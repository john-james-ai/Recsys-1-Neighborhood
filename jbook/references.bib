
@article{Arnold2016a,
	title = {Package '{coreNLP}' {\textbar} {Wrappers} {Around} {Stanford} {CoreNLP} {Tools}},
	author = {Arnold, Taylor and Tilton, Lauren},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Case,
	title = {Week 2 {Review} and {Examples}},
	author = {Case, Discrete},
	pages = {1--8},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T8DQ8L6P\\Bayesian Infernce - Week2_Review_Example.pdf:application/pdf},
}

@book{zipf32selective,
	title = {Selective {Studies} and the {Principle} of {Relative} {Frequency} in {Language}},
	publisher = {Harvard University Press},
	author = {Zipf, G K},
	year = {1932},
	keywords = {Applications, Natural Language Processing},
}

@article{Hinton2012,
	title = {Neural {Networks} for {Machine} {Learning} {Lecture} 6a {Overview} of mini-batch gradient descent.},
	abstract = {Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012},
	journal = {COURSERA: Neural Networks for Machine Learning},
	author = {Hinton, Geoffrey E and Srivastava, Nitish and Swersky, Kevin},
	year = {2012},
	pages = {29},
}

@misc{Cukier2007,
	title = {Study: {Hackers} {Attack} {Every} 39 {Seconds} {\textbar} {A}. {James} {Clark} {School} of {Engineering}, {University} of {Maryland}},
	url = {https://eng.umd.edu/news/story/study-hackers-attack-every-39-seconds},
	urldate = {2020-05-06},
	author = {Cukier, Michel},
	year = {2007},
	note = {Publication Title: University of Maryland},
}

@article{Freund2014,
	title = {Introduction to {Nonlinear} {Optimization} ; and {Optimality} {Conditions} for {Unconstrained} {Optimization} {Problems} {Introduction} to {Nonlinear} {Optimization}},
	author = {Freund, Robert M},
	year = {2014},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LXS5TLVW\\Introduction-to-Nonlinear-Optimization-and-Optimality-Conditions-fo.pdf:application/pdf},
}

@article{Jupudi2016,
	title = {Stochastic {Gradient} {Descent} using {Linear} {Regression} with {Python}},
	volume = {2},
	number = {7},
	journal = {International Journal of Advanced Engineering Research and Applications (IJA-ERA)},
	author = {Jupudi, Lakshmi},
	year = {2016},
	keywords = {big data, gradient descent, linear regression, machine learning, predictive analysis, www.ijaera.org},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VPPB9W6Y\\Stochastic Gradient Descent using Linear Regression with Python.pdf:application/pdf},
}

@article{keil,
	title = {Explanation and {Understanding}},
	volume = {57},
	url = {https://doi.org/10.1146/annurev.psych.57.102904.190100},
	doi = {10.1146/annurev.psych.57.102904.190100},
	abstract = {The study of explanation, while related to intuitive theories, concepts, and mental models, offers important new perspectives on high-level thought. Explanations sort themselves into several distinct types corresponding to patterns of causation, content domains, and explanatory stances, all of which have cognitive consequences. Although explanations are necessarily incomplete—often dramatically so in laypeople—those gaps are difficult to discern. Despite such gaps and the failure to recognize them fully, people do have skeletal explanatory senses, often implicit, of the causal structure of the world. They further leverage those skeletal understandings by knowing how to access additional explanatory knowledge in other minds and by being particularly adept at using situational support to build explanations on the fly in real time. Across development and cultures, there are differences in preferred explanatory schemes, but rarely are any kinds of schemes completely unavailable to a group.},
	number = {1},
	journal = {Annual Review of Psychology},
	author = {Keil, Frank C},
	year = {2006},
	pages = {227--254},
}

@misc{Brants2006,
	title = {Web \{{1T}\} 5-gram version 1},
	url = {https://catalog.ldc.upenn.edu/ldc2006t13},
	urldate = {2017-04-03},
	author = {Brants, T and Franz, A},
	year = {2006},
	note = {Publication Title: Linguistic Data Consortium, Philiadelphia},
	keywords = {Applications, Natural Language Processing},
}

@techreport{WMC2015,
	title = {{WMC} {Divided} 2015: {The} {Media} {Gender} {Gap} {\textbar} {Women}'s {Media} {Center}},
	url = {http://www.womensmediacenter.com/pages/2015-wmc-divided-media-gender-gap},
	author = {{The Women's Media Center}},
	year = {2015},
}

@article{Niu2017,
	title = {A breakthrough in {Speech} emotion recognition using {Deep} {Retinal} {Convolution} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1707.09917},
	abstract = {Speech emotion recognition (SER) is to study the formation and change of speaker's emotional state from the speech signal perspective, so as to make the interaction between human and computer more intelligent. SER is a challenging task that has encountered the problem of less training data and low prediction accuracy. Here we propose a data augmentation algorithm based on the imaging principle of the retina and convex lens, to acquire the different sizes of spectrogram and increase the amount of training data by changing the distance between the spectrogram and the convex lens. Meanwhile, with the help of deep learning to get the high-level features, we propose the Deep Retinal Convolution Neural Networks (DRCNNs) for SER and achieve the average accuracy over 99\%. The experimental results indicate that DRCNNs outperforms the previous studies in terms of both the number of emotions and the accuracy of recognition. Predictably, our results will dramatically improve human-computer interaction.},
	author = {Niu, Yafeng and Zou, Dongsheng and Niu, Yadong and He, Zhongshi and Tan, Hua},
	year = {2017},
	note = {arXiv: 1707.09917},
	pages = {1--7},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H8LY2K5H\\A breakthrough in Speech emotion recognition using Deep Retinal Convolution Neural Networks.pdf:application/pdf},
}

@book{Wasserman:2010:SCC:1965575,
	title = {All of {Statistics}: {A} {Concise} {Course} in {Statistical} {Inference}},
	isbn = {1-4419-2322-5 978-1-4419-2322-6},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Wasserman, Larry},
	year = {2010},
	keywords = {Mathematics, Statistics},
}

@article{Predict2017,
	title = {{HOW} {TO} : {DRIVE} {SERENDIPITOUS} {DISCOVERY} {WITH} {RECOMMENDATION} {Successfully} {Predict} {Content} {Users} {Will} {Like} ( {Even} {If} {They} {Don} ' t {Know} {They} ' ll {Like} {It} )},
	author = {Predict, Successfully and Users, Content and Like, Will},
	year = {2017},
	pages = {1--18},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HCYL9DAJ\\recommendation_engine_guidebook.pdf:application/pdf},
}

@article{Bryant,
	title = {{AIRBNB} {Plan}},
	author = {Bryant, Sarah and Morris, Joseph and Morrison, Elisa and Uhlending, Sofie},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PXN3DVK7\\Airbnb+Final+P.pdf:application/pdf},
}

@article{Sc,
	title = {Half-{Context} {Language} {Models}},
	abstract = {This article investigates the effects of different degrees of contextual granularity on language model performance. It presents a new language model that combines clustering and half-contextualization, a novel representation of contexts. Half-contextualization is based on the half-context hypothesis that states that the distributional characteristics of a word or bigram are best represented by treating its context distribution to the left and right separately and that only direc-tionally relevant distributional information should be used. Clustering is achieved using a new clustering algorithm for class-based language models that compares favorably to the exchange algorithm. When interpolated with a Kneser-Ney model, half-context models are shown to have better perplexity than commonly used interpolated n-gram models and traditional class-based approaches. A novel, fine-grained, context-specific analysis highlights those contexts in which the model performs well and those which are better treated by existing non-class-based models.},
	author = {Sc, Hinrich and Walsh, Michael},
	keywords = {Applications, Natural Language Processing},
}

@techreport{Dahl2016,
	title = {Package ‘xtable'},
	author = {Dahl, Author David B and Scott, Maintainer David},
	year = {2016},
	pages = {1--34},
}

@article{Ekstrom2017,
	title = {Package ‘ pwr '},
	author = {Ekstrom, Claus and Dalgaard, Peter and Gill, Jeffrey and Weibelzahl, Stephan and Ford, Clay and Volcic, Robert},
	year = {2017},
	pages = {1--22},
}

@techreport{Mcgregor2006,
	title = {No {Free} {Lunch} and {Algorithmic} {Randomness}},
	abstract = {Schumacher, Vose \& Whitley [1] have shown that Wolpert \& MacReady's celebrated No Free Lunch theorem [2] applies only to classes of target functions which are closed under permutation (c.u.p.). In the same paper, Schumacher et al. demonstrated that there exist both highly compressible and highly incompressible classes of objective functions for which NFL applies. However, I will show that there is a free lunch for the class of all n-compressible target functions f : X → Y given reasonable conditions on n, {\textbar}X {\textbar} and {\textbar}Y{\textbar}. While previous authors [3, 4] have considered NFL in the context of some form of complexity restriction on function classes, this paper appears to be the first to contain a proof using the general measure of Kolmogorov complexity. 1. NO FREE LUNCH When evolutionary algorithms were first introduced, it was hoped that they might provide a general-purpose "black box" search/optimisation tool. However, in [2], Wolpert \& MacReady proved that the average performance of all search algorithms, considered over the class of all possible target functions, is the same. Consequently evolutionary computing methods are "no better" than random search when considered over all possible fitness functions. It was subsequently proved in [1] that this "No Free Lunch" (NFL) result extends to classes of function other than the uniform class. NFL holds in the average case, regardless of the algorithm performance measure used, if and only if the class of functions under consideration is closed under permutation (c.u.p.). The lack of structure of c.u.p. classes has information-theoretic implications which have been explored in [5, 6] using Shannon-Weaver information theory; this paper investigates that lack of structure from an algorithmic information theory perspective using Kolmogorov complexity. I conclude that if we consider the class of algorithmically non-random (i.e. compressible) target functions, NFL does not hold. of largely theoretical interest to researchers using artificial evolution. This is due to a number of practical considerations such as local correlation structure in fitness spaces [7] and NFL assumptions ignoring algorithmic complexity [8]. The result presented in this paper is similarly theoretical rather than practical; nevertheless, it extends previously published work. 2. ASSUMPTIONS As in most previous NFL work, it is assumed that the search domain X and the set of objective values Y are sets of finite cardinality. Because the terms will be used frequently, I will use X = {\textbar}X {\textbar} and Y = {\textbar}Y{\textbar} to denote the cardinality of X and Y respectively. To avoid the trivial case where NFL necessarily holds, I will assume that Y ≥ 2. Additionally, it will be assumed that X ≫ 1 and that X ≥ Y. This is justified by the fact that in most practical search tasks the set of possible values of a target function is represented as a single fixed-width integer or floating-point number whilst the set of possible solutions is far larger. It is also justified in any case where target values are only used to rank possible solutions relative to one another, because the number of possible ranks is no larger than the number of possible solutions. 3. COMPRESSIBILITY 3.},
	author = {Mcgregor, Simon},
	year = {2006},
}

@misc{LawlerJohn1999,
	title = {English {Word} {List}},
	url = {http://www-personal.umich.edu/%7B~%7Djlawler/wordlist},
	author = {{LawlerJohn}},
	year = {1999},
	keywords = {Applications, Natural Language Processing},
}

@article{Kornai2002,
	title = {How {Many} {Words} {Are} {There}?},
	volume = {4},
	issn = {10542353},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.6.5245&rank=1},
	abstract = {The commonsensical assumption that any language has only finitely many words is shown to be false by a combination of formal and empirical arguments. Zipf's Law and related formulas are investigated and a more complex model is offered.},
	number = {2002},
	journal = {Glottometrics},
	author = {Kornai, András},
	year = {2002},
	pmid = {12374001},
	keywords = {Applications, Natural Language Processing, s law, vocabulary size, zipf},
	pages = {61--86},
}

@article{Anodot,
	title = {{ULTIMATE} {GUIDE} {TO} {BUILDING} {A} {MACHINE} {LEARNING} {ANOMALY} {DETECTION} {SYSTEM} {PART} 1 : {DESIGN} {PRINCIPLES} {Anomaly} detection is an imperative for digital businesses today ,},
	author = {{Anodot}},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QC9ILJR2\\Building a Large Scale Machine Learning-Based Anomaly Detection System, Part 1 - Design Principles.pdf:application/pdf},
}

@article{dorea1990,
	title = {Stopping {Rules} for a {Random} {Optimization} {Method}},
	volume = {28},
	url = {https://doi.org/10.1137/0328048},
	doi = {10.1137/0328048},
	number = {4},
	journal = {SIAM Journal on Control and Optimization},
	author = {Dorea, C},
	year = {1990},
	pages = {841--850},
}

@misc{Wikipedia2016,
	title = {Wikipedia:{List} of {English} contractions},
	url = {http://www.oxforddictionaries.com/us/definition/english/amn't},
	urldate = {2016-11-30},
	author = {{Wikipedia}},
	year = {2016},
	note = {Publication Title: Wikipedia},
	keywords = {Applications, Natural Language Processing},
}

@article{Schroff2015,
	title = {{FaceNet}: {A} unified embedding for face recognition and clustering},
	volume = {07-12-June},
	issn = {10636919},
	doi = {10.1109/CVPR.2015.7298682},
	abstract = {Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result [15] by 30\% on both datasets.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	year = {2015},
	note = {arXiv: 1503.03832
ISBN: 9781467369640},
	pages = {815--823},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P3KRRHVW\\Facenet.pdf:application/pdf},
}

@article{Smith2018,
	title = {Don't decay the learning rate, increase the batch size},
	abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate and scaling the batch size B ∝ . Finally, one can increase the momentum coefficient m and scale B ∝ 1/(1 − m), although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to 76.1\% validation accuracy in under 30 minutes.},
	number = {2017},
	journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
	author = {Smith, Samuel L and Kindermans, Pieter Jan and Ying, Chris and Le, Quoc V},
	year = {2018},
	note = {arXiv: 1711.00489v2},
	pages = {1--11},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\22PS8LPG\\DON’T DECAY THE LEARNING RATE, INCREASE BATCH SIZE.pdf:application/pdf},
}

@article{Evelson2018,
	title = {8 {Analytics} {Trends} to {Watch} in 2018 for the {Intelligent} {Enterprise} {The} {F} uture {Belongs} to the {Intelligent} {Enterprise}},
	author = {Evelson, Boris and Goetz, Michele},
	year = {2018},
	pages = {1--22},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZE8E375B\\8-Enterprise-Analytics-Trends-to-Watch-in-2018.pdf:application/pdf},
}

@article{Baroni2008,
	title = {Statistical {Analysis} of {Corpus} {Data} with {R}},
	volume = {6},
	author = {Baroni, Marco and Evert, Stefan},
	year = {2008},
	pages = {2008},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6R9VI5N4\\zipfR.slides.pdf:application/pdf},
}

@article{Covington2010,
	title = {Cutting the {Gordian} {Knot} : {The} {Moving}-{Average} {Type} – {Token} {Ratio} ( {MATTR} )},
	volume = {17},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09296171003643098},
	abstract = {Type–token ratio (TTR), or vocabulary size divided by text length (V/N), is a time- honoured but unsatisfactory measure of lexical diversity. The problem is that the TTR of a text sample is affected by its length. We present an algorithm for rapidly computing TTR through a moving window that is independent of text length, and we demonstrate that this measurement can detect changes within a text as well as differences between texts.},
	number = {2},
	journal = {Journal of Quantitative Linguistics},
	author = {Covington, Michael A and Mcfall, Joe D},
	year = {2010},
	pages = {94--100},
}

@article{BNC2005,
	title = {British {National} {Corpus}},
	url = {http://www.natcorp.ox.ac.uk/},
	abstract = {The written part of the (90\%) includes, for example, extracts from regional and national newspapers, specialist periodicals and journals for all ages and interests, academic books and popular fiction, published and unpublished letters and memoranda, school and university},
	number = {2001},
	journal = {British National Corpus},
	author = {BNC, Webmaster},
	year = {2005},
	note = {ISBN: Version 3, BNC XML Edition},
	keywords = {Applications, Natural Language Processing},
	pages = {6},
}

@article{Duffy2003,
	title = {Applying the decorator pattern for profiling object-oriented software},
	volume = {2003-May},
	issn = {10928138},
	doi = {10.1109/WPC.2003.1199192},
	abstract = {A profiler can provide valuable information to a developer to facilitate program optimization, debugging or testing. In this paper, we describe the use of the decorator pattern for non-intrusive profiling of object-oriented applications. We provide a formal specification of the decorator pattern, and show that the pattern can be used as a program transformation without altering the external, observable behavior of the system. We refer to such a transformation as a correctness preserving transformation, or CPT. As a CPT, the decorator pattern can be used to non-intrusively profile object-oriented applications and we illustrate this application with an invariant validator for enforcement of design by contract, and for profiling memory. We provide a case study to compare the cost trade-offs of validating invariants at different points in a program.},
	number = {May},
	journal = {Proceedings - IEEE Workshop on Program Comprehension},
	author = {Duffy, E B and Gibson, J P and Malloy, B A},
	year = {2003},
	note = {ISBN: 0769518834},
	pages = {84--93},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2I95VZEA\\Applying the Decorator Pattern for Profiling Object-Oriented Software.pdf:application/pdf},
}

@article{Raschka2018a,
	title = {Model {Evaluation} 4 : {Algorithm} {Comparisons}},
	author = {Raschka, Sebastian},
	year = {2018},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PYEJHEY8\\model evaluation and selection.pdf:application/pdf},
}

@article{Rastrow2012,
	title = {Practical and {Efficient} {Incorporation} of {Syntactic} {Features} into {Statistical} {Language} {Models}},
	volume = {Ph.D.},
	abstract = {Automatic Speech Recognition (ASR) and Statistical Machine Translation (SMT), among other natural language processing applications, rely on a language model (LM) to provide a strong linguistic prior over word sequences of the often prohibitively large and complex hypothesis space of these systems. The language models deployed in most state-of-the-art ASR and SMT systems are n-gram models. Several statistical frameworks have been proposed to build more complex models and " put (the syntactic structure of) language back into language modeling. " Yet, n-gram models, despite being linguistically nave, are still favored, because estimating them from text is well understood, they are computationally efficient, and integrating them into ASR and SMT systems is straightforward. This dissertation proposes novel algorithms and techniques that make it practical to estimate and apply more complex language models in ASR and SMT tasks, in particular syntactic modes for speech recognition. While yielding significantly better performance than n-gram models, the syntactic structured language models (SLM) can not be efficiently trained on a large amount of text data due to the impractical size of the resulting model. A general information-ii ABSTRACT theoretic pruning scheme is proposed to significantly reduce the size of the SLM while},
	author = {Rastrow, Ariya},
	year = {2012},
	keywords = {Applications, Natural Language Processing},
}

@book{mass,
	address = {New York},
	edition = {Fourth},
	title = {Modern {Applied} {Statistics} with {S}},
	url = {http://www.stats.ox.ac.uk/pub/MASS4},
	publisher = {Springer},
	author = {Venables, W N and Ripley, B D},
	year = {2002},
}

@article{facebook,
	title = {Facebook comment volume prediction},
	volume = {16},
	doi = {10.5013/IJSSST.a.16.05.16},
	journal = {International Journal of Simulation: Systems, Science \& Technology},
	author = {Singh, Kamaljot},
	year = {2015},
	pages = {16.1},
}

@article{Sandnes2015,
	title = {Reflective {Text} {Entry}: {A} {Simple} {Low} {Effort} {Predictive} {Input} {Method} {Based} on {Flexible} {Abbreviations}},
	volume = {67},
	issn = {18770509},
	url = {http://dx.doi.org/10.1016/j.procs.2015.09.254},
	doi = {10.1016/j.procs.2015.09.254},
	abstract = {Users with reduced physical functioning such as ALS patients need more time and effort to operate computers. Most of the previous assistive technologies use prefix based predictive text input algorithms. Prefix based predictive text entry is suitable for languages such as English where the average word length is approximately 5 characters. Other languages such as Norwegian and German have longer mean word lengths as words are combined into longer compound words and prefix approaches are thus less effective. This paper proposes a new abbreviation expansion algorithm. Users mentally determine an abbreviation of the word, typically comprising significant consonants and the system proposes words that contain the matched characters. The approach is non disruptive in that it does not require the user to learn a new system or abbreviation mnemonics, and it can be used with any text input device. The system is dynamic and adapts to the users style of abbreviated input. The algorithm is easier to implement than previous approaches and no a priori system training is required. Our experimental evaluations demonstrate that the algorithm achieves real time performance with modest computer hardware.},
	number = {1877},
	journal = {Procedia Computer Science},
	author = {Sandnes, Frode Eika},
	year = {2015},
	note = {Publisher: Elsevier Masson SAS},
	keywords = {abbreviation expansion, ALS, dyslexia, longest common subsequence, low physical effort, text input},
	pages = {105--112},
}

@article{Brunner2016,
	title = {Teaching data science},
	volume = {80},
	issn = {18770509},
	url = {http://dx.doi.org/10.1016/j.procs.2016.05.513},
	doi = {10.1016/j.procs.2016.05.513},
	abstract = {We describe an introductory data science course, entitled Introduction to Data Science, offered at the University of Illinois at Urbana-Champaign. The course introduced general programming concepts by using the Python programming language with an emphasis on data preparation, processing, and presentation. The course had no prerequisites, and students were not expected to have any programming experience. This introductory course was designed to cover a wide range of topics, from the nature of data, to storage, to visualization, to probability and statistical analysis, to cloud and high performance computing, without becoming overly focused on any one subject. We conclude this article with a discussion of lessons learned and our plans to develop new data science courses.},
	journal = {Procedia Computer Science},
	author = {Brunner, Robert J and Kim, Edward J},
	year = {2016},
	note = {arXiv: 1604.07397
Publisher: Elsevier Masson SAS},
	keywords = {Statistics, Cloud computing, Data science, Databases, High performance computing, Informatics, Probability, Python programming, Visualization},
	pages = {1947--1956},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\49L963CS\\Teaching Introduction to Data Science.pdf:application/pdf},
}

@article{10.2307/105741,
	title = {An {Essay} towards {Solving} a {Problem} in the {Doctrine} of {Chances}. {By} the {Late} {Rev}. {Mr}. {Bayes}, {F}. {R}. {S}. {Communicated} by {Mr}. {Price}, in a {Letter} to {John} {Canton}, {A}. {M}. {F}. {R}. {S}.},
	volume = {53},
	issn = {02607085},
	url = {http://www.jstor.org/stable/105741},
	journal = {Philosophical Transactions (1683-1775)},
	author = {Bayes, Mr. and Price, Mr.},
	year = {1763},
	note = {Publisher: The Royal Society},
	keywords = {Algorithms, Bayesian},
	pages = {370--418},
}

@misc{Feinerer2015a,
	title = {tm: {Text} {Mining} {Package}},
	url = {http://cran.r-project.org/package=tm},
	author = {Feinerer, Ingo and Hornik, Kurt},
	year = {2015},
	keywords = {Applications, Natural Language Processing},
}

@article{Fazly2003,
	title = {Testing the efficacy of part-of-speech information in word completion},
	url = {http://www.aclweb.org/anthology/W03-2502},
	abstract = {We investigate the effect of incorporat-ing syntactic information into a word-completion algorithm. We introduce lwo new algorithms lhal combine parl-of-speech tag trigrams with word bi-grams, and evaluate them with a test-bench constructed for the purpose. The results show a small but statistically sig-nificant improvement in keystroke sav-ings for one of our algorithms over base-lines that use only word n-grams.},
	number = {1991},
	journal = {Eacl},
	author = {Fazly, Afsaneh and Hirst, Graeme},
	year = {2003},
	keywords = {Applications, Natural Language Processing},
	pages = {9--16},
}

@article{Sun2020,
	title = {Gallery: {A} machine learning model management system at uber},
	volume = {2020-March},
	issn = {23672005},
	doi = {10.5441/002/edbt.2020.59},
	abstract = {Machine learning is critical to the success of many products across application domains. At Uber, we have a variety of machine learning applications including matching, pricing, recommendation, and personalization. As a result, we have a large number of machine learning models to manage in production. Generally, building machine learning models is an iterative process and machine learning models span across a set of stages of a lifecycle. In this paper, we describe Gallery, a machine learning model lifecycle management system to save and serve models and metrics and automatically orchestrate the flow of models across different stages in the lifecycle. We then use the Uber Marketplace Forecasting and Simulation platforms as examples to show how Uber uses Gallery in production and the benefits we get by using Gallery.},
	journal = {Advances in Database Technology - EDBT},
	author = {Sun, Chong and Azari, Nader and Turakhia, Chintan},
	year = {2020},
	note = {ISBN: 9783893180837},
	pages = {474--485},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UXYZTYXA\\Machine Learning Model Management System at Uber.pdf:application/pdf},
}

@article{Arora,
	title = {Gradient descent for convex functions : univariate case},
	author = {Arora, Sanjeev},
	pages = {1--8},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JPPAQ3EF\\cos 521 - Advanced Algorithm Design.pdf:application/pdf},
}

@article{Kremers,
	title = {{TEXT} {MESSAGING} {ON} {RELATIONAL} {INTIMACY} {The} {Effects} of {Text} {Messaging} on {Relational} {Intimacy}},
	author = {Kremers, Jerome},
	keywords = {Applications, Natural Language Processing},
}

@article{Tarjan1987,
	title = {Algorithm design},
	volume = {30},
	issn = {15577317},
	doi = {10.1145/214748.214752},
	abstract = {The quest for efficiency in computational methods yields not only fast algorithms, but also insights that lead to elegant, simple, and general problem-solving methods. ©1987, ACM. All rights reserved.},
	number = {3},
	journal = {Communications of the ACM},
	author = {Tarjan, Robert E},
	year = {1987},
	keywords = {John E. Hopcroft, Robert E. Tarjan, Turing Award},
	pages = {204--212},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RWAVDQ69\\Algorithm Design.pdf:application/pdf},
}

@misc{wiki:unsupervisedlearning,
	title = {Unsupervised learning --- {Wikipedia}\{,\} {The} {Free} {Encyclopedia}},
	url = {https://en.wikipedia.org/w/index.php?title=Unsupervised_learning&oldid=793838440},
	author = {{Wikipedia}},
	year = {2017},
}

@article{Nitanda2018,
	title = {Functional gradient boosting based on {Residual} {Network} perception},
	volume = {9},
	abstract = {Residual Networks (ResNets) have become stateof-the-art models in deep learning and several theoretical studies have been devoted to understanding why ResNet works so well. One attractive viewpoint on ResNet is that it is optimizing the risk in a functional space by combining an ensemble of effective features. In this paper, we adopt this viewpoint to construct a new gradient boosting method, which is known to be very powerful in data analysis. To do so, we formalize the gradient boosting perspective of ResNet mathematically using the notion of functional gradients and propose a new method called ResFGB for classification tasks by leveraging ResNet perception. Two types of generalization guarantees are provided from the optimization perspective: one is the margin bound and the other is the expected risk bound by the sample-splitting technique. Experimental results show superior performance of the proposed method over state-of-the-art methods such as LightGBM.},
	journal = {35th International Conference on Machine Learning, ICML 2018},
	author = {Nitanda, Atsushi and Suzuki, Taiji},
	year = {2018},
	note = {arXiv: 1802.09031
ISBN: 9781510867963},
	pages = {6126--6143},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UL5MM94B\\Functional Gradient Boosting based on Residual Network Perception.pdf:application/pdf},
}

@book{Chung2019,
	title = {Second edition},
	volume = {69},
	isbn = {978-1-78712-593-3},
	author = {Chung, Oscar},
	year = {2019},
	note = {Publication Title: Taiwan Review
Issue: 4
ISSN: 17275148},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WNXKK2BA\\PYTHON_MACHINE_LEARNING_SECOND_EDITION.pdf:application/pdf},
}

@article{Moh,
	title = {{TimeLine}: {A} {High} {Performance} {Archive} for a {Distributed} {Object} {Store}},
	abstract = {This paper describes TimeLine, an efficient archive service for a distributed storage system. TimeLine allows users to take snap-shots on demand. The archive is stored online so that it is easily accessible to users. It enables " time travel " in which a user runs a computation on an earlier system state. Archiving is challenging when storage is distributed. In par-ticular, a key issue is how to provide consistent snapshots, yet avoid stopping user access to stored state while a snapshot is be-ing taken. The paper defines the properties that an archive service ought to provide and describes an implementation approach that provides the desired properties yet is also efficient. TimeLine is designed to provide snapshots for a distributed persistent object store. However the properties and the implementation approach apply to file systems and databases as well. TimeLine has been implemented and we present the results of experiments that evaluate its performance. The experiments show that computations in the past run well when the archive store is nearby, e.g., on the same LAN, or connected by a high speed link. The results also show that taking snapshots has negligible impact on the cost of concurrently running computations, regardless of where the archived data is stored.},
	author = {Moh, Chuang-Hue and Liskov, Barbara},
	keywords = {Object Oriented Programming, Software Engineering},
}

@article{CenterforBehavioralHealthStatisticsandQuality2016,
	title = {Key substance use and mental health indicators in the {United} {States}: {Results} from the 2015 national survey on drug use and health},
	volume = {7},
	url = {http://store.samhsa.gov.},
	doi = {(HHS Publication No. SMA 16-4984, NSDUH Series H-51},
	number = {1},
	urldate = {2017-06-14},
	journal = {No.SMA 16-4984, NSDUH Series H-51},
	author = {{Center for Behavioral Health Statistics and Quality}},
	year = {2016},
	keywords = {alcohol use, behavioral health, co-occurring mental and substance use disorders, co-occurring mental and substance use disorders am, illicit drug use, major depressive episode among adults, major depressive episode among youths, mental health service use, mental health service use among adults with mental, mental health service use among those with co-occu, mental illness among adults, need for substance use treatment, prescription drug misuse, specialty substance use treatment, substance dependence or abuse, substance use, substance use disorders, substance use treatment, tobacco use, treatment for depression, trends},
	pages = {726--877},
}

@article{Poldrack2015,
	title = {Long-term neural and physiological phenotyping of a single human},
	volume = {6},
	issn = {20411723},
	doi = {10.1038/ncomms9885},
	abstract = {Psychiatric disorders are characterized by major fluctuations in psychological function over the course of weeks and months, but the dynamic characteristics of brain function over this timescale in healthy individuals are unknown. Here, as a proof of concept to address this question, we present the MyConnectome project. An intensive phenome-wide assessment of a single human was performed over a period of 18 months, including functional and structural brain connectivity using magnetic resonance imaging, psychological function and physical health, gene expression and metabolomics. A reproducible analysis workflow is provided, along with open access to the data and an online browser for results. We demonstrate dynamic changes in brain connectivity over the timescales of days to months, and relations between brain connectivity, gene expression and metabolites. This resource can serve as a testbed to study the joint dynamics of human brain and metabolic function over time, an approach that is critical for the development of precision medicine strategies for brain disorders.},
	journal = {Nature Communications},
	author = {Poldrack, Russell A and Laumann, Timothy O and Koyejo, Oluwasanmi and Gregory, Brenda and Hover, Ashleigh and Chen, Mei Yen and Gorgolewski, Krzysztof J and Luci, Jeffrey and Joo, Sung Jun and Boyd, Ryan L and Hunicke-Smith, Scott and Simpson, Zack Booth and Caven, Thomas and Sochat, Vanessa and Shine, James M and Gordon, Evan and Snyder, Abraham Z and Adeyemo, Babatunde and Petersen, Steven E and Glahn, David C and Mckay, D Reese and Curran, Joanne E and Göring, Harald H H and Carless, Melanie A and Blangero, John and Dougherty, Robert and Leemans, Alexander and Handwerker, Daniel A and Frick, Laurie and Marcotte, Edward M and Mumford, Jeanette A},
	year = {2015},
	pmid = {26648521},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QZ4BBZRN\\Poldrack_et_al-2015-Nature_Communications.pdf:application/pdf},
}

@article{Bottou2010,
	title = {Large-{Scale} {Machine} {Learning} with {Stochastic} {Gradient} {Descent}},
	doi = {10.1007/978-3-7908-2604-3_16},
	abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
	journal = {Proceedings of COMPSTAT'2010},
	author = {Bottou, Léon},
	year = {2010},
	keywords = {efficiency, online learning, stochastic gradient descent},
	pages = {177--186},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\55CWGKMP\\Large Scale Learning with Stochastic Gradient Descent.pdf:application/pdf},
}

@book{Goodfellow2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8Y8FZLPV\\deeplearning.pdf:application/pdf},
}

@article{Ley2008,
	title = {M {P} {RA} {On} the {Effect} of {Prior} {Assumptions} in {Bayesian} {Model} {Averaging} with {Applications} to {Growth} {Regression} {On} the {Effect} of {Prior} {Assumptions} in {Bayesian} {Model} {Averaging} with {Applications} to {Growth} {Regression}},
	url = {http://mpra.ub.uni-muenchen.de/6773/},
	abstract = {We consider the problem of variable selection in linear regression models. Bayesian model averaging has become an important tool in empirical settings with large numbers of potential regressors and relatively limited numbers of observations. We examine the effect of a variety of prior assumptions on the inference concerning model size, posterior inclusion probabilities of regressors and on predictive performance. We illustrate these issues in the context of cross-country growth regressions using three datasets with 41 to 67 potential drivers of growth and 72 to 93 observations. Finally, we recommend priors for use in this and related contexts.},
	author = {Ley, Eduardo and Steel, Mark F J},
	year = {2008},
	keywords = {Algorithms, Bayesian, Model size, Model uncertainty, O47, Posterior odds, Prediction, Prior odds, Robustness JEL Classification System C11},
}

@article{Mahsereci2017,
	title = {Early {Stopping} without a {Validation} {Set}},
	url = {http://arxiv.org/abs/1703.09580},
	abstract = {Early stopping is a widely used technique to prevent poor generalization performance when training an over-expressive model by means of gradient-based optimization. To find a good point to halt the optimizer, a common practice is to split the dataset into a training and a smaller validation set to obtain an ongoing estimate of the generalization performance. We propose a novel early stopping criterion based on fast-to-compute local statistics of the computed gradients and entirely removes the need for a held-out validation set. Our experiments show that this is a viable approach in the setting of least-squares and logistic regression, as well as neural networks.},
	author = {Mahsereci, Maren and Balles, Lukas and Lassner, Christoph and Hennig, Philipp},
	year = {2017},
	note = {arXiv: 1703.09580},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6SYK6XED\\Early Stopping without a Validation Set.pdf:application/pdf},
}

@misc{1564524869ProblemStatement,
	title = {1564524869\_Problem\_Statement\_Worksheet\_Template},
}

@article{Masrai2016,
	title = {How {Different} {Is} {Arabic} from {Other} {Languages} ? {The} {Relationship} between {Word} {Frequency} and {Lexical} {Coverage}},
	volume = {3},
	number = {1},
	journal = {Journal of Applied Linguistics and Language Research},
	author = {Masrai, Ahmed and Milton, James},
	year = {2016},
	keywords = {s law, zipf, arabic corpus, lexical coverage, vocabulary, word frequency},
	pages = {15--35},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2KXLFLMZ\\Relationship between Word Frequency and Lexical Coverage.pdf:application/pdf},
}

@article{Grouin2008,
	title = {Certification and cleaning up of a text corpus: {Towards} an evaluation of the "{Grammatical}" quality of a corpus},
	abstract = {We present in this article the methods we used for obtaining measures to ensure the quality and well-formedness of a text corpus. These measures allow us to determine the compatibility of a corpus with the treatments we want to apply on it. We called this method "certification of corpus". These measures are based upon the characteristics required by the linguistic treatments we have to apply on the corpus we want to certify. Since the certification of corpus allows us to highlight the errors present in a text, we developed modules to carry out an automatic correction. By applying these modules, we reduced the number of errors. In consequence, it increases the quality of the corpus making it possible to use a corpus that a first certification would not have admitted.},
	number = {January},
	journal = {Proceedings of the 6th International Conference on Language Resources and Evaluation, LREC 2008},
	author = {Grouin, Cyril},
	year = {2008},
	note = {ISBN: 2951740840},
	pages = {1083--1090},
}

@article{Sculley2015,
	title = {Hidden technical debt in machine learning systems},
	volume = {2015-Janua},
	issn = {10495258},
	abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean François and Dennison, Dan},
	year = {2015},
	pages = {2503--2511},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BL5AX2S2\\5656-hidden-technical-debt-in-machine-learning-systems.pdf:application/pdf},
}

@article{Wickham2016b,
	title = {Package ‘stringr'},
	author = {Wickham, Hadley},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@techreport{Baroni2006,
	title = {The {zipfR} package for lexical statistics: {A} tutorial introduction},
	abstract = {This document provides a tutorial introduction to the zipfR package (Evert and Baroni, 2007) for Large-Number-of-Rare-Events (LNRE) modeling of lexical distri- butions (Baayen, 2001). We assume that R is installed on your computer and that you have basic familiarity with it. If this is not the case, please start by visiting the R page at http://www.r-project.org/. The page provides links to download sites, documentation and introductory material, as well as a wide selection of textbooks on R programming.},
	author = {Baroni, Marco and Evert, Stefan},
	year = {2006},
	note = {Publication Title: R tutorial
Issue: August},
	keywords = {Applications, Natural Language Processing, R, R GNU},
	pages = {1--25},
}

@article{mandelbrot1965information,
	title = {Information theory and psycholinguistics},
	journal = {BB Wolman and E},
	author = {Mandelbrot, Beno\${\textbackslash}\${\textasciicircum}\${\textbackslash}\$it},
	year = {1965},
	keywords = {Applications, Natural Language Processing},
}

@techreport{He,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first 1 to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
}

@article{Supplies2018,
	title = {Five ways digital technologies boost clinical supply chain performance {Imperative}},
	author = {Supplies, Clinical and Group, Working},
	year = {2018},
	pages = {1--16},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9XVN3S3L\\Five Ways Digital Technologies Boost Clinical Supply Chain Performance.pdf:application/pdf},
}

@misc{Hornik2016,
	title = {Apache {OpenNLP} {Tools} {Interface}},
	abstract = {Description An interface to the Apache OpenNLP tools (version 1.5.3). The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text written in Java. It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. See {\textless}http://opennlp.apache.org/{\textgreater}for more information. Imports NLP ({\textgreater}= 0.1-6.3), openNLPdata ({\textgreater}= 1.5.3-1), rJava ({\textgreater}= 0.6-3)},
	author = {Hornik, Kurt},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@book{strang2006linear,
	address = {Belmont, CA},
	title = {Linear algebra and its applications},
	isbn = {0-03-010567-6 978-0-03-010567-8 0-534-42200-4 978-0-534-42200-4},
	url = {http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676},
	publisher = {Thomson, Brooks/Cole},
	author = {Strang, Gilbert},
	year = {2006},
	keywords = {book math to\_READ},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4SYQVSTG\\Linear Algebra and its Applications.pdf:application/pdf},
}

@book{Casella2006,
	title = {608 {Text} {Book}},
	volume = {102},
	isbn = {978-0-387-78188-4},
	url = {http://books.google.com/books?id=9tv0taI8l6YC},
	abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\textasciitilde}without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
	author = {Casella, George and Fienberg, Stephen and Olkin, Ingram},
	year = {2006},
	pmid = {10911016},
	doi = {10.1016/j.peva.2007.06.006},
	note = {Publication Title: Design
ISSN: 01621459},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AUQ5HDEH\\A_Modern_Approach_to_Regression.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G7BBW9KL\\A_Modern_Approach_to_Regression.pdf:application/pdf},
}

@article{Yihui2017,
	title = {Package printr},
	author = {Yihui, Xie},
	year = {2017},
}

@article{Dickinson2002,
	title = {• {Approaches}: – 4.},
	author = {Dickinson, Markus},
	year = {2002},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZHMCI4YU\\tokenization-4up.pdf:application/pdf},
}

@article{Yuan2008,
	title = {Step-sizes for the gradient method},
	doi = {10.1090/amsip/042.2/23},
	abstract = {Yuan, Y.X.: Step-sizes for the gradient method, Edited by K.S. Liu, Z.P. Xin and S.T. Yau. In: Third International Congress of Chinese Mathematicians (AMS/IP Studies in Advanced Mathematics), pp. 785–796 (2008)},
	author = {Yuan, Ya-xiang},
	year = {2008},
	keywords = {convergence, line search, steepest descent, unconstrained optimization},
	pages = {785--796},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WBNY9XLS\\Step Sizes for Gradient Descent.pdf:application/pdf},
}

@article{NumPy,
	title = {The {NumPy} {Array}: {A} {Structure} for {Efficient} {Numerical} {Computation}},
	volume = {13},
	number = {2},
	journal = {Computing in Science Engineering},
	author = {van der Walt, S and Colbert, S C and Varoquaux, G},
	year = {2011},
	pages = {22--30},
}

@article{Zurhellen2011,
	title = {{SMS} and {Oral} {Tradition}},
	volume = {26},
	number = {2},
	journal = {Oral Tradition},
	author = {Zurhellen, Sarah},
	year = {2011},
	pages = {637--642},
}

@misc{wiki:gradient_descent,
	title = {Gradient descent --- {Wikipedia} {The} {Free} {Encyclopedia}},
	url = {https://en.wikipedia.org/w/index.php?title=Gradient_descent&oldid=893231138},
	author = {contributors, Wikipedia},
	year = {2019},
	keywords = {Algorithms, Optimization, Gradient Descent},
}

@article{Zhang2017a,
	title = {A {Survey} on {Multi}-{Task} {Learning}},
	url = {http://arxiv.org/abs/1707.08114},
	abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL. First, we classify different MTL algorithms into several categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach, and decomposition approach, and then discuss the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, batch MTL models are difficult to handle this situation and online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing are reviewed to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works. Finally, we present theoretical analyses and discuss several future directions for MTL.},
	author = {Zhang, Yu and Yang, Qiang},
	year = {2017},
	note = {arXiv: 1707.08114},
	pages = {1--20},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SUWAGHI3\\A Survey on Multi-Task Learning.pdf:application/pdf},
}

@article{Portilla2018,
	title = {Python {Visualization} {Dashboards} with {Plotly} ' s {Dash} {Library}},
	author = {Portilla, Marcial},
	year = {2018},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZH6DFV5N\\PYTHON DASHBOARDS.pdf:application/pdf},
}

@article{Schuster2012,
	title = {Classification using {Logistic} {Regression}},
	abstract = {1. Properties of dorsal horn interneurones that process information from group II muscle afferents in the sacral segments of the spinal cord have been investigated in the cat using both intracellular and extracellular recording. 2. The interneurones were excited by group II muscle afferents and cutaneous afferents but not by group I muscle afferents. They were most effectively excited by group II afferents of the posterior biceps, semitendinosus, triceps surae and quadriceps muscle nerves and by cutaneous afferents running in the cutaneous femoris, pudendal and sural nerves. The earliest synaptic actions were evoked monosynaptically and were very tightly locked to the stimuli. 3. EPSPs evoked monosynaptically by group II muscle afferents and cutaneous afferents of the most effective nerves were often cut short by disynaptic IPSPs. As a consequence of this negative feedback the EPSPs gave rise to single or double spike potentials and only a minority of interneurones responded with repetitive discharges. However, the neurones that did respond repetitively did so at a very high frequency of discharges (0.8-1.2 ms intervals between the first 2- 3 spikes). 4. Sacral dorsal horn group II interneurones do not appear to act directly upon motoneurones because: (i) these interneurones are located outside the area within which last order interneurones have previously been found and (ii) the latencies of PSPs evoked in motoneurones by stimulation of the posterior biceps and semitendinosus, cutaneous femoris and pudendal nerves (i.e. the main nerves providing input to sacral interneurones) are compatible with a tri- but not with a disynaptic coupling. Spatial facilitation of EPSPs and IPSPs following synchronous stimulation of group II and cutaneous afferents of these nerves shows, however, that sacral interneurones may induce excitation or inhibition of motoneurones via other interneurones. 5. Comparison of the properties of group II interneurones in the sacral segments with those of previously studied group II interneurones in the midlumbar segments leads to the conclusion that these two populations of neurones are specialized for the processing of information from different muscles and skin areas. In addition, equivalents of only one of the two subpopulations of midlumbar interneurones have been found at the level of the pudendal nucleus: neurones with input from group II but not from group I muscle afferents. Neurones integrating information from group I and II muscle afferents and in direct contact with motoneurones thus seem to be scarce in the sacral segments.},
	journal = {Notes},
	author = {Schuster, Ingmar and Jähnichen, Patrick},
	year = {2012},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YIPT72WF\\TMI05.2_logistic_regression.pdf:application/pdf},
}

@article{Clyde2017,
	title = {Bayesian {Variable} {Selection} and {Model} {Averaging} using {Bayesian} {Adaptive} {Sampling}},
	url = {https://github.com/merliseclyde/BAS},
	abstract = {Depends R ({\textgreater}= 3.0), Imports stats, graphics, utils, grDevices Suggests MASS, knitr, GGally, rmarkdown, roxygen2 Description Package for Bayesian Variable Selection and Model Averaging in linear models and generalized linear models using stochastic or deterministic sampling without replacement from posterior distributions. Prior distributions on coefficients are from Zellner's g-prior or mixtures of g-priors corresponding to the Zellner-Siow Cauchy Priors or the mixture of g-priors from Liang et al (2008) {\textless}DOI:10.1198/016214507000001337{\textgreater}for linear models or mixtures of g-priors in GLMs of Li and Clyde (2015) {\textless}arXiv:1503.06913{\textgreater}. Other model selection criteria include AIC, BIC and Empirical Bayes estimates of g. Sampling probabilities may be updated based on the sampled models using Sampling w/out Replacement or an efficient MCMC algorithm samples models using the BAS tree structure as an efficient hash table. Uniform priors over all models or beta-binomial prior distributions on model size are allowed, and for large p truncated priors on the model space may be used. The user may force variables to always be included. Details behind the sampling algorithm are provided in Clyde, Ghosh and Littman (2010) {\textless}DOI:10.1198/jcgs.},
	author = {Clyde, Merlise},
	year = {2017},
	keywords = {Algorithms, Bayesian},
}

@article{Brieman2001,
	title = {Random {Forests}},
	volume = {45},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * * , 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	journal = {Machine learning},
	author = {Brieman, Leo},
	year = {2014},
	note = {tex.ids= breimanRandomForests2001},
	keywords = {classification, ensemble, regression},
	pages = {5--32},
	file = {Breiman - 2001 - Random Forests.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Breiman - 2001 - Random Forests.pdf:application/pdf;Random Forests.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Random Forests.md:text/plain},
}

@book{Kiraz2019,
	title = {Praise for the {First} {Edition}},
	isbn = {978-0-321-35668-0},
	abstract = {TextbfRaves for the First Edition! emphI sure wish I had this book ten years ago. Some might think that I dont need any Java books, but I need this one. James Gosling, fellow and vice president, Sun Microsystems, Inc. emphAn excellent book, crammed with good advice on using the Java programming language and object-oriented programming in general. Gilad Bracha, coauthor of emphThe Java Language Specification, Third aspiring to write good Java code that others will appreciate reading and maintaining should be required to own a copy of this book. This is one of those rare books where the information wont become obsolete with subsequent releases of the JDK library. Peter Tran, bartender, JavaRanch.com emphThe best Java book yet written.... Really great; very readable and eminently useful. I cant say enough good things about this book. At JavaOne 2001, James Gosling said, Go buy this book! Im glad I did, and I couldnt agree more. Keith Edwards, senior member of research staff, Computer Science Lab at the Palo Alto Research Center (PARC), and author of emphCore JINI (Prentice Hall, 2000) emphThis is a truly excellent book done by the guy who designed several of the better recent Java platform APIs (including the Collections API). James Clark, technical lead of the XML Working Group during the creation of the XML 1.0 Recommendation, editor of the XPath and XSLT content. Analogous to Scott Meyers classic Effective C++. emphIf you know the basics of Java, this has to be your next book. Gary K. Evans, OO mentor and consultant, Evanetics, Inc emphJosh Bloch gives great insight into best practices that really can only be discovered after years of study and experience. Mark Mascolino, software engineer emphThis is a superb book. It clearly covers many of the language/platform subtleties and trickery you need to learn to become a real Java master. Victor Wiewiorowski, vice president development and code quality manager, ValueCommerce Co., Tokyo, Japan emphI like books that under-promise in their titles and over-deliver in their contents. This book has 57 items of programming advice that are well chosen. Each item reveals a clear, deep grasp of the language. Each one illustrates in simple, practical terms the limits of programming on intuition alone, or taking the most direct path to a solution without fully understanding what the language offers. Michael Ernest, Inkling Research, Inc. emphI dont find many programming books that make me want to read every pagethis is one of them. Matt Tucker, chief technical officer, Jive Software emphGreat how-to resource for the experienced developer. John Zukowski, author of numerous Java technology books emphI picked this book up two weeks ago and can safely say I learned more about the Java language in three days of reading than I did in three months of study! An excellent book and a welcome addition to my Java library. Jane Griscti, I/T advisory specialist Are you looking for a deeper understanding of the Java programming language so that you can write code that is clearer, more correct, more robust, and more reusable? Look no further! emphtextbfEffective Java, Second Edition, brings together seventy-eight indispensable programmers rules of thumb: working, best-practice solutions for the programming challenges you encounter every day. This highly anticipated new edition of the classic, Jolt Award-winning work has been thoroughly updated to cover Java SE 5 and Java SE 6 features introduced since the first edition. Bloch explores new design patterns and language idioms, showing you how to make the most of features ranging from generics to enums, annotations to autoboxing. Each chapter in the book consists of several items presented in the form of a short, standalone essay that provides specific advice, insight into Java platform subtleties, and outstanding code examples. The comprehensive descriptions and explanations for each item illuminate what to do, what not to do, and why. Highlights include: New coverage of generics, enums, annotations, autoboxing, the for-each loop, varargs, concurrency utilities, and much more Updated techniques and best practices on classic topics, including objects, classes, libraries, methods, and serialization How to avoid the traps and pitfalls of commonly misunderstood subtleties of the language Focus on the language and its most fundamental libraries: java.lang, java.util, and, to a lesser extent, java.util.concurrent and java.io Simply put, emphtextbfEffective Java, Second Edition, presents the most practical, authoritative guidelines available for writing efficient, well-designed programs.},
	author = {Kiraz, George},
	year = {2019},
	doi = {10.31826/9781463209674-001},
	note = {Publication Title: Comparative Edition of the Syriac Gospels},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5BQBK6SP\\Effective Java, 2nd Edition.pdf:application/pdf},
}

@article{George1993,
	title = {Variable {Selection} {Via} {Gibbs} {Sampling}},
	volume = {88},
	issn = {0162-1459},
	doi = {10.1080/01621459.1993.10476353},
	journal = {Journal of the American Statistical Association},
	author = {George, Edward I and Mcculloch, Robert E},
	year = {1993},
	keywords = {Algorithms, Bayesian},
	pages = {881--889},
}

@article{Tsai1998,
	title = {The examination of residual plots},
	volume = {8},
	issn = {10170405},
	abstract = {Linear and squared residual plots are proposed to assess nonlinearity and heteroscedasticity in regression diagnostics. It is shown that linear residual plots are useful for diagnosing nonlinearity and squared residual plots are powerful for detecting nonconstant variance. A paradigm for the graphical interpretation of residual plots is presented.},
	number = {2},
	journal = {Statistica Sinica},
	author = {Tsai, Chih Ling and Cai, Zongwu and Wu, Xizhi},
	year = {1998},
	keywords = {Heteroscedasticity, Leverages, Nonlinearity, Outliers},
	pages = {445--465},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y5C2SERP\\THE EXAMINATION OF RESIDUAL PLOTS.pdf:application/pdf},
}

@article{Haussler1988,
	title = {Quantifying inductive bias: {AI} learning algorithms and {Valiant}'s learning framework},
	volume = {36},
	issn = {00043702},
	doi = {10.1016/0004-3702(88)90002-1},
	abstract = {We show that the notion of inductive bias in concept learning can be quantified in a way that directly relates to learning performance in the framework recently introduced by Valiant. Our measure of bias is based on the growth function introduced by Vapnik and Chervonenkis, and on the Vapnik-Chervonenkis dimension. We measure some common language biases, including restriction to conjunctive concepts, conjunctive concepts with internal disjunction, k-DNF and k-CNF concepts. We also measure certain types of bias that result from a preference for simpler hypotheses. Using these bias measurements we analyze the performance of the classical learning algorithm for conjunctive concepts from the perspective of Valiant's learning framework. We then augment this algorithm with a hypothesis simplification routine that uses a greedy heuristic and show how this improves learning performance on simpler target concepts. Improved learning algorithms are also developed for conjunctive concepts with internal disjunction, k-DNF and k-CNF concepts. We show that all our algorithms are within a logarithmic factor of optimal in terms of the number of examples they require to achieve a given level of learning performance in the Valiant framework. Our results hold for arbitrary attribute-based instance spaces defined by either tree-structured or linear attributes. ©1988.},
	number = {2},
	journal = {Artificial Intelligence},
	author = {Haussler, David},
	year = {1988},
	pages = {177--221},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SML85HQL\\valiants learning framework.pdf:application/pdf},
}

@article{Francis1979,
	title = {Brown corpus manual},
	volume = {5},
	url = {http://clu.uni.no/icame/manuals/BROWN/INDEX.HTM},
	number = {2},
	journal = {Letters to the Editor},
	author = {Francis, W N and Kucera, H},
	year = {1979},
	keywords = {Applications, Natural Language Processing},
	pages = {7},
}

@article{Kneser1997,
	title = {Language model adaptation using dynamic marginals},
	volume = {2},
	abstract = {A new method is presented to quickly adapt a given language model to local text characteristics. The ba-sic approach is to choose the adaptive models as close as possible to the background estimates while con-straining them to respect the locally estimated uni-gram probabilities. Several means are investigated to speed up the calculations. We measure both perplex-ity and word error rate to gauge the quality of our model.},
	number = {140403},
	author = {Kneser, Reinhard and Peters, Jochen and Klakow, Dietrich},
	year = {1997},
	keywords = {Applications, Natural Language Processing},
	pages = {1--24},
}

@misc{EconomicGraphTeam2017,
	title = {{LinkedIn}'s 2017 {U}.{S}. {Emerging} {Jobs} {Report}},
	url = {https://economicgraph.linkedin.com/research/LinkedIns-2017-US-Emerging-Jobs-Report},
	abstract = {The job market in the U.S. is brimming right now with fresh and exciting opportunities for professionals in a range of emerging roles.},
	urldate = {2020-08-13},
	author = {{Economic Graph Team}},
	year = {2017},
	note = {Publication Title: LinkedIn},
}

@misc{Bailey2016,
	title = {Brown {Corpus} {Word} {Frequency} {List}},
	url = {http://www.lextutor.ca/freq/lists_download/},
	author = {Bailey, Cathy},
	year = {2016},
	note = {Publication Title: Georgetown Linguistics},
	keywords = {Applications, Natural Language Processing},
}

@book{LudwigWittgensteinStanford,
	title = {Ludwig {Wittgenstein} ({Stanford} {Encyclopedia} of {Philosophy})},
	url = {http://plato.stanford.edu/entries/wittgenstein},
	publisher = {Plato.stanford.edu},
	keywords = {Applications, Natural Language Processing},
}

@article{Analysis,
	title = {{CH4}-{Regression} {Diagnostics} and {Advanced} {Regression} {Topics}},
	abstract = {We continue our discussion of regression by talking about residuals and outliers, and then look at some more advanced approaches for linear regression, including nonlinear models and sparsity-and robustness-oriented approaches.},
	journal = {Statistics for Research Projects},
	author = {Analysis, Residual and Again, Quartet Revisited},
	pages = {1--12},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PGPG8VQA\\Regression Diagnostics and Advanced Regression Topics (Lecture 4).pdf:},
}

@book{Hansen2001,
	title = {Understanding active noise cancellation},
	isbn = {0-415-23377-1},
	abstract = {Ch. 1. A Little History -- Ch. 2. Foundations of Active Control. 2.1. Physical Mechanisms. 2.2. Basic Structure of Active Noise Control Systems. 2.3. Control System Optimization -- Ch. 3. The Electronic Control System. 3.1. Introduction. 3.2. Digital Filters (Adaptive Control Filters). 3.3. Adaptation Algorithms for Adaptive Filters. 3.4. Waveform Synthesis. 3.5. Important Controller Implementation Issues -- Ch. 4. Active Noise Control Sources. 4.1. Introduction. 4.2. Acoustic Sources. 4.3. Vibration Sources -- Ch. 5. Reference and Error Sensing. 5.1. Microphones. 5.2. Tachometer Reference Signal. 5.3. Sound Intensity. 5.4. Energy Density. 5.5. Virtual Sensing. 5.6. Vibration Sensing of Sound Radiation.},
	publisher = {Spon Press},
	author = {Hansen, Colin H},
	year = {2001},
}

@article{Schelter2017,
	title = {Automatically {Tracking} {Metadata} and {Provenance} of {Machine} {Learning} {Experiments}},
	abstract = {We present a lightweight system to extract, store and manage metadata and prove-nance information of common artifacts in machine learning (ML) experiments: datasets, models, predictions, evaluations and training runs. Our system accelerates users in their ML workflow, and provides a basis for comparability and repeata-bility of ML experiments. We achieve this by tracking the lineage of produced artifacts and automatically extracting metadata such as hyperparameters of models, schemas of datasets or layouts of deep neural networks. Our system provides a general declarative representation of said ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon.},
	journal = {Machine Learning Systems Workshop at NIPS},
	author = {Schelter, Sebastian and Böse, Joos-Hendrik and Kirschnick, Johannes and Klein, Thoralf and Seufert, Stephan},
	year = {2017},
	pages = {1--8},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LXXVP5WY\\Automatically Tracking Metadata of Machine Learning Experiments.pdf:},
}

@article{PettinatiHM2011,
	title = {Comorbid {Depression} and {Alcohol} {Dependence}},
	url = {http://www.psychiatrictimes.com/major-depressive-disorder/comorbid-depression-and-alcohol-dependence/page/0/1},
	journal = {Psychiatric Times},
	author = {Pettinati HM, Dundon W D},
	year = {2011},
}

@article{Lagaris2008,
	title = {Stopping rules for box-constrained stochastic global optimization},
	volume = {197},
	issn = {00963003},
	doi = {10.1016/j.amc.2007.08.001},
	abstract = {We present three new stopping rules for Multistart based methods. The first uses a device that enables the determination of the coverage of the bounded search domain. The second is based on the comparison of asymptotic expectation values of observable quantities to the actually measured ones. The third offers a probabilistic estimate for the number of local minima inside the search domain. Their performance is tested and compared to that of other widely used rules on a host of test problems in the framework of Multistart.},
	number = {2},
	journal = {Applied Mathematics and Computation},
	author = {Lagaris, I E and Tsoulos, I G},
	year = {2008},
	keywords = {multistart, stochastic global optimization, stopping rules},
	pages = {622--632},
}

@article{Markov1906,
	title = {Rasprostranenie zakona bol'shih chisel na velichiny, zavisyaschie drug ot druga},
	volume = {2-ya seriy},
	number = {tom 15},
	journal = {Izvestiya Fiziko-matematicheskogo obschestva pri Kazanskom universitete},
	author = {Markov, A},
	year = {1906},
	keywords = {Applications, Natural Language Processing},
	pages = {135--156},
}

@article{Gelman2008,
	title = {A {WEAKLY} {INFORMATIVE} {DEFAULT} {PRIOR} {DISTRIBUTION} {FOR} {LOGISTIC} {AND} {OTHER} {REGRESSION} {MODELS}},
	volume = {2},
	doi = {10.1214/08-AOAS191},
	abstract = {We propose a new prior distribution for classical (nonhierarchical) lo-gistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-t prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine ap-plied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also au-tomatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-t prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting pref-erences, a small bioassay experiment, and an imputation model for a public health data set.},
	number = {4},
	journal = {The Annals of Applied Statistics},
	author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
	year = {2008},
	keywords = {linear regression, Algorithms, Bayesian, Bayesian inference, generalized linear model, hierarchical model, least squares, logistic regression, multilevel model, noninformative prior distribution, weakly informative prior distribution},
	pages = {1360--1383},
}

@article{Hansen,
	title = {Model {Selection} and the {Principle} of {Minimum} {Description} {Length}},
	abstract = {This article reviews the principle of minimum description length (MDL) for problems of model selection. By viewing statistical modeling as a means of generating descriptions of observed data, the MDL framework discriminates between competing models based on the complexity of each description. This approach began with Kolmogorov's theory of algorithmic complexity, matured in the literature on information theory, and has recently received renewed attention within the statistics community. Here we review both the practical and the theoretical aspects of MDL as a tool for model selection, emphasizing the rich connections between information theory and statistics. At the boundary between these two disciplines we nd many interesting interpretations of popular frequentist and Bayesian procedures. As we show, MDL provides an objective umbrella under which rather disparate approaches to statistical modeling can coexist and be compared. We illustrate the MDL principle by considering problems in regression, nonparametric curve estimation, cluster analysis, and time series analysis. Because model selection in linear regression is an extremely common problem that arises in many applications, we present detailed derivations of several MDL criteria in this context and discuss their properties through a number of examples. Our emphasis is on the practical application of MDL, and hence we make extensive use of real datasets. In writing this review, we tried to make the descriptive philosophy of MDL natural to a statistics audience by examining classical problems in model selection. In the engineering literature, however, MDL is being applied to ever more exotic modeling situations. As a principle for statistical modeling in general, one strength of MDL is that it can be intuitively extended to provide useful tools for new problems. The principle of parsimony, or Occam's razor, implicitly motivates the process of data analysis and statistical model-ing and is the soul of model selection. Formally, the need for model selection arises when investigators must decide among model classes based on data. These classes might be indistin-guishable from the standpoint of existing subject knowledge or scientii c theory, and the selection of a particular model class implies the conn rmation or revision of a given theory. To implement the parsimony principle, one must quantify " parsi-mony" of a model relative to the available data. Applying this measure to a number of candidates, we search for a concise model that provides a good t to the data. Rissanen (1978) distilled such thinking in his principle of minimum descrip-tion length (MDL): Choose the model that gives the shortest description of data. In this framework a concise model is one that is easy to describe, whereas a good t implies that the model captures or describes the important features evident in the data. MDL has its intellectual roots in the algorithmic or descriptive complexity theory of Kolmogorov, Chaitin, and Solomonoff (cf. Li and Vitányi 1996). Kolmogorov, the founder of axiomatic probability theory, examined the rela-tionship between mathematical formulations of randomness and their application to real-world phenomena. He ultimately turned to algorithmic complexity as an alternative means of expressing random events. A new characterization of prob-ability emerged based on the length of the shortest binary computer program that describes an object or event.},
	author = {Hansen, Mark H and Yu, Bin},
	keywords = {Algorithms, Bayesian, A IC, Bayes information criterion, Bayesian methods, Cluster analysis, Code length, Coding redundancy, Information theory, Model selection, Pointwise and minimax lower bounds, Regression, time series},
}

@article{Madigan,
	title = {Strategies for {Graphical} {Model} {Selection}},
	author = {Madigan, David and Raftery, Adrian E and Bradshaw, Jeerey M},
	keywords = {Algorithms, Bayesian},
}

@article{Holkamp2014,
	title = {A {Comparison} of {Human} and {Machine} {Learning}-based {Accuracy} for {Valence} {Classification} of {Subjects} in {Video} {Fragments}},
	author = {Holkamp, Y H},
	year = {2014},
	pages = {27--30},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F2NT6CC4\\Human vs Machine Classification Performance.pdf:application/pdf},
}

@misc{UniversityofWash,
	title = {Machine {Learning} - {University} of {Washington} {\textbar} {Coursera}},
	url = {https://www.coursera.org/specializations/machine-learning},
	abstract = {This Specialization from leading researchers at the University of Washington introduces you to the exciting, high-demand field of Machine Learning. Through a series of practical case studies, you will gain applied experience in major areas of Machine Learning including Prediction, Classification, Clustering, and Information Retrieval. You will learn to analyze large and complex datasets, create systems that adapt and improve over time, and build intelligent applications that can make predictions from data.},
	urldate = {2017-09-08},
	author = {{University of Washington}},
	keywords = {certificates, Coursera, courses, education, free, mooc, online, specializations},
}

@article{Msr2018,
	title = {Alors : {An} algorithm recommender system {To} cite this version : {HAL} {Id} : hal-01419874 {Alors} : an {Algorithm} {Recommender} {System}},
	author = {Mısır, Mustafa and Sebag, Michèle and An, Alors},
	year = {2018},
	keywords = {Applications, Recommender Systems},
	pages = {291--314},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IKNKQJVX\\Alors an Algorithm Recommendation System.pdf:application/pdf},
}

@article{Zhang2012,
	title = {Numerical optimization},
	issn = {21931577},
	doi = {10.1007/978-1-4471-2224-1_2},
	abstract = {Numerical optimization is introduced as the mathematical foundation for this book, focusing on two basic unconstrained optimization algorithms: line search and trust-region methods. Line search optimization methods are relatively simple and commonly used gradient descent based methods. Their strength lies in their simplicity and ease of implementation, but their convergence properties degrade as the nonlinearity and complexity of the function to be optimized increases. Trust-region, or “restricted step” methods are presented as an often more practical alternative to line search that involved the construction of an approximation, or “model,” of the function to be minimized, together with a dynamic estimate of the region where this model is sufficiently valid. There are a large number of optimization methods, and the interested reader is referred to the bibliographical citations presented in this chapter. In this book we restrict our attention mainly to line search and trust-region methods, since we will use them in the context of their application to extremum seeking control and stability proofs thereof.},
	number = {9781447122234},
	journal = {Advances in Industrial Control},
	author = {Zhang, Chunlei and Ordóñez, Raúl},
	year = {2012},
	pages = {31--45},
}

@techreport{Duggan2013a,
	title = {Additional {Demographic} {Analysis} {\textbar} {Pew} {Research} {Center}},
	url = {http://www.pewinternet.org/2013/09/19/additional-demographic-analysis/},
	author = {Duggan, Maeve},
	year = {2013},
}

@article{Hasan2012a,
	title = {Performance {Analysis} of {Different} {Smoothing} {Methods} on n-grams for {Statistical} {Machine} {Translation}},
	volume = {46},
	abstract = {Smoothing techniques adjust the maximum likelihood estimate of probabilities to produce more accurate probabilities. This is one of the most important tasks while building a language model with a limited number of training data. Our main contribution of this paper is to analyze the performance of different smoothing techniques on n-grams. Here we considered three most widely-used smoothing algorithms for language modeling: Witten-Bell smoothing, Kneser-Ney smoothing, and Modified Kneser-Ney smoothing. For the evaluation we use BLEU (Bilingual Evaluation Understudy) and NIST (National Institute of Standards and Technology) scoring techniques. A detailed evaluation of these models is performed by comparing the automatically produced word alignment. We use Moses Statistical Machine Translation System for our work (i.e.Moses decoder, GIZA++, mkcls, SRILM, IRSTLM, Pharaoh, BLEU Scoring Tool). Here machine translation approach has been tested on German to English and English to German task. Our obtain results are significantly better than those obtained with alternative approaches to machine translation. This paper addresses several aspects of Statistical Machine Translation (SMT). The emphasis is put on the architecture and modeling of an SMT system.},
	number = {2},
	journal = {International Journal of Computer Applications},
	author = {Hasan, Mahmudul and Islam, Saria and Rahman, M Arifur},
	year = {2012},
	keywords = {Applications, Natural Language Processing, BLEU, Machine Translation, n-gram, NIST, parallel corpora, smoothing, SMT},
	pages = {975--8887},
}

@article{LossFunctionsBinary2005,
	title = {Loss {Functions} for {Binary} {Classification} and {Class}},
	year = {2005},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3ANW6MY5\\LOSS FUNCTIONS FOR BINARY CLASSIFICATION AND CLASS.pdf:application/pdf},
}

@techreport{Roughgarden2018,
	title = {{CS168}: {The} {Modern} {Algorithmic} {Toolbox} {Gradient} {Descent} {Basics}},
	abstract = {Gradient descent is an extremely simple algorithm-simpler than most of the algorithms you studied in CS161-that has been around for centuries. These days, the main "killer app" is machine learning. Model-fitting often reduces to optimization-for example, maximizing the likelihood of observed data over a family of generative models. A remarkably large fraction of modern machine learning research, including some of the much-hyped recent work on "deep learning," boils down to implementing variants of gradient descent on a very large scale (i.e., for huge training sets). Indeed, the choice of models in many machine learning applications is driven as much by computational considerations-whether or not gradient descent can be implemented quickly-as by any other criteria. The first goal of these notes is to develop the geometry and intuition behind gradient descent, to the point that the algorithm seems totally obvious. The second goal is to make the general method concrete with a case study on linear regression. (The method is also very useful for many other problems.) Throughout these notes, you might want to keep Figure 1 in mind. The figures show a succession of lines that are an increasingly good fit for a collection of points in the plane. "Linear regression" just means computing the best-fitting line, and this succession of lines is generated by successive iterations of gradient descent. This is not all supposed to make 100\% sense yet, of course-the rest of the notes explains what's going on-but this example should provide you with a concrete picture to refer back to as the lecture proceeds. *},
	author = {Roughgarden, Tim and Valiant, Gregory},
	year = {2018},
}

@article{Zhang2015,
	title = {Cross-validation for selecting a model selection procedure},
	volume = {187},
	issn = {18726895},
	doi = {10.1016/j.jeconom.2015.02.006},
	abstract = {While there are various model selection methods, an unanswered but important question is how to select one of them for data at hand. The difficulty is due to that the targeted behaviors of the model selection procedures depend heavily on uncheckable or difficult-to-check assumptions on the data generating process. Fortunately, cross-validation (CV) provides a general tool to solve this problem. In this work, results are provided on how to apply CV to consistently choose the best method, yielding new insights and guidance for potentially vast amount of application. In addition, we address several seemingly widely spread misconceptions on CV.},
	number = {1},
	journal = {Journal of Econometrics},
	author = {Zhang, Yongli and Yang, Yuhong},
	year = {2015},
	keywords = {Adaptive procedure selection, Cross-validation, Cross-validation paradox, Data splitting ratio, Information criterion, LASSO, MCP, SCAD},
	pages = {95--112},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GTT7DF24\\Cross-Validation for Selecting a Model Selection Procedure.pdf:application/pdf},
}

@article{VinhNguyen2017,
	title = {Package 'inference' {Title} {Functions} to extract inferential values of a fitted model object},
	url = {http://r-forge.r-project.org/projects/inference/},
	abstract = {Description Collection of functions to extract inferential values (point estimates, confidence intervals, p-values, etc) of a fitted model object into a matrix-like object that can be used for table/report generation; transform point estimates via the delta method. 1 2 infer,-methods inference-package Extract inferential information from different statistical models. Description Extract inferential information from different statistical models. Details This package provides functions to extract point estimates, standard errors, confidence intervals, p-values, and sample size from a fitted model in a matrix-like object. The purpose is to have all inferential numbers be readily accessible, especially in the construction of summary tables (R -{\textgreater}LaTeX -{\textgreater}html -{\textgreater}Word) for publication and collaboration.},
	author = {Vinh Nguyen, Author and Vinh Nguyen, Maintainer},
	year = {2017},
}

@article{Gelman2008a,
	title = {Objections to {Bayesian} statistics},
	volume = {3},
	doi = {10.1214/08-BA318},
	abstract = {Bayesian inference is one of the more controversial approaches to statistics. The fundamental objections to Bayesian methods are twofold: on one hand, Bayesian methods are presented as an automatic inference engine, and this raises suspicion in anyone with applied experience. The second objection to Bayes comes from the opposite direction and addresses the subjective strand of Bayesian inference. This article presents a series of objections to Bayesian inference, written in the voice of a hypothetical anti-Bayesian statistician. The article is intended to elicit elaborations and extensions of these and other arguments from non-Bayesians and responses from Bayesians who might have different perspectives on these is-sues. Bayesian inference is one of the more controversial approaches to statistics, with both the promise and limitations of being a closed system of logic. There is an extensive literature, which sometimes seems to overwhelm that of Bayesian inference itself, on the advantages and disadvantages of Bayesian approaches. Bayesians' contributions to this discussion have included defense (explaining how our methods reduce to classical methods as special cases, so that we can be as inoffensive as anybody if needed), af-firmation (listing the problems that we can solve more effectively as Bayesians), and attack (pointing out gaps in classical methods). The present article is unusual in representing a Bayesian's presentation of what he views as the strongest non-Bayesian arguments. Although this originated as an April Fool's blog entry (Gelman, 2008), I realized that these are strong arguments to be taken seriously—and ultimately accepted in some settings and refuted in others.},
	number = {3},
	journal = {Bayesian Analysis},
	author = {Gelman, Andrew},
	year = {2008},
	keywords = {Algorithms, Bayesian, Comparisons to other methods, Foundations},
	pages = {445--450},
}

@article{Gh2001,
	title = {Contribution to {Word} {Prediction} in {Spanish} and its {Integration} in {Technial} {Aids} for {People} with {Physical} {Disabilities}},
	author = {Gh, Derudwrulr and Gh, Hfqrorjtdv and Gh, S W R and Gh, Rolwpfqlfd and Navarro, Santiago Aguilera},
	year = {2001},
	keywords = {Applications, Natural Language Processing},
}

@article{Schwartz,
	title = {On {Using} {Written} {Language} {Training} {Data} for {Spoken} {Language} {Modeling}},
	abstract = {We attemped to improve recognition accuracy by reduc-ing the inadequacies of the lexicon and language model. Specifically we address the following three problems: (1) the best size for the lexicon, (2) conditioning written text for spoken language recognition, and (3) using additional training outside the text distribution. We found that in-creasing the lexicon 20,000 words to 40,000 words re-duced the percentage of words outside the vocabulary from over 2\% to just 0.2\%, thereby decreasing the error rate substantially. The error rate on words already in the vocabulary did not increase substantially. We modified the language model training text by applying rules to sim-ulate the differences between the training text and what people actually said. Finally, we found that using another three years' of training text -even without the appropri-ate preprocessing, substantially improved the language model We also tested these approaches on spontaneous news dictation and found similar improvements.},
	author = {Schwartz, R and Nguyen, L and Kubala, F and Chou, G and Zavaliagkos, G and Makhoul, J},
	keywords = {Applications, Natural Language Processing},
}

@article{Xiong2015,
	title = {Global supervised descent method},
	volume = {07-12-June},
	issn = {10636919},
	doi = {10.1109/CVPR.2015.7298882},
	abstract = {Mathematical optimization plays a fundamental role in solving many problems in computer vision (e.g., camera calibration, image alignment, structure from motion). It is generally accepted that second order descent methods are the most robust, fast, and reliable approaches for nonlinear optimization of a general smooth function. However, in the context of computer vision, second order descent methods have two main drawbacks: 1) the function might not be analytically differentiable and numerical approximations are impractical, and 2) the Hessian may be large and not positive definite. Recently, Supervised Descent Method (SDM), a method that learns the 'weighted averaged gradients' in a supervised manner has been proposed to solve these issues. However, SDM is a local algorithm and it is likely to average conflicting gradient directions. This paper proposes Global SDM (GSDM), an extension of SDM that divides the search space into regions of similar gradient directions. GSDM provides a better and more efficient strategy to minimize non-linear least squares functions in computer vision problems. We illustrate the effectiveness of GSDM in two problems: non-rigid image alignment and extrinsic camera calibration.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Xiong, Xuehan and De La Torre, Fernando},
	year = {2015},
	note = {ISBN: 9781467369640},
	pages = {2664--2673},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DR4AWUFE\\Xiong_Global_Supervised_Descent_2015_CVPR_paper.pdf:application/pdf},
}

@techreport{Ma,
	title = {Identifying {Suspicious} {URLs}: {An} {Application} of {Large}-{Scale} {Online} {Learning}},
	url = {www.ebay.com'},
	abstract = {This paper explores online learning approaches for detecting malicious Web sites (those involved in criminal scams) using lexical and host-based features of the associated URLs. We show that this application is particularly appropriate for on-line algorithms as the size of the training data is larger than can be efficiently processed in batch and because the distribution of features that typify malicious URLs is changing continuously. Using a real-time system we developed for gathering URL features, combined with a real-time source of labeled URLs from a large Web mail provider, we demonstrate that recently-developed online algorithms can be as accurate as batch techniques, achieving classification accuracies up to 99\% over a balanced data set.},
	institution = {Proceedings of the International Conference on Machine Learning (ICML)},
	author = {Ma, Justin and Saul, Lawrence K and Savage, Stefan and Voelker, Geoffrey M},
	pages = {681--688},
}

@article{NewGuideArtificial1991,
	title = {A new guide to artificial intelligence},
	volume = {29},
	issn = {0009-4978},
	doi = {10.5860/choice.29-0944},
	abstract = {AI: what is it? -- AI and the science of computer usage: the forging of a methodology -- The major paradigms -- The Babel of AI languages -- Current expert systems technology (CEST) -- Knowledge representation: a problem of both structure and function -- Vision: seeing is perceiving -- Language processing: what you hear is what you are -- Learning to do it right -- Foundations of AI: can we find any? -- Prognostications, or w(h)ither AI?},
	number = {02},
	journal = {Choice Reviews Online},
	year = {1991},
	pages = {29--944},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VNJR9T9Z\\ArtificialIntelligenceinHealthcare.pdf:application/pdf},
}

@article{Misra2017,
	title = {Optimization for {Deep} {Networks}},
	issn = {0024-3205},
	abstract = {Alpha1-adrenergic receptors (AR) are members of the superfamily of G protein-coupled receptors (GPCRs) which mediate the effects of the sympathetic nervous system. Alpha1-AR comprise a heterogeneous family of three distinct isoforms of alpha1A, alpha1B and alpha1D; however, very little is known about their difference in physiological role or regulation. We have recently observed a subtype-specific differences in subcellular localization of alpha1-ARs; thus, alpha1A-AR predominantly localize intracellularly, while alpha1B-AR on the cell surface. To examine the molecular mechanism for the subtype-specific differences in subcellular localization, we conducted a search for novel proteins that interact with the alpha1B-AR, specifically focusing on the carboxyl-terminal cytoplasmic domain. Using interaction cloning and biochemical techniques, we demonstrate that gC1q-R interacts with alpha1B-AR in vitro and in vivo through the specific site, and that in cells which co-express alpha1B-AR and gC1q-R, the subcellular localization of alpha1B-AR is markedly altered and its expression is down-regulated. These results suggest that gC1q-R plays a role in the regulation of the subcellular localization as well as the function of alpha1B-ARs.},
	author = {Misra, Ishan},
	year = {2017},
	pmid = {11358335},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\B65T8FP4\\Optimization for Deep Networks.pdf:},
}

@book{Lax2018,
	title = {Linear {Algebra} {Linear} {Algebra}},
	isbn = {978-0-471-75156-4},
	author = {Lax, Peter D},
	year = {2018},
	note = {Publication Title: Mathematics for Machine Learning},
	keywords = {bases, canonical forms, duality, inner product spaces, normality, scalars, similarity, transformations, vectors},
}

@techreport{Feinerer2015,
	title = {Package 'tm' {Text} {Mining} {Package}},
	url = {https://cran.r-project.org/web/packages/tm/index.html},
	abstract = {A framework for text mining applications within R.},
	author = {Feinerer, Ingo},
	year = {2015},
	keywords = {Applications, Natural Language Processing},
	pages = {58},
}

@article{FightingChronicDisease,
	title = {Fighting {Chronic} {Disease}},
	abstract = {Amount spent each year treating patients with one or more chronic disease. Chronic illnesses account for 75\% of the \$2.2 trillion we spend on health care each year in the U.S. How Do Chronic Diseases Impact the Economy? The annual economic impact on the U.S. of the seven most common chronic diseases is estimated to be \$1.3 trillion, which could balloon to nearly \$6 trillion by 2050. What's the Solution? Investing in prevention and disease management will save lives and money now and well into the future. Programs that encourage healthy lifestyles – like healthy eating, fitness and not smoking – are effective and inexpensive ways to reduce our nation's chronic disease burden. We can also improve health and manage costs by catching chronic diseases early and treating them appropriately to avoid costly complications. Preventing and better managing disease would eliminate, not just shift, costs from our health care system. Today, nearly half of all Americans live with one or more chronic disease. The impact of these illnesses on our nation's health and prosperity is startling, and Congress is currently considering proposals to better prevent and manage disease in order to save both lives and money. How CBO Scoring Fits In: The Congressional Budget Office (CBO) provides Congress with independent, non-partisan analyses of how proposed legislation – including legislation to reform health care – would impact the federal budget. Members of Congress rely on CBO to estimate how much a piece of legislation will increase or decrease federal spending and revenue. The fiscal feasibility of legislative proposals often rests on CBO projections. CBO is highly credible and uses some of the most robust analysis techniques. However, we know a great deal more about the impact of disease on health care costs than we did just a few years ago, and that new data could be used to enhance CBO methods. This would more fully capture the benefits of chronic disease prevention and management and provide lawmakers with even better information to guide health reform decisions. The U.S. has developed a wealth of knowledge in the prevention and treatment of many chronic diseases. We need policy changes that help us put the full value of that knowledge to work by investing in chronic disease prevention and management. Policies that improve health in these ways will not only improve Americans' quality of life, but they will save our economy billions of dollars by eliminating – not just shifting – costs from our health care system, and by increasing U.S. productivity.},
}

@techreport{Piech2016,
	title = {Logistic {Regression}},
	author = {Piech, Chris},
	year = {2016},
}

@article{Porter2001,
	title = {Strategy and the {Internet}.},
	volume = {79},
	issn = {00178012},
	doi = {10.2469/dig.v31.n4.960},
	abstract = {Many of the pioneers of Internet business, both dot-coms and established companies, have competed in ways that violate nearly every precept of good strategy. Rather than focus on profits, they have chased customers indiscriminately through discounting, channel incentives, and advertising. Rather than concentrate on delivering value that earns an attractive price from customers, they have pursued indirect revenues such as advertising and click-through fees. Rather than make trade-offs, they have rushed to offer every conceivable product or service. It did not have to be this way--and it does not have to be in the future. When it comes to reinforcing a distinctive strategy, Michael Porter argues, the Internet provides a better technological platform than previous generations of IT. Gaining competitive advantage does not require a radically new approach to business; it requires building on the proven principles of effective strategy. Porter argues that, contrary to recent thought, the Internet is not disruptive to most existing industries and established companies. It rarely nullifies important sources of competitive advantage in an industry; it often makes them even more valuable. And as all companies embrace Internet technology, the Internet itself will be neutralized as a source of advantage. Robust competitive advantages will arise instead from traditional strengths such as unique products, proprietary content, and distinctive physical activities. Internet technology may be able to fortify those advantages, but it is unlikely to supplant them. Porter debunks such Internet myths as first-mover advantage, the power of virtual companies, and the multiplying rewards of network effects. He disentangles the distorted signals from the marketplace, explains why the Internet complements rather than cannibalizes existing ways of doing business, and outlines strategic imperatives for dot-coms and traditional companies.},
	number = {3},
	journal = {Harvard business review},
	author = {Porter, M E},
	year = {2001},
	pmid = {11246925},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AHBEPTIL\\Strategy_and_the_Internet.pdf:application/pdf},
}

@article{Johnson1944,
	title = {Studies in {Language} {Behaviour}: {I}. \{\{\}{A}\{\}\} {Program} of {Research}},
	volume = {56},
	journal = {Psychological Monographs},
	author = {Johnson, W},
	year = {1944},
	keywords = {Applications, Natural Language Processing},
	pages = {1--15},
}

@techreport{Marti,
	title = {Intelligent {Multi}-{Start} {Methods}},
	abstract = {Heuristic search procedures aimed at finding globally optimal solutions to hard combinatorial optimization problems usually require some type of diversification to overcome local optimality. One way to achieve diversification is to restart the procedure from a new solution once a region has been explored, which constitutes a multi-start procedure. In this chapter we describe the best known multi-start methods for solving optimization problems. We also describe their connections with other metaheuristic methodologies. We propose classifying these methods in terms of their use of randomization, memory and degree of rebuild. We also present a computational comparison of these methods on solving the Maximum Diversity Problem to illustrate the efficiency of the multi-start methodology in terms of solution quality and diversification power.},
	author = {Martí, R and Aceves, R and León, M T and Moreno-Vega, J M and Duarte, A and Martí, Rafael and Aceves, Ricardo and León, Maria Teresa and Moreno-Vega, Jose Marcos and Duarte, Abraham},
}

@article{EatonJohn2017,
	title = {Octave},
	issn = {1342-6907},
	url = {https://www.gnu.org/software/octave/},
	doi = {10.3169/itej.65.790},
	author = {Eaton, John, W},
	year = {2017},
}

@article{Zanette2005,
	title = {Dynamics of {Text} {Generation} with {Realistic} {Zipf}'s {Distribution}},
	volume = {12},
	issn = {0929-6174},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09296170500055293},
	doi = {10.1080/09296170500055293},
	number = {1},
	journal = {Journal of Quantitative Linguistics},
	author = {Zanette, Damián and Montemurro, Marcelo},
	month = apr,
	year = {2005},
	keywords = {Applications, Natural Language Processing},
	pages = {29--40},
}

@misc{octave,
	title = {{GNU} {Octave} : version 5.2.0 manual: a high-level interactive language for numerical computations},
	url = {https://www.gnu.org/software/octave/doc/v5.2.0/},
	author = {Eaton, John W and Bateman, David and Hauberg, Søren and Wehbring, Rik},
	year = {2020},
}

@article{MitOcw2011,
	title = {Symmetric matrices and positive definite ­ ness},
	abstract = {Symmetric Matrices and Positive Definiteness\${\textbackslash}\$n\${\textbackslash}\$nInstructor: David Shirokoff\${\textbackslash}\$n\${\textbackslash}\$nView the complete course: http://ocw.mit.edu/18-06SCF11\${\textbackslash}\$n\${\textbackslash}\$nLicense: Creative Commons BY-NC-SA\${\textbackslash}\$nMore information at http://ocw.mit.edu/terms\${\textbackslash}\$nMore courses at http://ocw.mit.edu},
	author = {{Mit Ocw}},
	year = {2011},
	pages = {1--4},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VFBBKQ5L\\Symmetric Matrices.pdf:},
}

@article{Buck,
	title = {N -gram {Counts} and {Language} {Models} from the {Common} {Crawl}},
	abstract = {We contribute 5-gram counts and language models trained on the Common Crawl corpus, a collection over 9 billion web pages. This release improves upon the Google n-gram counts in two key ways: the inclusion of low-count entries and deduplication to reduce boilerplate. By preserving singletons, we were able to use Kneser-Ney smoothing to build large language models. This paper describes how the corpus was processed with emphasis on the problems that arise in working with data at this scale. Our unpruned Kneser-Ney English 5-gram language model, built on 975 billion deduplicated tokens, contains over 500 billion unique n-grams. We show gains of 0.5–1.4 BLEU by using large language models to translate into various languages.},
	author = {Buck, Christian and Heafield, Kenneth and Van Ooyen, Bas},
	keywords = {Applications, Natural Language Processing, language models, multilingual, web corpora},
}

@misc{TylerWRinker2016,
	title = {qdap: {Quantitative} {Discourse} {Analysis} {Package}},
	url = {https://trinker.github.io/qdap/},
	author = {{Tyler W Rinker}},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Ng2000a,
	title = {{CS229} {Lecture} notes {Deep} {Learning}},
	abstract = {We now begin our study of deep learning. In this set of notes, we give an overview of neural networks, discuss vectorization and discuss training neural networks with backpropagation.},
	author = {Ng, Andrew},
	year = {2000},
	pages = {1--30},
}

@misc{Christensen2016,
	title = {{HC} {Corpora}},
	url = {http://www.corpora.heliohost.org/},
	author = {Christensen, Hans},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Lluisa2007,
	title = {The {Texture} of {Internet}: {Netlinguistics} in {Progress}},
	url = {http://www.cambridgescholars.com/download/sample/61182},
	abstract = {Internet and Information and Communication Technologies represent the largest network of human online communication ever. Language is the material that enables communication to flow in this ever-growing digital world of emails,},
	journal = {Portal.Acm.Org},
	author = {Lluïsa, Santiago Posteguillo、María José Esteve、M.},
	year = {2007},
	note = {ISBN: 1-84718-173-2; ISBN 13: 9781847181732},
	pages = {235},
}

@article{Pickhardt2014,
	title = {A {Generalized} {Language} {Model} as the {Combination} of {Skipped} n-grams and {Modified} {Kneser}-{Ney} {Smoothing}},
	abstract = {We introduce a novel approach for building language models based on a systematic, re-cursive exploration of skip n-gram models which are interpolated using modified Kneser-Ney smoothing. Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case. In this paper we motivate, for-malize and present our approach. In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduc-tion of perplexity between 3.1\% and 12.7\% in comparison to traditional language models using modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improve-ments. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small training data set of only 736 KB text we yield improvements of even 25.7\% reduction of perplexity.},
	author = {Pickhardt, Rene and Gottron, Thomas and Körner, Martin and Staab, Steffen and Wagner, Paul Georg and Speicher, Till and Gbr, Typology},
	year = {2014},
	note = {arXiv: 1404.3377v1},
	keywords = {Applications, Natural Language Processing, ()},
}

@article{Hiranandani2019,
	title = {Multiclass {Performance} {Metric} {Elicitation}},
	abstract = {Metric Elicitation is a principled framework for selecting the performance metric that best reflects implicit user preferences. However, available strategies have so far been limited to binary classification. In this paper, we propose novel strategies for eliciting multiclass classification performance metrics using only relative preference feedback. We also show that the strategies are robust to both finite sample and feedback noise.},
	number = {NeurIPS},
	journal = {NeurIPS},
	author = {Hiranandani, Gaurush and Boodaghians, Shant and Mehta, Ruta and Koyejo, Oluwasanmi},
	year = {2019},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T2TJJ5KJ\\9133-multiclass-performance-metric-elicitation.pdf:application/pdf},
}

@article{Strnadova2014,
	title = {Clustering at {Large} {Scales}},
	abstract = {A1;A2},
	author = {Strnadová, Veronika},
	year = {2014},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JJKXQVMS\\MAE_LargeScaleClustering_strnadova.pdf:application/pdf},
}

@article{Learning2019,
	title = {Lecture 3 : {Steepest} and {Gradient} {Descent}-{Part} {I} {Methods} for {Choosing} {Stepsizes}},
	author = {Learning, Reinforcement and Moradzadeh, Alireza},
	year = {2019},
	pages = {1--7},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4DM234HU\\Steepest and Gradient Descent-Part I.pdf:application/pdf},
}

@article{Ferri2009,
	title = {An experimental comparison of performance measures for classification},
	volume = {30},
	issn = {01678655},
	url = {http://dx.doi.org/10.1016/j.patrec.2008.08.010},
	doi = {10.1016/j.patrec.2008.08.010},
	abstract = {Performance metrics in classification are fundamental in assessing the quality of learning methods and learned models. However, many different measures have been defined in the literature with the aim of making better choices in general or for a specific application area. Choices made by one metric are claimed to be different from choices made by other metrics. In this work, we analyse experimentally the behaviour of 18 different performance metrics in several scenarios, identifying clusters and relationships between measures. We also perform a sensitivity analysis for all of them in terms of several traits: class threshold choice, separability/ranking quality, calibration performance and sensitivity to changes in prior class distribution. From the definitions and experiments, we make a comprehensive analysis of the relationships between metrics, and a taxonomy and arrangement of them according to the previous traits. This can be useful for choosing the most adequate measure (or set of measures) for a specific application. Additionally, the study also highlights some niches in which new measures might be defined and also shows that some supposedly innovative measures make the same choices (or almost) as existing ones. Finally, this work can also be used as a reference for comparing experimental results in pattern recognition and machine learning literature, when using different measures.},
	number = {1},
	journal = {Pattern Recognition Letters},
	author = {Ferri, C and Hernández-Orallo, J and Modroiu, R},
	year = {2009},
	note = {Publisher: Elsevier B.V.},
	pages = {27--38},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9V5IPPNG\\An experimental comparison of performance measures for classification.pdf:application/pdf},
}

@article{Heafield2011,
	title = {{KenLM} : {Faster} and {Smaller} {Language} {Model} {Queries}},
	abstract = {We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs. The PROBING data structure uses linear probing hash tables and is de signed for speed. Compared with the widely-used SRILM, our PROBING model is 2.4 times as fast while using 57\% of the memory. The TRIE data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption. TRIE simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline. Our code is open-source, thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations.},
	number = {2009},
	journal = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
	author = {Heafield, Kenneth},
	year = {2011},
	note = {ISBN: 978-1-937284-12-1},
	keywords = {Applications, Natural Language Processing},
	pages = {187--197},
}

@article{Jurish,
	title = {Word and {Sentence} {Tokenization} with {Hidden} {Markov} {Models}},
	abstract = {We present a novel method for the segmentation of text into tokens and sentences. Our approach makes use of a Hidden Markov Model for the detection of segment boundaries. Model parameters can be estimated from pre-segmented text which is widely available in the form of treebanks or aligned multi-lingual corpora. We formally define the boundary detection model and evaluate its performance on corpora from various languages as well as a small corpus of computer-mediated communication.},
	author = {Jurish, Bryan and Würzner, Kay-Michael},
	keywords = {Applications, Natural Language Processing},
}

@misc{StanfordUniversity,
	title = {Markov {Chains}},
	abstract = {A Markov Chain consists of a countable (possibly finite) set S (called the state space) together with a countable family of random variables X • , X 1 , X 2 , ···with values in S such that P [X l+1 = s {\textbar} X l = s l , X l−1 = s l−1 , ···, X • = s • ] = P [X l+1 = s {\textbar} X l = s l ]. We refer to this fundamental equation as the Markov property. The random variables X • , X 1 , X 2 , ···are dependent. Markov chains are among the few sequences of dependent random variables which are of a general character and have been successfully investigated with deep results about their behavior. Later we will discuss martingales which also provide examples of sequences of dependent random variables. Martingales have many applications to probability theory. One often thinks of the subscript l of the random variable X l as representing the time (discretely), and the random variables represent the evolution of a system whose behavior is only probabilistically known. Markov property expresses the assumption that the knowledge of the present (i.e., X l = s l) is relevant to predictions about the future of the system, however additional information about the past (X j = s j , j ≤ l − 1) is irrelevant. What we mean by the system is explained later in this subsection. These ideas will be clarified by many examples. Since the state space is countable (or even finite) it customary (but not always the case) to use the integers Z or a subset such as Z + (non-negative integers), the natural numbers N = \{1, 2, 3, ···\} or \{0, 1, 2, ···, m\} as the state space. The specific Markov chain under consideration often determines the natural notation for the state space. In the general case where no specific Markov chain is singled out, we often use N or Z + as the state space. We set P l,l+1 ij = P [X l+1 = j {\textbar} X l = i] For fixed l the (possibly infinite) matrix P l = (P l,l+1 ij},
	author = {{Stanford University}},
	keywords = {Applications, Natural Language Processing},
}

@article{NoelVanErp,
	title = {Deriving {Proper} {Uniform} {Priors} for {Regression} {Coefficients}, {Parts} {I}, {II}, and {III}},
	doi = {10.3390/e19060250},
	abstract = {It is a relatively well-known fact that in problems of Bayesian model selection, improper priors should, in general, be avoided. In this paper we will derive and discuss a collection of four proper uniform priors which lie on an ascending scale of informativeness. It will turn out that these priors lead us to evidences that are closely associated with the implied evidence of the Bayesian Information Criterion (BIC) and the Akaike Information Criterion (AIC). All the discussed evidences are then used in two small Monte Carlo studies, wherein for different sample sizes and noise levels the evidences are used to select between competing C-spline regression models. Also, there is given, for illustrative purposes, an outline on how to construct simple trivariate C-spline regression models. In regards to the length of this paper, only one half of this paper consists of theory and derivations, the other half consists of graphs and outputs of the two Monte Carlo studies.},
	author = {Noel Van Erp, H R and Linger, Ronald O and Van Gelder, Pieter H A J M},
	keywords = {Algorithms, Bayesian, Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), model selection, non-linear, proper uniform priors, regression analysis, regression coefficients, splines},
}

@article{Khmaladze1988,
	title = {The statistical analysis of a large number of rare events},
	number = {R 8804},
	journal = {Department of Mathematical Statistics},
	author = {Khmaladze, E V},
	month = jan,
	year = {1988},
	note = {Publisher: CWI},
	keywords = {Applications, Natural Language Processing},
	pages = {1--21},
}

@book{Simpson1989,
	edition = {2},
	title = {The {Oxford} {English} {Dictionary}.},
	isbn = {0-19-861186-2},
	abstract = {2nd ed. / prepared by J.A. Simpson and E.S.C. Weiner. Provides definitions of approximately 290,500 English words, arranged alphabetically in twenty volumes, with cross-references, etymologies, and pronunciation keys, and includes a bibliography. v. 1. A-Bazouki -- v. 2. B.B.C.-Chalypsography -- v. 3. Cham-Creeky -- v. 4. Creel-Duzepere -- v. 5. Dvandva-Follis -- v. 6. Follow-Haswed -- v. 7. Hat-Intervacuum -- v. 8. Interval-Looie -- v. 9. Look-Mouke -- v. 10. Moul-Ovum -- v. 11. Ow-Poisant -- v. 12. Poise-Quelt -- v. 13. Quemadero-Roaver -- v. 14. Rob-Sequyle -- v. 15. Ser-Soosy -- v. 16. Soot-Styx -- v. 17. Su-Thrivingly -- v. 18. Thro-Unelucidated -- v. 19. Unemancipated-Wau-wau -- v. 20. Wave-Zyxt. Bibliography.},
	publisher = {Clarendon Press},
	author = {Simpson, J A and Weiner, E S C and {Oxford University Press.}},
	year = {1989},
	keywords = {Applications, Natural Language Processing},
}

@article{Lunt2012,
	title = {Sample {Size} and {Power} {Calculations} {Power} {Calculations}},
	author = {Lunt, Mark},
	year = {2012},
	pages = {437--456},
}

@article{rozenblit,
	title = {The {Misunderstood} {Limits} of {Folk} {Science}: {An} {Illusion} of {Explanatory} {Depth}},
	volume = {26},
	doi = {10.1207/s15516709cog2605_1},
	journal = {Cognitive science},
	author = {Rozenblit, Leon and Keil, Frank},
	year = {2002},
	pages = {521--562},
}

@misc{KaggleInc2016,
	title = {House {Prices}: {Advanced} {Regression} {Techniques} - {Prizes}},
	url = {https://www.kaggle.com/c/house-prices-advanced-regression-techniques},
	urldate = {2018-10-29},
	author = {{Kaggle Inc}},
	year = {2016},
	note = {Publication Title: kaggle.com},
}

@article{ReferenceBayesianTest,
	title = {A {Reference} {Bayesian} {Test} for {Nested} {Hypotheses} and {Its} {Relationship} to the {Schwarz} {Criterion}},
	keywords = {Algorithms, Bayesian},
}

@article{Rengaswamy2020,
	title = {Robust f0 extraction from monophonic signals using adaptive sub-band filtering},
	volume = {116},
	issn = {01676393},
	url = {https://doi.org/10.1016/j.specom.2019.11.006},
	doi = {10.1016/j.specom.2019.11.006},
	abstract = {Fundamental frequency (f0) extraction plays an important role in processing of monophonic signals such as speech and song. It is essential in various real-time applications such as emotion recognition, speech/singing voice discrimination and so on. Several f0 extraction methods have been proposed over the years, but no one algorithm works well for both speech and song. In this paper, we propose a novel approach that can accurately estimate f0 from speech as well as songs. First, voiced/unvoiced detection is performed using a novel RNN-LSTM based approach. Then, each voiced frame is decomposed into several sub-bands. From each sub-band of a voiced frame, the candidate pitch periods are identified using autocorrelation and non-linear operations. Finally, Viterbi decoding is used to form the final pitch contours. The performance of the proposed method is evaluated using popular speech (Keele, CMU-ARCTIC), and song (MIR-1K, LYRICS) databases. The evaluation results show that the proposed method performs equally well for speech and monophonic songs, and is better than the state-of-the-art methods. Further, the efficacy of proposed f0 extraction method is demonstrated by developing an interactive SARGAM learning tool.},
	number = {November 2019},
	journal = {Speech Communication},
	author = {Rengaswamy, Pradeep and Kiran Reddy, M and Sreenivasa Rao, Krothapalli and Dasgupta, Pallab},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Autocorrelation, Fundamental frequency, LSTM, Non-linear filtering, SARGAM, Song, Speech},
	pages = {77--85},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VAKIINN3\\Robust-f0-extraction-from-monophonic-signals-using-adapt_2020_Speech-Communi.pdf:application/pdf},
}

@book{matlab,
	address = {Natick, Massachusetts},
	title = {9.7.0.1190202 ({R2019b})},
	publisher = {The MathWorks Inc.},
	author = {{MATLAB}},
	year = {2018},
	file = {9.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\9.pdf:application/pdf},
}

@article{Yoseph2020,
	title = {The impact of big data market segmentation using data mining and clustering techniques},
	volume = {38},
	issn = {18758967},
	doi = {10.3233/JIFS-179698},
	abstract = {Targeted marketing strategy is a prominent topic that has received substantial attention from both industries and academia. Market segmentation is a widely used approach in investigating the heterogeneity of customer buying behavior and profitability. It is important to note that conventional market segmentation models in the retail industry are predominantly descriptive methods, lack sufficient market insights, and often fail to identify sufficiently small segments. This study also takes advantage of the dynamics involved in the Hadoop distributed file system for its ability to process vast dataset. Three different market segmentation experiments using modified best fit regression, i.e., Expectation-Maximization (EM) and K-Means++ clustering algorithms were conducted and subsequently assessed using cluster quality assessment. The results of this research are twofold: i) The insight on customer purchase behavior revealed for each Customer Lifetime Value (CLTV) segment; ii) performance of the clustering algorithm for producing accurate market segments. The analysis indicated that the average lifetime of the customer was only two years, and the churn rate was 52\%. Consequently, a marketing strategy was devised based on these results and implemented on the departmental store sales. It was revealed in the marketing record that the sales growth rate up increased from 5\% to 9\%.},
	number = {5},
	journal = {Journal of Intelligent and Fuzzy Systems},
	author = {Yoseph, Fahed and Ahamed Hassain Malim, Nurul Hashimah and Heikkilä, Markku and Brezulianu, Adrian and Geman, Oana and Paskhal Rostam, Nur Aqilah},
	year = {2020},
	keywords = {customer lifetime value (CLTV), data mining, Market segmentation, RFM model (recency frequency monetary)},
	pages = {6159--6173},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3Q6M87ZV\\Impact of Big Data Market Segmentation.pdf:application/pdf},
}

@article{Fazly2002,
	title = {The use of syntax in word completion utilities},
	author = {Fazly, a},
	year = {2002},
	keywords = {Applications, Natural Language Processing},
}

@book{LinearModelsStatistics2001,
	title = {Linear models in statistics},
	volume = {38},
	isbn = {978-0-471-75498-5},
	year = {2001},
	doi = {10.5860/choice.38-3948},
	note = {Publication Title: Choice Reviews Online
Issue: 07
ISSN: 0009-4978},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AZ4HJ5MF\\LinearModelsInStatistics.pdf:application/pdf},
}

@misc{Flixter,
	title = {Rotten {Tomatoes}: {Movies} {\textbar} {TV} {Shows} {\textbar} {Movie} {Trailers} {\textbar} {Reviews}},
	url = {http://www.rottentomatoes.com/},
	urldate = {2017-11-24},
	author = {{Flixter}},
	keywords = {Algorithms, Bayesian},
}

@article{Maruyama2011,
	title = {{FULLY} {BAYES} {FACTORS} {WITH} {A} {GENERALIZED} g-{PRIOR}},
	volume = {39},
	doi = {10.1214/11-AOS917},
	abstract = {For the normal linear model variable selection problem, we propose se-lection criteria based on a fully Bayes formulation with a generalization of Zellner's g-prior which allows for p {\textgreater}n. A special case of the prior formu-lation is seen to yield tractable closed forms for marginal densities and Bayes factors which reveal new model evaluation characteristics of potential inter-est.},
	number = {5},
	journal = {The Annals of Statistics},
	author = {Maruyama, Yuzo and George, Edward I},
	year = {2011},
	keywords = {Algorithms, Bayesian, 62C10, 62F07, 62F15, Bayes factor, model selection consistency, ridge regression, singular value decomposition, variable selection},
	pages = {2740--2765},
}

@article{Gogh2015,
	title = {Text analysis pipelines},
	volume = {9383},
	issn = {16113349},
	doi = {10.1007/978-3-319-25741-9_2},
	abstract = {The understanding of natural language is one of the primary abilities that provide the basis for human intelligence. Since the invention of computers, people have thought about how to operationalize this ability in software applications (Jurafsky and Martin 2009). The rise of the internet in the 1990s then made explicit the practical need for automatically processing natural language in order to access relevant information. Search engines, as a solution, have revolutionalized the way we can find such information ad-hoc in large amounts of text (Manning et al. 2008). Until today, however, search engines excel in finding relevant texts rather than in understanding what information is relevant in the texts. Chapter 1 has proposed text mining as a means to achieve progress towards the latter, thereby making information search more intelligent. At the heart of every text mining application lies the analysis of text, mostly realized in the form of text analysis pipelines. In this chapter, we present the basics required to follow the approaches of this book to improve such pipelines for enabling text mining ad-hoc on large amounts of text as well as the state of the art in this respect. Text mining combines techniques from information retrieval, natural language processing, and data mining. In Sect. 2.1, we first provide a focused overview of those techniques referred to in this book. Then, we define the text analysis processes and pipelines that we consider in our proposed approaches (Sect. 2.2). We evaluate the different approaches based on texts and pipelines from a number of case studies introduced in Sect. 2.3. Finally, Sect. 2.4 surveys and discusses related existing work in the broad context of ad-hoc large-scale text mining.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Gogh, Vincent Van},
	year = {2015},
	note = {ISBN: 9783319257402},
	pages = {19--53},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\92AHNEZ2\\Text Analysis Pipelines.pdf:application/pdf},
}

@article{Niesler1997,
	title = {Category-based statistical language models},
	abstract = {Synopsis Language models are computational techniques and structures that describe word sequences produced by human subjects, and the work presented here considers primarily their application to automatic speech-recognition systems. Due to the very complex nature of natural languages as well as the need for robust recognition, statistically-based language models, which assign probabilities to word sequences, have proved most successful. This thesis focuses on the use of linguistically defined word categories as a means of improving the performance of statistical language models. In particular, an approach that aims to capture both general grammatical patterns, as well as particular word dependencies, using different model components is proposed, developed and evaluated. To account for grammatical patterns, a model employing variable-length n-grams of part-of-speech word categories is developed. The often local syntactic patterns in English text are captured conveniently by the n-gram structure, and reduced sparseness of the data allows larger n to be employed. A technique that optimises the length of individual n-grams is proposed, and experimental tests show it to lead to improved results. The model allows words to belong to multiple categories in order to cater for different grammatical functions, and may be employed as a tagger to assign category classifications to new text. While the category-based model has the important advantage of generalisation to unseen word se-quences, it is by nature not able to capture relationships between particular words. An experimental comparison with word-based n-gram approaches reveals this ability to be important to language model quality, and consequently two methods allowing the inclusion of word relations are developed. The first method allows the incorporation of selected word n-grams within a backoff framework. The number of word n-grams added may be controlled, and the resulting tradeoff between size and accuracy is shown to surpass that of standard techniques based on n-gram cutoffs. The second technique ad-dresses longer-range word-pair relationships that arise due to factors such as the topic or the style of the text. Empirical evidence is presented demonstrating an approximately exponentially decaying behaviour when considering the probabilities of related words as a function of an appropriately defined separating distance. This definition, which is fundamental to the approach, is made in terms of the category assign-ments of the words. It minimises the effect syntax has on word co-occurrences while taking particular advantage of the grammatical word classifications implicit in the operation of the category model. Since only related words are treated, the model size may be constrained to reasonable levels. Methods by means of which related word pairs may be identified from a large corpus, as well as techniques allow-ing the estimation of the parameters of the functional dependence, are presented and shown to lead to performance improvements.},
	author = {Niesler, Thomas},
	year = {1997},
	keywords = {Applications, Natural Language Processing},
}

@article{Le2016,
	title = {Domain-driven design patterns: {A} metadata-based approach},
	doi = {10.1109/RIVF.2016.7800302},
	abstract = {Design pattern is the most common form of object oriented software reuse. In object oriented domain driven design, a number of high-level patterns have been identified and applied for over a decade. However, no concrete design patterns for domain modeling in this method have been published in the literature. A primary challenge in defining these design patterns is how to express their form in a way that eases their application to a specific domain that uses a specific object oriented programming platform. In this work, we propose a set of concrete design patterns, whose form is expressed in a domain class modeling language (DCML). DCML is based on UML and uses implementation-aware meta-attributes to define design metadata. We extend this language with new meta-attributes to support the proposed design patterns. Further, we discuss how domain-specific examples of these patterns are translated to physical class models for automatically generating software prototypes.},
	number = {December 2018},
	journal = {2016 IEEE RIVF International Conference on Computing and Communication Technologies: Research, Innovation, and Vision for the Future, RIVF 2016 - Proceedings},
	author = {Le, Duc Minh and Dang, Duc Hanh and Nguyen, Viet Ha},
	year = {2016},
	note = {ISBN: 9781509041336},
	pages = {247--252},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SUBSFLGI\\domain-driven-design-patterns-no-copyright.pdf:application/pdf},
}

@misc{wiki:hebbian,
	title = {Generalized {Hebbian} {Algorithm} --- {Wikipedia},{The} {Free} {Encyclopedia}},
	url = {https://en.wikipedia.org/w/index.php?title=Generalized_Hebbian_Algorithm&oldid=814765473},
	author = {contributors, Wikipedia},
	year = {2017},
}

@misc{explain,
	title = {If you can't explain something in simple terms, you don't understand it},
	url = {https://kottke.org/17/06/if-you-cant-explain-something-in-simple-terms-you-dont-understand-it},
	urldate = {2020-05-08},
	author = {Kottke, Jason},
}

@misc{Upwork2017,
	title = {The {Hottest} {Freelance} {Skills} on {Upwork}: {Q3} 2017 - {Upwork} {Blog}},
	url = {https://www.upwork.com/blog/2017/11/freelance-skills-upwork-q3-2017/},
	urldate = {2018-08-19},
	author = {{Upwork}},
	year = {2017},
	keywords = {Applications, Natural Language Processing},
}

@article{Zhou2018,
	title = {On the {Convergence} of {Adaptive} {Gradient} {Methods} for {Nonconvex} {Optimization}},
	url = {http://arxiv.org/abs/1808.05671},
	abstract = {Adaptive gradient methods are workhorses in deep learning. However, the convergence guarantees of adaptive gradient methods for nonconvex optimization have not been sufficiently studied. In this paper, we provide a sharp analysis of a recently proposed adaptive gradient method namely partially adaptive momentum estimation method (Padam) (Chen and Gu, 2018), which admits many existing adaptive gradient methods such as RMSProp and AMSGrad as special cases. Our analysis shows that, for smooth nonconvex functions, Padam converges to a first-order stationary point at the rate of \$O{\textbackslash}backslashbig(({\textbackslash}backslashsum\{{\textbackslash}\_\}\{i=1\}{\textasciicircum}d{\textbackslash}{\textbar}{\textbackslash}backslashmathbf\{{\textbackslash}\{\}g\{{\textbackslash}\}\}\_\{1:T,i\}{\textbackslash}{\textbar}\_2){\textasciicircum}\{1/2\}/T{\textasciicircum}\{3/4\} + d/T{\textbackslash}backslashbig)\$, where \$T\$ is the number of iterations, \$d\$ is the dimension, \${\textbackslash}backslashmathbf\{{\textbackslash}\{\}g\{{\textbackslash}\}\}\_1,{\textbackslash}backslashldots,{\textbackslash}backslashmathbf\{{\textbackslash}\{\}g\{{\textbackslash}\}\}\_T\$ are the stochastic gradients, and \${\textbackslash}backslashmathbf\{{\textbackslash}\{\}g\{{\textbackslash}\}\}\_\{1:T,i\} = [g\_\{1,i\},g\_\{2,i\},{\textbackslash}backslashldots,g\_\{T,i\}]{\textasciicircum}{\textbackslash}backslashtop\{{\textbackslash}\$\}. Our theoretical result also suggests that in order to achieve faster convergence rate, it is necessary to use Padam instead of AMSGrad. This is well-aligned with the empirical results of deep learning reported in Chen and Gu (2018).},
	author = {Zhou, Dongruo and Tang, Yiqi and Yang, Ziyan and Cao, Yuan and Gu, Quanquan},
	year = {2018},
	note = {arXiv: 1808.05671},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\K92IJRRT\\On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization.pdf:application/pdf},
}

@misc{RPubsNaturallanguageprocessing,
	title = {{RPubs} - natural-language-processing},
	url = {https://rpubs.com/lmullen/nlp-chapter},
	keywords = {Applications, Natural Language Processing},
}

@article{Johnson2017,
	title = {The {Marriage} and {Family} {Therapy} {Practice} {Research} {Network} ({MFT}-{PRN}): {Creating} a {More} {Perfect} {Union} {Between} {Practice} and {Research}},
	volume = {43},
	issn = {0194472X},
	url = {http://doi.wiley.com/10.1111/jmft.12238},
	doi = {10.1111/jmft.12238},
	number = {4},
	journal = {Journal of Marital and Family Therapy},
	author = {Johnson, Lee N and Miller, Richard B and Bradford, Angela B and Anderson, Shayne R},
	month = oct,
	year = {2017},
	pmid = {28426921},
	pages = {561--572},
}

@article{Health2010,
	title = {Major {Depression} {Among} {Adults}},
	url = {https://www.nimh.nih.gov/health/statistics/prevalence/major-depression-among-adults.shtml},
	abstract = {Major Depression Among Adults},
	journal = {Health and Education},
	author = {Health, Mental and States, United},
	year = {2010},
	pages = {2010--2012},
}

@article{Martin2016,
	title = {Rules of {Machine} {Learning} : {Best} {Practices} for {ML} {Engineering}},
	url = {papers3://publication/uuid/4CC57281-97A2-4896-BC0D-94B065C5B70B},
	abstract = {This document is intended to help those with a basic knowledge of machine learning get the benefit of best practices in machine learning from around Google. It presents a style for machine learning, similar to the Google C++ Style Guide and other popular guides to practical programming. If you have taken a class in machine learning, or built or worked on a machine?learned model, then you have the necessary background to read this document. Terminology},
	journal = {Google},
	author = {Martin, Zinkevich},
	year = {2016},
	pages = {1--24},
}

@inproceedings{Shindjalova2014,
	title = {Modeling data for tilted implants in grafted with bio-oss maxillary sinuses using logistic regression},
	volume = {1631},
	isbn = {978-0-7354-1270-5},
	doi = {10.1063/1.4902458},
	abstract = {The aim of this study is to define the prognostic factors for implant survival of immediately loaded tilted implants in the edentulous maxillae placed into Bio-Oss grafted sinuses. A total of 44 tilted Bredent implants in 24 grafted sinuses were inserted 9 to 10 months after the augmentation procedures. Loading was applied within 12 hours of implant surgery. Patients were scheduled for follow-up at 12 months. Marginal bone loss, implant plaque level, pocket probing depth and bleeding scores, CBCT measurements of mineral density (coDiagnostiX™) were assessed and recorded. Four implants failed (survival rate of 90,91\%). The average marginal loss is 0,43 mm (SD 0,42). The mean bone density of the grafts is 964 (SD 120,36) HU, ranging between min 477 and max 1068 HU. All implant failures occurred in lower mineral density grafts. Statistical analyses strongly support correlation between bone density at the grafted sites and implant survival. ©2014 AIP Publishing LLC.},
	booktitle = {{AIP} {Conference} {Proceedings}},
	author = {Shindjalova, R and Prodanova, K and Svechtarov, V},
	year = {2014},
	note = {arXiv: 1412.6980v9
ISSN: 15517616},
	keywords = {logistic regression, bone density, grafted maxillary sinuses, prognostic factors, Tilted implants},
	pages = {58--62},
}

@article{Box1964,
	title = {An {Analysis} of {Transformations}},
	volume = {26},
	url = {http://links.jstor.org/sici?sici=0035-9246%281964%2926%3A2%3C211%3AAAOT%3E2.0.CO%3B2-6},
	number = {2},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Box, G E P and Cox, D R},
	year = {1964},
	keywords = {Algorithms, Bayesian},
	pages = {211--252},
}

@article{Girshick2014,
	title = {1043.0690},
	volume = {1},
	issn = {10636919},
	url = {http://arxiv.},
	doi = {10.1109/CVPR.2014.81},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra and Berkeley, U C and Malik, Jitendra},
	year = {2014},
	pmid = {26656583},
	note = {arXiv: 1311.2524
ISBN: 978-1-4799-5118-5},
	pages = {5000},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5CCKKB5M\\Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8WNX2FVL\\Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf:application/pdf},
}

@article{Tan2012,
	title = {A {Scalable} {Distributed} {Syntactic}, {Semantic}, and {Lexical} {Language} {Model}},
	volume = {38},
	issn = {0891-2017},
	doi = {10.1162/COLI_a_00107},
	abstract = {This paper presents an attempt at building a large scale distributed composite language model that is formed by seamlessly integrating an n-gram model, a structured language model, and probabilistic latent semantic analysis under a directed Markov random field paradigm to simultaneously account for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the Bleu score and "readability" of translations when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system.},
	number = {3},
	journal = {Computational Linguistics},
	author = {Tan, Ming and Zhou, Wenli and Zheng, Lei and Wang, Shaojun},
	year = {2012},
	note = {ISBN: 1530-9312},
	keywords = {Applications, Natural Language Processing},
	pages = {631--671},
}

@article{Arnold2016,
	title = {Introduction {To} ggthemes},
	issn = {00335770},
	url = {https://cran.r-project.org/web/packages/ggthemes/vignettes/ggthemes.html},
	doi = {10.1007/978-1-61779-968-6},
	author = {Arnold, Jeffrey B},
	year = {2016},
	pmid = {18963060},
	note = {arXiv: 0712.0689
ISBN: 9780470057247},
	keywords = {Applications, Natural Language Processing},
}

@article{Baber2012,
	title = {Relational maintenance: {An} examination of how gender, relational maintenance strategies, and commitment affect the use of text messages in romantic relationships},
	abstract = {This communication study explores the relation between commitment, relational maintenance strategies, and text message use in romantic partners. This study examines how these three factors are connected through the use of surveys. It was hypothesized that romantic partners who were more committed to one another would use text messages to communicate about certain relational maintenance strategies. Results showed romantic partners who used more relational maintenance strategies did in fact use text messages to communicate about these issues more often. Also, couples who were more committed to their partner did use more positivity when communicating through text messages with their partner. It was also found that males use the relational maintenance strategy of openness more often than females when communicating through text messages.},
	author = {Baber, Vashaun M and Shen, Sam and Tew, Michael and Stacey, Kathleen and Ypsilanti, Michigan},
	year = {2012},
	keywords = {Applications, Natural Language Processing},
}

@article{Pollock2018,
	title = {Speech and language processing},
	doi = {10.4324/9780203461891-3},
	abstract = {An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology – at all levels and with all modern technologies – this book takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. Builds each chapter around one or more worked examples demonstrating the main idea of the chapter, usingthe examples to illustrate the relative strengths and weaknesses of various approaches. Adds coverage of statistical sequence labeling, information extraction, question answering and summarization, advanced topics in speech recognition, speech synthesis. Revises coverage of language modeling, formal grammars, statistical parsing, machine translation, and dialog processing. A useful reference for professionals in any of the areas of speech and language processing.},
	journal = {Day-to-Day Dyslexia in the Classroom},
	author = {Pollock, Joy and Waller, Elisabeth and Politt, Rody},
	year = {2018},
	pages = {16--28},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S5U9ZE38\\Speech and Language Processing - jurafsky_martin.pdf:application/pdf},
}

@article{SciPy,
	title = {{SciPy} 1.0: {Fundamental} {Algorithms} for {Scientific} {Computing} in {Python}},
	volume = {17},
	doi = {https://doi.org/10.1038/s41592-019-0686-2},
	journal = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J and Brett, Matthew and Wilson, Joshua and Jarrod Millman, K and Mayorov, Nikolay and Nelson, Andrew R.{\textasciitilde}J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, \${\textbackslash}\$.Ilhan and Feng, Yu and Moore, Eric W and Vand erPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E.{\textasciitilde}A. and Harris, Charles R and Archibald, Anne M and Ribeiro, Antônio H and Pedregosa, Fabian and van Mulbregt, Paul and Contributors, SciPy 1. 0},
	year = {2020},
	pages = {261--272},
}

@article{Kim2001,
	title = {Smoothing {Issues} in the {Structured} {Language} {Model}},
	volume = {1},
	abstract = {The Structured Language Model (SLM) recently introduced by Chelba and Jelinek is a powerful general formalism for exploit-ing syntactic dependencies in a left-to-right language model for applications such as speech and handwriting recognition, spelling correction, machine translation, etc. Unlike traditional N-gram models, optimal smoothing techniques – discounting methods and hierarchical structures for back-off – are still be-ing developed for the SLM. In the SLM, the statistical depen-dencies of a word on immediately preceding words, preced-ing syntactic heads, non-terminal labels, etc., are parameterized as overlapping N-gram dependencies. Statistical dependencies in the parser and tagger used by the SLM also have N-gram like structure. Deleted interpolation has been used to combine these N-gram like models. We demonstrate on two different corpora – WSJ and Switchboard – that more recent modified back-off strategies and nonlinear interpolation methods consid-erably lower the perplexity of the SLM. Improvement in word error rate is also demonstrated on the Switchboard corpus.},
	author = {Kim, Woosung and Khudanpur, Sanjeev and Wu, Jun},
	year = {2001},
	keywords = {Applications, Natural Language Processing},
	pages = {717--720},
}

@article{Wild2015,
	title = {Package ‘ lsa '},
	abstract = {The basic idea of latent semantic analysis (LSA) is, that text do have a higher order (=latent semantic) structure which, however, is obscured by word usage (e.g. through the use of synonyms or polysemy). By using conceptual indices that are derived statistically via a truncated singular value decomposition (a two-mode factor analysis) over a given document-term matrix, this variability problem can be overcome.},
	author = {Wild, Fridolin},
	year = {2015},
	keywords = {Applications, Natural Language Processing},
	pages = {1--11},
}

@article{Ravaut2018,
	title = {Gradient descent revisited via an adaptive online learning rate},
	url = {http://arxiv.org/abs/1801.09136},
	abstract = {Any gradient descent optimization requires to choose a learning rate. With deeper and deeper models, tuning that learning rate can easily become tedious and does not necessarily lead to an ideal convergence. We propose a variation of the gradient descent algorithm in the which the learning rate is not fixed. Instead, we learn the learning rate itself, either by another gradient descent (first-order method), or by Newton's method (second-order). This way, gradient descent for any machine learning algorithm can be optimized.},
	author = {Ravaut, Mathieu and Gorti, Satya},
	year = {2018},
	note = {arXiv: 1801.09136},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RULZU9SM\\Gradient Descent Revisited via an Adaptive Online Learning Rate.pdf:application/pdf},
}

@article{Dataiku2017,
	title = {Machine learning {Basics} - {An} {Illustrated} {Guide} for {Non}-{Technical} {Readers}},
	url = {https://pages.dataiku.com/machine-learning-basics-thank-you?submissionGuid=80d21f82-ac46-45d0-969a-cd9914d06af9},
	abstract = {Overview of ML Algorithms},
	journal = {Daiku},
	author = {{Dataiku}},
	year = {2017},
	pages = {15},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\REE6845W\\GUIDEBOOK MACHINE LEARNING BASICS.pdf:application/pdf},
}

@article{LanguageModelingSearch,
	title = {Language {Modeling} and {Search}},
	keywords = {Applications, Natural Language Processing},
	pages = {323},
}

@article{Bo-JuneHsu2009,
	title = {Language {Modeling} for {Limited}-{Data} {Domains}},
	author = {{by Bo-June Hsu}},
	year = {2009},
	keywords = {Applications, Natural Language Processing},
}

@article{Thurlow2003,
	title = {Generation {Txt} ? {The} sociolinguistics of young people ' s text-messaging},
	issn = {14777843},
	url = {http://extra.shu.ac.uk/daol/articles/v1/n1/a3/thurlow2002003-paper.html},
	abstract = {Abstract: The so called 'net generation' is popularly assumed to be naturally media literate and to be necessarily reinventing conventional linguistic and communicative practices. With this in mind, this essay centres around discursive analyses of qualitative data arising from an investigation of 159 older teenagers' use of mobile telephone text-messaging - or SMS (i.e. short-messaging services). In particular, against a backdrop of media commentaries, we examine the linguistic forms and communicative functions in a corpus of 544 participants' actual text-messages. While young people are surely using their mobile phones as a novel, creative means of enhancing and supporting intimate relationships and existing social networks, popular discourses about the linguistic exclusivity and impenetrability of this particular technologically-mediated discourse appear greatly exaggerated. Serving the sociolinguistic 'maxims' of (a) brevity and speed, (b) paralinguistic restitution and (c) phonological approximation, young people's messages are both linguistically unremarkable and communicatively adept.},
	journal = {Discourse Analysis Online},
	author = {Thurlow, Crispin},
	year = {2003},
	note = {ISBN: 1477-7843},
	keywords = {Applications, Natural Language Processing, 1, adolescents, all tables, communication technologies, computer-mediated discourse, figures and images are, introduction and background, multimedia, new, presented in pdf format, sms, sociolinguistics, text-messaging},
	pages = {1--31},
}

@article{Caffo2015a,
	title = {Advanced linear models for data science},
	author = {Caffo, Brian},
	year = {2015},
	pages = {69},
}

@article{AdaptiveApproachProduct2015,
	title = {An {Adaptive} {Approach} to {Product} {Development}: {Exploratory} {PD} ®({ExPD})},
	url = {www.exploratorypd.com},
	year = {2015},
}

@article{Ramisch2008,
	title = {N-gram models for language detection},
	abstract = {In this document we report a set of experiments using n-gram lan-guage models for automatic language detection of text. We will start with a brief explanation of the concepts and of the mathematics behind n-gram language models and discuss some applications and domains in which they are widely used. We will also present an overview of related work in language detection. Then, we will describe the resources used in the experiments, namely a subset of the Europarl corpus and the SRILM toolkit. We will then perform a toy experiment in order to explain in detail our methodology. Afterwards, we will evaluate the performance of different language models and parameters through a precision measure based on the perplexity of a text according to a model. We conclude that n-gram models are indeed a simple and efficient tool for automatic language detection.},
	author = {Ramisch, Carlos},
	year = {2008},
	keywords = {Applications, Natural Language Processing},
}

@techreport{BaptisteAuguie2016,
	title = {R package {gridExtra}},
	author = {Baptiste Auguie, Anton Antonov},
	year = {2016},
	pages = {1--10},
}

@article{Hosmer2000,
	title = {Applied {Logistic} {Regression}.pdf},
	issn = {1476-4687},
	url = {http://as.wiley.com/WileyCDA/WileyTitle/productCd-0470582472.html},
	doi = {10.1038/461726a},
	abstract = {Science is finding evidence of genetic diversity among groups of people as well as among individuals. This discovery should be embraced, not feared, say Bruce T. Lahn and Lanny Ebenstein. \${\textbackslash}\$n\${\textbackslash}\$nSummary \${\textbackslash}\$n * Promoting biological sameness in humans is illogical, even dangerous\${\textbackslash}\$n * To ignore the possibility of group diversity is to do poor science and poor medicine\${\textbackslash}\$n * A robust moral position is one that embraces this diversity as among humanity's great assets},
	author = {Hosmer, David W and Lemeshow, Stanley},
	year = {2000},
	pmid = {19812654},
	note = {ISBN: 0-471-35632-8},
	pages = {161--164},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6C4BMSZ8\\Applied Logistic Regression.pdf:application/pdf},
}

@article{Pastor,
	title = {Size matters: {A} quantitative approach to corpus representativeness},
	author = {Pastor, Gloria Corpas and Seghiri, Miriam},
	keywords = {Applications, Natural Language Processing},
}

@misc{blogstats2020,
	title = {Revealing {Blogging} {Statistics} {\textbar} {The} {State} {Of} {The} {Industry} {In} 2020},
	url = {https://techjury.net/stats-about/blogging/#gref},
	urldate = {2020-05-06},
	author = {G., Nick},
}

@article{Allan2006,
	title = {Writing up research: a statistical perspective},
	abstract = {The main audience for this guide is natural resources research scientists. The aim is to provide them with help on how to report results of investigations in a way that clearly demonstrates the evidence base for research recommendations. The guide has been written primarily with the inexperienced user of statistics in mind,},
	number = {January},
	author = {Allan, E F and Abeyasekera, S and Stern, R D},
	year = {2006},
}

@article{Magnuson2002,
	title = {Dept. for {Speech}, {Music} and {Hearing} {Quarterly} {Progress} and {Status} {Report} {Measuring} the effectiveness of word prediction: {The} advantage of long-term use},
	url = {http://www.speech.kth.se/qpsr},
	author = {Magnuson, T and Hunnicutt, S},
	year = {2002},
	keywords = {Applications, Natural Language Processing},
	pages = {57--67},
}

@incollection{Jurafsky2006,
	title = {Word {Classes} and {Part}-of-{Speech} {Tagging}},
	author = {Jurafsky, Daniel and Martin, James},
	year = {2006},
	keywords = {Applications, Natural Language Processing},
}

@article{Zhang2020,
	title = {Emotion recognition using multi-modal data and machine learning techniques: {A} tutorial and review},
	volume = {59},
	issn = {15662535},
	url = {https://doi.org/10.1016/j.inffus.2020.01.011},
	doi = {10.1016/j.inffus.2020.01.011},
	abstract = {In recent years, the rapid advances in machine learning (ML) and information fusion has made it possible to endow machines/computers with the ability of emotion understanding, recognition, and analysis. Emotion recognition has attracted increasingly intense interest from researchers from diverse fields. Human emotions can be recognized from facial expressions, speech, behavior (gesture/posture) or physiological signals. However, the first three methods can be ineffective since humans may involuntarily or deliberately conceal their real emotions (so-called social masking). The use of physiological signals can lead to more objective and reliable emotion recognition. Compared with peripheral neurophysiological signals, electroencephalogram (EEG) signals respond to fluctuations of affective states more sensitively and in real time and thus can provide useful features of emotional states. Therefore, various EEG-based emotion recognition techniques have been developed recently. In this paper, the emotion recognition methods based on multi-channel EEG signals as well as multi-modal physiological signals are reviewed. According to the standard pipeline for emotion recognition, we review different feature extraction (e.g., wavelet transform and nonlinear dynamics), feature reduction, and ML classifier design methods (e.g., k-nearest neighbor (KNN), naive Bayesian (NB), support vector machine (SVM) and random forest (RF)). Furthermore, the EEG rhythms that are highly correlated with emotions are analyzed and the correlation between different brain areas and emotions is discussed. Finally, we compare different ML and deep learning algorithms for emotion recognition and suggest several open problems and future research directions in this exciting and fast-growing area of AI.},
	number = {March 2019},
	journal = {Information Fusion},
	author = {Zhang, Jianhua and Yin, Zhong and Chen, Peng and Nichele, Stefano},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Emotion recognition, Deep learning, Affective computing, Data fusion, Feature dimensionality reduction, Machine learning, Physiological signals},
	pages = {103--126},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JKL8Y9JJ\\Emotion-recognition-using-multi-modal-data-and-machine-learn_2020_Informatio.pdf:application/pdf},
}

@misc{TextBasedEmoticons,
	title = {Text-{Based} {Emoticons}},
	url = {http://pc.net/emoticons/},
}

@article{Samuel,
	title = {4.3.3 {Some} {Studies} in {Machine} {Learning} {Using} the {Game} of {Checkers} {Some} {Studies} in {Machine} {Learning} {Using} the {Game} of {Checkers}},
	abstract = {Two machine-learning procedures have been investigated in some detail usi!Jg the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to playa better game of checkers than can be played by the person who wrote the program. Further-more, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these 'experiments are, of course, applicable to many other situations.},
	author = {Samuel, Arthur L},
}

@article{Krekeler1979,
	title = {Die {Einheilung} des {Spongiosatransplantates} in der parodontalen {Knochentasche} ({Tierexperimentelle} {Studie}).},
	volume = {34},
	issn = {00121029},
	abstract = {Healing of cancellous bone grafts taken from the iliac crest which were implanted in alveolar bone pockets were followed with polychromatic sequence marking in animal experiments using beagle dogs. Healing of the cancellous bone graft is unproblematic only in the case of one- and two-walled defects. A periodontal fissure of normal width, but perforated by ankyloses, formed within seven weeks. The fissures are considerably more pronounced when the dentin is denuded than when the cement layer has been preserved.},
	number = {4},
	journal = {Deutsche zahnarztliche Zeitschrift},
	author = {Krekeler, G and Düker, J and Fabinger, A},
	year = {1979},
	pages = {313--316},
}

@techreport{Smith,
	title = {Cyclical {Learning} {Rates} for {Training} {Neural} {Networks}},
	url = {www.cs.toronto.edu/},
	abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of mono-tonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds"-linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
	author = {Smith, Leslie N},
	note = {arXiv: 1506.01186v6},
}

@article{Mustaqeem2020a,
	title = {Clustering-{Based} {Speech} {Emotion} {Recognition} by {Incorporating} {Learned} {Features} and {Deep} {BiLSTM}},
	volume = {8},
	issn = {21693536},
	doi = {10.1109/ACCESS.2020.2990405},
	abstract = {Emotional state recognition of a speaker is a difficult task for machine learning algorithms which plays an important role in the field of speech emotion recognition (SER). SER plays a significant role in many real-time applications such as human behavior assessment, human-robot interaction, virtual reality, and emergency centers to analyze the emotional state of speakers. Previous research in this field is mostly focused on handcrafted features and traditional convolutional neural network (CNN) models used to extract high-level features from speech spectrograms to increase the recognition accuracy and overall model cost complexity. In contrast, we introduce a novel framework for SER using a key sequence segment selection based on redial based function network (RBFN) similarity measurement in clusters. The selected sequence is converted into a spectrogram by applying the STFT algorithm and passed into the CNN model to extract the discriminative and salient features from the speech spectrogram. Furthermore, we normalize the CNN features to ensure precise recognition performance and feed them to the deep bi-directional long short-term memory (BiLSTM) to learn the temporal information for recognizing the final state of emotion. In the proposed technique, we process the key segments instead of the whole utterance to reduce the computational complexity of the overall model and normalize the CNN features before their actual processing, so that it can easily recognize the Spatio-temporal information. The proposed system is evaluated over different standard dataset including IEMOCAP, EMO-DB, and RAVDESS to improve the recognition accuracy and reduce the processing time of the model, respectively. The robustness and effectiveness of the suggested SER model is proved from the experimentations when compared to state-of-the-art SER methods with an achieve up to 72.25\%, 85.57\%, and 77.02\% accuracy over IEMOCAP, EMO-DB, and RAVDESS dataset, respectively.},
	journal = {IEEE Access},
	author = {{Mustaqeem} and Sajjad, Muhammad and Kwon, Soonil},
	year = {2020},
	keywords = {deep bidirectional long shot term memory, key segment sequence selection, normalization of CNN features, radial-based function network (RBFN), Speech emotion recognition},
	pages = {79861--79875},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z3PTR3DH\\Clustering-Based Speech Emotion Recognition by Incorporating Learned Features and Deep BiLSTM.pdf:application/pdf},
}

@book{Geron,
	title = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn}},
	isbn = {978-1-4919-6229-9},
	author = {Géron, Aurélien},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JSY9HXD9\\Hands on Machine Learning with Scikit Learn and TensorFlow.pdf:application/pdf},
}

@article{Zarowski2004,
	title = {Unconstrained {Optimization}},
	doi = {10.1002/0471650412.ch8},
	journal = {An Introduction to Numerical Analysis for Electrical and Computer Engineers},
	author = {Zarowski, Christopher J},
	year = {2004},
	pages = {341--368},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8YVYTFDP\\Unconstrained Optimization (SFU).pdf:application/pdf},
}

@article{Lawrence2008,
	title = {Explorase: {Multivariate} exploratory analysis and visualization for systems biology},
	volume = {25},
	issn = {15487660},
	doi = {10.18637/jss.v025.i09},
	abstract = {The datasets being produced by high-throughput biological experiments, such as microarrays, have forced biologists to turn to sophisticated statistical analysis and visualization tools in order to understand their data. We address the particular need for an open-source exploratory data analysis tool that applies numerical methods in coordination with interactive graphics to the analysis of experimental data. The software package, known as explorase, provides a graphical user interface (GUI) on top of the R platform for statistical computing and the GGobi software for multivariate interactive graphics. The GUI is designed for use by biologists, many of whom are unfamiliar with the R language. It displays metadata about experimental design and biological entities in tables that are sortable and filterable. There are menu shortcuts to the analysis methods implemented in R, including graphical interfaces to linear modeling tools. The GUI is linked to data plots in GGobi through a brush tool that simultaneously colors rows in the entity information table and points in the GGobi plots. explorase is an R package publicly available from Bioconductor and is a tool in the MetNet platform for the analysis of systems biology data.},
	number = {9},
	journal = {Journal of Statistical Software},
	author = {Lawrence, Michael and Cook, Dianne and Lee, Eun Kyung and Babka, Heather and Wurtele, Eve Syrkin},
	year = {2008},
	keywords = {Visualization, Bioconductor, Bioinformatics, Exploratory data analysis, Graphical user interface, Interactive graphics, Metabolomics, Microarray, Proteomics},
	pages = {1--23},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5D2XQVAQ\\Multivariate Exploratory Analysis for Visualization for Systems Biology.pdf:application/pdf},
}

@article{BayesianVariableSelection2017,
	title = {Bayesian {Variable} {Selection} and {Model} {Averaging} using {Bayesian} {Adaptive} {Sampling}},
	abstract = {Depends R ({\textgreater}= 3.0), Imports stats, graphics, utils, grDevices Suggests MASS, knitr, GGally, rmarkdown, roxygen2 Description Package for Bayesian Variable Selection and Model Averaging in linear models and generalized linear models using stochastic or deterministic sampling without replacement from posterior distributions. Prior distributions on coefficients are from Zellner's g-prior or mixtures of g-priors corresponding to the Zellner-Siow Cauchy Priors or the mixture of g-priors from Liang et al (2008) {\textless}DOI:10.1198/016214507000001337{\textgreater}for linear models or mixtures of g-priors in GLMs of Li and Clyde (2015) {\textless}arXiv:1503.06913{\textgreater}. Other model selection criteria include AIC, BIC and Empirical Bayes estimates of g. Sampling probabilities may be updated based on the sampled models using Sampling w/out Replacement or an efficient MCMC algorithm samples models using the BAS tree structure as an efficient hash table. Uniform priors over all models or beta-binomial prior distributions on model size are allowed, and for large p truncated priors on the model space may be used. The user may force variables to always be included. Details behind the sampling algorithm are provided in Clyde, Ghosh and Littman (2010) {\textless}DOI:10.1198/jcgs.},
	year = {2017},
}

@article{Krhenbhl2015DatadependentIO,
	title = {Data-dependent {Initializations} of {Convolutional} {Neural} {Networks}},
	volume = {abs/1511.0},
	journal = {CoRR},
	author = {Krähenbühl, Philipp and Doersch, Carl and Donahue, Jeff and Darrell, Trevor},
	year = {2015},
}

@article{Reid2004,
	title = {Insights into the {Social} and {Psychological} {Effects} of {SMS} {Text} {Messaging}},
	abstract = {Abstract: The increasingly widespread use of text -messaging has led to the questioning of the social and psychological effects of this novel communication medium. A selection of findings from an online questionnaire that was developed by the author to answer this pertinent question are presented. McKennas recent work on the way the Internet can help some people develop relationships is drawn upon and taken a step further by exploring the differences between those who prefer texting (Texters) and those who prefer talking on their mobiles (Talkers). A large sample of 982 respondents completed the questionnaire. Results showed there was a clear distinction between Texters and Talkers in the way they used their mobiles and their underlying motivations. The key finding to emerge in the preliminary analyses was that Texters seemed to form close knit text circles with their own social ecology, interconnecting with a close group of friends in perpetual text contact. Compared to Talkers, Texters were found to be more lonely and socially anxious, and more likely to disclose their real-self through text than via face-toface or voice call exchanges. Structural equation modeling produced a model showing that where respondents located their real-self and whether they were a Texter or a Talker mediated between the loneliness and social anxiety measures and the impact of these on relational outcomes, in line with McKennas theoretical framework. Thus it appears that there is something special about texting that allows some people to translate their loneliness and/or social anxiety into productive relationships whilst for others the mobile does not afford the same effect. .Applications and explorations for future research are discussed.},
	number = {February},
	journal = {The Social and Psychological Effects of Text},
	author = {Reid, Donna and Reid, Fraser},
	year = {2004},
	pages = {1--11},
}

@article{RafteryUniversityOF1998,
	title = {Bayes {Factors} and {BIC}: {Comment} o n {W} eakliem},
	abstract = {Weakliem agrees that Bayes factors are useful for model selection and hypothesis testing. He reminds us that the simple and convenient BIC approximation corresponds most closely to one particular prior on the parameter space, the unit information prior, a n d p o i n ts out that researchers may h a ve diierent prior information or opinions. Clearly a prior that represents the available information should be used, although the unit information prior often seems reasonable in the absence of strong prior information. It seems that, among the Bayes factors likely to be used in practice, BIC is conservative in the sense of tending to provide less evidence for additional parameters or \${\textbackslash}\$eeects". Thus if a Bayes factor based on additional prior information favors an eeect, but BIC does not, the prior information is playing a crucial role and this should be made clear when the research is reported. BIC may w ell have a role as a baseline reference analysis to be provided in routine reporting of research results, perhaps along with Bayes factors based on other priors. In Weakliem's 2 2 table examples, BIC and Bayes factors based on Weakliem's preferred priors lead to similar substantive conclusions, but both diier from those based on P values. When there is additional prior information, the technology now exists to express it as a prior probability distribution and to compute the corresponding Bayes factors. This can be done for a wide range of families of statistical models. Prior assessment is facilitated by deening a parsimonious family of prior distributions, and a reference set of priors can be deened for sensitivity analysis. The integrals needed to compute Bayes factors can often be evaluated almost exactly using the Laplace method. The GLIB software automates much o f this process for generalized linear models, which include linear regression, logistic regression and log-linear models. Weakliem considers a much-analyzed cross-national social mobility data set, and discovers two new models for it. He contends that the fact that previous researchers who used BIC failed to discover these models reeects badly on BIC. However, BIC strongly favors the model that he prefers, so this seems to be a non sequitur, especially as other researchers who did not use BIC did not discover these models either. With complex observational data it is important not to stop the model selection process just because BIC favors one model over another, but to continue searching for better models using formal and informal diagnostic checking and residual analysis methods as long as substantial amounts of deviance remain to be explained or the current best model seems overparameterized. I would argue that Bayes factors should remain the enal criterion for model comparison.},
	author = {Raftery University O F, Adrian E and Ashington, W},
	year = {1998},
	keywords = {Algorithms, Bayesian},
}

@article{Tokenization,
	title = {Tokenization},
	abstract = {The process of segmenting running text into words and sentences. Electronic text is a linear sequence of symbols (characters or words or phrases). Naturally, before any real text processing is to be done, text needs to be segmented into linguistic units such as words, punctuation, numbers, alpha-numerics, etc. This process is called tokenization. In English, words are often separated from each other by blanks (white space), but not all white space is equal. Both " Los Angeles " and " rock 'n' roll " are individual thoughts despite the fact that they contain multiple words and spaces. We may also need to separate single words like " I'm " into separate words " I " and " am " . Tokenization is a kind of pre-processing in a sense; an identification of basic units to be processed. It is conventional to concentrate on pure analysis or generation while taking basic units for granted. Yet without these basic units clearly segregated it is impossible to carry out any analysis or generation. The identification of units that do not need to be further decomposed for subsequent processing is an extremely important one. Errors made at this stage are very likely to induce more errors at later stages of text processing and are therefore very dangerous. What counts as a token in NLP? The notion of a token must first be defined before computational processing can proceed. There is more to the issue than simply identifying strings delimited on both sides by spaces or punctuation. Different notions depend on different objectives, and often different language backgrounds. A token is 1. Linguistically significant 2. Methodologically useful Webster and Kit suggest that finding significant tokens depends on the ability to recognize patterns displaying significant collocation. Rather than simply relying on wehther a string is bounded by delimters on either side, segmentation into significant tokens relies on a kind of pattern recognition. Consider this hypothetical speech transcription: where is meadows dr who asked Collocation patterns could help determine if this is about meadows dr (Drive) or dr (Doctor) who. Standard (White Space) Tokenization Word tokenization may seem simple in a language that separates words by a special 'space' character. However, not every language does this (e.g. Chinese, Japanese, Thai), and a closer examination will Page 1 -16 make it clear that white space alone is not sufficient even for English. Addressing Specific Challenges Tokenization is generally considered as easy relative to other tasks in natural language, and one of the more uninteresting tasks (for English and other segmented languages). However, errors made in this phase will propogate into later phases and cause problems. To address this problem, a number of advanced methods which deal with specific challenges in tokenization have been developed to complement standard tokenizers. Bob Carpenter states that tokenization is particularly vexing in the bio-medical text domain, where there are tons of words (or at least phrasal lexical entries) that contain parentheses, hyphens, and so on, and that this turned out to be a problem for WordNet)},
	keywords = {Applications, Natural Language Processing},
}

@misc{Columbus2017,
	title = {{LinkedIn}'s {Fastest}-{Growing} {Jobs} {Today} {Are} {In} {Data} {Science} {And} {Machine} {Learning}},
	url = {https://www.forbes.com/sites/louiscolumbus/2017/12/11/linkedins-fastest-growing-jobs-today-are-in-data-science-machine-learning/#67365b3351bd},
	urldate = {2020-08-13},
	author = {Columbus, Louis},
	year = {2017},
	note = {Publication Title: Forbes},
}

@article{ZELLNER1985187,
	title = {Bayesian regression diagnostics with applications to international consumption and income data},
	volume = {29},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/0304407685900399},
	doi = {https://doi.org/10.1016/0304-4076(85)90039-9},
	number = {1},
	journal = {Journal of Econometrics},
	author = {Zellner, Arnold and Moulton, Brent R},
	year = {1985},
	pages = {187--211},
}

@article{Goulart,
	title = {Hybrid {Model} {For} {Word} {Prediction} {Using} {Naive} {Bayes} and {Latent} {Information}},
	abstract = {Historically, the Natural Language Processing area has been given too much attention by many researchers. One of the main motivation beyond this interest is related to the word prediction problem, which states that given a set words in a sentence, one can recommend the next word. In literature, this problem is solved by methods based on syntactic or semantic analysis. Solely, each of these analysis cannot achieve practical results for end-user applications. For instance, the Latent Semantic Analysis can handle semantic features of text, but cannot suggest words considering syntactical rules [1]. On the other hand, there are models that treat both methods together and achieve state-of-the-art results, e.g. Deep Learning. These models can demand high computational effort, which can make the model infeasible for certain types of applications. With the advance of the technology and mathematical models, it is possible to develop faster systems with more accuracy. This work proposes a hybrid word suggestion model, based on Naive Bayes and Latent Semantic Analysis, considering neighbouring words around unfilled gaps. Results show that this model could achieve 44.2\% of accuracy in the MSR Sentence Completion Challenge.},
	author = {Goulart, Henrique X and Tosi, Mauro D L and Soares-Gonçalves, Daniel and Maia, Rodrigo F and Wachs-Lopes, Guilherme},
	keywords = {Applications, Natural Language Processing, Index Terms Naive Bayes, Latent Semantic Analysis, Sentence Completion, Word Prediction},
}

@book{Bauer1983,
	address = {Cambridge},
	title = {English word-formation},
	isbn = {0-521-28492-9},
	abstract = {Interest in word-formation is probably as old as interest in language itself. As Dr. Bauer points out in his Introduction, many of the questions that scholars are asking now were also being asked in the seventeenth, eighteenth and nineteenth centuries. However, there is still little agreement on methodology in the study of word-formation or theoretical approaches to it; even the kind of data relevant to its study is open to debate. Dr. Bauer here provides students and general linguists alike with a new perspective on what is a confused and often controversial field of study, providing a resolution to the terminological confusion which currently reigns in this area. In doing so, he clearly demonstrates the challenge and intrinsic fascination of the study of word-formation. Linguists have recently become increasingly aware of the relevance of word-formation to work in syntax and semantics, phonology and morphology, and Dr Bauer discusses - within a largely synchronic and transformational framework - the theoretical issues involved. He considers topics where word-formation has a contribution to make to other areas of linguistics and, without pretending to provide a fully-fledged theory of word-formation, develops those points which he sees as being central to its study. -- Publisher description. 1. Introduction --- 2. Some basic concepts --- 3. Lexicalization --- 4. Productivity --- 5. Phonological issues in word-formation --- 6. Syntactic and semantic issues in word-formation --- 7. An outline of English word-formation --- 8. Theory and practice --- 9. Conclusion.},
	publisher = {Cambridge University Press},
	author = {Bauer, Laurie},
	year = {1983},
	keywords = {Applications, Natural Language Processing},
}

@article{Qin2018,
	title = {Enhancing person-job fit for talent recruitment: {An} ability-aware neural network approach},
	doi = {10.1145/3209978.3210025},
	abstract = {The wide spread use of online recruitment services has led to information explosion in the job market. As a result, the recruiters have to seek the intelligent ways for Person-Job Fit, which is the bridge for adapting the right job seekers to the right positions. Existing studies on Person-Job Fit have a focus on measuring the matching degree between the talent qualification and the job requirements mainly based on the manual inspection of human resource experts despite of the subjective, incomplete, and inefficient nature of the human judgement. To this end, in this paper, we propose a novel end-to-end A bility-aware P erson-J ob F it N eural N etwork (APJFNN) model, which has a goal of reducing the dependence on manual labour and can provide better interpretation about the fitting results. The key idea is to exploit the rich information available at abundant historical job application data. Specifically, we propose a word-level semantic representation for both job requirements and job seekers' experiences based on Recurrent Neural Network (RNN). Along this line, four hierarchical ability-aware attention strategies are designed to measure the different importance of job requirements for semantic representation, as well as measuring the different contribution of each job experience to a specific ability requirement. Finally, extensive experiments on a large-scale real-world data set clearly validate the effectiveness and interpretability of the APJFNN framework compared with several baselines.},
	journal = {41st International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2018},
	author = {Qin, Chuan and Zhu, Hengshu and Xu, Tong and Zhu, Chen and Jiang, Liang and Chen, Enhong and Xiong, Hui},
	year = {2018},
	note = {ISBN: 9781450356572},
	keywords = {Applications, Recommender Systems, ★, Job, Neural network, Person-job fit, Recruitment analysis},
	pages = {25--34},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8TF2FBG5\\Enhancing Person-Job Fit for Talent Recruitment  An Ability-aware Neural Network Approach.pdf:application/pdf},
}

@book{moss2003conceptual,
	title = {Conceptual {Representation}},
	isbn = {978-1-84169-958-5},
	url = {https://books.google.com/books?id=yV2JGnlZTnUC},
	publisher = {Psychology Press},
	author = {Moss, H and Hampton, J and Hampton, J A},
	year = {2003},
	note = {Series Title: Language and cognitive processes ; v.18, no.5-6},
}

@book{Jeffreys61,
	address = {Oxford, England},
	edition = {Third},
	title = {Theory of {Probability}},
	publisher = {Oxford},
	author = {Jeffreys, H},
	year = {1961},
	note = {Publication Title: Theory of Probability},
	keywords = {Applications, Natural Language Processing},
}

@misc{Sarle2014,
	title = {Should {I} normalize/standardize/rescale the},
	url = {http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html},
	abstract = {FAQ related to re-scaling/normalising NN (and other related machine learning methods) input data. Covers both "feature" and "case" based data re-scaling.},
	urldate = {2018-10-06},
	author = {Sarle, Warren},
	year = {2014},
	keywords = {normalisation, SVM, SVM\_features},
}

@misc{Duggan2015,
	title = {Mobile {Messaging} and {Social} {Media} 2015 {\textbar} {Pew} {Research} {Center}},
	url = {http://www.pewinternet.org/2015/08/19/mobile-messaging-and-social-media-2015/},
	urldate = {2016-12-14},
	author = {Duggan, Maeve},
	year = {2015},
	keywords = {Applications, Natural Language Processing},
}

@article{Wasserman,
	title = {Larry {Wasserman} {All} of {Statistics}},
	author = {Wasserman, Larry},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y6BPPK2I\\all_of_statistics.pdf:application/pdf},
}

@article{Piironen2017,
	title = {Comparison of {Bayesian} predictive methods for model selection},
	volume = {27},
	issn = {15731375},
	doi = {10.1007/s11222-016-9649-y},
	abstract = {The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classification and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation (CV) score is liable to finding overfitted models due to relatively high variance in the utility estimates when the data is scarce. This can also lead to substantial selection induced bias and optimism in the performance evaluation for the selected model. From a predictive viewpoint, best results are obtained by accounting for model uncertainty by forming the full encompassing model, such as the Bayesian model averaging solution over the candidate models. If the encompassing model is too complex, it can be robustly simplified by the projection method, in which the information of the full model is projected onto the submodels. This approach is substantially less prone to overfitting than selection based on CV-score. Overall, the projection method appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly benefit from using cross-validation outside the searching process both for guiding the model size selection and assessing the predictive performance of the finally selected model.},
	number = {3},
	journal = {Statistics and Computing},
	author = {Piironen, Juho and Vehtari, Aki},
	year = {2017},
	note = {arXiv: 1503.08650
Publisher: Springer US},
	keywords = {Cross-validation, Bayesian model selection, Projection, Reference model, Selection bias},
	pages = {711--735},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2XSEHYE6\\Comparison of Bayesian predictive methods for model selection.pdf:application/pdf},
}

@article{Driscoll2005,
	title = {Intro to {Learning} theories},
	abstract = {for troubleshooting information. If further assistance is required, please send a description of the problem to libraryreserves@psu.edu that includes the course and instructor for which the material is on reserve, as well as the title of the material. Bransford, J., et al. Learning theories and education: Toward a decade of synergy.},
	number = {814},
	author = {{Driscoll}},
	year = {2005},
	note = {ISBN: 0205375197},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TPZZBM7M\\Machine Learning and Learning Theory.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\W98QF462\\Machine Learning and Learning Theory.pdf:application/pdf},
}

@book{Lester,
	title = {What readers are saying about {Technical} {Blogging}},
	isbn = {978-1-934356-88-3},
	author = {Lester, Andy},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DB87MV9D\\technical_blogging.pdf:application/pdf},
}

@article{Christian2013,
	title = {Implementation of {Modified} {Kneser}-{Ney} {Smoothing} on {Top} of {Generalized} {Language} {Models} for {Next} {Word} {Prediction}},
	author = {Christian, Martin and Erstgutachter, Körner and Staab, Steffen},
	year = {2013},
	keywords = {Applications, Natural Language Processing},
}

@article{Greene,
	title = {How {Many} {Topics}? {Stability} {Analysis} for {Topic} {Models}},
	abstract = {Topic modeling refers to the task of discovering the under-lying thematic structure in a text corpus, where the output is commonly presented as a report of the top terms appearing in each topic. Despite the diversity of topic modeling algorithms that have been proposed, a common challenge in successfully applying these techniques is the selec-tion of an appropriate number of topics for a given corpus. Choosing too few topics will produce results that are overly broad, while choosing too many will result in the " over-clustering " of a corpus into many small, highly-similar topics. In this paper, we propose a term-centric stability analysis strategy to address this issue, the idea being that a model with an appropriate number of topics will be more robust to perturbations in the data. Using a topic modeling approach based on matrix factorization, evaluations performed on a range of corpora show that this strategy can successfully guide the model selection process.},
	author = {Greene, Derek and O 'callaghan, Derek and Cunningham, Pádraig},
	keywords = {Applications, Natural Language Processing},
}

@techreport{Dozatb,
	title = {Incorporating {Nesterov} {Momentum} into {Adam}},
	url = {http://mattmahoney.net/dc/text8.zip},
	author = {Dozat, Timothy},
}

@article{Kilgarriff2001,
	title = {29 {COMPARING} {CORPORA}},
	volume = {6},
	abstract = {Corpus linguistics lacks strategies for describing and compar-ing corpora. Currently most descriptions of corpora are textual, and questions such as 'what sort of a corpus is this?', or 'how does this corpus compare to that?' can only be answered impressionistically. This paper considers various ways in which different corpora can be compared more objectively. First we address the issue, 'which words are particularly characteristic of a corpus?', reviewing and critiquing the statistical methods which have been applied to the question and proposing the use of the Mann-Whitney ranks test. Results of two corpus com-parisons using the ranks test are presented. Then, we consider measures for corpus similarity. After discussing limitations of the idea of corpus similarity, we present a method for evaluat-ing corpus similarity measures. We consider several measures and establish that a \$χ\$ 2 -based one performs best. All methods considered in this paper are based on word and ngram fre-quencies; the strategy is defended.},
	number = {1},
	journal = {International Journal of Corpus Linguistics},
	author = {Kilgarriff, Adam},
	year = {2001},
	keywords = {Applications, Natural Language Processing},
	pages = {97--133},
}

@techreport{Sugiyama,
	title = {Learning under {Non}-stationarity: {Covariate} {Shift} {Adaptation} by {Importance} {Weighting}},
	url = {http://sugiyama-www.cs.titech.ac.jp/%CB%9Csugi},
	abstract = {The goal of supervised learning is to estimate an underlying input-output function from its input-output training samples so that output values for unseen test input points can be predicted. A common assumption in supervised learning is that the training input points follow the same probability distribution as the test input points. However, this assumption is not satisfied, for example, when outside of the training region is extrapolated. The situation where the training and test input points follow different distributions while the conditional distribution of output values given input points is unchanged is called covariate shift. Since almost all existing learning methods assume that the training and test samples are drawn from the same distribution, their fundamental theoretical properties such as consistency or efficiency no longer hold under covariate shift. In this chapter, we review recently proposed techniques for covariate shift adaptation.},
	author = {Sugiyama, Masashi},
}

@article{Fox2009,
	title = {Package ‘ effects '},
	journal = {Source},
	author = {Fox, John},
	year = {2009},
}

@article{Darken1989,
	title = {Note on {Learning} {Rate} {Schedules} for {Stochastic} {Optimization}},
	url = {http://www.informatik.uni-trier.de/%7B~%7Dley/db/conf/nips/nips1990.html%5Cnhttp://books.nips.cc/nips03.html},
	abstract = {We present and compare learning rate schedules for stochastic gradient descent, a general algorithm which includes LMS, on-line backpropagation, and k-means clustering as special cases. We introduce "search-then-converge" type schedules which outperform the classical constant and "running average" (1/t) schedules both in speed of convergence and quality of solution.},
	journal = {Advances in Neural Information Processing Systems (NIPS)},
	author = {Darken, Christian and Moody, John E},
	year = {1989},
	note = {ISBN: 1-55860-100-7},
	pages = {832--838},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PXBEQRBK\\Notes on Learning Rate Schedules for Stochastic Optimization.pdf:application/pdf},
}

@article{Liang2015,
	title = {{CS229T}/{STAT231}: {Statistical} {Learning} {Theory} ({Winter} 2015)},
	url = {papers3://publication/uuid/7D5405C2-EE27-4CE3-9136-35C25CC09BFE},
	abstract = {These lecture notes will be updated periodically as the course goes on. Please let us know if you find typos. Section A.1 describes the basic notation and definitions; Section A.2 describes some basic theorems.},
	number = {Winter},
	author = {Liang, Percy},
	year = {2015},
	pages = {1--192},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4TS9HMYH\\Statistical Learning Theory (Winter 2015).pdf:application/pdf},
}

@article{F-test2008,
	title = {Generalised {Linear} {Models}},
	doi = {10.1007/978-0-387-73251-0_10},
	number = {1984},
	author = {F-test, The},
	year = {2008},
	pages = {301--333},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YGYES8AS\\GLM.pdf:application/pdf},
}

@article{Pappas2012,
	title = {A {SURVEY} {ON} {LANGUAGE} {MODELING} {USING} {NEURAL} {NETWORKS}},
	author = {Pappas, Nikolaos and Meyer, Thomas},
	year = {2012},
	keywords = {Applications, Natural Language Processing},
}

@article{Jin2019,
	title = {Machine {Learning}: {Why} {Do} {Simple} {Algorithms} {Work} {So} {Well}?},
	author = {Jin, Chi},
	year = {2019},
}

@article{Clyde2000,
	title = {Flexible {Empirical} {Bayes} {Estimation} for {Wavelets}},
	volume = {62},
	url = {http://links.jstor.org/sici?sici=1369-7412%282000%2962%3A4%3C681%3AFEBEFW%3E2.0.CO%3B2-B},
	number = {4},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Clyde, Merlise and George, Edward I},
	year = {2000},
	keywords = {Algorithms, Bayesian},
	pages = {681--698},
}

@article{NESTEROV1983,
	title = {A method for unconstrained convex minimization problem with the rate of convergence o(1/k{\textasciicircum}2)},
	volume = {269},
	url = {http://ci.nii.ac.jp/naid/20001173129/en/},
	journal = {Doklady AN USSR},
	author = {{NESTEROV} and {Y.}},
	year = {1983},
	pages = {543--547},
}

@misc{Rossenstock,
	title = {{NGramQuickTour} {\textless}{GRM} {\textless}{TWiki}},
	url = {http://openfst.cs.nyu.edu/twiki/bin/view/GRM/NGramQuickTour#NgramCounting},
	urldate = {2017-04-03},
	author = {Rosenstock, Jesse},
	keywords = {Applications, Natural Language Processing},
}

@book{Taylor2015,
	title = {The {Oxford} {Handbook} of the {Word}},
	author = {Taylor, John},
	year = {2015},
	keywords = {Applications, Natural Language Processing},
}

@article{Leyffer2016,
	title = {Optimization: {Applications}, {Algorithms}, and {Computation} 24 {Lectures} on {Nonlinear} {Optimization} and {Beyond}},
	author = {Leyffer, Sven},
	year = {2016},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2GL3HXIJ\\NonLinOpt.pdf:},
}

@book{Miceli2018,
	title = {Isolating {Random} and {Bias} {Covariances} in {Tracks}},
	isbn = {978-0-9964527-6-2},
	abstract = {In addition to the typical random errors that vary between consecutive measurements, the measurements for most all sensors used for target tracking include bias errors that remain relatively fixed during a target tracking episode and are typically characterized by an a priori mean and covariance. Since the bias errors are approximately fixed during a tracking episode, those errors violate the typical assumption of the measurement errors being white noise. Inflating the measurement covariance of the random errors by adding the bias covariance gives track covariances that poorly represent the true errors. The Schmidt-Kalman filter can be used to prevent the track covariances from becoming artificially too small. However, the Schmidt-Kalman filter produces a track covariance that encompasses the random and bias errors. In this paper, the authors formulate the target tracking as a least-square estimation (LSE) problem and show that the track covariance due to the bias errors can be isolated from the track covariance due the random errors. The authors utilize Monte Carlo simulations to verify and illustrate the accuracy of isolation of the bias and random covariances.},
	author = {Miceli, P A and Blair, W Dale and Brown, M M},
	year = {2018},
	doi = {10.23919/ICIF.2018.8455530},
	note = {Publication Title: 2018 21st International Conference on Information Fusion, FUSION 2018},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ULNRQDIW\\Deep Learning with Python.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DLI4YRZK\\Deep Learning with Python.pdf:application/pdf},
}

@book{Axler1996,
	title = {Linear algebra done right},
	volume = {33},
	isbn = {978-3-319-11079-0},
	abstract = {As humans, we perceive the three-dimensional structure of the world around us with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem, and what is the current state of the art?Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging and fun consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos.More than just a source of "recipes", this text/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting this process to produce the best possible descriptions of a scene. Exercises are presented throughout the book, with a heavy emphasis on testing algorithms.Suitable for either an undergraduate or a graduate-level course in computer vision, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries.Dr. Richard Szeliski has over twenty years' experience in computer vision research, most notably at Digital Equipment Corporation and Microsoft.},
	author = {Axler, Sheldon},
	year = {1996},
	doi = {10.5860/choice.33-6354},
	note = {Publication Title: Choice Reviews Online
Issue: 11
ISSN: 0009-4978},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SY8V2F47\\linear_algebra_done_right.pdf:application/pdf},
}

@article{jia2014caffe,
	title = {Caffe: {Convolutional} {Architecture} for {Fast} {Feature} {Embedding}},
	journal = {arXiv preprint arXiv:1408.5093},
	author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	year = {2014},
}

@article{Behrens1997,
	title = {Principles and {Procedures} of {Exploratory} {Data} {Analysis}},
	volume = {2},
	issn = {1082989X},
	doi = {10.1037/1082-989X.2.2.131},
	abstract = {Exploratory data analysis (EDA) is a well-established statistical tradition that provides conceptual and computational tools for discovering patterns to foster hypothesis development and refinement. These tools and attitudes complement the use of significance and hypothesis tests used in confirmatory data analysis (CDA). Although EDA complements rather than replaces CDA, use of CDA without EDA is seldom warranted. Even when well-specified theories are held, EDA helps one interpret the results of CDA and may reveal unexpected or misleading patterns in the data. This article introduces the central heuristics and computational tools of EDA and contrasts it with CDA and exploratory statistics in general. EDA techniques are illustrated using previously published psychological data. Changes in statistical training and practice are recommended to incorporate these tools.},
	number = {2},
	journal = {Psychological Methods},
	author = {Behrens, John T},
	year = {1997},
	pages = {131--160},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CFYCYRMY\\Principles and Procedures of Exploratory Data Analysis.pdf:application/pdf},
}

@book{Goddard2002,
	address = {Amsterdam},
	title = {Meaning and {Universal} {Grammar}: {Theory} and {Empirical} {Findings}},
	publisher = {John Benjamins},
	author = {Goddard, Cliff},
	year = {2002},
	keywords = {Applications, Natural Language Processing},
}

@article{StateEnterpriseMachine2018,
	title = {The {State} of {Enterprise} {Machine} {Learning} {\textbar} {The} {State} of {Enterprise} {Machine} {Learning}},
	number = {October},
	year = {2018},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UHTEA6RZ\\State of ML Report Final.pdf:application/pdf},
}

@misc{WikimediaFoundation,
	title = {Wikipedia {Corpus}},
	url = {https://support.microsoft.com/en-us/help/14200/windows-compress-uncompress-zip-files},
	urldate = {2017-03-17},
	author = {{Wikimedia Foundation}},
	note = {Publication Title: Microsoft},
	keywords = {Applications, Natural Language Processing},
}

@article{Jurafsky2007,
	title = {N-grams chapter {Draft}},
	issn = {14321238},
	url = {http://books.google.com/books?id=y0xQAAAAMAAJ},
	doi = {10.1007/s00134-010-1760-5},
	abstract = {Draft of July 30, 2007 - Do not cite without permission - This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora.Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language},
	journal = {Language},
	author = {Jurafsky, Daniel S and Martin, James H},
	year = {2007},
	pmid = {20213416},
	note = {ISBN: 0131873210},
	pages = {934},
}

@misc{08DesignandAnalysisExptsPdf,
	title = {08\_Design-and-{Analysis}-{Expts}.{Pdf}},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\K9MPPMXK\\Design and Analysis of Learning Experiments.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E553CY2S\\Design and Analysis of Learning Experiments.pdf:application/pdf},
}

@article{MWCReynaertdrPaijmans2008,
	title = {Explorations into {Unsupervised} {Corpus} {Quality} {Assessment}},
	author = {MWC Reynaert dr Paijmans, dr J J},
	year = {2008},
}

@article{Boulton2007,
	title = {A {Vision} for a {Healthier} {Future}},
	volume = {6},
	issn = {15443450},
	url = {http://www.cdc.gov/nccdphp/overview.htm},
	doi = {10.1331/JAPhA.2007.08541},
	abstract = {t Overweight rates have been climbing over the past few decades among children. About 9 million (or roughly one in six kids ages 6–19) were overweight in 2004 – more than triple the number of overweight children in 1980. 14 t Given current trends, one in three children born in 2000 will develop diabetes over the course of a lifetime. 15 Chronic Diseases: Often Preventable, Frequently Manageable Many chronic diseases could be prevented, delayed, or alleviated, through simple lifestyle changes. t The U.S. Centers for Disease Control and Prevention (CDC) 16 estimates that eliminating three risk factors – poor diet, inactivity, and smoking – would prevent: t 80\% of heart disease and stroke; t 80\% of type 2 diabetes; and, t 40\% of cancer. 7 To get this number, total spending on chronic disease during 2005 (\$1.5 trillion) was divided by the total population (300 million Americans). 15 Laino C. One in three kids will develop diabetes. Web MD [serial online].},
	number = {17},
	journal = {Accessed Aprilppt. Accessed on April},
	author = {Boulton, Aj and Vileikyte, L and {Ragnarson} and Tennvall, G and Apelqvist, J},
	year = {2007},
	pages = {1719--1724},
}

@article{Hoefling2014,
	title = {Reproducible research for large-scale data analysis},
	doi = {10.1201/b16868},
	abstract = {The opinions expressed in this chapter are solely those of the authors and not necessarily those of Novartis. Novartis does not guarantee the accuracy or reliability of the information provided herein.},
	journal = {Implementing Reproducible Research},
	author = {Hoefling, Holger and Rossini, Anthony},
	year = {2014},
	note = {ISBN: 9781466561601},
	pages = {219--240},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2KTZN9R5\\Reproducible Research for Large Scale Data Analysis.pdf:application/pdf},
}

@misc{boston1980,
	title = {Boston {Housing} {Data}},
	url = {http://lib.stat.cmu.edu/datasets/boston},
	abstract = {Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the demand for clean air', J. Environ. Economics \& Management, vol.5, 81-102, 1978.},
	urldate = {2020-05-04},
	author = {Harrison, D. Rubinfeld, D L},
	year = {1993},
	note = {Publication Title: J. Environ. Economics \& Management},
}

@article{Vanwinckelen2012,
	title = {On estimating model accuracy with repeated cross-validation},
	url = {https://lirias.kuleuven.be/handle/123456789/346385},
	abstract = {Evaluation of predictive models is a ubiq- uitous task in machine learning and data mining. Cross-validation is often used as a means for evaluating models. There appears to be some confusion among re- searchers, however, about best practices for cross-validation, and about the interpreta- tion of cross-validation results. In particular, repeated cross-validation is often advocated, and so is the reporting of standard devia- tions, confidence intervals, or an indication of ”significance”. In this paper, we argue that, under many practical circumstances, when the goal of the experiments is to see how well the model returned by a learner will perform in practice in a particular domain, repeated cross-validation is not useful, and the report- ing of confidence intervals or significance is misleading. Our arguments are supported by experimental results.},
	journal = {21st Belgian-Dutch Conference on Machine Learning},
	author = {Vanwinckelen, Gitte and Blockeel, Hendrik},
	year = {2012},
	keywords = {conditional prediction error, predictive model evaluation, repeated cross-validation},
	pages = {39--44},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BSHU4BHI\\OnEstimatingModelAccuracy.pdf:application/pdf},
}

@article{Manning2014,
	title = {The {Stanford} {CoreNLP} {Natural} {Language} {Processing} {Toolkit}},
	issn = {1098-6596},
	url = {http://aclweb.org/anthology/P14-5010},
	doi = {10.3115/v1/P14-5010},
	abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
	journal = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
	author = {Manning, Christopher D and Bauer, John and Finkel, Jenny and Bethard, Steven J and Surdeanu, Mihai and McClosky, David},
	year = {2014},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISBN: 9781941643006},
	keywords = {Applications, Natural Language Processing},
	pages = {55--60},
}

@article{Domino2017,
	title = {The {Practical} {Guide} to {Managing} {Data} {Science} at {Scale}},
	journal = {Domino},
	author = {{Domino}},
	year = {2017},
	pages = {1--25},
}

@article{Muller,
	title = {{PREDICTING} {THE} {OUT}-{OF}-{VOCABULARY} {RATE} {AND} {THE} {REQUIRED} {VOCABULARY} {SIZE} {FOR} {SPEECH} {PROCESSING} {APPLICATIONS}},
	abstract = {This paper describes an approach for predicting both the vocabu-lary size and the resulting out-of-vocabulary rate (OOV-rate) for a hypothetical extension of an existing text corpus. By splitting the original corpus into two different sub-corpora, vocabulary and OOV-rate can be determined for that special constellation. Average values are calculated for all combinations of sub-cor-pora and can be approximated by analytic function terms. These functions enable the easy prediction of the vocabulary size and the OOV-rate. The prediction accuracy results in a relative error below 4.6\%.},
	author = {Müller, Johannes and Stahl, Holger and Lang, Manfred},
	keywords = {Applications, Natural Language Processing, vocabulary size, OOV-rate, out-of-vocabulary rate, test corpus, text corpus, training corpus},
}

@article{Finnoff1993ImprovingMS,
	title = {Improving model selection by nonconvergent methods},
	volume = {6},
	journal = {Neural Networks},
	author = {Finnoff, William and Hergert, Ferdinand and Zimmermann, Hans-Georg},
	year = {1993},
	pages = {771--783},
}

@techreport{Wolpert1992,
	title = {On the {Connection} between {In}-sample {Testing} and {Generalization} {Error}},
	abstract = {This paper pr oves t hat it is impossibl e to justify a corre la-tio n between rep roducti on of a training set and generali zation err or off of t he training set usin g only a pr iori reasoning. As a resu lt , the use in t he real world of any genera lizer t hat fits a hypothesis functi on to a training set (e.g., the use of back-propagation) is implicitl y pr edic at ed on an ass umpt ion abo ut the physical universe. This pap er shows how this ass umpt ion can be expressed in te rms of a non-Euclidean inn er product between two vectors, one represent ing t he ph ysical uni verse a nd on e representing t he generalizer. In deriving this result , a novel formalism for ad dress ing mac hine learni ng is develop ed. T his new formalism can be viewed as an exte nsion of t he conventional "Bayesian" formalism , to (among other t hings). allow one to address t he case in which one's assumed "priors" are not exactly correct. The most im-p or ta nt fea ture of this new formalism is t hat it uses an ext remely low-level event space, consis ting of triples of \{target fun ction , hypothesis fun cti on , train ing set \}. P artly as a resu lt of this fea ture, most other form alisms that have been constructed to address machine lea rn ing (e.g., PAC , the Bayesian formalism , and th e "st a tist ical mechanics" for malism) are sp ecial cases of t he form alism presented in this paper. Consequent ly such for malisms are capable of addressin g only a subset of t he issues ad dress ed in this pap er. In fact , the formalism of t his paper can be used to address all generalization issues of which the author is aware: overt ra in ing , the need to restrict the number of free para meters in t he hypothesis funct ion , th e problems associated wit h a "non-represent a tive" training set , whether and when cross-validat ion work s, wh ether and when stacked gene ra lizat ion work s, whe the r a nd wh en a particu lar regu lari zer will work , and so for th. A sum mary of som e of the more im port ant resu lt s of t his pap er conce rn ing t hese and related topi cs can be found in the conclusion .},
	author = {Wolpert, David H},
	year = {1992},
	note = {Publication Title: Com p lex Systems
Volume: 6},
	pages = {47--94},
}

@misc{Needham1990,
	title = {{IMDb} - {Movies}, {TV} and {Celebrities}},
	url = {http://www.imdb.com/},
	abstract = {IMDb, the world's most popular and authoritative source for movie, TV and celebrity content.},
	urldate = {2017-11-24},
	author = {Needham, Col},
	year = {1990},
	keywords = {Algorithms, Bayesian},
}

@article{DeJonge2013,
	title = {An introduction to data cleaning with {R}},
	abstract = {Data cleaning, or data preparation is an essential part of statistical analysis. In fact, in practice it is often more time-consuming than the statistical analysis itself. These lecture notes describe a range of techniques, implemented in the R statistical environment, that allow the reader to build data cleaning scripts for data suffering from a wide range of errors and inconsistencies, in textual format. These notes cover technical as well as subject-matter related aspects of data cleaning. Technical aspects include data reading, type conversion and string matching and manipulation. Subject-matter related aspects include topics like data checking, error localization and an introduction to imputation methods in R. References to relevant literature and R packages are provided throughout.},
	journal = {Statistics Netherlands},
	author = {de Jonge, Edwin and van der Loo, Mark},
	year = {2013},
	note = {ISBN: 1572-0314},
	keywords = {data editing, methodology, statistical software},
	pages = {53},
}

@article{Latif2020,
	title = {Multi-{Task} {Semi}-{Supervised} {Adversarial} {Autoencoding} for {Speech} {Emotion} {Recognition}},
	issn = {19493045},
	doi = {10.1109/TAFFC.2020.2983669},
	abstract = {Inspite the emerging importance of Speech Emotion Recognition (SER), the state-of-the-art accuracy is quite low and needs improvement to make commercial applications of SER viable. A key underlying reason for the low accuracy is the scarcity of emotion datasets, which is a challenge for developing any robust machine learning model in general. In this paper, we propose a solution to this problem: a multi-task learning framework that uses auxiliary tasks for which data is abundantly available. We show that utilisation of this additional data can improve the primary task of SER for which only limited labelled data is available. In particular, we use gender identifications and speaker recognition as auxiliary tasks, which allow the use of very large datasets, e.g., speaker classification datasets. To maximise the benefit of multi-task learning, we further use an adversarial autoencoder (AAE) within our framework, which has a strong capability to learn powerful and discriminative features. Furthermore, the unsupervised AAE in combination with the supervised classification networks enables semi-supervised learning which incorporates a discriminative component in the AAE unsupervised training pipeline. The proposed model is rigorously evaluated for categorical and dimensional emotion, and cross-corpus scenarios. Experimental results demonstrate that the proposed model achieves state-of-the-art performance on two publicly available dataset.},
	journal = {IEEE Transactions on Affective Computing},
	author = {Latif, Siddique and Rana, Rajib and Khalifa, Sara and Jurdak, Raja and Epps, Julien and Schuller, Bjorn Wolfgang},
	year = {2020},
	note = {arXiv: 1907.06078},
	keywords = {Emotion recognition, Speech emotion recognition, Australia, Hidden Markov models, multi task learning, representation learning, Semisupervised learning, Speech recognition, Task analysis, Training},
	pages = {1--13},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JBPQE722\\Multi-Task Semi-Supervised Adversarial Autoencoding for Speech Emotion Recognition.pdf:application/pdf},
}

@book{strang09,
	address = {Wellesley, MA},
	edition = {Fourth},
	title = {Introduction to {Linear} {Algebra}},
	isbn = {978-0-9802327-1-4 0-9802327-1-6 978-0-9802327-2-1 0-9802327-2-4 978-81-7596-811-0 81-7596-811-7},
	abstract = {Book Description: Gilbert Strang's textbooks have changed the entire approach to learning linear algebra -- away from abstract vector spaces to specific examples of the four fundamental subspaces: the column space and nullspace of A and A'. Introduction to Linear Algebra, Fourth Edition includes challenge problems to complement the review problems that have been highly praised in previous editions. The basic course is followed by seven applications: differential equations, engineering, graph theory, statistics, Fourier methods and the FFT, linear programming, and computer graphics. Thousands of teachers in colleges and universities and now high schools are using this book, which truly explains this crucial subject.},
	publisher = {Wellesley-Cambridge Press},
	author = {Strang, Gilbert},
	year = {2009},
	keywords = {linear.algebra matrix strang textbook},
}

@article{Perez-Sabater,
	title = {The {Linguistics} of {Social} {Networking}: {A} {Study} of {Writing} {Conventions} on {Facebook}},
	abstract = {Scholarly research on computer-mediated communication discourse has mainly centred upon the linguistic characteristics of emails, focusing on the formal and informal features and the orality involved in this form of communication. This paper presents a new insight into the study of computer-mediated communication (CMC) by analysing a fairly recent genre of computer-mediated communication, comments posted on the new social networking websites. The research undertaken examines the comments published on the official Facebook sites of some universities to observe the level of formality/informality of online communication in English. The distinction between online writings by native and non-native speakers of English has been considered as well. The study focuses on the formulae of etiquette and protocol used for salutation, opening, pre-closing and closing as an indicator of the degree of orality and informality in online writing. Data reveal that, in the specific context of the university, the use of Facebook is not conventionalised, as the comments posted on Facebook present important stylistic variations. Moreover, in most instances non-native speakers of English display more formal traits than native speakers when communicating electronically on social networking sites in the academic world. 1 Introduction In the last few years, information and communication technologies have evolved rapidly and have created new forms of literacies. As a result, new online genres have emerged in academic genres and discourses (cf. Kuteeva 2011). The incorporation of the new electronic genres to the academic world has affected the use of established traditional communication exchange media in university organisations. Computer-mediated interaction in the academic world has changed the way we write and the genres we create (cf. Hyland/Hamp-Lyons 2002). Focusing on this specific context, the development of social networking and its increasing importance in the academic world creates the need for scholarly research on the issue. The research undertaken examines the use of the English language in the comments posted on the social networking website Facebook, the most popular social networking website at the moment (cf. Facebook Inc. 2012). As will be detailed below, Facebook is used as a research context in order to determine whether a specific tool of this networking application, that of posting comments, is a conventionalised genre of computer-mediated communication, despite its relative novelty.},
	author = {Pérez-Sabater, Carmen},
	keywords = {Applications, Natural Language Processing},
}

@article{Cawley2010,
	title = {On over-fitting in model selection and subsequent selection bias in performance evaluation},
	volume = {11},
	issn = {15324435},
	abstract = {Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a nonnegligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds. ©2010 Gavin C. Cawley and Nicola L. C. Talbot.},
	journal = {Journal of Machine Learning Research},
	author = {Cawley, Gavin C and Talbot, Nicola L C},
	year = {2010},
	keywords = {Model selection, Selection bias, Bias-variance trade-off, Overfitting, Performance evaluation},
	pages = {2079--2107},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V3HM3H7G\\On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation.pdf:application/pdf},
}

@article{TheRFoundation2015,
	title = {What is {R}?},
	issn = {1609-3631},
	url = {https://www.r-project.org/about.html},
	doi = {10.1016/B978-0-12-417113-8.09994-X},
	journal = {Mathematical Statistics with Applications in R},
	author = {{The R Foundation}},
	year = {2015},
	pmid = {21196786},
	note = {ISBN: 0387947256},
	keywords = {Applications, Natural Language Processing},
	pages = {745},
}

@techreport{RecommendationSystems,
	title = {Recommendation {Systems}},
	abstract = {There is an extensive class of Web applications that involve predicting user responses to options. Such a facility is called a recommendation system. We shall begin this chapter with a survey of the most important examples of these systems. However, to bring the problem into focus, two good examples of recommendation systems are: 1. Offering news articles to on-line newspaper readers, based on a prediction of reader interests. 2. Offering customers of an on-line retailer suggestions about what they might like to buy, based on their past history of purchases and/or product searches. Recommendation systems use a number of different technologies. We can classify these systems into two broad groups. • Content-based systems examine properties of the items recommended. For instance, if a Netflix user has watched many cowboy movies, then recommend a movie classified in the database as having the "cowboy" genre. • Collaborative filtering systems recommend items based on similarity measures between users and/or items. The items recommended to a user are those preferred by similar users. This sort of recommendation system can use the groundwork laid in Chapter 3 on similarity search and Chapter 7 on clustering. However, these technologies by themselves are not sufficient , and there are some new algorithms that have proven effective for recommendation systems. 9.1 A Model for Recommendation Systems In this section we introduce a model for recommendation systems, based on a utility matrix of preferences. We introduce the concept of a "long-tail," 319},
	keywords = {Applications, Recommender Systems, Clustering, Unsupervised Learning},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RBRSSHAE\\Recommendation Systems (Stanford).pdf:application/pdf},
}

@article{Roffo,
	title = {Towards {Personality}-{Aware} {Recommendation} {The} {ADS} {Dataset}},
	doi = {10.475/123},
	abstract = {In the last decade new ways of shopping online have increased the possibility of buying products and services more easily and faster than ever. In this new context, personality is a key determinant in the decision making of the consumer when shopping. The two main reasons are: firstly, a person's buying choices are influenced by psychological factors like impulsiveness, and secondly, some consumers may be more susceptible to making impulse purchases than others. To the best of our knowledge, the impact of personality factors on advertisements has been largely neglected at the level of recommender systems. This work proposes a highly innovative research which uses a personality perspective to determine the unique associations among the consumer's buying tendency and advert recommendations. As a matter of fact, the lack of a publicly available benchmark for computational advertising do not allow both the exploration of this intriguing research direction and the evaluation of state-of-the-art algorithms. We present the ADS Dataset, a publicly available benchmark for computational advertising enriched with Big-Five users' personality factors and 1,200 personal users' pictures. The proposed benchmark allows two main tasks: rating prediction over 300 real advertisements (i.e., Rich Media Ads, Image Ads, Text Ads) and click-through rate prediction. Moreover, this work carries out experiments, reviews various evaluation criteria used in the literature, and provides a library for each one of them within one integrated toolbox.},
	author = {Roffo, Giorgio},
	keywords = {Applications, Recommender Systems, CCS Concepts •Information systems → Computational, Collabo-rative search, Keywords Computational Advertising, Online Adverti, Test collections},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F23VLEZE\\Towards Personality-Aware Recommendation.pdf:application/pdf},
}

@article{Ghayoomi,
	title = {A {POS}-based {Word} {Prediction} {System} for the {Persian} {Language}},
	abstract = {Word prediction is the problem of guessing the words which are likely to follow in a given text segment by displaying a list of the most probable words that could appear in that position. In this research, we designed and implemented three word predictors for Persian. Our baseline is a statistical-based system which uses language models. The first system uses word statistics; in the second one we use the main syntactic categories of a Persian POS tagged corpus; and the last one uses the main syntactic categories along with their morphological, syntactic and semantic subcategories. Using KeyStroke Saving (KSS) as the most important metrics to evaluate systems' performance, the primary word-based statistical system achieved 37\% KSS, and the second system that used only the main syntactic categories with word-statistics achieved 38.95\% KSS. Our last system which used all of the available information to the words get the best result by 42.45\% KSS.},
	author = {Ghayoomi, Masood and Daroodi, Ehsan},
	keywords = {Applications, Natural Language Processing, word prediction, POS tagging, statistical language modeling},
}

@article{Lehman2004,
	title = {Mathematics for {Computer} {Science} {Eric} {Lehman} and {Tom} {Leighton} 2004},
	journal = {Science},
	author = {Lehman, Eric and Leighton, Tom},
	year = {2004},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZTIZY7AI\\Mathematics for Computer Science.pdf:application/pdf},
}

@inproceedings{paszke2017automatic,
	title = {Automatic differentiation in {PyTorch}},
	booktitle = {{NIPS}-{W}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017},
}

@article{Saxe2013ExactST,
	title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
	volume = {abs/1312.6},
	journal = {CoRR},
	author = {Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
	year = {2013},
}

@article{Goodman2001,
	title = {A bit of progress in language modeling},
	volume = {15},
	url = {http://www.idealibrary.com},
	doi = {10.1006/csla.2001.0174},
	abstract = {In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n-grams, skipping, interpolated Kneser–Ney smoothing, and clustering. We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential. While all of these techniques have been studied separately, they have rarely been studied in combination. We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs. We achieve perplexity reductions between 38 and 50\% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9\%. Our perplexity reductions are perhaps the highest reported compared to a fair baseline.},
	journal = {Computer Speech and Language},
	author = {Goodman, Joshua T},
	year = {2001},
	keywords = {Applications, Natural Language Processing},
	pages = {403--434},
}

@article{mehta2019,
	title = {A high-bias, low-variance introduction to {Machine} {Learning} for physicists.},
	volume = {810},
	issn = {0370-1573 (Print)},
	doi = {10.1016/j.physrep.2019.03.001},
	abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias-variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton-proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute.},
	journal = {Physics reports},
	author = {Mehta, Pankaj and Wang, Ching-Hao and Day, Alexandre G R and Richardson, Clint and Bukov, Marin and Fisher, Charles K and Schwab, David J},
	month = may,
	year = {2019},
	pmid = {31404441},
	pages = {1--124},
}

@article{Clarkson1997,
	title = {Statistical {Language} {Modeling} {Using} the \{{CMU}\}--{Cambridge} {Toolkit}},
	url = {http://repository.cmu.edu/compsci},
	journal = {Proceedings Eurospeech '97},
	author = {Clarkson, Philip and Rosenfeld, Ronald},
	year = {1997},
	keywords = {Applications, Natural Language Processing},
	pages = {2707--2710},
}

@article{Lidstone1920,
	title = {Note on the general case of the bayes-laplace formula for inductive or a posteriori probabilities},
	volume = {8},
	journal = {Transactions of the Faculty of Actuaries},
	author = {Lidstone, George James},
	year = {1920},
	keywords = {Applications, Natural Language Processing, mathematics},
	pages = {182--192},
}

@article{Sichel1986,
	title = {Word frequency distributions and type-token characteristics},
	volume = {11},
	journal = {Mathematical Scientist},
	author = {Sichel, H},
	year = {1986},
	keywords = {Applications, Natural Language Processing},
	pages = {45--72},
}

@incollection{Biber1993a,
	address = {New York},
	title = {An {Analytical} {Framework} for {Register} {Studies}},
	booktitle = {Sociolinguistic {Perspectives} on {Register}},
	publisher = {Oxford University Press},
	author = {Biber, Douglas and Finegan, E},
	year = {1993},
	keywords = {Applications, Natural Language Processing},
	pages = {31--58},
}

@article{Nalavade,
	title = {{PreText}: {A} {Predictive} {Text} {Entry} {System} for {Mobile} {Phones}},
	volume = {III},
	abstract = {Majority of the currently used predictive text entry systems (like T9 for lower end mobile phones) do not provide word prediction. In these systems, the average number of key-taps per word is high resulting in higher typing efforts on the part of the user. At times, T9 provides options (words) that may not fit into the context of the message, are wrong grammatically and are not valid English words. Also, T9 is slower to adapt to usage patterns. PreText predicts the word that the user is typing with the help of grammar rules for the English language, making word prediction more precise, reducing the number of key taps required, saving the user's time and achieving an optimisation over the existing systems. It also adapts to the user's usage pattern with the help of a frequency model. The metric used here to evaluate the performance of text entry systems is KSPC [1] (keystrokes per character). The KSPC was found to be 0.7360 for PreText, providing an average improvement of 26.91\% over T9 which has a KSPC of 1.023 [1].},
	journal = {Proceedings of the World Congress on …},
	author = {Nalavade, Deepti and Mahule, Tushar and Ketkar, Harshvardhan},
	year = {2008},
	note = {ISBN: 9789881701244},
	keywords = {adaptive, KSPC, predictive, text entry},
	pages = {2--7},
}

@book{Oliphant2006,
	title = {Guide to {NumPy}},
	author = {Oliphant, Travis},
	year = {2006},
}

@book{Jackson2018,
	title = {Secret {Recipes} of the {Python} {Ninja}},
	isbn = {978-1-78829-487-4},
	author = {Jackson, Cody},
	year = {2018},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YPRRDTEP\\Secret_Recipes_of_the_Python_Ninja_f6c3.pdf:application/pdf},
}

@article{Yi2019,
	title = {An enhanced optimization scheme based on gradient descent methods for machine learning},
	volume = {11},
	issn = {20738994},
	doi = {10.3390/sym11070942},
	abstract = {A The learning process of machine learning consists of finding values of unknown weights in a cost function by minimizing the cost function based on learning data. However, since the cost function is not convex, it is conundrum to find the minimum value of the cost function. The existing methods used to find the minimum values usually use the first derivative of the cost function. When even the local minimum (but not a global minimum) is reached, since the first derivative of the cost function becomes zero, the methods give the local minimum values, so that the desired global minimum cannot be found. To overcome this problem, in this paper we modified one of the existing schemes-the adaptive momentum estimation scheme-by adding a new term, so that it can prevent the new optimizer from staying at local minimum. The convergence condition for the proposed scheme and the convergence value are also analyzed, and further explained through several numerical experiments whose cost function is non-convex.},
	number = {7},
	journal = {Symmetry},
	author = {Yi, Dokkyun and Ji, Sangmin and Bu, Sunyoung},
	year = {2019},
	keywords = {Machine learning, Adam optimization, Cost function, Local minimum, Non-convex},
	pages = {1--17},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AYGJMCHS\\An_Enhanced_Optimization_Scheme_Based_on_Gradient_.pdf:application/pdf},
}

@article{Lenhart2012,
	title = {Teens , {Smartphones} \& {Texting}},
	url = {http://pewinternet.org/Reports/2012/Teens-and-smartphones.aspx},
	abstract = {Texting volume is up while the frequency of voice calling is down. About one in four teens say they own smartphones. The volume of texting among teens has risen from 50 texts a day in 2009 to 60 texts for the median teen text user. Older teens, boys, and blacks are leading the increase. Texting is the dominant daily mode of communication between teens and all those with whom they communicate.},
	journal = {Pew Research Center},
	author = {Lenhart, Amanda},
	year = {2012},
	keywords = {Applications, Natural Language Processing},
	pages = {34},
}

@book{Pustejovsky2013,
	title = {Natural language annotation for machine learning},
	isbn = {978-1-4493-0666-3},
	url = {http://it-ebooks.info/book/681/%5Cnpapers3://publication/uuid/906A922E-DE39-4CAB-8067-F222D065ACEF},
	abstract = {Create your own natural language training corpus for machine learning. Whether you're working with English, Chinese, or any other natural language, this hands-on book guides you through a proven annotation development cycle—the process of adding metadata to your training corpus to help ML algorithms work more efficiently. You don't need any programming or linguistics experience to get started. Using detailed examples at every step, you'll learn how the MATTER Annotation Development Process helps you Model, Annotate, Train, Test, Evaluate, and Revise your training corpus. You also get a complete walkthrough of a real-world annotation project. Define a clear annotation goal before collecting your dataset (corpus) Learn tools for analyzing the linguistic content of your corpus Build a model and specification for your annotation project Examine the different annotation formats, from basic XML to the Linguistic Annotation Framework Create a gold standard corpus that can be used to train and test ML algorithms Select the ML algorithms that will process your annotated data Evaluate the test results and revise your annotation task Learn how to use lightweight software for annotating texts and adjudicating the annotations This book is a perfect companion to O'Reilly's Natural Language Processing with Python.},
	author = {Pustejovsky, J and Stubbs, a},
	year = {2013},
	pmid = {25246403},
	doi = {1332788036},
	note = {arXiv: 1011.1669v3
Publication Title: Vasa
ISSN: 1098-6596},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SRP9D3UM\\natural_language_annotation_for_machine_learning.pdf:application/pdf},
}

@article{Deng2020,
	title = {A {Sparse} {Deep} {Factorization} {Machine} for {Efficient} {CTR} prediction},
	url = {http://arxiv.org/abs/2002.06987},
	abstract = {Click-through rate (CTR) prediction is a crucial task in online display advertising and the key part is to learn important feature interactions. The mainstream models are embedding-based neural networks that provide end-to-end training by incorporating hybrid components to model both low-order and high-order feature interactions. These models, however, slow down the prediction inference by at least hundreds of times due to the deep neural network (DNN) component. Considering the challenge of deploying embedding-based neural networks for online advertising, we propose to prune the redundant parameters for the first time to accelerate the inference and reduce the run-time memory usage. Most notably, we can accelerate the inference by 46X on Criteo dataset and 27X on Avazu dataset without loss on the prediction accuracy. In addition, the deep model acceleration makes an efficient model ensemble possible with low latency and significant gains on the performance.},
	author = {Deng, Wei and Pan, Junwei and Zhou, Tian and Flores, Aaron and Lin, Guang},
	year = {2020},
	note = {arXiv: 2002.06987},
	keywords = {chine, deep model acceleration, factorization ma-, field importance, model compression, structural pruning},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YHCCNWA6\\A Sparse Deep Factorization Machine for Efficient CTR prediction.pdf:application/pdf},
}

@article{Siivola2007a,
	title = {On {Growing} and {Pruning} {Kneser} – {Ney} {Smoothed} {N} -{Gram} {Models}},
	volume = {15},
	issn = {15587916},
	doi = {10.1109/TASL.2007.896666},
	abstract = {— -gram models are the most widely used language models in large vocabulary continuous speech recognition. Since the size of the model grows rapidly with respect to the model order and available training data, many methods have been proposed for pruning the least relevant -grams from the model. However, correct smoothing of the -gram probability distri-butions is important and performance may degrade significantly if pruning conflicts with smoothing. In this paper, we show that some of the commonly used pruning methods do not take into account how removing an -gram should modify the backoff distributions in the state-of-the-art Kneser–Ney smoothing. To solve this problem, we present two new algorithms: one for pruning Kneser–Ney smoothed models, and one for growing them incrementally. Experiments on Finnish and English text corpora show that the proposed pruning algorithm provides considerable improvements over previous pruning algorithms on Kneser–Ney-smoothed models and is also better than the baseline entropy pruned Good–Turing smoothed models. The models created by the growing algorithm provide a good starting point for our pruning algorithm, leading to further improvements. The improvements in the Finnish speech recognition over the other Kneser–Ney smoothed models are statistically significant, as well.},
	number = {5},
	journal = {IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
	author = {Siivola, Vesa and Hirsimäki, Teemu and Virpioja, Sami},
	year = {2007},
	keywords = {Applications, Natural Language Processing, Speech recognition, Index Terms—Modeling, Modeling, natural languages, Natural languages, smoothing methods, Smoothing methods, speech recognition},
	pages = {1617--1624},
}

@article{Im2017,
	title = {Empirical {Analysis} of {Deep} {Network} {Loss}},
	abstract = {The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through sev-eral experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.},
	author = {Im, Daniel Jiwoong and Tao, Michael and Branson, Kristin},
	year = {2017},
	note = {arXiv: 1612.04010v1},
	pages = {1--18},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2H26Y3G9\\AN EMPIRICAL ANALYSIS OF DEEP NETWORK LOSS SURFACES.pdf:application/pdf},
}

@article{Chaloner1988,
	title = {A {Bayesian} approach to outlier detection and residual analysis},
	volume = {75},
	issn = {00063444},
	doi = {10.1093/biomet/75.4.651},
	abstract = {An approach to detecting outliers in a linear model is developed. An outlier is defined to be an observation with a large random error, generated by the linear model under consideration. Outliers are detected by examining the posterior distribution of the random errors. An augmented residual plot is also suggested as a graphical aid in finding outliers. ©1988 Biometrika Trust.},
	number = {4},
	journal = {Biometrika},
	author = {Chaloner, Kathryn and Brant, Rollin},
	year = {1988},
	keywords = {Leverage, Linear model, Posterior distribution, Residual plot},
	pages = {651--659},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XP9L3VKF\\Bayesian Approach to Outlier Detection & Residual Analysis.pdf:application/pdf},
}

@article{Tong2013,
	title = {Cross-{Validation}},
	doi = {10.1007/978-1-4419-9863-7_941},
	abstract = {Data Mining, Interference, and Prediction},
	journal = {Encyclopedia of Systems Biology},
	author = {Tong, Joo Chuan},
	year = {2013},
	pages = {508},
}

@techreport{Bengio2012,
	title = {Practical {Recommendations} for {Gradient}-{Based} {Training} of {Deep} {Architectures}},
	url = {http://deeplearning.net/software/pylearn2},
	abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
	author = {Bengio, Yoshua},
	year = {2012},
	note = {arXiv: 1206.5533v2},
	keywords = {Applications, Recommender Systems, (), Clustering, Unsupervised Learning, deepcvr},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YU6MBN2N\\Practical Recommendations for Gradient-Based Training of Deep Architectures.pdf:application/pdf},
}

@techreport{DuchiJDUCHI2011,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization} * {Elad} {Hazan}},
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function , which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regu-larization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	author = {Duchi JDUCHI, John and Singer, Yoram},
	year = {2011},
	note = {Publication Title: Journal of Machine Learning Research
Volume: 12},
	keywords = {online learning, adaptivity, stochastic convex optimization, subgradient methods},
	pages = {2121--2159},
}

@misc{Kennon2015,
	title = {Blog {Demographics} 2015 {Edition}: {If} {Life} {Were} a {Game}, {You} {All} {Would} {Be} {Champions}},
	url = {http://www.joshuakennon.com/blog-demographics-2015-edition-if-life-were-a-game-you-would-be-champions/},
	urldate = {2016-12-13},
	author = {Kennon, Joshua},
	year = {2015},
	keywords = {Applications, Natural Language Processing},
}

@article{Geza2002,
	title = {Multilingual {Statistical} {Text} {Analysis}, {Zipf}'s {Law} and {Hungarian} {Speech} {Generation}},
	volume = {49},
	issn = {1216-8076},
	doi = {10.1556/ALing.49.2002.3-4.8},
	abstract = {The practical challenge of creating a Hungarian e-mail reader has initiated our work on statistical text analysis. The starting point was statistical analysis for automatic discrimination of the language of texts. Later it was extended to automatic re-generation of diacritic signs and more detailed language structure analysis. Parallel study of three different languages -Hungarian. German and English -using text corpora of similar size explores both similarities and differences. Corpora of publicly available Internet sources were used. The corpus size was the same (approximately 20Mbytes, 2.5-3.5 million word forms) for all languages. Besides traditional corpus coverage, word length and occurence statistics, some new features about prosodic boundaries (sentence beginning and final positions, preceding and following a comma) were also computed. Among others, it was found, that the coverage of corpora by the most frequent words follows a parallel logarithmic rule for all languages in the 40-85\% coverage range, known as Zipf's law in linguistics. The functions are much nearer for English and German than for Hungarian. Further conclusions are also drawn. The language detection and diacritic re-generation applications are discussed in detail with implications on Hungarian speech generation. Diverse further application domains, such as predictive text input, word hyphenation, language modeling in speech recognition, corpus-based speech synthesis, etc. are also foreseen.},
	number = {2001},
	journal = {Corpus},
	author = {Geza, Nemeth and {Csaba Zainko}},
	year = {2002},
	keywords = {Applications, Natural Language Processing, language modeling, corpus analysis, corpus-based speech synthesis, multilinguality, text corpora, word length, Zipf's law},
	pages = {3--4},
}

@article{Spitkovsky,
	title = {Capitalization {Cues} {Improve} {Dependency} {Grammar} {Induction}},
	abstract = {We show that orthographic cues can be helpful for unsupervised parsing. In the Penn Tree-bank, transitions between upper-and lower-case tokens tend to align with the boundaries of base (English) noun phrases. Such signals can be used as partial bracketing constraints to train a grammar inducer: in our experiments, directed dependency accuracy increased by 2.2\% (average over 14 languages having case information). Combining capitalization with punctuation-induced constraints in inference further improved parsing performance, attain-ing state-of-the-art levels for many languages.},
	author = {Spitkovsky, Valentin I and Alshawi, Hiyan and Jurafsky, Daniel},
	keywords = {Applications, Natural Language Processing},
}

@misc{Nga,
	title = {Machine {Learning} {\textbar} {Coursera}},
	url = {https://www.coursera.org/learn/machine-learning/home/info},
	abstract = {Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.},
	urldate = {2017-09-16},
	author = {Ng, Andrew},
}

@article{Moore2010,
	title = {Intelligent {Selection} of {Language} {Model} {Training} {Data}},
	abstract = {We address the problem of selecting non- domain-specific language model training data to build auxiliary language models for use in tasks such as machine transla- tion. Our approach is based on comparing the cross-entropy, according to domain- specific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.},
	number = {July},
	journal = {Proceedings of ACL},
	author = {Moore, Robert C and Lewis, William},
	year = {2010},
	note = {ISBN: 9781617388088},
	keywords = {Applications, Natural Language Processing},
	pages = {220--224},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QKF7MXGZ\\Intelligent Selection of Language Model Training Data.pdf:application/pdf},
}

@article{Webroot2019,
	title = {2019 {Webroot} {Threat} {Report}},
	journal = {Webroot},
	author = {{Webroot}},
	year = {2019},
}

@misc{Benoit2016a,
	title = {Quanteda {\textbar} {Quantitative} {Text} {Analysis} with {R}},
	url = {https://kbenoit.github.io/quanteda/intro/overview.html},
	author = {Benoit, Kenneth},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Kurt2016,
	title = {Package ‘ {RWeka} '},
	author = {Kurt, Author and Karatzoglou, Alexandros and Meyer, David},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Definitions,
	title = {in {Procurement}},
	author = {Definitions, Including},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5H6D4KIB\\AI in procurement.pdf:application/pdf},
}

@article{Ahmadi2016,
	title = {Basic notation and terminology in optimization {Optimization} problems},
	author = {Ahmadi, Instructor A A},
	year = {2016},
	pages = {1--13},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MSG4Q35U\\Optimality Conditions for Unconstrained Optimization.pdf:application/pdf},
}

@article{Garcia-Garcia2017,
	title = {A {Review} on {Deep} {Learning} {Techniques} {Applied} to {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1704.06857},
	abstract = {Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.},
	number = {April},
	author = {Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Garcia-Rodriguez, Jose},
	year = {2017},
	note = {arXiv: 1704.06857},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T8J5GLYZ\\A_Review_on_Deep_Learning_Techniques_Applied_to_Se.pdf:application/pdf},
}

@article{Mythily2019,
	title = {A design pattern structure specification to extract statistical report},
	volume = {8},
	issn = {22783075},
	doi = {10.35940/ijitee.J9508.0881019},
	abstract = {Design patterns are ecological abstract documents which offer acceptable solutions for the continual issues. It allows developers to communicate in a well-known way with the system design. This paper reviewing set of research articles to know the current trend on design pattern's applications. This research also introduces the structure of a new pattern that acts as data analyzer of any software given as input. This pattern acts as centralized data storage of concrete class objects. The instances are analyzed by various functionalities of analyzer in-order to derive results. It helps to solve some of the day to day problems faced by the software designer. This pattern provides services for a group of concrete members to design set of functionalities in order to support data analytics. The structure and its applicability on software are examined, using a case study.},
	number = {10},
	journal = {International Journal of Innovative Technology and Exploring Engineering},
	author = {Mythily, M and Nambiar, Radhika and Kethsy Prabhavathy, A and Joseph, Iwin Thanakumar and Deepa Kanmani, S},
	year = {2019},
	keywords = {Application of design patterns, Data analytics, Design pattern},
	pages = {3066--3070},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4GXA2F5T\\A Design Pattern Structure Specification to Extract Statistical Information.pdf:application/pdf},
}

@article{Bustos2018,
	title = {Learning eligibility in cancer clinical trials using deep neural networks},
	volume = {8},
	issn = {20763417},
	doi = {10.3390/app8071206},
	abstract = {Interventional cancer clinical trials are generally too restrictive, and some patients are often excluded on the basis of comorbidity, past or concomitant treatments, or the fact that they are over a certain age. The efficacy and safety of new treatments for patients with these characteristics are, therefore, not defined. In this work, we built a model to automatically predict whether short clinical statements were considered inclusion or exclusion criteria. We used protocols from cancer clinical trials that were available in public registries from the last 18 years to train word-embeddings, and we constructed a dataset of 6M short free-texts labeled as eligible or not eligible. A text classifier was trained using deep neural networks, with pre-trained word-embeddings as inputs, to predict whether or not short free-text statements describing clinical information were considered eligible. We additionally analyzed the semantic reasoning of the word-embedding representations obtained and were able to identify equivalent treatments for a type of tumor analogous with the drugs used to treat other tumors. We show that representation learning using deep neural networks can be successfully leveraged to extract the medical knowledge from clinical trial protocols for potentially assisting practitioners when prescribing treatments.},
	number = {7},
	journal = {Applied Sciences (Switzerland)},
	author = {Bustos, Aurelia and Pertusa, Antonio},
	year = {2018},
	note = {arXiv: 1803.08312},
	keywords = {Natural language processing, Clinical decision support system, Clinical trials, Deep neural networks, Word embeddings},
	pages = {1--19},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HIZVILSK\\Learning Eligibility in Cancer Clinical Trials Using Neural Networks.pdf:application/pdf},
}

@article{Unido,
	title = {An {Object} {Oriented} {Framework} for {Robust} {Multivariate} {Analysis}},
	abstract = {This introduction to the R package rrcov is a (slightly) modified version of Todorov and Filzmoser (2009), published in the Journal of Statistical Software. Taking advantage of the S4 class system of the programming environment R, which facil-itates the creation and maintenance of reusable and modular components, an object ori-ented framework for robust multivariate analysis was developed. The framework resides in the packages robustbase and rrcov and includes an almost complete set of algorithms for computing robust multivariate location and scatter, various robust methods for princi-pal component analysis as well as robust linear and quadratic discriminant analysis. The design of these methods follows common patterns which we call statistical design patterns in analogy to the design patterns widely used in software engineering. The application of the framework to data analysis as well as possible extensions by the development of new methods is demonstrated on examples which themselves are part of the package rrcov.},
	author = {Unido, Valentin Todorov and Filzmoser, Peter},
	keywords = {Object Oriented Programming, Software Engineering, R, MCD, multivariate analysis, robustness, statistical design patterns},
}

@article{Ghaoui2013,
	title = {Convex {Optimization} {Lecture} {Notes} for {EE} {227BT} {Draft}, {Fall} 2013},
	abstract = {Convex Optimization Lecture Notes for EE 227BT Draft, Fall ：凸优化课堂讲稿的EE 227bt草案，秋天EE,帮助,Notes,for,227BT,Draft,notes,Fall,凸优化,反馈意见},
	author = {Ghaoui, Laurent El},
	year = {2013},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NZB7K3PH\\Convex Optimization.pdf:application/pdf},
}

@article{Iizuka2003,
	title = {Oligonucleotide microarray for prediction of early intrahepatic recurrence of hepatocellular carcinoma after curative resection},
	volume = {361},
	issn = {01406736},
	url = {www.thelancet.com},
	doi = {10.1016/S0140-6736(03)12775-4},
	abstract = {Background: Hepatocellular carcinoma has a poor prognosis because of the high intrahepatic recurrence rate. There are technological limitations to traditional methods such as TNM staging for accurate prediction of recurrence, suggesting that new techniques are needed. Methods: We investigated mRNA expression profiles in tissue specimens from a training set, comprising 33 patients with hepatocellular carcinoma, with high-density oligonucleotide microarrays representing about 6000 genes. We used this training set in a supervised learning manner to construct a predictive system, consisting of 12 genes, with the Fisher linear classifier. We then compared the predictive performance of our system with that of a predictive system with a support vector machine (SVM-based system) on a blinded set of samples from 27 newly enrolled patients. Findings: Early intrahepatic recurrence within 1 year after curative surgery occurred in 12 (36\%) and eight (30\%) patients in the training and blinded sets, respectively. Our system correctly predicted early intrahepatic recurrence or non-recurrence in 25 (93\%) of 27 samples in the blinded set and had a positive predictive value of 88\% and a negative predictive value of 95\%. By contrast, the SVM-based system predicted early intrahepatic recurrence or non-recurrence correctly in only 16 (60\%) individuals in the blinded set, and the result yielded a positive predictive value of only 38\% and a negative predictive value of 79\%. Interpretation: Our system predicted early intrahepatic recurrence or non-recurrence for patients with hepatocellular carcinoma much more accurately than the SVM-based system, suggesting that our system could serve as a new method for characterising the metastatic potential of hepatocellular carcinoma.},
	number = {9361},
	journal = {Lancet},
	author = {Iizuka, Norio and Oka, Masaaki and Yamada-Okabe, Hisafumi and Nishida, Minekatsu and Maeda, Yoshitaka and Mori, Naohide and Takao, Takashi and Tamesa, Takao and Tangoku, Akira and Tabuchi, Hisahiro and Hamada, Kenji and Nakayama, Hironobu and Ishitsuka, Hideo and Miyamoto, Takanobu and Hirabayashi, Akira and Uchimura, Shunji and Hamamoto, Yoshihiko},
	year = {2003},
	pmid = {12648972},
	note = {ISBN: 0140-6736 LA - eng PT - Journal Article},
	pages = {923--929},
}

@article{Barry2011,
	title = {Doing {Bayesian} {Data} {Analysis}: {A} {Tutorial} with {R} and {BUGS}},
	volume = {7},
	issn = {1841-0413},
	doi = {10.5964/ejop.v7i4.163},
	abstract = {Although Bayesian models of mind have attracted great interest from cognitive scientists, Bayesian methods for data analysis have not. This article reviews several advantages of Bayesian data analysis over traditional null-hypothesis significance testing. Bayesian methods provide tremendous flexibility for data analytic models and yield rich information about parameters that can be used cumulatively across progressive experiments. Because Bayesian statistical methods can be applied to any data, regardless of the type of cognitive model (Bayesian or otherwise) that motivated the data collection, Bayesian methods for data analysis will continue to be appropriate even if Bayesian models of mind lose their appeal.},
	number = {4},
	journal = {Europe's Journal of Psychology},
	author = {Barry, John},
	year = {2011},
}

@article{Cauchy1847Methode,
	title = {Méthode générale pour la résolution des systèmes d'équations simultanées},
	volume = {S'erie A},
	number = {25},
	journal = {Compte Rendu des S'eances de L'Acad'emie des Sciences XXV},
	author = {Cauchy, Augustin-Louis},
	month = oct,
	year = {1847},
	keywords = {first, steepest-descent},
	pages = {536--538},
}

@article{Luyckx,
	title = {Shallow {Text} {Analysis} and {Machine} {Learning} for {Authorship} {Attri}- bution},
	abstract = {Current advances in shallow parsing and machine learning allow us to use results from these fields in a methodology for Authorship Attribution. We report on experiments with a corpus that consists of newspaper articles about national current affairs by different journalists from the Belgian newspaper De Standaard. Because the documents are in a similar genre, register, and range of topics, token-based (e.g., sentence length) and lexical features (e.g., vocabulary richness) can be kept roughly constant over the different authors. This allows us to focus on the use of syntax-based features as possible predictors for an author's style, as well as on those token-based features that are predictive to author style more than to topic or register. These style characteristics are not under the author's conscious control and therefore good clues for Authorship Attribution. Machine Learning methods (TiMBL and the WEKA software package) are used to select informative combinations of syntactic, token-based and lexical features and to predict authorship of unseen documents. The combination of these features can be considered an implicit profile that characterizes the style of an author.},
	author = {Luyckx, Kim and Daelemans, Walter},
	keywords = {Applications, Natural Language Processing},
}

@article{Taigman2014,
	title = {{DeepFace}: {Closing} the gap to human-level performance in face verification},
	issn = {10636919},
	doi = {10.1109/CVPR.2014.220},
	abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ={\textgreater}align ={\textgreater}represent ={\textgreater}classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35\% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27\%, closely approaching human-level performance.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
	year = {2014},
	note = {ISBN: 9781479951178},
	pages = {1701--1708},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NVYKI97T\\deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf:application/pdf},
}

@misc{CommonStopWords,
	title = {Common {Stop} {Words}},
	url = {https://code.google.com/archive/p/stop-words/downloads},
}

@article{Lin2014,
	title = {Network in network},
	abstract = {We propose a novel deep network structure called “Network In Network”(NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
	journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
	author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
	year = {2014},
	note = {arXiv: 1312.4400v3},
	pages = {1--10},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IFXATHM9\\Network in Network.pdf:application/pdf},
}

@article{Lennox1993,
	title = {The cost of depression-complicated alcoholism: {Health}-{Care} utilization and treatment effectiveness},
	volume = {20},
	issn = {1556-3308},
	url = {http://dx.doi.org/10.1007/BF02519238},
	doi = {10.1007/BF02519238},
	abstract = {Clinical and epidemiologic evidence suggests that alcoholism complicated by concurrent or a lifetime history of depression is slower to remit and more prone to relapse than uncomplicated alcoholism. Consequently, alcoholics with a history of depressive illness, on average, are likely to use more health care and to have higher treatment costs than those without depression complications. This article contrasts evidence of the suitability of three models for predicting the impact of depression on an alcoholic's health-care use: a null model (assuming no differences) a cumulative-effect model (arguing for a linear increase associated with comorbid depression), and a synergistic model (wherein alcoholism complicated with depression is qualitatively as well as quantitatively different than uncomplicated alcoholism). To test these models, health-care costs and utilization of 491 ``pure'' alcoholics (those with no history of depression diagnosis) and 199 depression-complicated alcoholics, who received alcohol treatment while enrolled in a self-insured health-care program of a major U.S. manufacturing company, were compared. Results are discussed in terms of the implications for cost containment and the likelihood of relapse among the depression-complicated alcoholism group.},
	number = {2},
	journal = {The journal of mental health administration},
	author = {Lennox, Richard D and Scott-Lennox, Jane A and Bohlig, E Michael},
	year = {1993},
	pages = {138--152},
}

@misc{mathstack2018,
	title = {calculus - {Proof} that derivative is the best linear approximation? - {Mathematics} {Stack} {Exchange}},
	url = {https://math.stackexchange.com/questions/3551202/proof-that-derivative-is-the-best-linear-approximation},
	urldate = {2020-05-10},
}

@inproceedings{1716540,
	title = {On the {Probability} of {Finding} {Local} {Minima} in {Optimization} {Problems}},
	doi = {10.1109/IJCNN.2006.247318},
	abstract = {The standard method in optimization problems consists in a random search of the global minimum: a neuron network relaxes in the nearest local minimum from some randomly chosen initial configuration. This procedure is to be repeated many times in order to find as deep energy minimum as possible. However the question about the reasonable number of such random starts and if the result of the search can be treated as successful remains always open. In this paper by analyzing the generalized Hopfield model we obtain expressions, which yield the relationship between the depth of a local minimum and the size of the basin of attraction. Based on this, we present the probability of finding a local minimum as a function of the depth of the minimum. Such a relation can be used in optimization applications: it allows one, basing on a series of already found minima, to estimate the probability of finding a deeper minimum, and decide in favor of or against further running the program. The theory is in a good agreement with experimental results.},
	booktitle = {The 2006 {IEEE} {International} {Joint} {Conference} on {Neural} {Network} {Proceedings}},
	author = {Kryzhanovsky, Boris and Magomedov, Bashir and Fonarev, Anatoly},
	month = jul,
	year = {2006},
	note = {ISSN: 2161-4393},
	keywords = {Hopfield neural nets, optimisation, probability, rand},
	pages = {3243--3248},
}

@article{Matiasek2003,
	title = {Exploiting {Long} {Distance} {Collocational} {Relations} in {Predictive} {Typing}},
	abstract = {In this paper, we report about some preliminary experiments in which we tried to improve the performance of a state-of-the-art Predictive Typing sys-tem for the German language by adding a collocation-based prediction compo-nent. This component tries to ex-ploit the fact that texts have a topic and are semantically coherent. Thus, the appearance in a text of a certain word can be a cue that other, semanti-cally related words are likely to appear soon. The collocation-based module ex-ploits this kind of topical/semantic re-latedness by relying on statistics about the co-occurrence of words within a large window of text in the training corpus. Our current experimental re-sults indicate that using the collocation-based prediction module has a small but consistent positive effect on the perfor-mance of the system.},
	journal = {Proceedings of the EACL Workshop on Language Modeling for Text Entry Methods},
	author = {Matiasek, Johannes and Baroni, Marco},
	year = {2003},
	keywords = {Applications, Natural Language Processing, word completion, text completion},
	pages = {1--8},
}

@article{Mandelbrot1953,
	title = {An {Informational} {Theory} of the {Statistical} {Structure} of {Language}},
	number = {2},
	journal = {Communication Theory},
	author = {Mandelbrot, Benoit},
	year = {1953},
	keywords = {Applications, Natural Language Processing},
	pages = {486--502},
}

@article{Al-Mubaid,
	title = {Context-{Based} {Word} {Prediction} and {Classification}},
	author = {Al-Mubaid, Hisham},
	keywords = {Applications, Natural Language Processing},
	file = {Context-Based Word Prediction and Classification.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Context-Based Word Prediction and Classification.pdf:application/pdf},
}

@article{Press2018,
	title = {Top 10 {Hot} {Artificial} {Intelligence} ({AI}) {Technologies}},
	url = {https://www.forbes.com/sites/gilpress/2017/01/23/top-10-hot-artificial-intelligence-ai-technologies/#4f8ff2821928},
	abstract = {Based on Forrester's analysis in its latest TechRadar report, here's my list of the 10 hottest AI technologies.},
	number = {4},
	urldate = {2018-08-19},
	journal = {Forbes/ Tech},
	author = {Press, Gil},
	year = {2018},
	keywords = {Applications, Natural Language Processing},
}

@article{Moniz2018,
	title = {Multi-{Source} {Social} {Feedback} of {Online} {News} {Feeds}},
	url = {http://arxiv.org/abs/1801.07055},
	abstract = {The profusion of user generated content caused by the rise of social media platforms has enabled a surge in research relating to fields such as information retrieval, recommender systems, data mining and machine learning. However, the lack of comprehensive baseline data sets to allow a thorough evaluative comparison has become an important issue. In this paper we present a large data set of news items from well-known aggregators such as Google News and Yahoo! News, and their respective social feedback on multiple platforms: Facebook, Google+ and LinkedIn. The data collected relates to a period of 8 months, between November 2015 and July 2016, accounting for about 100,000 news items on four different topics: economy, microsoft, obama and palestine. This data set is tailored for evaluative comparisons in predictive analytics tasks, although allowing for tasks in other research areas such as topic detection and tracking, sentiment analysis in short text, first story detection or news recommendation.},
	author = {Moniz, Nuno and Torgo, Luís},
	year = {2018},
	note = {arXiv: 1801.07055},
	pages = {1--8},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VLIT67PG\\Multi-Source Social Feedback of Online News Feeds.pdf:application/pdf},
}

@inproceedings{Wolpert2001,
	title = {The {Supervised} {Learning} {No}-{Free}-{Lunch} {Theorems}},
	doi = {10.1007/978-1-4471-0123-9_3},
	booktitle = {Proceedings of the 6th {Online} {World} {Conference} on {Soft} {Computing} in {Industrial} {Applications}},
	author = {Wolpert, David},
	year = {2001},
}

@article{Outline,
	title = {Introduction to {Mathematical} {Optimization}},
	abstract = {Engineering tools and mathematical optimization are applied in this study to plan the work of the agents of the cow artificial insemination service (inseminator) in Israel. Time is crucial in insemination as the chances of conception decline with increasing delay between the start of estrus and insemination. About 1,090 artificial inseminations of cows are performed daily in Israel. They involve 412 farms in 283 villages, and are performed by 29 inseminators; the work plan should balance the work load among the inseminators. To this end, the working time of an inseminator in each village is required. Thus, a model to predict the working time in a village was developed. Subsequently, a mathematical optimization model was designed and solved, which aims to allocate customers to trips and to determine the itinerary of each trip to minimize total distance/time. The main benefits included a 21.4\% reduction in total traveling time and a 55\% reduction in the difference between the lengths of the longest and shortest working days. Moreover, the longest delay in reaching an estrous cow is reduced from 7.6 to 5.9 h (i.e., by 1.7 h), which may increase the conception ratio by some 7\%. In addition, the trade-off between work balance and total traveling time was studied.},
	author = {Outline, Course},
}

@article{Simon2003,
	title = {Health care costs of primary care patients with recognized depression},
	volume = {54},
	journal = {Biological Psychiatry},
	author = {Simon, G E and VonKorff, M and Barlow, W},
	year = {2003},
	pages = {216--226},
}

@article{bloom,
	title = {Taxonomy of educational objectives : {The} classification of educational goals},
	url = {https://ci.nii.ac.jp/naid/10025289849/en/},
	journal = {Cognitive domain},
	author = {BLOOM, B S},
	year = {1956},
	note = {Publisher: Longman},
}

@article{CS474NaturalLanguage,
	title = {{CS474} {Natural} {Language} {Processing} {HMM} s for p-o-s tagging},
}

@misc{Avvo,
	title = {New {Avvo} survey explores modern attitudes around love, sex, and dating - {AvvoStories}},
	url = {http://stories.avvo.com/relationships/new-avvo-survey-explores-modern-attitudes-around-love-sex-dating.html},
	urldate = {2017-10-27},
	author = {{Avvo}},
}

@article{Young2018,
	title = {Recent trends in deep learning based natural language processing [{Review} {Article}]},
	volume = {13},
	issn = {15566048},
	doi = {10.1109/MCI.2018.2840738},
	abstract = {Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.},
	number = {3},
	journal = {IEEE Computational Intelligence Magazine},
	author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
	year = {2018},
	note = {arXiv: 1708.02709v7},
	keywords = {and engineering, ek laboratories, nanyang technological university, school of computer science, singapore},
	pages = {55--75},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JX2SC8NC\\Recent Trends in Deep Learning Based Natural Language Processingr.pdf:application/pdf},
}

@article{Reid2004a,
	title = {The social and psychological effects of {Text} {Insights} into the {Social} and {Psychological} {Effects} of {SMS} {Text} {Messaging}},
	url = {www.plymouth.ac.uk},
	abstract = {The increasingly widespread use of text -messaging has led to the questioning of the social and psychological effects of this novel communication medium. A selection of findings from an online questionnaire that was developed by the author to answer this pertinent question are presented. McKenna's recent work on the way the Internet can help some people develop relationships is drawn upon and taken a step further by exploring the differences between those who prefer texting ('Texters') and those who prefer talking on their mobiles ('Talkers'). A large sample of 982 respondents completed the questionnaire. Results showed there was a clear distinction between Texters and Talkers in the way they used their mobiles and their underlying motivations. The key finding to emerge in the preliminary analyses was that Texters seemed to form close knit 'text circles' with their own social ecology, interconnecting with a close group of friends in perpetual text contact. Compared to Talkers, Texters were found to be more lonely and socially anxious, and more likely to disclose their 'real-self' through text than via face-to-face or voice call exchanges. S tructural equation modeling produced a model showing that where respondents located their real-self and whether they were a Texter or a Talker mediated between the loneliness and social anxiety measures and the impact of these on relational outcomes, in line with McKenna's theoretical framework. Thus it appears that there is something special about texting that allows some people to translate their loneliness and/or social anxiety into productive relationships whilst for others the mobile does not afford the same effect. .Applications and explorations for future research are discussed.},
	author = {Reid, Donna and Reid, Fraser},
	year = {2004},
	keywords = {Applications, Natural Language Processing},
}

@article{Ghosh2017,
	title = {On the {Use} of {Cauchy} {Prior} {Distributions} for {Bayesian} {Logistic} {Regression}},
	abstract = {In logistic regression, separation occurs when a linear combination of the predictors can perfectly classify part or all of the observations in the sample, and as a result, finite maximum likelihood estimates of the regression coefficients do not exist. Gelman et al. (2008) recommended independent Cauchy distributions as default priors for the regression coefficients in logistic regression, even in the case of separation, and reported posterior modes in their analyses. As the mean does not exist for the Cauchy prior, a natural question is whether the posterior means of the regression coefficients exist under separation. We prove theorems that provide necessary and sufficient conditions for the existence of posterior means under independent Cauchy priors for the logit link and a general family of link functions, including the probit link. We also study the existence of posterior means under multivariate Cauchy priors. For full Bayesian inference, we develop a Gibbs sampler based on Pólya-Gamma data augmentation to sample from the posterior distribution under independent Student-t priors including Cauchy priors, and provide a companion R package in the supplement. We demonstrate empirically that even when the posterior means of the regression coefficients exist under separation, the magnitude of the posterior samples for Cauchy priors may be unusually large, and the corresponding Gibbs sampler shows extremely slow mixing. While alternative algorithms such as the No-U-Turn Sampler in Stan can greatly improve mixing, in order to resolve the issue of extremely heavy tailed posteriors for Cauchy priors under separation, one would need to consider lighter tailed priors such as normal priors or Student-t priors with degrees of freedom larger than one.},
	author = {Ghosh, Joyee and Li, Yingbo and Mitra, Robin},
	year = {2017},
	note = {arXiv: 1507.07170v2},
	keywords = {Algorithms, Bayesian},
}

@article{Byrne2017,
	title = {Ciara {Byrne} {Development} {Workflows} for {Data} {Scientists}},
	url = {http://oreilly.com/safari},
	journal = {O'Reilly},
	author = {Byrne, Ciara and Farnham, Boston and Tokyo, Sebastopol and Boston, Beijing and Sebastopol, Farnham and Beijing, Tokyo},
	year = {2017},
	note = {ISBN: 978-1-491-98330-0},
	keywords = {data science, workflow},
}

@book{Katamba2005,
	title = {English words : structure, history, usage},
	isbn = {0-415-29893-8},
	abstract = {2nd ed. 'English Words' assumes no prior knowledge of linguistics in introducing the vocabulary of modern English usage. It covers meaning, history, pronunciation \& the structure of words. This new edition has been extensively updated with new chapters, new exercises, an improved index \& links to web resources. 1. Introduction -- 2. What is a word? -- 3. Close encounters of a morphemic kind -- 4. Building words -- 5. lexicon with layers -- 6. Word meaning -- 7. lexical mosaic : sources of English vocabulary -- 8. Words galore : innovation and change -- 9. Should English be spelt as she is spoke? -- 10. Speech recognition -- 11. Speech production.},
	publisher = {Routledge},
	author = {Katamba, Francis},
	year = {2005},
	keywords = {Applications, Natural Language Processing},
}

@book{Cormen2001,
	title = {Introduction to {Algorithms}, {Second} {Edition}},
	volume = {7},
	isbn = {0-262-03293-7},
	url = {https://labs.xjtudlc.com/labs/wldmt/reading},
	author = {Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford and London, Massachusetts and Company, Mcgraw-hill Book and Ridge, Boston Burr},
	year = {2001},
	pmid = {22929350},
	doi = {10.2307/2583667},
	note = {arXiv: 2010 (ret. 29.4.2010)
ISSN: 15580768},
	keywords = {0262033844, 0262533057, 9780262033848, 9780262533058},
}

@misc{lasagne,
	title = {Lasagne: {First} release.},
	url = {http://dx.doi.org/10.5281/zenodo.27878},
	author = {Dieleman, Sander and Schlüter, Jan and Raffel, Colin and Olson, Eben and Sønderby, Søren Kaae and Nouri, Daniel and Maturana, Daniel and Thoma, Martin and Battenberg, Eric and Kelly, Jack and Fauw, Jeffrey De and Heilman, Michael and de Almeida, Diogo Moitinho and McFee, Brian and Weideman, Hendrik and Takács, Gábor and de Rivaz, Peter and Crall, Jon and Sanders, Gregory and Rasul, Kashif and Liu, Cong and French, Geoffrey and Degrave, Jonas},
	month = aug,
	year = {2015},
	doi = {10.5281/zenodo.27878},
}

@book{Kuhn2013,
	title = {Applied predictive modeling},
	isbn = {978-1-4614-6849-3},
	abstract = {Applied Predictive Modeling covers the overall predictive modeling process, beginning with the crucial steps of data preprocessing, data splitting and foundations of model tuning. The text then provides intuitive explanations of numerous common and modern regression and classification techniques, always with an emphasis on illustrating and solving real data problems. The text illustrates all parts of the modeling process through many hands-on, real-life examples, and every chapter contains extensive R code for each step of the process. This multi-purpose text can be used as an introduction to predictive models and the overall modeling process, a practitioner's reference handbook, or as a text for advanced undergraduate or graduate level predictive modeling courses. To that end, each chapter contains problem sets to help solidify the covered concepts and uses data available in the book's R package. This text is intended for a broad audience as both an introduction to predictive models as well as a guide to applying them. Non-mathematical readers will appreciate the intuitive explanations of the techniques while an emphasis on problem-solving with real data across a wide variety of applications will aid practitioners who wish to extend their expertise. Readers should have knowledge of basic statistical ideas, such as correlation and linear regression analysis. While the text is biased against complex equations, a mathematical background is needed for advanced topics.},
	author = {Kuhn, Max and Johnson, Kjell},
	year = {2013},
	doi = {10.1007/978-1-4614-6849-3},
	note = {Publication Title: Applied Predictive Modeling},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2A6HXQKI\\2013_Book_AppliedPredictiveModeling.pdf:application/pdf},
}

@article{Good1953,
	title = {The population frequencies of spiecies and the estimation of population parameters},
	volume = {40},
	issn = {0006-3444},
	url = {http://links.jstor.org/sici?sici=0006-3444%28195312%2940%3A3%2F4%3C237%3ATPFOSA%3E2.0.CO%3B2-K},
	doi = {10.1093/biomet/40.3-4.237},
	abstract = {A random sample is drawn from a population of animals of various species. (The theory may also be applied to studies of literary vocabulary, for example.) If a particular species is represented r times in the sample of size N, then r/N is not a good estimate of the population frequency, p, when r is small. Methods are given for estimating p, assuming virtually nothing about the underlying population. The estimates are expressed in terms of smoothed values of the numbers nr (r= 1, 2, 3, ...), where nr is the number of distinct species that are each represented r times in the sample. (nr may be described as ‘the frequency of the frequency r'.) Turing is acknowledged for the most interesting formula in this part of the work. An estimate of the proportion of the population represented by the species occurring in the sample is an immediate corollary. Estimates are made of measures of heterogeneity of the population, including Yule's ‘characteristic' and Shannon's ‘entropy'. Methods are then discussed that do depend on assumptions about the underlying population. It is here that most work has been done by other writers. It is pointed out that a hypothesis can give a good fit to the numbers nr but can give quite the wrong value for Yule's characteristic. An example of this is Fisher's fit to some data of Williams's on Macrolepidoptera.},
	number = {3-4},
	journal = {Biometrika},
	author = {Good, I J},
	year = {1953},
	keywords = {Applications, Natural Language Processing},
	pages = {237--264},
}

@article{Yao2007,
	title = {On early stopping in gradient descent learning},
	volume = {26},
	issn = {01764276},
	doi = {10.1007/s00365-006-0663-2},
	abstract = {In this paper we study a family of gradient descent algorithms to approximate the regression function from reproducing kernel Hilbert spaces (RKHSs), the family being characterized by a polynomial decreasing rate of step sizes (or learning rate). By solving a bias-variance trade-off we obtain an early stopping rule and some probabilistic upper bounds for the convergence of the algorithms. We also discuss the implication of these results in the context of classification where some fast convergence rates can be achieved for plug-in classifiers. Some connections are addressed with Boosting, Landweber iterations, and the online learning algorithms as stochastic approximations of the gradient descent method. ©2007 Springer.},
	number = {2},
	journal = {Constructive Approximation},
	author = {Yao, Yuan and Rosasco, Lorenzo and Caponnetto, Andrea},
	year = {2007},
	keywords = {Boosting, Early stopping, Gradient descent method, Landweber iteration, Regularization, Reproducing kernel Hilbert space},
	pages = {289--315},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XJZSMMU7\\On_Early_Stopping_in_Gradient_Descent_Learning.pdf:application/pdf},
}

@article{LanguageModelingProbability,
	title = {Language modeling and probability},
	keywords = {Applications, Natural Language Processing},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UR3GT9KH\\Language Modeling and Probability.pdf:},
}

@article{Reynaert,
	title = {Corpus-{Induced} {Corpus} {Clean}-up},
	abstract = {We explore the feasibility of using only unsupervised means to identify non-words, i.e. typos, in a frequency list derived from a large corpus of Dutch and to distinguish between these non-words and real-words in the language. We call the system we built and evaluate in this paper CICCL, which stands for 'Corpus-Induced Corpus Clean-up'. The algorithm on which CICCL is primarily based is the anagram-key hashing algorithm introduced by (Reynaert, 2004). The core correction mechanism is a simple and effective method which translates the actual characters which make up a word into a large natural number in such a way that all the anagrams, i.e. all the words composed of precisely the same subset of characters, are allocated the same natural number. In effect, this constitutes a novel approximate string matching algorithm for indexed text search. This is because by simple addition, subtraction or a combination of both, all variants within reach of the range of numerical values defined in the alphabet are retrieved by iterating over the alphabet. CICCL's input consists primarily of corpus derived frequency lists, from which it derives valuable morphological and compounding information by performing frequency counts over the substrings of the words. These counts are then used to perform decompounding, as well as for distinguishing between most likely correctly spelled words and typos.},
	author = {Reynaert, Martin},
	keywords = {Applications, Natural Language Processing},
}

@book{BULM37,
	address = {Cambridge, Mass.},
	edition = {2. ed.},
	title = {Principles of statistics},
	url = {http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA&SRT=YOP&IKT=1016&TRM=ppn+568832809&sourceid=fbw_bibsonomy},
	publisher = {M.I.T. Press},
	author = {Bulmer, Michael George},
	year = {1967},
	keywords = {Mathematics, Statistics, Biometrie Statistik},
}

@article{StefanEvert2015,
	title = {Package "{zipfR}"},
	abstract = {Statistical models and utilities for the analysis of word frequency distributions. The utilities include functions for loading, manipulating and visualizing word frequency data and vocabulary growth curves. The package also implements several statistical models for the distribution of word frequencies in a population. (The name of this library derives from the most famous word frequency distribution, Zipf's law.)},
	journal = {R Package},
	author = {Evert, Stefan},
	year = {2015},
	keywords = {Applications, Natural Language Processing},
	pages = {94},
}

@misc{Gartner2018,
	title = {Chatbots {Will} {Appeal} to {Modern} {Workers}},
	url = {https://www.gartner.com/smarterwithgartner/chatbots-will-appeal-to-modern-workers/},
	abstract = {Chatbots are on the rise thanks to their ability to mimic conversations and offer instant, digital connections.},
	urldate = {2018-08-19},
	author = {{Gartner}},
	year = {2018},
	note = {Publication Title: Technology Trends - Smarter With Gartner},
	keywords = {Applications, Natural Language Processing},
}

@article{Montemurro2001,
	title = {Beyond the {Zipf}-{Mandelbrot} law in quantitative linguistics},
	volume = {300},
	issn = {03784371},
	url = {www.elsevier.com/locate/physa},
	doi = {10.1016/S0378-4371(01)00355-7},
	abstract = {In this paper the Zipf-Mandelbrot law is revisited in the context of linguistics. Despite its widespread popularity the Zipf-Mandelbrot law can only describe the statistical behaviour of a rather restricted fraction of the total number of words contained in some given corpus. In particular, we focus our attention on the important deviations that become statistically relevant as larger corpora are considered and that ultimately could be understood as salient features of the underlying complex process of language generation. Finally, it is shown that all the different observed regimes can be accurately encompassed within a single mathematical framework recently introduced by C. Tsallis. ?? 2001 Elsevier Science B.V. All rights reserved.},
	number = {3-4},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Montemurro, Marcelo A},
	year = {2001},
	pmid = {172092800018},
	note = {arXiv: cond-mat/0104066},
	keywords = {Applications, Natural Language Processing, Human language, Zipf-Mandelbrot law},
	pages = {567--578},
}

@article{FernahNdez2001,
	title = {Benchmark priors for {Bayesian} model averaging},
	volume = {100},
	url = {https://eclass.aueb.gr/modules/document/file.php/OIK164/Fernandez},
	abstract = {In contrast to a posterior analysis given a particular sampling model, posterior model probabilities in the context of model uncertainty are typically rather sensitive to the speci"cation of the prior. In particular, \&di!use' priors on model-speci"c parameters can lead to quite unexpected consequences. Here we focus on the practically relevant situation where we need to entertain a (large) number of sampling models and we have (or wish to use) little or no subjective prior information. We aim at providing an \&automatic' or \&benchmark' prior structure that can be used in such cases. We focus on the normal linear regression model with uncertainty in the choice of regressors. We propose a partly non-informative prior structure related to a natural conjugate g-prior speci"cation, where the amount of subjective information requested from the user is limited to the choice of a single scalar hyperparameter g H . The consequences of di!erent choices for g H are examined. We investigate theoretical properties, such as consistency of the implied Bayesian procedure. Links with classical information criteria are provided. More importantly, we examine the "nite sample implications of several choices of g H in a simulation study. The use of the MC algorithm of Madigan and York (Int. Stat. Rev. 63 (1995) 215), combined with e\$cient coding in Fortran, makes it feasible to conduct large simulations. In addition to posterior criteria, we shall also compare the predictive performance of di!erent priors. A classic example concerning the economics of crime will also be provided and contrasted with results in the literature. The main "ndings of the 0304-4076/01/\$ -see front matter 2001 Elsevier Science S.A. All rights reserved. PII: S 0 3 0 4 -4 0 7 6 (0 0) 0 0 0 7 6 -2},
	journal = {Journal of Econometrics},
	author = {Fernah Ndez, Carmen and Ley, Eduardo and Steel, Mark F J},
	year = {2001},
	keywords = {Algorithms, Bayesian},
	pages = {381--427},
}

@article{Hodges1987,
	title = {Uncertainty, {Policy} {Analysis} and {Statisitcs}},
	volume = {2},
	number = {3},
	journal = {Statistical Science},
	author = {Hodges, James S},
	year = {1987},
	keywords = {Algorithms, Bayesian},
	pages = {259--291},
}

@article{Kuo2011,
	title = {On {Word} {Prediction} {Methods}},
	url = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2011/EECS-2011-147.html},
	abstract = {This paper evaluates prediction and topic modelling methods through the task of word pre-diction. In our word prediction experiment, we compare some existing and two novel methods, including a version of Cooccurrence, two versions of K-Nearest-Neighbor method and Latent semantic indexing[5], against a baseline algorithm. Furthermore, we explore the effects of us-ing different similarity functions on the accuracies of our prediction methods. Finally, without much modifications to the framework, we were also able to perform tag classification on Stack-Overflow posts.},
	author = {Kuo, Darren and Zhao, Chenyu and Wang, Di and Cook, James and Rao, Satish},
	year = {2011},
	keywords = {Applications, Natural Language Processing},
}

@article{Geoff2012,
	title = {Lecture 5 : {Gradient} {Desent} {Revisited} {Choose} step size},
	journal = {Notes},
	author = {Geoff, Lecturer and Ryan, Gordon},
	year = {2012},
	pages = {1--10},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GJ7I4X8S\\Lecture 5 - Gradient Desent Revisited.pdf:application/pdf},
}

@article{Wallach,
	title = {Topic {Modeling}: {Beyond} {Bag}-of-{Words}},
	abstract = {Some models of textual corpora employ text generation methods involving n-gram statis-tics, while others use latent topic variables inferred using the " bag-of-words " assump-tion, in which word order is ignored. Pre-viously, these methods have not been com-bined. In this work, I explore a hierarchi-cal generative probabilistic model that in-corporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchi-cal Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs EM algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hi-erarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by func-tion words than are topics discovered using unigram statistics, potentially making them more meaningful.},
	author = {Wallach, Hanna M},
	keywords = {Applications, Natural Language Processing},
}

@article{Bellegarda,
	title = {Exploiting {Latent} {Semantic} {Information} in {Statistical} {Language} {Modeling}},
	abstract = {Statistical language models used in large-vocabulary speech recognition must properly encapsulate the various constraints, both local and global, present in the language. While local constraints are readily captured through n-gram modeling, global constraints, such as long-term semantic dependencies, have been more diffi-cult to handle within a data-driven formalism. This paper focuses on the use of latent semantic analysis, a paradigm that automat-ically uncovers the salient semantic relationships between words and documents in a given corpus. In this approach, (discrete) words and documents are mapped onto a (continuous) semantic vector space, in which familiar clustering techniques can be applied. This leads to the specification of a powerful framework for automatic semantic classification, as well as the derivation of several lan-guage model families with various smoothing properties. Because of their large-span nature, these language models are well suited to complement conventional n-grams. An integrative formulation is proposed for harnessing this synergy, in which the latent se-mantic information is used to adjust the standard n-gram proba-bility. Such hybrid language modeling compares favorably with the corresponding n-gram baseline: experiments conducted on the Wall Street Journal domain show a reduction in average word error rate of over 20\%. This paper concludes with a discussion of intrinsic tradeoffs, such as the influence of training data selection on the re-sulting performance. Keywords—Latent semantic analysis, multispan integration, n-grams, speech recognition, statistical language modeling.},
	author = {Bellegarda, Jerome R},
	keywords = {Applications, Natural Language Processing},
}

@article{Duggan2013,
	title = {Cell {Phone} {Activities} 2013},
	url = {http://pewinternet.org/%7B~%7D/media//Files/Reports/2013/PIP_Cell},
	abstract = {Fully 91\% of American adults own a cell phone and many use the devices for much more than phone calls. In our most recent nationally representative survey, we checked in on some of the most popular activities people perform on their cell phones: Cell phone activities The \% of cell phone owners who use their cell phone to…81 send or receive text messages 60 access the internet 52 send or receive email 50 download apps 49 get directions, recommendations, or other location-based information 48 listen to music 21 participate in a video call or video chat 8 “check in” or share your location Source: Pew Research Center's Internet \& American Life Project Spring Tracking Survey, April 17 – May 19, 2013. N=2,076 cell phone owners. Interviews were conducted in English and Spanish and on landline and cell phones. The margin of error for results based on all cell phone owners is +/- 2.4 percentage points. Texting, accessing the internet and sending and receiving email remain popular. Some 50\% of cell owners download apps—up from 22\% in 2009. Many use certain location-based services like getting directions or recommendations. Nearly half of cell owners (48\%) use their phones to listen to music. The proportion of cell owners who use video calling has tripled since May 2011. Overall, almost all activities have seen steady upward growth over time},
	author = {Duggan, Maeve},
	year = {2013},
	keywords = {Applications, Natural Language Processing},
	pages = {18},
}

@book{Adger2003,
	address = {Oxford},
	title = {Core syntax : a minimalist approach},
	isbn = {0-19-924370-0},
	abstract = {1. Core Concepts -- 2. Morphosyntactic Features -- 3. Constituency and Theta Roles -- 4. Representing Phrase Structure -- 5. Functional Categories I -- TP -- 6. Subjects and Objects -- 7. Functional Categories II -- the DP 8. Functional Categories III -- CP -- 9. Wh-movement -- 10. Locality.},
	publisher = {Oxford University Press},
	author = {Adger, David.},
	year = {2003},
	keywords = {Applications, Natural Language Processing},
}

@article{Box1982,
	title = {An {Analysis} of {Transformations} {Revisited}, {Rebutted}},
	volume = {77},
	issn = {01621459},
	doi = {10.2307/2287791},
	abstract = {In the analysis of data it is often assumed that observations y,, y,,...,y, are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters 0. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples. 1.},
	number = {377},
	journal = {Journal of the American Statistical Association},
	author = {Box, G E P and Cox, D R},
	year = {1982},
	pmid = {16703375},
	note = {arXiv: 1011.1669v3
ISBN: EE000178 00359246 DI993152 99P02493},
	pages = {209},
}

@article{Raschka2018,
	title = {Model evaluation , model selection , and algorithm selection in machine learning {Performance} {Estimation} : {Generalization} {Performance} {Vs} . {Model} {Selection}},
	abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. In the context of discussing the bias-variance trade-off, leave-one-out cross-validation is compared to k-fold cross-validation, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm com-parisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alter-native methods for algorithm selection, such as 5×2cv cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
	number = {January},
	author = {Raschka, Sebastian},
	year = {2018},
	note = {arXiv: 1811.12808
ISBN: 9780511563928},
	keywords = {Modeling, Model Selection, Process},
	pages = {1--13},
}

@article{Lee2012,
	title = {Numerically efficient methods for solving least squares problems},
	abstract = {Computing the solution to Least Squares Problems is of great importance in a wide range of fields ranging from numerical linear algebra to econometrics and optimization. This paper aims to present numerically stable and computationally efficient algorithms for computing the solution to Least Squares Problems. In order to evaluate and compare the stability and efficiency of our proposed algorithms, the theoretical complexities and numerical results have been analyzed. Contents},
	author = {Lee, D O Q},
	year = {2012},
	pages = {1--15},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9FXTJ5XN\\NUMERICALLY EFFICIENT METHODS FOR SOLVING LEAST SQUARES PROBLEM.pdf:application/pdf},
}

@article{Entner2019,
	title = {A {Systematic} {Approach} for the {Selection} of {Optimization} {Algorithms} including {End}-{User} {Requirements} {Applied} to {Box}-{Type} {Boom} {Crane} {Design}},
	volume = {2},
	issn = {2571-5577},
	doi = {10.3390/asi2030020},
	abstract = {In engineering design, optimization methods are frequently used to improve the initial design of a product. However, the selection of an appropriate method is challenging since many methods exist, especially for the case of simulation-based optimization. This paper proposes a systematic procedure to support this selection process. Building upon quality function deployment, end-user and design use case requirements can be systematically taken into account via a decision matrix. The design and construction of the decision matrix are explained in detail. The proposed procedure is validated by two engineering optimization problems arising within the design of box-type boom cranes. For each problem, the problem statement and the respectively applied optimization methods are explained in detail. The results obtained by optimization validate the use of optimization approaches within the design process. The application of the decision matrix shows the successful incorporation of customer requirements to the algorithm selection.},
	number = {3},
	journal = {Applied System Innovation},
	author = {Entner, Doris and Fleck, Philipp and Vosgien, Thomas and Münzer, Clemens and Finck, Steffen and Prante, Thorsten and Schwarz, Martin},
	year = {2019},
	keywords = {optimization, algorithm selection, engineering design},
	pages = {20},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\M72555ZM\\A Systematic Approach for the Selection of Optimization Algorithm.pdf:application/pdf},
}

@article{HowBuildData,
	title = {How to {Build} a {Data} {Analytics} {Platform} t o s u p p o r t yo u r e n d - t o - e n d d ata j o u r n e y},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AIZ58A77\\Data Analytics Platform_Matillion.pdf:application/pdf},
}

@misc{SmartphoneUse2015,
	title = {U.{S}. {Smartphone} {Use} in 2015 {\textbar} {Pew} {Research} {Center}},
	url = {http://www.pewinternet.org/2015/04/01/us-smartphone-use-in-2015/},
	keywords = {Applications, Natural Language Processing},
}

@article{Tagg2015,
	title = {a {Corpus} {Linguistics} {Study} of {Sms} {Text} {Messaging}},
	abstract = {This thesis reports a study using a corpus of text messages in English (CorTxt) to explore linguistic features which define texting as a language variety. It focuses on how the language of texting, Txt, is shaped by texters actively fulfilling interpersonal goals. The thesis starts with an overview of the literature on texting, which indicates the need for thorough linguistic investigation of Txt based on a large dataset. It then places texting within the tradition of research into the speech-writing continuum, which highlights limitations of focusing on mode at the expense of other user-variables. The thesis also argues the need for inductive investigation alongside the quantitative corpus-based frameworks that dominate the field. A number of studies are then reported which explore the unconventional nature of Txt. Firstly, drawing on the argument that respelling constitutes a meaning-making resource, spelling variants are retrieved using word-frequency lists and categorised according to form and function. Secondly, identification of everyday creativity in CorTxt challenges studies focusing solely on spelling as a creative resource, and suggests that creativity plays an important role in texting because of, rather than despite, physical constraints. Thirdly, word frequency analysis suggests that the distinct order of the most frequent words in CorTxt can be explained with reference to the frequent phrases in which they occur. Finally, application of a spoken grammar model reveals similarities and differences between spoken and texted interaction. The distinct strands of investigation highlight, on the one hand, the extent to which texting differs from speech and, on the other, the role of user agency, awareness and choice in shaping Txt. The argument is made that this can be explained through performativity and, in particular, the observation that texters perform brevity, speech-like informality and group deviance in construing identities through Txt.},
	number = {March},
	journal = {University of Birmingham},
	author = {Tagg, Caroline},
	year = {2015},
	pages = {412},
}

@article{Andrychowicz2016,
	title = {Learning to learn by gradient descent by gradient descent},
	issn = {10495258},
	abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
	number = {Nips},
	journal = {Advances in Neural Information Processing Systems},
	author = {Andrychowicz, Marcin and Denil, Misha and Colmenarejo, Sergio Gómez and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
	year = {2016},
	note = {arXiv: 1606.04474},
	pages = {3988--3996},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7I53MLIT\\Learning to learn by gradient descent by gradient descent.pdf:application/pdf},
}

@misc{KbotCompleteHand,
	title = {kbot\_complete\_hand\_written\_example (1).pdf},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XUXS3IAN\\kbot_complete_hand_written_example (1).pdf:application/pdf},
}

@article{Natarajan2012,
	title = {Gradient-based boosting for statistical relational learning: {The} relational dependency network case},
	volume = {86},
	issn = {08856125},
	doi = {10.1007/s10994-011-5244-9},
	abstract = {Dependency networks approximate a joint probability distribution over multiple random variables as a product of conditional distributions. Relational Dependency Networks (RDNs) are graphical models that extend dependency networks to relational domains. This higher expressivity, however, comes at the expense of a more complex model-selection problem: an unbounded number of relational abstraction levels might need to be explored. Whereas current learning approaches for RDNs learn a single probability tree per random variable, we propose to turn the problem into a series of relational function-approximation problems using gradient-based boosting. In doing so, one can easily induce highly complex features over several iterations and in turn estimate quickly a very expressive model. Our experimental results in several different data sets show that this boosting method results in efficient learning of RDNs when compared to state-of-the-art statistical relational learning approaches. ©2011 The Author(s).},
	number = {1},
	journal = {Machine Learning},
	author = {Natarajan, Sriraam and Khot, Tushar and Kersting, Kristian and Gutmann, Bernd and Shavlik, Jude},
	year = {2012},
	keywords = {Ensemble methods, Graphical models, Statistical relational learning},
	pages = {25--56},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T5NF9D79\\Gradient-based boosting for statistical relational.pdf:application/pdf},
}

@book{gleick2011genius,
	title = {Genius: {The} {Life} and {Science} of {Richard} {Feynman}},
	isbn = {978-1-4532-1043-7},
	url = {https://books.google.com/books?id=j42RD66g72oC},
	publisher = {Open Road Media},
	author = {Gleick, J},
	year = {2011},
	note = {Series Title: EBL-Schweitzer},
}

@article{Carlberg2009,
	title = {Lecture 2 : {Unconstrained} {Optimization}},
	abstract = {Outline and terminologies First-order optimality: Unconstrained problems First-order optimality: Constrained problems Second-order optimality conditions Algorithms Lecture 3: Constrained Optimization},
	journal = {Optimization},
	author = {Carlberg, Kevin},
	year = {2009},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XK3BR6CA\\Unconstrained Optimization.pdf:application/pdf},
}

@article{Evert,
	title = {A lightweight and efficient tool for cleaning {Web} pages},
	abstract = {Originally conceived as a " naive " baseline experiment using traditional n-gram language models as classifiers, the NCLEANER system has turned out to be a fast and lightweight tool for cleaning Web pages with state-of-the-art accuracy (based on results from the CLEANEVAL competition held in 2007). Despite its simplicity, the algorithm achieves a significant improvement over the baseline (i.e. plain, uncleaned text dumps), trading off recall for substantially higher precision. NCLEANER is available as an open-source software package. It is pre-configured for English Web pages, but can adapted to other languages with minimal amounts of manually cleaned training data. Since NCLEANER does not make use of HTML structure, it can also be applied to existing Web corpora that are only available in plain text format, with a minor loss in classfication accuracy.},
	author = {Evert, Stefan},
	keywords = {Applications, Natural Language Processing},
}

@article{Tibshirani2015,
	title = {Lecture 5 : {Gradient} {Descent} {Unconstrained} minimization problems and {Gradient} descent {How} to choose step sizes},
	author = {Tibshirani, Lecturer Ryan},
	year = {2015},
	pages = {1--7},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UDEMG7CH\\05-grad-descent-scribed (1).pdf:application/pdf},
}

@article{Mello2010,
	title = {5 {Best} {Practices} for {Telling} {Great} {Stories} {With} {Data} and {Why} {It} {Will} {Make} {You} a {Better} {Analyst}},
	abstract = {Every one who has data to analyze has stories to tell, whether it's diagnosing the reasons for manufacturing defects, selling a new idea in a way that captures the imagination of your target audience or informing colleagues about a particular customer service improvement program. And when it's telling the story behind a big strategic choice so that you and your senior management team can make a solid decision, providing a fact-based story can be especially challenging. In all cases, it's a big job. You want to be interesting and memorable; you know you need to keep it simple for your busy executives and colleagues. Yet you also know you have to be factual, detail-oriented and data-driven, especially in today's metriccentric world.},
	author = {Mello, Anthony De},
	year = {2010},
	keywords = {analyst, and why it will, est practices for telling, great stories with data, make you a better},
	pages = {1--8},
	file = {5 Best Practices for Telling Great Stories With Data and Why It Will Make You a.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\5 Best Practices for Telling Great Stories With Data and Why It Will Make You a.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2PR9T5XX\\best-practices_telling_great_stories.pdf:application/pdf},
}

@article{Ripley2014,
	title = {Package '{MASS}' {Title} {Support} {Functions} and {Datasets} for {Venables} and {Ripley}'s {MASS}},
	url = {http://www.stats.ox.ac.uk/pub/MASS4/},
	author = {Ripley, Brian},
	year = {2014},
	keywords = {Algorithms, Bayesian},
}

@article{Hladek,
	title = {Text {Gathering} and {Processing} {Agent} for {Language} {Modeling} {Corpus}},
	abstract = {an approach for acquisition, preprocessing and storage of large quantity of text for creation of the Slovak language model is presented. Text, downloaded from web, is preprocessed by automatically generated parser. Heuristic parser rules are used to identify entities in text such as abbreviations or end of sentence. Raw and processed text is stored in the relational database. Effective filters for erroneous and duplicate sentences are proposed. Evaluation of the results includes effects of the proposed filters on corpus and tendency of the database growth.},
	author = {Hladek, Daniel and Stas, Jan},
	keywords = {corpus, language model, parser, text processing},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T9Y29KKG\\Text Gathering and Processing Agent for Language Modeling Corpus.pdf:application/pdf},
}

@article{Hsu,
	title = {{GENERALIZED} {LINEAR} {INTERPOLATION} {OF} {LANGUAGE} {MODELS}},
	abstract = {Despite the prevalent use of model combination techniques to improve speech recognition performance on domains with limited data, little prior research has focused on the choice of the actual interpolation model. For merging language models, the most popular approach has been the simple linear interpo-lation. In this work, we propose a generalization of linear in-terpolation that computes context-dependent mixture weights from arbitrary features. Results on a lecture transcription task yield up to a 1.0\% absolute improvement in recognition word error rate (WER).},
	author = {Hsu, Bo-June},
	keywords = {Applications, Natural Language Processing, adap-tation, Index Terms— Language modeling, interpolation, mixture models},
}

@article{Madnani,
	title = {Getting {Started} on {Natural} {Language} {Processing} with {Python}},
	url = {http://www.acm.},
	abstract = {The intent of this article is to introduce the readers to the area of Natural Language Processing, commonly referred to as NLP. However, rather than just describing the salient concepts of NLP, this article uses the Python programming language to illustrate them as well. For readers unfamiliar with Python, the article provides a number of references to learn how to program in Python.},
	author = {Madnani, Nitin},
	keywords = {Applications, Natural Language Processing},
}

@book{Herdan1960,
	title = {Type-token {Mathematics}},
	publisher = {Mouton},
	author = {Herdan, Gustav},
	year = {1960},
	keywords = {Applications, Natural Language Processing},
}

@article{Stolcke2011,
	title = {{SRILM} at {Sixteen} : {Update} and {Outlook}},
	abstract = {The SRI Language Modeling Toolkit (SRILM for short) is an open source software toolkit for statistical language modeling and related tasks. It was first conceived and implemented— with minimal functionality—in 1995, followed by a first public (beta) release in 1999. Since then SRILM has found wide use in the speech and natural language research community. A 2002 paper [1] presented an overview of the toolkit's design and functionality, of which we provide only a brief summary here. This paper takes stock of SRILM developments since then, as well as extensions and applications of SRILM. We also summarize activities in the SRILM user community, give an overall assessment of developments so far, and point out possible future directions.},
	journal = {Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
	author = {Stolcke, Andreas and Zheng, Jing and Wang, Wen and Abrash, Victor},
	year = {2011},
	keywords = {Applications, Natural Language Processing},
	pages = {5--9},
}

@book{Harper2005,
	title = {The {Inside} {Text} : {Social}, {Cultural} and {Design} {Perspectives} on {SMS} ({The} {Computer} {Supported} {Cooperative} {Work} {Series})},
	isbn = {1-4020-3060-6},
	url = {http://www.amazon.fr/exec/obidos/ASIN/1402030592/citeulike04-21},
	abstract = {SMS or Text is one of the most popular forms of messaging. Yet, despite its immense popularity, SMS has remained unexamined by science. Not only that, but the commercial organisations, who have been forced to offer SMS by a demanding public, have had very little idea why it has been successful. Indeed, they have, until very recently, planned to replace SMS with other messaging services such as MMS. This book is the first to bring together scientific studies into the values that ‘texting' provides, examining both cultural variation in countries as different as the Philippines and Germany, as well as the differences between SMS and other communications channels like Instant Messaging and the traditional letter. It presents usability and design research which explores how SMS will evolve and what is likely to be the pattern of person-to-person messaging in the future. In short, Inside Text is a fundamental resource for anyone interested in mobile communications at the start of the 21st Century The book will be of interest to anyone in the CHI, CSCW and mobile communications research areas, as well as sociologists, anthropologists, communications scientists and policy makers.},
	publisher = {Springer},
	author = {Harper, R and Palen, L and Taylor, A},
	editor = {Harper, Richard and Palen, Leysia and Taylor, Alex},
	year = {2005},
	keywords = {Applications, Natural Language Processing, no-tag},
}

@article{Koehn2009,
	title = {Chapter 7 {Language} models {Language} models},
	journal = {Statistical machine translation},
	author = {Koehn, Philipp},
	year = {2009},
}

@article{Chelba2000,
	title = {Structured language modeling},
	volume = {14},
	url = {http://www.idealibrary.com},
	doi = {10.1006/csla.2000.0147},
	abstract = {This paper presents an attempt at using the syntactic structure in natural language for improved language models for speech recognition. The structured language model merges techniques in automatic parsing and language modeling using an original probabilistic parameterization of a shift-reduce parser. A maximum likelihood re-estimation procedure belonging to the class of expectation-maximization algorithms is employed for training the model. Experiments on the Wall Street Journal and Switchboard corpora show improvement in both perplexity and word error rate—word lattice rescoring—over the standard 3-gram language model.},
	journal = {Computer Speech and Language},
	author = {Chelba, Ciprian and Jelinek, Frederick},
	year = {2000},
	keywords = {Applications, Natural Language Processing},
	pages = {283--332},
}

@article{Amini,
	title = {{BAYESIAN} {MODEL} {AVERAGING} {IN} {R}},
	abstract = {Bayesian model averaging has increasingly witnessed applications across an array of empirical contexts. However, the dearth of available statistical software which allows one to engage in a model averaging exercise is limited. It is common for consumers of these methods to develop their own code, which has obvious appeal. However, canned statistical software can ameliorate one's own analysis if they are not intimately familiar with the nuances of computer coding. Moreover, many researchers would prefer user ready software to mitigate the inevitable time costs that arise when hard coding an econometric estimator. To that end, this paper describes the relative merits and attractiveness of several competing packages in the statistical environment R to implement a Bayesian model averaging exercise.},
	author = {Amini, Shahram M and Parmeter, Christopher F},
	keywords = {Algorithms, Bayesian},
}

@article{Moisl2009,
	title = {Exploratory multivariate analysis},
	volume = {2},
	doi = {10.1007/978-0-387-21706-2_11},
	journal = {Corpus Linguistics: An International Handbook},
	author = {Moisl, Hermann},
	year = {2009},
	note = {ISBN: 9783110213881},
	pages = {874--899},
}

@article{MaxKuhnContributionsfromJedWing2017,
	title = {Package 'caret' {Classification} and {Regression} {Training} {Description} {Misc} functions for training and plotting classification and regression models},
	author = {Max Kuhn Contributions from Jed Wing, Author and Weston, Steve and Williams, Andre and Keefer, Chris and Engelhardt, Allan and Cooper, Tony and Mayer, Zachary and Benesty, Michael and Lescarbeau, Reynald and Ziem, Andrew and Scrucca, Luca and Tang, Yuan and Candan, Can and Hunt, Tyler and Max Kuhn, Maintainer},
	year = {2017},
	keywords = {Algorithms, Bayesian},
}

@misc{NORC,
	title = {About {NORC} {Menu} {\textbar} {NORC}.org},
	url = {http://www.norc.org/About/Pages/default.aspx},
	urldate = {2017-09-30},
	author = {{NORC}},
}

@article{Chelba2008,
	title = {Exploiting {Syntactic} {Structure} for {Natural} {Language} {Modeling}},
	number = {December},
	author = {Chelba, Ciprian},
	year = {2008},
	note = {arXiv: cs/0001020v1},
}

@article{Tagg2009a,
	title = {A {CORPUS} {LINGUISTICS} {STUDY} {OF} {SMS} {TEXT} {MESSAGING}},
	author = {Tagg, Caroline},
	year = {2009},
	keywords = {Applications, Natural Language Processing},
}

@article{UnitRepresentativenessBalance,
	title = {Unit 2 {Representativeness}, balance and sampling},
	keywords = {Applications, Natural Language Processing},
}

@article{Goyvaerts2007,
	title = {The {Complete} {Tutorial}},
	abstract = {Explains the relationship between computers and multiple intelligences and explores its potential. Topics include behaviorism and cognitivism; constructivism; multiple representations in the human mind, including various forms of intelligence; individual differences; the challenge of altering early representations; and machine versus human tutors. (LRW)},
	author = {Goyvaerts, Jan},
	year = {2007},
	note = {ISBN: 1-4116-7760-9},
	pages = {1--197},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TQ6MLI7W\\Regular-Expressions.pdf:application/pdf},
}

@book{feynman1992surely,
	title = {"{Surely} {You}'re {Joking}, {Mr}. {Feynman}!": {Adventures} of a {Curious} {Character}},
	isbn = {978-0-09-917331-1},
	url = {https://books.google.com/books?id=N3eLdCTynR0C},
	publisher = {Vintage},
	author = {Feynman, R P and Leighton, R and Hutchings, E},
	year = {1992},
}

@article{Hoeting1999,
	title = {Bayesian {Model} {Averaging}: {A} {Tutorial}},
	volume = {14},
	abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident in-ferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for ac-counting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-of-sample predictive performance. We also provide a catalogue of currently available BMA software.},
	number = {4},
	journal = {Statistical Science},
	author = {Hoeting, Jennifer A and Madigan, David and Raftery, Adrian E and Volinsky, Chris T},
	year = {1999},
	keywords = {Algorithms, Bayesian, and phrases, Bayesian graphical models, Bayesian model averaging, learning, Markov chain Monte Carlo, model uncertainty},
	pages = {382--417},
}

@article{CBinsights2018,
	title = {What is {Blockchain} {Technology}},
	doi = {10.1227/01.NEU.0000210001.75597.81},
	abstract = {Peter C. B. Phillips has made fundamental contributions to unit root econometrics. This paper provides a selective review of Peter's contribution to unit roots, with a focus on unit root asymptotics, unit root tests, and testing for stationarity against the unit root alternative. The discussion puts a relatively heavier weight on Peter's most recent work.},
	journal = {Accounting Education News},
	author = {{CBinsights}},
	year = {2018},
	pages = {1--20},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U3M3883M\\CB-Insights_Blockchain-Explainer.pdf:application/pdf},
}

@misc{OutliersinstatisticaldataprobabilityampmathePdf,
	title = {outliers-in-statistical-data-probability-amp-mathe.pdf},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8RME595Z\\outliers-in-statistical-data-probability-amp-mathe.pdf:application/pdf},
}

@article{Maclaurin2015,
	title = {Gradient-based {Hyperparameter} {Optimization} through {Reversible} {Learning}},
	url = {http://arxiv.org/abs/1502.03492},
	abstract = {Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.},
	author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P},
	year = {2015},
	note = {arXiv: 1502.03492},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3HET26D3\\Gradient Descent Hyperparameter Tuning.pdf:application/pdf},
}

@article{Miche2008,
	title = {A methodology for building regression models using extreme learning machine: {OP}-{ELM}},
	abstract = {This paper proposes a methodology named OP-ELM, based on a recent development -the Extreme Learning Machine- decreasing drastically the training speed of networks. Variable selection is beforehand performed on the original dataset for proper results by OP-ELM: The network is first created using Extreme Learning Process, selection of the most relevant nodes is performed using Least Angle Regression (LARS) ranking of the nodes and a Leave-One-Out estimation of the performances. Results are globally equivalent to LSSVM ones with reduced computational time.},
	journal = {ESANN 2008 Proceedings, 16th European Symposium on Artificial Neural Networks - Advances in Computational Intelligence and Learning},
	author = {Miche, Yoan and Bas, Patrick and Jutten, Christian and Simula, Olli and Lendasse, Amaury},
	year = {2008},
	note = {ISBN: 2930307080},
	pages = {247--252},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5NM4PPVM\\A_methodology_for_building_regression_mo.pdf:application/pdf},
}

@misc{Cool-Smileys2010,
	title = {List of {Text} {Emoticons}: {The} {Ultimate} {Resource}},
	url = {http://cool-smileys.com/text-emoticons},
	author = {{Cool-Smileys}},
	year = {2010},
	keywords = {Applications, Natural Language Processing},
}

@misc{Fritz2008,
	title = {Box {Office} {Mojo}},
	url = {http://www.boxofficemojo.com/},
	author = {Fritz, ‎Ben},
	year = {2008},
	note = {Publication Title: wikipedia},
	keywords = {Algorithms, Bayesian},
}

@article{Dayan1999,
	title = {Unsupervised {Learning}},
	volume = {47},
	issn = {0042-6989},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/17850840},
	doi = {10.1016/j.visres.2007.07.023},
	abstract = {Unsupervised learning studies how systems can learn to represent particular input patterns in a way that reflects the statistical structure of the overall collection of input patterns. By contrast with SUPERVISED LEARNING or REINFORCEMENT LEARNING, there are no explicit target outputs or environmental evaluations associated with each input; rather the unsupervised learner brings to bear prior biases as to what aspects of the structure of the input should be captured in the output.},
	number = {22},
	urldate = {2017-09-08},
	journal = {The MIT Encyclopedia of the Cognitive Sciences},
	author = {Dayan, Peter},
	year = {1999},
	pmid = {17850840},
	keywords = {Learning, Animals, Cats, Learning: physiology, Models, Neurological, Neuronal Plasticity, Neuronal Plasticity: physiology, Neurons, Neurons: physiology, Orientation, Orientation: physiology, Photic Stimulation, Photic Stimulation: methods, Sensory Deprivation, Sensory Deprivation: physiology, Visual Cortex, Visual Cortex: physiology, Visual Perception, Visual Perception: physiology},
	pages = {1--29},
}

@article{Yasuda,
	title = {Method of {Selecting} {Training} {Sets} to {Build} {Compact} and {Efficient} {Statistical} {Language} {Model}},
	abstract = {For statistical language model training, tar-get task matched corpora are required. How-ever, training corpora sometimes include both target task matched and unmatched sentences. In such a case, training set selec-tion is effective for both model size reduc-tion and model performance improvement. In this paper, training set selection method for statistical language model training is de-scribed. The method provides two advan-tages for training a language model. One is its capacity to improve the language model performance, and the other is its capacity to reduce computational loads for the language model. The method has four steps. 1) Sen-tence clustering is applied to all available corpora. 2) Language models are trained on each cluster. 3) Perplexity on the devel-opment set is calculated using the language models. 4) For the final language model training, we use the clusters whose language models yield low perplexities. The experi-mental results we obtained indicate the lan-guage model trained on the data selected by our method gives lower perplexity on an open test set than a language model trained on all available corpora.},
	author = {Yasuda, Keiji and Yamamoto, Hirofumi and Sumita, Eiichiro},
	keywords = {Applications, Natural Language Processing},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RRVCRWJB\\Method of Selecting Training Sets to Build Compact and Efficient Statistical Language Model.pdf:application/pdf},
}

@misc{WHO2012,
	title = {Fact sheet 369: {Depression}},
	url = {http://www.who.int/mediacentre/factsheets/fs369/en/},
	abstract = {WHO fact sheet on depression providing key facts and information on types and symptoms, contributing factors, diagnosis and treatment, WHO response.},
	urldate = {2017-06-13},
	publisher = {World Health Organization},
	author = {{WHO}},
	year = {2012},
	doi = {/entity/mediacentre/factsheets/fs369/en/index.html},
	note = {Publication Title: World Health Organization},
	keywords = {chronic diseases, communicable disease [subject], depression [subject], depressive symptoms, Fact sheet [doctype], infectious diseases, mental disorder [subject], mental health [subject], mental health promotion, mental illness, noncommunicable disease [subject], psychiatric illness, Region of the Americas [region], suicide [subject], suicide prevention, United States of America [country]},
}

@article{Bickel2005,
	title = {Predicting sentences using {N}-gram language models},
	url = {http://dx.doi.org/10.3115/1220575.1220600},
	doi = {10.3115/1220575.1220600},
	abstract = {We explore the benefit that users in several application areas can\${\textbackslash}\$nexperience from a "tab-complete" editing assistance function. We\${\textbackslash}\$ndevelop an evaluation metric and adapt N-gram language models to\${\textbackslash}\$nthe problem of predicting the subsequent words, given an initial\${\textbackslash}\$ntext fragment. Using an instance-based method as baseline, we empirically\${\textbackslash}\$nstudy the predictability of call-center emails, personal emails,\${\textbackslash}\$nweather reports, and cooking recipes.},
	journal = {Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing},
	author = {Bickel, Steffen and Haider, Peter and Scheffer, Tobias},
	year = {2005},
	keywords = {Applications, Natural Language Processing},
	pages = {193--200},
}

@book{savov2017no,
	title = {No {Bullshit} {Guide} to {Linear} {Algebra}},
	isbn = {978-0-9920010-2-5},
	url = {https://books.google.com/books?id=A4WzswEACAAJ},
	abstract = {Linear algebra is the foundation of science and engineering. Knowledge of linear algebra is a prerequisite for studying statistics, machine learning, computer graphics, signal processing, chemistry, economics, quantum mechanics, and countless other applications. Indeed, linear algebra offers a powerful toolbox for modelling the real world. The NO BULLSHIT guide to LINEAR ALGEBRA shows the connections between the computational techniques of linear algebra, their geometric interpretations, and the theoretical foundations. This university-level textbook contains lessons on linear algebra written in a style that is precise and concise. Each concept is illustrated through definitions, formulas, diagrams, explanations, and examples of real-world applications. Readers build their math superpowers by solving practice problems and learning to use the computer algebra system SymPy to speed up tedious matrix arithmetic tasks.},
	publisher = {Minireference Publishing},
	author = {Savov, I},
	year = {2017},
}

@article{Tofallis2015,
	title = {A better measure of relative prediction accuracy for model selection and model estimation},
	volume = {66},
	issn = {14769360},
	doi = {10.1057/jors.2014.103},
	abstract = {Surveys show that the mean absolute percentage error (MAPE) is the most widely used measure of prediction accuracy in businesses and organizations. It is, however, biased: when used to select among competing prediction methods it systematically selects those whose predictions are too low. This has not been widely discussed and so is not generally known among practitioners. We explain why this happens. We investigate an alternative relative accuracy measure which avoids this bias: the log of the accuracy ratio, that is, log (prediction/actual). Relative accuracy is particularly relevant if the scatter in the data grows as the value of the variable grows (heteroscedasticity). We demonstrate using simulations that for heteroscedastic data (modelled by a multiplicative error factor) the proposed metric is far superior to MAPE for model selection. Another use for accuracy measures is in fitting parameters to prediction models. Minimum MAPE models do not predict a simple statistic and so theoretical analysis is limited. We prove that when the proposed metric is used instead, the resulting least squares regression model predicts the geometric mean. This important property allows its theoretical properties to be understood.},
	number = {8},
	journal = {Journal of the Operational Research Society},
	author = {Tofallis, Chris},
	year = {2015},
	keywords = {regression, time series, model selection, forecasting, loss function, prediction},
	pages = {1352--1362},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S38V3N2C\\A_Better_Measure_of_Relative_prediction_accuracy_for_model_selection_and_fitting_Preprint_JORS_.pdf:application/pdf},
}

@misc{Bradford2017,
	title = {8 {Ways} {You} {Can} {Succeed} {In} {A} {Machine} {Learning} {Career}},
	url = {https://www.forbes.com/sites/laurencebradford/2017/07/28/8-ways-you-can-succeed-in-a-machine-learning-career/#1b0e26bf3c32},
	abstract = {Machine learning is exploding, with smart algorithms being used everywhere from email to smartphone apps to marketing campaigns. Translation: if you're looking for an in-demand career, setting yourself up with the skills to work with smart machines/artificial intelligence is a good move.

With input from Florian Douetteau, CEO of Dataiku, here are some things you can start doing today to position yourself for a future career in machine learning.},
	urldate = {2019-03-31},
	author = {Bradford, Laurence},
	year = {2017},
	note = {Publication Title: Forbes},
}

@article{Fan2019,
	title = {Deep social collaborative filtering},
	doi = {10.1145/3298689.3347011},
	abstract = {Recommender systems are crucial to alleviate the information overload problem in online worlds. Most of the modern recommender systems capture users' preference towards items via their interactions based on collaborative fltering techniques. In addition to the user-item interactions, social networks can also provide useful information to understand users' preference as suggested by the social theories such as homophily and infuence. Recently, deep neural networks have been utilized for social recommendations, which facilitate both the user-item interactions and the social network information. However, most of these models cannot take full advantage of the social network information. They only use information from direct neighbors, but distant neighbors can also provide helpful information. Meanwhile, most of these models treat neighbors' information equally without considering the specifc recommendations. However, for a specifc recommendation case, the information relevant to the specifc item would be helpful. Besides, most of these models do not explicitly capture the neighbor's opinions to items for social recommendations, while diferent opinions could afect the user diferently. In this paper, to address the aforementioned challenges, we propose DSCF, a Deep Social Collaborative Filtering framework, which can exploit the social relations with various aspects for recommender systems. Comprehensive experiments on two-real world datasets show the efectiveness of the proposed framework.},
	journal = {RecSys 2019 - 13th ACM Conference on Recommender Systems},
	author = {Fan, Wenqi and Wang, Jianping and Ma, Yao and Tang, Jiliang and Yin, Dawei and Li, Qing},
	year = {2019},
	note = {arXiv: 1907.06853
ISBN: 9781450362436},
	keywords = {Applications, Recommender Systems, Neural Networks, Random Walk, Recurrent Neural Network, Social Network, Social Recommendation},
	pages = {305--313},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\779SIB8V\\Deep Social Collaborative Filtering.pdf:application/pdf},
}

@article{Freund2004,
	title = {Introduction to {Optimization} , and {Optimality} {Conditions} for {Unconstrained} {Problems}},
	author = {Freund, Robert M},
	year = {2004},
	pages = {1--47},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6PJDY7LW\\Unconstrained Optimality.pdf:application/pdf},
}

@article{Reddi2018,
	title = {A generic approach for escaping saddle points},
	volume = {84},
	abstract = {A central challenge to using first-order methods for optimizing nonconvex problems is the presence of saddle points. First-order methods often get stuck at saddle points, greatly deteriorating their performance. Typically, to escape from saddles one has to use second-order methods. However, most works on second-order methods rely extensively on expensive Hessian-based computations, making them impractical in large-scale settings. To tackle this challenge, we introduce a generic framework that minimizes Hessian-based computations while at the same time provably converging to second- order critical points. Our framework carefully alternates between a first-order and a second-order subroutine, using the latter only close to saddle points, and yields convergence results competitive to the state-of-the-art. Empirical results suggest that our strategy also enjoys a good practical performance.},
	journal = {International Conference on Artificial Intelligence and Statistics, AISTATS 2018},
	author = {Reddi, Sashank J and Zaheer, Manzil and Sra, Suvrit and Póczos, Barnabás and Salakhutdinov, Ruslan and Bach, Francis and Smola, Alexander J},
	year = {2018},
	note = {arXiv: 1709.01434},
	pages = {1233--1242},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QDJ9I5KY\\A Generic Approach for Escaping Saddle points.pdf:application/pdf},
}

@article{Lorch2016,
	title = {Visualizing {Deep} {Network} {Training} {Trajectories} with {PCA}},
	volume = {48},
	abstract = {Neural network training is a form of numeri-cal optimization in a high-dimensional parame-ter space. Such optimization processes are rarely visualized because of the difficulty of represent-ing high-dimensional dynamics in a visually in-telligible way. We present a simple method for rendering the optimization trajectories of deep networks with low-dimensional plots, using lin-ear projections obtained by principal component analysis. We show that such plots reveal visually distinctive properties of the training processes, and outline opportunities for future investigation.},
	journal = {The 33rd International Conference on Machine Learning},
	author = {Lorch, Eliana},
	year = {2016},
	pages = {1--5},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NAQ2Q5AP\\Visualizing Deep Network Training Trajectories with PCA.pdf:application/pdf},
}

@article{Benoit2008,
	title = {A scaling model for estimating time-series party positions from texts},
	volume = {97},
	abstract = {The ultimate quantitative extreme in textual data analysis uses scaling procedures borrowed from item response theory methods developed originally in psychometrics. Both Jon Slapin and Sven-Oliver Proksch's Poisson scaling model and Burt Monroe and Ko Maeda's similar scaling method assume that word frequencies are generated by a probabi-listic function driven by the author's position on some latent scale of interest and can be used to estimate those latent positions relative to the posi-tions of other texts. Such methods may be applied to word frequency matrixes constructed from texts with no human decision making of any kind. The disadvantage is that while the scaled estimates resulting from the procedure represent relative dif-ferences between texts, they must be interpreted if a researcher is to understand what politically signifi-cant differences the scaled results represent. This interpretation is not always self-evident. Recent textual data analysis methods used in political science have also focused on classifica-tion: determining which category a given text belongs to. Recent examples include methods to categorize the topics debated in the U.S. Congress as a means of measuring political agendas. Variants on classification include recently developed meth-ods designed to estimate accurately the propor-tions of categories of opinions about the U.S. presidency from blog postings, even though the classifier on which it is based performs poorly for individual texts. New methodologies for drawing more information from political texts continue to be developed, using clustering methodologies, more advanced item response theory models, sup-port vector machines, and semisupervised and unsupervised machine learning techniques.},
	number = {523},
	journal = {American Political Science Review Politburo images of Stalin. World Politics American Journal of Political Science},
	author = {Benoit, Kenneth},
	year = {2008},
	note = {Publisher: Cambridge University Press Oxford University Press},
	keywords = {Applications, Natural Language Processing, Archival, Discourse Analysis, Expert, Interviews, Party Manifesto Further Readings, See also Data},
	pages = {311--331},
}

@article{Hiemstra2009,
	title = {Language {Models}},
	doi = {10.1007/978-0-387-39940-9_923},
	journal = {Encyclopedia of Database Systems},
	author = {Hiemstra, Djoerd},
	year = {2009},
	pages = {1591--1594},
}

@article{Wilson2015,
	title = {The {R} {Project} for {Statistical} {Computing} {The} {R} {Project} for {Statistical} {Computing}},
	volume = {3},
	url = {https://www.r-project.org/},
	journal = {URL: http://www. r-project. org/254},
	author = {Wilson, Adam and Norden, Natalia},
	year = {2015},
}

@article{Even-Zohar2000,
	title = {A {Classification} {Approach} to {Word} {Prediction}},
	abstract = {The eventual goal of a language model is to accurately predict the value of a missing word given its context. We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context. This approach raises a few new questions that we address. First, in order to learn good word representations it is necessary to use an expressive representation of the context. We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction. Second, since the number of words ``competing'' for each prediction is large, there is a need to ``focus the attention'' on a smaller subset of these. We exhibit the contribution of a ``focus of attention'' mechanism to the performance of the word predictor. Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks.},
	journal = {Proceedings of the First NorthAmerican Conference on Computational Linguistics},
	author = {Even-Zohar, Yair and Roth, Dan},
	year = {2000},
	note = {arXiv: cs/0009027},
	pages = {8},
}

@book{Division,
	title = {Lllllllllllllllllllllllllllllllllll 11587},
	isbn = {0-87389-247-X},
	author = {Division, Statistics},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JUMDSGCJ\\How to Detect and Handle Outliers.pdf:application/pdf},
}

@article{Schwarz,
	title = {Estimating the {Dimension} of a {Model}},
	volume = {6},
	issn = {0090-5364},
	doi = {10.1214/aos/1176344136},
	abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
	number = {2},
	journal = {The Annals of Statistics},
	author = {Schwarz, Gideon},
	year = {1978},
	pmid = {2958889},
	note = {arXiv: 1011.1669v3
ISBN: 0780394224},
	keywords = {Algorithms, Bayesian},
	pages = {461--464},
}

@article{WeekReviewPosterior,
	title = {Week 1 {Review} and {Posterior} {Probability} {Calculation} {Example}},
	pages = {1--6},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8K5VA76N\\Bayesian Inference - Week1_Review_Example.pdf:application/pdf},
}

@article{Genereux,
	title = {Introducing the {Reference} {Corpus} of {Contemporary} {Portuguese} {Online}},
	abstract = {We present our work in processing the Reference Corpus of Contemporary Portuguese and its publication online. After discussing how the corpus was built and our choice of meta-data, we turn to the processes and tools involved for the cleaning, preparation and annotation to make the corpus suitable for linguistic inquiries. The Web platform is described, and we show examples of linguistic resources that can be extracted from the platform for use in linguistic studies or in NLP.},
	author = {Généreux, Michel and Hendrickx, Iris and Mendes, Amália},
	keywords = {Applications, Natural Language Processing, Corpus management, Portuguese, Preprocessing},
}

@article{doi:10.1093/mind/XLI.164.409,
	title = {I.—{PROBABILITY}: {THE} {DEDUCTIVE} {AND} {INDUCTIVE} {PROBLEMS}},
	volume = {XLI},
	url = {+},
	doi = {10.1093/mind/XLI.164.409},
	number = {164},
	journal = {Mind},
	author = {JOHNSON, W E},
	year = {1932},
	keywords = {Applications, Natural Language Processing},
	pages = {409},
}

@article{Reiss2019,
	title = {Deep {PPG}: {Large}-{Scale} {Heart} {Rate} {Estimation} with {Convolutional} {Neural} {Networks}},
	volume = {19},
	issn = {1424-8220},
	doi = {10.3390/s19143079},
	abstract = {Photoplethysmography (PPG)-based continuous heart rate monitoring is essential in a number of domains, e.g., for healthcare or fitness applications. Recently, methods based on time-frequency spectra emerged to address the challenges of motion artefact compensation. However, existing approaches are highly parametrised and optimised for specific scenarios of small, public datasets. We address this fragmentation by contributing research into the robustness and generalisation capabilities of PPG-based heart rate estimation approaches. First, we introduce a novel large-scale dataset (called PPG-DaLiA), including a wide range of activities performed under close to real-life conditions. Second, we extend a state-of-the-art algorithm, significantly improving its performance on several datasets. Third, we introduce deep learning to this domain, and investigate various convolutional neural network architectures. Our end-to-end learning approach takes the time-frequency spectra of synchronised PPG- and accelerometer-signals as input, and provides the estimated heart rate as output. Finally, we compare the novel deep learning approach to classical methods, performing evaluation on four public datasets. We show that on large datasets the deep learning model significantly outperforms other methods: The mean absolute error could be reduced by 31 \% on the new dataset PPG-DaLiA, and by 21 \% on the dataset WESAD.},
	number = {14},
	journal = {Sensors},
	author = {Reiss, Attila and Indlekofer, Ina and Schmidt, Philip and Van Laerhoven, Kristof},
	month = jul,
	year = {2019},
	note = {Publisher: MDPI AG},
	pages = {3079},
}

@misc{AmericanEnglishText,
	title = {American {English} {Text} {Corpus}},
	url = {http://cybersecurity.cit.purduecal.edu/content/dl/master_corpus.txt},
	keywords = {Applications, Natural Language Processing},
}

@article{Griffiths2003,
	title = {Prediction and {Semantic} {Association}},
	volume = {15},
	issn = {10495258},
	abstract = {We explore the consequences of viewing semantic association as the result of attempting to predict the concepts likely to arise in a particular context. We argue that the success of existing accounts of semantic representation comes as a result of indirectly addressing this problem, and show that a closer correspondence to human data can be obtained by taking a probabilistic approach that explicitly models the generative structure of language.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Griffiths, Thomas L and Steyvers, Mark},
	year = {2003},
	note = {ISBN: 0262025507},
	keywords = {Applications, Natural Language Processing, Distributional semantics},
	pages = {11--18},
}

@misc{wiki:supervisedlearning,
	title = {Supervised learning --- {Wikipedia}\{,\} {The} {Free} {Encyclopedia}},
	url = {https://en.wikipedia.org/w/index.php?title=Supervised_learning&oldid=791408094},
	author = {{Wikipedia}},
	year = {2017},
}

@article{Pecina,
	title = {{LEXICAL} {ASSOCIATION} {MEASURES} {Collocation} {Extraction}},
	author = {Pecina, Pavel},
	keywords = {Applications, Natural Language Processing},
}

@article{Sampling,
	title = {➔ {Effects} of {Linear} {Transformations}: ◆ a + b*mean mean},
	abstract = {new = ◆ a + b*median median new = ◆ *stdev stdev new = b {\textbar} {\textbar} ◆ *IQR IQR new = b {\textbar} {\textbar} ➔ Z­score ­ new data set will have mean 0 and variance 1 z = S X X Empirical Rule ➔ Only for mound­shaped data Approx. 95\% of data is in the interval: x s , x s) s (2 x + 2 x = x + / 2 x ➔ only use if you just have mean and std. dev. Chebyshev's Rule ➔ Use for any set of data and for any number k, greater than 1 (1.2, 1.3, etc.) ➔ 1 1 k 2 ➔ (Ex) for k=2 (2 standard deviations), 75\% of data falls within 2 standard deviations Detecting Outliers ➔ Classic Outlier Detection ◆ doesn't always work ◆ z {\textbar} {\textbar} = {\textbar} {\textbar} S X X {\textbar} {\textbar} ≥ 2 ➔ The Boxplot Rule ◆ Value X is an outlier if: X{\textless}Q1­1.5(Q3­Q1) or X{\textgreater}Q3+1.5(Q3­Q1) Skewness ➔ measures the degree of asymmetry exhibited by data ◆ negative values= skewed left ◆ positive values= skewed right ◆ if = don't need .8 skewness {\textbar} {\textbar} {\textless}0 to transform data Measurements of Association ➔ Covariance ◆ Covariance {\textgreater}0 = larger x, larger y ◆ Covariance {\textless}0 = larger x, smaller y ◆ s (x)(y) xy = 1 n 1 ∑ n i=1 x y ◆ Units = Units of x Units of y ◆ Covariance is only +, ­, or 0 (can be any number) ➔ Correlation ­ measures strength of a linear relationship between two variables ◆ r xy = covariance xy (std.dev.)(std. dev.) x y ◆ correlation is between ­1 and 1 ◆ Sign: direction of relationship ◆ Absolute value: strength of relationship (­0.6 is stronger relationship than +0.4) ◆ Correlation doesn't imply causation ◆ The correlation of a variable with itself is one Combining Data Sets ➔ Mean (Z) = X Y Z = a + b ➔ Var (Z) = V ar(X) V ar(Y) s z 2 = a 2 + b 2 + abCov(X,) 2 Y Portfolios ➔ Return on a portfolio: R R R p = w A A + w B B ◆ weights add up to 1 ◆ return = mean ◆ risk = std. deviation ➔ Variance of return of portfolio s s s 2 p = w 2 A 2 A + w 2 B 2 B + w w (s) 2 A B A,B ◆ Risk(variance) is reduced when stocks are negatively correlated. (when there's a negative covariance) Probability ➔ measure of uncertainty ➔ all outcomes have to be exhaustive (all options possible) and mutually exhaustive (no 2 outcomes can occur at the same time)},
	number = {X},
	author = {Sampling, Simple Random and Study, Observational and Study, Experimental and Deviation, Standard and Plot, Whisker and Part, Descriptive Statistics},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7SHTRDS4\\Stat 100 Final Cheat Sheets - Google Docs (2).pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4WX2NRQS\\Stat 100 Final Cheat Sheets - Google Docs (2).pdf:application/pdf},
}

@article{Carlberger1997b,
	title = {Profet, {A} {New} {Generation} of {Word} {Prediction}:},
	abstract = {Prolet, a word prediction program, has been in use for the last ten years as a writing aid, and was designed to accelerate the writing process and minimize the writing effort for persons with motor dysfunction.},
	journal = {In Proceedings of the 2nd Workshop on NLP for Communication Aids},
	author = {Carlberger, Alice and Carlberger, Johan and Magnuson, Tina and Hunnicutt, M Sharon and Palazuelos-cagigas, Sira E and Navarro, Santiago A and Electronica, Ingenieria},
	year = {1997},
	keywords = {Applications, Natural Language Processing, word-prediction},
	pages = {23--28},
}

@misc{indeed,
	title = {Indeed: {AI} job-posting rate slows and interest dips {\textbar} {VentureBeat}},
	url = {https://venturebeat.com/2019/06/28/indeed-ai-job-posting-growth-slows-and-interest-in-jobs-dips/},
	urldate = {2020-08-13},
}

@book{brunschwig2003guide,
	title = {A {Guide} to {Greek} {Thought}: {Major} {Figures} and {Trends}},
	isbn = {978-0-674-02156-3},
	url = {https://books.google.com/books?id=AXl9j4n2iP4C},
	publisher = {Belknap Press of Harvard University Press},
	author = {Brunschwig, J and Lloyd, G E R and Lloyd, G E R and Pellegrin, P and Porter, C},
	year = {2003},
}

@article{Than2013,
	title = {Modeling the diversity and log-normality of data},
	abstract = {We investigate two important properties of real data: diversity and log-normality. Log-normality accounts for the fact that data follow the lognormal distribution, whereas diversity measures variations of the at-tributes in the data. To our knowledge, these two inherent properties have not been paid much attention from the machine learning community, es-pecially from the topic modeling community. In this article, we fill in this gap in the framework of topic modeling. We first investigate whether or not these two properties can be captured by the most well-known Latent Dirichlet Allocation model (LDA), and find that LDA behaves inconsis-tently with respect to diversity. Particularly, it favors data of low diver-sity, but works badly on data of high diversity. Then, we argue that these two inherent properties can be captured well by endowing the topic-word distributions in LDA with the lognormal distribution. This treatment leads to a new model, named Dirichlet-lognormal topic model (DLN). Us-ing the lognormal distribution complicates the learning and inference of DLN, compared with those of LDA. Hence, we used variational method, in which model learning and inference are reduced to solving convex op-timization problems. Extensive experiments strongly suggest that (1) the predictive power of DLN is consistent with respect to diversity, and that (2) DLN works consistently better than LDA for datasets whose diversity is large, and for datasets which contain many log-normally distributed attributes. Justifications for these results require insights into the used statistical distributions and will be discussed in the article.},
	author = {Than, Khoat and Ho, Tu Bao},
	year = {2013},
	keywords = {Applications, Natural Language Processing},
}

@article{Suleman2014,
	title = {{HONOURS} {PROJECT} {REPORT} {Afri}-{Web}},
	abstract = {Zulu is one of South Africa's indigenous languages understood by over 16 million speakers. There is a large and increasing amount of Zulu documents on the Internet, however there is no easy and effective means to get access to them. This creates a need for a Zulu information retrieval system. This paper outlines procedures followed to develop a language identification and crawling system for Zulu documents. The project managed to obtain a collection of over 60,000 Zulu documents and developed a Zulu identification system capable of identifying Zulu strings with an accuracy of 98.4\% . The documents collected were subsequently indexed and a search platform provided for efficient querying.},
	author = {Suleman, Hussein},
	year = {2014},
	keywords = {Applications, Natural Language Processing},
}

@incollection{zellner1986,
	title = {On assessing prior distributions and {Bayesian} regression analysis with g-prior distributions},
	isbn = {0-444-87712-6},
	url = {http://www.worldcat.org/isbn/0444877126},
	booktitle = {Bayesian inference and decision techniques: essays in honor of {Bruno} de {Finetti}},
	publisher = {North-Holland ; Sole distributors for the U.S.A. and Canada, Elsevier Science Pub. Co.},
	author = {Zellner, Arnold},
	year = {1986},
	keywords = {Algorithms, Bayesian, regression, bayesian},
	pages = {233--243},
}

@article{Palmer2000,
	title = {Tokenisation and {Sentence} {Segmentation}},
	journal = {Handbook of Natural Language Processing},
	author = {Palmer, David D},
	year = {2000},
	keywords = {Applications, Natural Language Processing},
	pages = {11--35},
}

@article{Devriendt2020,
	title = {Why you should stop predicting customer churn and start using uplift models},
	issn = {00200255},
	url = {https://doi.org/10.1016/j.ins.2019.12.075},
	doi = {10.1016/j.ins.2019.12.075},
	abstract = {Uplift modeling has received increasing interest in both the business analytics research community and the industry as an improved paradigm for predictive analytics for data-driven operational decision-making. The literature, however, does not provide conclusive empirical evidence that uplift modeling outperforms predictive modeling. Case studies that directly compare both approaches are lacking, and the performance of predictive models and uplift models as reported in various experimental studies cannot be compared indirectly since different evaluation measures are used to assess their performance. Therefore, in this paper, we introduce a novel evaluation metric called the maximum profit uplift (MPU) measure that allows assessing the performance in terms of the maximum potential profit that can be achieved by adopting an uplift model. This measure, developed for evaluating customer churn uplift models, extends the maximum profit measure for evaluating customer churn prediction models. While introducing the MPU measure, we describe the generally applicable liftup curve and liftup measure for evaluating uplift models as counterparts of the lift curve and lift measure that are broadly used to evaluate predictive models. These measures are subsequently applied to assess and compare the performance of customer churn prediction and uplift models in a case study that applies uplift modeling to customer retention in the financial industry. We observe that uplift models outperform predictive models and lead to improved profitability of retention campaigns.},
	number = {xxxx},
	journal = {Information Sciences},
	author = {Devriendt, Floris and Berrevoets, Jeroen and Verbeke, Wouter},
	year = {2020},
	note = {Publisher: Elsevier Inc.},
	keywords = {Customer churn prediction, Customer retention, Maximum profit, Prescriptive analytics, Uplift modeling},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NVSV2IEX\\Why You Should Stop Predicting Customer Churn and Start Using Uplift Models.pdf:application/pdf},
}

@book{Zipf49,
	title = {Human {Behavior} and the {Principle} of {Least} {Effort}},
	publisher = {Addison-Wesley (Reading MA)},
	author = {Zipf, George K},
	year = {1949},
	keywords = {Applications, Natural Language Processing, bibtex-import},
}

@book{street2020great,
	title = {The {Great} {Mental} {Models}: {General} {Thinking} {Concepts}},
	isbn = {978-1-9994490-0-1},
	url = {https://books.google.com/books?id=ffUpywEACAAJ},
	publisher = {LATTICEWORK PUB Incorporated},
	author = {Street, F},
	year = {2020},
	note = {Issue: v. 1},
}

@article{Brants,
	title = {Large {Language} {Models} in {Machine} {Translation}},
	abstract = {This paper reports on the benefits of large-scale statistical language modeling in ma-chine translation. A distributed infrastruc-ture is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabil-ities for fast, single-pass decoding. We in-troduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.},
	author = {Brants, Thorsten and Popat, Ashok C and Xu, Peng and Och, Franz J and Dean, Jeffrey},
	keywords = {Applications, Natural Language Processing},
	pages = {858--867},
}

@article{Heeman,
	title = {{POS} {Tags} and {Decision} {Trees} for {Language} {Modeling}},
	url = {http://www.aclweb.org/anthology/W99-0617},
	abstract = {Language models for speech recognition con-centrate solely on recognizing the words that were spoken. In this paper, we advocate re-defining the speech recognition problem so that its goal is to find both the best sequence of words and their POS tags, and thus incorpo-rate POS tagging. To use POS tags effectively, we use clustering and decision tree algorithms, which allow generalizations between POS tags and words to be effectively used in estimating the probability distributions. We show that our POS model gives, a reduction in word error rate and perplexity for the Trains corpus in compar-ison to word and class-based approaches. By using the Wall Street Journal corpus, we show that this approach scales up when more training data is available.},
	author = {Heeman, Peter A},
	keywords = {Applications, Natural Language Processing},
}

@article{Witten:2006:ZPE:2263404.2271157,
	title = {The {Zero}-frequency {Problem}: {Estimating} the {Probabilities} of {Novel} {Events} in {Adaptive} {Text} {Compression}},
	volume = {37},
	issn = {0018-9448},
	url = {http://dx.doi.org/10.1109/18.87000},
	doi = {10.1109/18.87000},
	number = {4},
	journal = {IEEE Trans. Inf. Theor.},
	author = {Witten, I H and Bell, T C},
	month = sep,
	year = {2006},
	note = {Publisher: IEEE Press
Place: Piscataway, NJ, USA},
	pages = {1085--1094},
}

@misc{BeautifulEvidencePdf,
	title = {Beautiful {Evidence}.pdf},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MC8CH3CK\\Beautiful_Evidence.pdf:application/pdf},
}

@article{Taylor2010,
	title = {The {Decline} of {Marriage} {And} {Rise} of {New} {Families}},
	url = {http://pewsocialtrends.org},
	abstract = {The transformative trends of the past 50 years that have led to a sharp decline in marriage and a rise of new family forms have been shaped by attitudes and behaviors that differ by class, age and race, according to a new Pew Research Center nationwide survey complemented by an analysis of demographic and economic data from the U.S. Census Bureau. A new ―marriage gap‖ in the United States is increasingly aligned with a growing income gap. Marriage, while declining among all groups, remains the norm for adults with a college education and good income but is now markedly less prevalent among those on the lower rungs of the socio-economic ladder. The survey finds that those in this less-advantaged group are as likely as others to want to marry, but they place a higher premium on economic security as a condition for marriage. This is a bar that many may not meet. The survey also finds striking differences by generation. In 1960, two-thirds (68\%) of all twenty-somethings were married. In 2008, just 26\% were. How many of today's youth will eventually marry is an open question. For now, the survey finds that the young are much more inclined than their elders to view cohabitation without marriage and other new family forms — such as same sex marriage and interracial marriage — in a positive light. Even as marriage shrinks, family— in all its emerging varieties — remains resilient. The survey finds that Americans have an expansive definition of what constitutes a family. And the vast majority of adults consider their own family to be the most important, most satisfying element of their lives.},
	journal = {Pew Research Center},
	author = {Taylor, Paul},
	year = {2010},
}

@book{engelhard2016synthetic,
	title = {Synthetic {Biology} {Analysed}: {Tools} for {Discussion} and {Evaluation}},
	isbn = {978-3-319-25145-5},
	url = {https://books.google.com/books?id=894FDAAAQBAJ},
	publisher = {Springer International Publishing},
	author = {Engelhard, M},
	year = {2016},
	note = {Series Title: Ethics of Science and Technology Assessment},
}

@article{Report2018,
	title = {Procurement automation trend report},
	author = {Report, Trend},
	year = {2018},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2PHT3KEV\\paystream-procurement-insight-report.pdf:application/pdf},
}

@article{Crystal,
	title = {Txting: {The} gr8 db8},
	author = {Crystal, David and Clark, Rykia and Timmons, Taylor and Kanski, Alison and Campbell, Jeanna and Reese, Hanna and Murphy, Katey},
	keywords = {Applications, Natural Language Processing},
}

@incollection{Jurafsky2016b,
	title = {Regular {Expressions}, {Text} {Normalization}, {Edit} {Distance}},
	author = {Jurafsky, Daniel and Martin, James H and {Martin}},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Taylor2014,
	title = {Simpson's paradox: {A} data set and discrimination case study exercise},
	volume = {22},
	issn = {10691898},
	doi = {10.1080/10691898.2014.11889697},
	abstract = {In this article, we present a data set and case study exercise that can be used by educators to teach a range of statistical concepts including Simpson's paradox. The data set and case study are based on a real-life scenario where there was a claim of discrimination based on ethnicity. The exercise highlights the importance of performing rigorous statistical analysis and how data interpretations can accurately inform or misguide decision makers. ©2014 by Stanley A.},
	number = {1},
	journal = {Journal of Statistics Education},
	author = {Taylor, Stanley A and Mickel, Amy E},
	year = {2014},
	keywords = {Outliers, Bivariate analysis, Simpson's paradox, Specific variation, Univariate analysis, Weighted average},
	pages = {3--5},
}

@article{2DI70StatisticalLearning2016,
	title = {{2DI70} - {Statistical} {Learning} {Theory} {Lecture} {Notes}},
	year = {2016},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N26RHERN\\Statistical Learning Theory Lecture Notes - R. Castro.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PPRCB323\\Statistical Learning Theory Lecture Notes - R. Castro.pdf:application/pdf},
}

@misc{Xie2016a,
	title = {knitr: {A} {General}-{Purpose} {Package} for {Dynamic} {Report} {Generation} in {R}},
	url = {https://github.com/yihui/knitr},
	publisher = {https://github.com/yihui/knitr},
	author = {Xie, Yihui},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Harrison,
	title = {Hedonic housing prices and the demand for clean air},
	volume = {5},
	issn = {10960449},
	doi = {10.1016/0095-0696(78)90006-2},
	abstract = {This paper investigates the methodological problems associated with the use of housing market data to measure the willingness to pay for clean air. With the use of a hedonic housing price model and data for the Boston metropolitan area, quantitative estimates of the willingness to pay for air quality improvements are generated. Marginal air pollution damages (as revealed in the housing market) are found to increase with the level of air pollution and with household income. The results are relatively sensitive to the specification of the hedonic housing price equation, but insensitive to the specification of the air quality demand equation. ©1978.},
	number = {1},
	journal = {Journal of Environmental Economics and Management},
	author = {Harrison, David and Rubinfeld, Daniel L},
	year = {1978},
	pages = {81--102},
}

@techreport{Bengio2012a,
	title = {{ADVANCES} {IN} {OPTIMIZING} {RECURRENT} {NETWORKS}},
	abstract = {After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.},
	author = {Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan and Montreal, U},
	year = {2012},
	note = {arXiv: 1212.0901v2},
	keywords = {Algorithms, (), Deep Learning, Recurrent Neural Networks, RNN},
}

@article{Tatarenko2017,
	title = {Non-{Convex} {Distributed} {Optimization}},
	volume = {62},
	issn = {00189286},
	doi = {10.1109/TAC.2017.2648041},
	abstract = {We study distributed non-convex optimization on a time-varying multi-agent network. Each node has access to its own smooth local cost function, the collective goal is to minimize the sum of these functions. The perturbed push-sum algorithm was previously used for convex distributed optimization. We generalize the result obtained for the convex case to the case of non-convex functions. Under some additional technical assumptions on the gradients we prove the convergence of the distributed push-sum algorithm to some critical point of the objective function. By utilizing perturbations on the update process, we show the almost sure convergence of the perturbed dynamics to a local minimum of the global objective function, if the objective function has no saddle points. Our analysis shows that this perturbed procedure converges at a rate of O(1/t).},
	number = {8},
	journal = {IEEE Transactions on Automatic Control},
	author = {Tatarenko, Tatiana and Touri, Behrouz},
	year = {2017},
	note = {arXiv: 1512.00895},
	keywords = {Non-convex optimization, time-varying multi-agent},
	pages = {3744--3757},
}

@misc{BYUCorporaBillions,
	title = {{BYU} corpora: billions of words of data: free online access},
	url = {http://corpus.byu.edu/},
	urldate = {2017-03-17},
	keywords = {Applications, Natural Language Processing},
}

@article{Kolter2015,
	title = {15-780 : {Optimization}},
	author = {Kolter, J Zico},
	year = {2015},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZKMD7P9L\\optimization.pdf:application/pdf},
}

@article{Zhang2019c,
	title = {Manifold: {A} {Model}-{Agnostic} {Framework} for {Interpretation} and {Diagnosis} of {Machine} {Learning} {Models}},
	volume = {25},
	issn = {19410506},
	doi = {10.1109/TVCG.2018.2864499},
	abstract = {Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zhang, Jiawei and Wang, Yang and Molino, Piero and Li, Lezhi and Ebert, David S},
	year = {2019},
	pmid = {30130197},
	note = {arXiv: 1808.00196},
	keywords = {Interactive machine learning, model comparison, model debugging, performance analysis},
	pages = {364--373},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WANVTYH3\\Manifold - A Model-Agnostic Framework for Interpretationand Diagnosis of Machine Learning Models.pdf:application/pdf},
}

@article{Crystal2009,
	title = {Txtng: the {Gr8} {Db8}},
	volume = {44},
	issn = {0046208X},
	url = {http://books.google.es/books?id=Nxp3PgAACAAJ},
	doi = {10.1177/0094306110361589k},
	abstract = {The article reviews the book “Txtng: the gr8 db8," by David Crystal.},
	number = {1},
	journal = {English in Australia},
	author = {Crystal, David},
	year = {2009},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISBN: 019156267X},
	pages = {81--83},
}

@misc{Silvola,
	title = {{VariKN} toolkit},
	url = {http://vsiivola.github.io/variKN/},
	urldate = {2017-04-03},
	author = {Silvola, Vesa},
	keywords = {Applications, Natural Language Processing},
}

@article{Murray2013,
	title = {Measuring the {Global} {Burden} of {Disease}},
	volume = {369},
	issn = {0028-4793},
	url = {http://www.nejm.org/doi/10.1056/NEJMra1201534},
	doi = {10.1056/NEJMra1201534},
	abstract = {The burden that disease places on societies around the world is complex and changing as communicable diseases are replaced by noncommunicable diseases. This article summarizes these changes and the current burden.},
	number = {5},
	journal = {New England Journal of Medicine},
	author = {Murray, Christopher J L and Lopez, Alan D},
	month = aug,
	year = {2013},
	pmid = {23902484},
	note = {ISBN: 1533-4406 (Electronic)\${\textbackslash}\$r0028-4793 (Linking)},
	pages = {448--457},
}

@book{Guiraud1954,
	address = {Paris},
	title = {Les caracteres statistique du vocabulaire},
	publisher = {Presses Universitaires de France},
	author = {Guiraud, Pierre},
	year = {1954},
	keywords = {Applications, Natural Language Processing},
}

@incollection{JelMer80,
	address = {Amsterdam},
	title = {Interpolated estimation of \{{M}\}arkov source parameters from sparse data},
	booktitle = {Proceedings, {Workshop} on {Pattern} {Recognition} in {Practice}},
	publisher = {North Holland},
	author = {Jelinek, Fred and Mercer, Robert L},
	editor = {Gelsema, Edzard S and Kanal, Laveen N},
	year = {1980},
	keywords = {Applications, Natural Language Processing, 2000 book nlp},
	pages = {381--397},
}

@article{Wang,
	title = {Exploiting {Syntactic}, {Semantic} and {Lexical} {Regularities} in {Language} {Modeling} via {Directed} {Markov} {Random} {Fields}},
	abstract = {We present a directed Markov random field (MRF) model that combines n-gram models, probabilistic context free grammars (PCFGs) and probabilistic latent semantic analysis (PLSA) for the purpose of statistical language modeling. Even though the composite directed MRF model potentially has an exponential number of loops and becomes a context sensitive grammar, we are nevertheless able to estimate its parameters in cu-bic time using an efficient modified EM method, the generalized inside-outside algorithm, which extends the inside-outside algorithm to incorpo-rate the effects of the n-gram and PLSA lan-guage models. We generalize various smooth-ing techniques to alleviate the sparseness of n-gram counts in cases where there are hidden vari-ables. We also derive an analogous algorithm to calculate the probability of initial subsequence of a sentence, generated by the composite lan-guage model. Our experimental results on the Wall Street Journal corpus show that we obtain significant reductions in perplexity compared to the state-of-the-art baseline trigram model with Good-Turing and Kneser-Ney smoothings.},
	author = {Wang, Shaojun and Wang, Shaomin and Greiner, Russell and Schuurmans, Dale and Cheng, Li},
	keywords = {Applications, Natural Language Processing},
}

@inproceedings{8473275,
	title = {A state-of-the-art {Recommender} {Systems}: {An} overview on {Concepts}, {Methodology} and {Challenges}},
	doi = {10.1109/ICICCT.2018.8473275},
	abstract = {Recommender systems have become applications which probe large information space and assist customers in selecting items of their interest. For decades, many techniques have been proposed and improved on recommendation such as content-based, collaborative, knowledge-based etc. This paper aims at presenting a structural model for the recommendation systems, the current generation of recommendation methods and challenges in them. By providing state-of-the-art knowledge, the paper will help researchers and practical professional in their understanding of recommender systems and also leads towards the future direction in the development of recommendations.},
	booktitle = {2018 {Second} {International} {Conference} on {Inventive} {Communication} and {Computational} {Technologies} ({ICICCT})},
	author = {Jariha, P and Jain, S K},
	month = apr,
	year = {2018},
	keywords = {Applications, Recommender Systems, groupware;recommender systems;recommendation metho},
	pages = {1769--1774},
}

@article{Chen,
	title = {{IBM} {Research} {Report} {Performance} {Prediction} for {Exponential} {Language} {Models} {Performance} {Prediction} for {Exponential} {Language} {Models}},
	abstract = {We investigate the task of performance prediction for language models belonging to the exponential family. First, we attempt to empirically discover a formula for predicting test set cross-entropy for n-gram language models. We build models over varying domains, data set sizes, and n-gram orders, and perform linear regression to see whether we can model test set performance as a simple function of training set performance and various model statistics. Re-markably, we discover a very simple relationship that predicts test performance with a correla-tion of 0.9996. We provide analysis of why this relationship holds, and show how this relation-ship can be used to motivate two heuristics for improving existing language models. We use the first heuristic to develop a novel class-based language model that outperforms a baseline word trigram model by up to 28\% in perplexity and 2.1\% absolute in speech recognition word-error rate on Wall Street Journal data. We use the second heuristic to provide a new motivation for minimum discrimination information (MDI) models (Della Pietra et al., 1992), and show how this method outperforms other methods for domain adaptation on a Wall Street Journal data set.},
	author = {Chen, Stanley F},
	keywords = {Applications, Natural Language Processing},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KGLM4SCW\\Performance Prediction for Exponential Language Models.pdf:application/pdf},
}

@article{Mann,
	title = {On a {Test} of {Whether} {One} of {Two} {Random} {Variables} is {Stochastically} {Larger} {Than} {The} {Other}},
	author = {Mann, H B and D.R., Whitney},
}

@misc{AmericanPsychiatricAssociation2017,
	title = {What {Is} {Depression}?},
	url = {https://www.psychiatry.org/patients-families/depression/what-is-depression},
	urldate = {2017-06-13},
	author = {{American Psychiatric Association}},
	year = {2017},
}

@article{Gries2014,
	title = {Quantitative corpus approaches to linguistic analysis: seven or eight levels of resolution and the lessons they teach us},
	volume = {i},
	doi = {10.1017/CBO9781139833882.005},
	number = {3},
	journal = {Developments in English: expanding electronic evidence},
	author = {Gries, Stefan Th.},
	year = {2014},
	note = {ISBN: 9781107038509},
	keywords = {Applications, Natural Language Processing},
	pages = {29--47},
}

@article{Heafield,
	title = {Efficient {Language} {Modeling} {Algorithms} with {Applications} to {Statistical} {Machine} {Translation}},
	url = {www.lti.cs.cmu.edu},
	abstract = {N -gram language models are an essential component in statistical natural language processing systems for tasks such as machine translation, speech recognition, and optical character recognition. They are also re-sponsible for much of the computational costs. This thesis contributes efficient algorithms for three language modeling problems: estimating probabilities from corpora, representing a model in memory, and searching for high-scoring output when log language model probability is part of the score. Most existing language modeling toolkits operate in RAM, effectively limiting model size. This work contributes disk-based streaming algorithms that use a configurable amount of RAM to estimate Kneser-Ney language models 7.13 times as fast as the popular SRILM toolkit. Scaling to 126 billion tokens led to first-place performance in the 2013 Workshop on Machine Translation for all three language pairs where submissions were made. Query speed is critical because a machine translation system makes millions of queries to translate one sentence. Thus, language models are typically queried in RAM, where size is a concern. This work contributes two near-lossless data structures for efficient storage and querying. The first, based on linear probing hash tables, responds to queries 2.42 times as fast as the SRILM toolkit while using 57\% of the memory. The second, based on sorted arrays, is faster than all baselines and uses less memory than all lossless baselines. Searching for high-scoring output is difficult because log language model probabilities do not sum when strings are concatenated. This thesis contributes a series of optimizations that culminate in a new approx-imate search algorithm. The algorithm applies to search spaces expressed as lattices and, more generally, hypergraphs that arise in many natural language tasks. Experiments with syntactic machine translation show that the new algorithm attains various levels of accuracy 3.25 to 10.01 times as fast as the popular cube pruning algorithm with SRILM.},
	author = {Heafield, Kenneth and Lavie, Alon and Dyer, Chris and Raj, Bhiksha and Koehn, Philipp},
	keywords = {Applications, Natural Language Processing},
}

@book{Bober2013,
	title = {Introduction to {Numerical} and {Analytical} {Methods} with {MATLAB} for {Engineers} and {Scientists}},
	isbn = {978-1-4665-7609-4},
	abstract = {Introduction to Numerical and Analytical Methods with MATLAB®for Engineers and Scientists provides the basic concepts of programming in MATLAB for engineering applications. • Teaches engineering students how to write computer programs on the MATLAB platform • Examines the selection and use of numerical and analytical methods through examples and case studies • Demonstrates mathematical concepts that can be used to help solve engineering problems, including matrices, roots of equations, integration, ordinary differential equations, curve fitting, algebraic linear equations, and more The text covers useful numerical methods, including interpolation, Simpson's rule on integration, the Gauss elimination method for solving systems of linear algebraic equations, the Runge-Kutta method for solving ordinary differential equations, and the search method in combination with the bisection method for obtaining the roots of transcendental and polynomial equations. It also highlights MATLAB's built-in functions. These include interp1 function, the quad and dblquad functions, the inv function, the ode45 function, the fzero function, and many others. The second half of the text covers more advanced topics, including the iteration method for solving pipe flow problems, the Hardy-Cross method for solving flow rates in a pipe network, separation of variables for solving partial differential equations, and the use of Laplace transforms to solve both ordinary and partial differential equations. This book serves as a textbook for a first course in numerical methods using MATLAB to solve problems in mechanical, civil, aeronautical, and electrical engineering. It can also be used as a textbook or as a reference book in higher level courses.},
	author = {Bober, William},
	year = {2013},
	doi = {10.1201/b16030},
	note = {Publication Title: Introduction to Numerical and Analytical Methods with MATLAB for Engineers and Scientists},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9X9H8P7K\\Introduction_to_Numerical_and_Analytical.pdf:application/pdf},
}

@misc{Wickham2016,
	title = {R {Package} 'ggplot2' {\textbar} {Create} {Elegant} {Data} {Visualisations} {Using} the {Grammar} of {Graphics}},
	author = {Wickham, Hadley},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Chatzimichailidis2019,
	title = {{GradVis}: {Visualization} and second order analysis of optimization surfaces during the training of deep neural networks},
	doi = {10.1109/MLHPC49564.2019.00012},
	abstract = {Current training methods for deep neural networks boil down to very high dimensional and non-convex optimization problems which are usually solved by a wide range of stochastic gradient descent methods. While these approaches tend to work in practice, there are still many gaps in the theoretical understanding of key aspects like convergence and generalization guarantees, which are induced by the properties of the optimization surface (loss landscape). In order to gain deeper insights, a number of recent publications proposed methods to visualize and analyze the otimization surfaces. However, the computational cost of these methods are very high, making it hardly possible to use them on larger networks. In this paper, we present the GradVis Toolbox, an open source library for efficient and scalable visualization and analysis of deep neural network loss landscapes in Tesorflow and PyTorch. Introducing more efficient mathematical formulations and a novel parallelization scheme, GradVis allows to plot 2d and 3d projections of optimization surfaces and trajectories, as well as high resolution second order gradient information for large networks.},
	journal = {Proceedings of MLHPC 2019: 5th Workshop on Machine Learning in HPC Environments - Held in conjunction with SC 2019: The International Conference for High Performance Computing, Networking, Storage and Analysis},
	author = {Chatzimichailidis, Avraam and Keuper, Janis and Pfreundt, Franz Josef and Gauger, Nicolas R},
	year = {2019},
	note = {arXiv: 1909.12108
ISBN: 9781728159850},
	keywords = {Visualization, Deeplearning, Eigenvalues, Parallelization, Second-order},
	pages = {66--74},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DFBQVZJQ\\GradVis  Visualization and Second Order Analysis of Optimization.pdf:application/pdf},
}

@article{Pentland2012,
	title = {The new science of building great teams},
	volume = {90},
	issn = {00178012},
	number = {4},
	journal = {Harvard Business Review},
	author = {Pentland, Alex Sandy},
	year = {2012},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DHMFKEV4\\hbr-new-science-of building great teams 2012.pdf:application/pdf},
}

@article{Databricks,
	title = {Solving {Four} {Big} {Problems} in {Data} {Science} {Table} of {Contents}},
	author = {{Databricks}},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P6ZQ5B7C\\Solving-Four-Big-Problems-in-Data-Science.pdf:application/pdf},
}

@article{Lyddy,
	title = {An {Analysis} of {Language} in {University} {Students}' {Text} {Messages} *},
	doi = {10.1111/jcc4.12045},
	abstract = {Concerns over effects of 'textisms' on literacy have been reinforced by research identifying processing costs associated with reading textisms. But to what extent do such studies reflect actual textism use? This study examined the textual characteristics of 936 text messages in English (13391 words). Message length, nonstandard spelling, sender and message characteristics and word frequency were analyzed. The data showed that 25\% of word content used nonstandard spelling, the most frequently occurring category involving omission of capital letters. Types of nonstandard spelling varied only slightly depending on the purpose of the text message, while the overall proportion of nonstandard spelling did not differ significantly. Less than 0.2\% of content was 'semantically unrecoverable.' Implications for experimental studies of textisms are discussed. Text messaging, short message service (SMS) or 'texting' continues to be a popular means of communi-cation, among young people in particular. A report by Lenhart, Ling, Campbell, and Purcell (2010) high-lighted the rapid increase in text messaging in the United States, where 72\% of teenagers use text messag-ing, compared to 51\% in 2006. In a British survey, 52\% of young people aged 11-18 (n = 1000), along with 28\% of adults aged 18-65 (n = 2000), named texting as the most important form of communication that they use to stay in touch with friends (Mobile Life Report, 2008); for the young people surveyed, texting ranked above instant messaging (17\%), e-mail (12\%), calls via mobile (9\%) or landline (10\%), and letters (0\%). A British survey of 2117 adults shows the increasing popularity of texting from 2005 to 2010, with 62\% of those aged 16-24 preferring texting over other means of communicating with friends (Ofcom, 2011). Ling's (2010) cross-sectional analysis suggests that texting follows a life-phase pattern, with older teens and those in their early 20s making the most use of the medium, with usage dropping off with age. The authors would like to thank Maria Bakardjieva and two anonymous reviewers for helpful criticisms and suggestions regarding data presentation and analysis.},
	author = {Lyddy, Fiona and Farina, Francesca and Hanney, James and Farrell, Lynn and Kelly, Niamh and Neill, O '},
	keywords = {Applications, Natural Language Processing, Language use, Linguistic, Literacy, Mobile phones, Text messaging},
}

@misc{IMDbstats2015,
	title = {{IMDb} {Database} {Statistics}},
	url = {http://www.imdb.com/stats},
	urldate = {2017-11-24},
	author = {{IMDbstats}},
	year = {2015},
	note = {Pages: 14-15
Publication Title: IMDb.com
Volume: 009},
	keywords = {Algorithms, Bayesian},
}

@article{Baayen2015,
	title = {Package '{languageR}' {Title} {Data} sets and {functionswiAnalyzing} {Linguistic} {Data}: {A} practical introduction to statistics''},
	author = {Baayen, R H and Baayen, Maintainer R H},
	year = {2015},
}

@article{Raskutti2014,
	title = {Early stopping and non-parametric regression: {An} optimal data-dependent stopping rule},
	volume = {15},
	issn = {15337928},
	abstract = {Early stopping is a form of regularization based on choosing when to stop running an iterative algorithm. Focusing on non-parametric regression in a reproducing kernel Hilbert space, we analyze the early stopping strategy for a form of gradient-descent applied to the least-squares loss function. We propose a data-dependent stopping rule that does not involve hold-out or cross-validation data, and we prove upper bounds on the squared error of the resulting function estimate, measured in either the L2 (ℙ) and L2 (ℙn) norm. These upper bounds lead to minimax-optimal rates for various kernel classes, including Sobolev smoothness classes and other forms of reproducing kernel Hilbert spaces. We show through simulation that our stopping rule compares favorably to two other stopping rules, one based on hold-out data and the other based on Stein's unbiased risk estimate. We also establish a tight connection between our early stopping strategy and the solution path of a kernel ridge regression estimator. ©2014 Garvesh Raskutti, Martin J. Wainwright and Bin Yu.},
	journal = {Journal of Machine Learning Research},
	author = {Raskutti, Garvesh and Wainwright, Martin J and Yu, Bin},
	year = {2014},
	note = {arXiv: 1306.3574},
	keywords = {Early stopping, Empirical processes, Kernel ridge regression, Non-parametric regression, Rademacher complexity, Reproducing kernel hilbert space, Stopping rule},
	pages = {335--366},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GE98B2XT\\Early Stopping and Non-parametric Regression - An Optimal Data-dependent Stopping Rule.pdf:application/pdf},
}

@book{Perez2003,
	title = {Python {Data} visualization},
	isbn = {978-1-107-67181-2},
	author = {Pérez, Maraña},
	year = {2003},
	pmid = {26840611},
	doi = {10.1002/ejoc.201200111},
	note = {arXiv: 1011.1669v3
Publication Title: WD info
ISSN: 0196-6553},
}

@techreport{Jin2017,
	title = {How to {Escape} {Saddle} {Points} {Efficiently}},
	abstract = {This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost "dimension-free"). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.},
	author = {Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
	year = {2017},
	note = {arXiv: 1703.00887v1
ISBN: 1703.00887v1},
	keywords = {()},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HUCXJHYQ\\How to Escape Saddle Points Efficiently.pdf:},
}

@article{Clauset2009,
	title = {{POWER}-{LAW} {DISTRIBUTIONS} {IN} {EMPIRICAL} {DATA}},
	abstract = {Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution—the part of the distribution representing large but rare events— and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.},
	author = {Clauset, Aaron and Rohilla Shalizi, Cosma and J Newman, M E},
	year = {2009},
	note = {arXiv: 0706.1062v2},
	keywords = {Applications, Natural Language Processing, ()},
}

@book{Zipf1935,
	title = {The {Psycho}-{Biology} {Of} {Language}: {AN} {INTRODUCTION} {TO} {DYNAMIC} {PHILOLOGY}},
	volume = {ix},
	isbn = {978-0-415-20976-2},
	url = {http://psycnet.apa.org/psycinfo/1935-04756-000},
	abstract = {Frequency counts of phonemes, morphemes, and words in samples of written discourse in diverse languages are presented in support of the generalization that the more complex any speech element, the less frequently does it occur. Thus, the greater the frequency of occurrence of words, the less tends to be their average length, and the smaller also is the number of different words. The relation between frequency and number of different words is said to be expressed by the formula ab2 = k, in which a represents the number of different words of a given frequency and b the frequency. The relationship between the magnitude of speech elements and their frequency is attributed to the operation of a "law" of linguistic change: that as the frequency of phonemes or of linguistic forms increases, their magnitude decreases. There is thus a tendency to "maintain an equilibrium" between length and frequency, and this tendency rests upon an "underlying law of economy." Human beings strive to maintain an "emotional equilibrium" between variety and repetitiveness of environmental factors and behavior. A speaker's discourse must represent a compromise between variety and repetitiveness adapted to the hearer's "tolerable limits of change in maintaining emotional equilibrium." This accounts for the maintenance of the relationship ab2 = k; the exponent of b expresses this "rate of variegation."},
	publisher = {Houghton Mifflin Company},
	author = {Zipf, George Kingsley},
	year = {1935},
	pmid = {6891221},
	doi = {10.1097/00005053-193701000-00041},
	note = {ISSN: 0022-3018},
	keywords = {Applications, Natural Language Processing},
}

@article{SanfeliceBazanella2012,
	title = {Iterative optimization},
	issn = {21977119},
	doi = {10.1007/978-94-007-2300-9_4},
	abstract = {In many practical cases the theoretical conditions required by the “one-shot” solutions are not met. Moreover, operational constraints of the process often require that only small adjustments to existing parameter settings can be made, which precludes the application of VRFT in these situations. In such cases the data-driven control design must be performed through iterative procedures in which each iteration requires collecting more data, each time with a different controller in the loop. In Chap. 4 a general review of basic optimization theory is given, setting the stage for the chapters to follow. The basic convergence properties of the basic optimization algorithms—steepest descent and Newton-Raphson in particular—are analyzed. Some robustness properties, that is, convergence under imprecise information, of these algorithms are also demonstrated.},
	number = {9789400722996},
	journal = {Communications and Control Engineering},
	author = {Sanfelice Bazanella, Alexandre and Campestrini, Lucíola and Eckhard, Diego},
	year = {2012},
	pages = {69--88},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IKWNYCNY\\Iterative Methods for Optimization.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\K4I68E9A\\Iterative Methods for Optimization.pdf:application/pdf},
}

@article{Puri1971,
	title = {Introduction to {Statistics}.},
	volume = {27},
	issn = {0006341X},
	doi = {10.2307/2528617},
	number = {3},
	journal = {Biometrics},
	author = {Puri, S C and Bevan, J M},
	year = {1971},
	pages = {751},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NINU9YMJ\\Online_Statistics_Education.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YLTTICHH\\Online_Statistics_Education.pdf:application/pdf},
}

@article{Chiu2008,
	title = {Introduction to {Data} {Mining}},
	doi = {10.1016/b978-0-7506-8234-3.00007-1},
	abstract = {OBJECTIVES: Reductions in exposure to environmental tobacco smoke have been shown to attenuate the risk of cardiovascular disease. We examined whether the 2003 implementation of a comprehensive smoking ban in New York State was associated with reduced hospital admissions for acute myocardial infarction and stroke, beyond the effect of moderate, local and statewide smoking restrictions, and independent of secular trends. METHODS: We analyzed trends in county-level, age-adjusted, monthly hospital admission rates for acute myocardial infarction and stroke from 1995 to 2004 to identify any association between admission rates and implementation of the smoking ban. We used regression models to adjust for the effects of pre-existing smoking restrictions, seasonal trends in admissions, differences across counties, and secular trends. RESULTS: In 2004, there were 3813 fewer hospital admissions for acute myocardial infarction than would have been expected in the absence of the comprehensive smoking ban. Direct health care cost savings of \$56 million were realized in 2004. There was no reduction in the number of admissions for stroke. CONCLUSIONS: Hospital admission rates for acute myocardial infarction were reduced by 8\% as a result of a comprehensive smoking ban in New York State after we controlled for other relevant factors. Comprehensive smoking bans constitute a simple, effective intervention to substantially improve the public's health},
	journal = {Data Mining and Market Intelligence for Optimal Marketing Returns},
	author = {Chiu, Susan and Tavella, Domingo},
	year = {2008},
	pages = {137--192},
}

@article{StateDevelopmentOperations2019,
	title = {The {State} of {Development} and {Operations} of {AI} {Applications}},
	year = {2019},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L7MM3W47\\Dotscience_Survey-Report-2019.pdf:application/pdf},
}

@article{Brownlee2018,
	title = {Basics of {Linear} {Algebra} for {Machine} {Learning}},
	abstract = {There is no doubt that linear algebra is important in machine learning. Linear algebra is the mathematics of data. It's all vectors and matrices of numbers. Modern statistics is described using the notation of linear algebra and modern statistical methods harness the tools of linear algebra. Modern machine learning methods are described the same way, using the notations and tools drawn directly from linear algebra. Even some classical methods used in the field, such as linear regression via linear least squares and singular-value decomposition, are linear algebra methods, and other methods, such as principal component analysis, were born from the marriage of linear algebra and statistics. To read and understand machine learning, you must be able to read and understand linear algebra.},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	year = {2018},
	pages = {196},
}

@article{Oreski2014,
	title = {Comparison of {Feature} {Selection} {Techniques} in {Knowledge} {Discovery} {Process}},
	volume = {3},
	url = {www.temjournal.com},
	abstract = {–The process of knowledge discovery in data consists of five steps. Data preparation, which includes data cleaning and feature selection, takes away from 60\% to 95\% total time of the whole process. Thus, it is crucial phase of the process. The purpose of this research is to investigate feature selection techniques performance by conducting empirical research. Our comparison of three feature selection techniques reveals significant difference in feature selection techniques performance.},
	number = {4},
	journal = {TEM Journal},
	author = {Oreski, Dijana and Novosel, Tomislav},
	year = {2014},
	keywords = {big data, – Data mining, classification accuracy, feature selection},
	pages = {285--290},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MSDUD83H\\Comparisonoffeatureselectiontechniquesinknowledgediscoveryprocess.pdf:application/pdf},
}

@book{Wierzbicka1996,
	title = {Semantics : primes and universals},
	isbn = {0-19-870002-4},
	abstract = {Includes previously published material rev. and expanded for this publication. 1. Introduction -- 2. Survey of Semantic Primitives -- 3. Universal Grammar: The Syntax of Universal Semantic Primitives -- 4. Prototypes and Invariant -- 5. Semantic Primitives and Semantic Fields -- 6. Semantics and "Primitive Thought" -- 7. Semantic Complexity and the Role of Ostension in the Acquisition of Concepts -- 8. Against "Against Definitions" -- 9. Semantics and Lexicography -- 10. Meaning of Colour Terms and the Universals of Seeing -- 11. Semantics of Natural Kinds -- 12. Semantics and Ethnobiology -- 13. Semantic Rules in Grammar -- 14. Semantic Basis for Grammatical Description and Typology: Transitivity and Reflexives -- 15. Comparing Grammatical Categories across Languages: The Semantics of Evidentials.},
	publisher = {Oxford University Press},
	author = {Wierzbicka, Anna.},
	year = {1996},
	keywords = {Applications, Natural Language Processing},
}

@inproceedings{DBLP:journals/corr/ZhangCL14a,
	title = {Deep learning with {Elastic} {Averaging} \{{SGD}\}},
	url = {http://arxiv.org/abs/1412.6651},
	booktitle = {\{{ICLR}\} ({Workshop})},
	author = {Zhang, Sixin and Choromanska, Anna and LeCun, Yann},
	year = {2015},
}

@article{TurningOurExperience,
	title = {Turning our experience into your expertise {BSI} {Medical} {Devices}},
}

@article{sabanésbové2011,
	title = {Hyper-\$g\$ priors for generalized linear models},
	volume = {6},
	url = {https://doi.org/10.1214/11-BA615},
	doi = {10.1214/11-BA615},
	number = {3},
	journal = {Bayesian Anal.},
	author = {Sabanés Bové, Daniel and Held, Leonhard},
	year = {2011},
	note = {Publisher: International Society for Bayesian Analysis},
	pages = {387--410},
}

@article{Beyazit2019,
	title = {Online {Learning} from {Data} {Streams} with {Varying} {Feature} {Spaces}},
	volume = {33},
	issn = {2159-5399},
	doi = {10.1609/aaai.v33i01.33013232},
	abstract = {We study the problem of online learning with varying feature spaces. The problem is challenging because, unlike traditional online learning problems, varying feature spaces can introduce new features or stop having some features without following a pattern. Other existing methods such as online streaming feature selection (Wu et al. 2013), online learning from trapezoidal data streams (Zhang et al. 2016), and learning with feature evolvable streams (Hou, Zhang, and Zhou 2017) are not capable to learn from arbitrarily varying feature spaces because they make assumptions about the feature space dynamics. In this paper, we propose a novel online learning algorithm OLVF to learn from data with arbitrarily varying feature spaces. The OLVF algorithm learns to classify the feature spaces and the instances from feature spaces simultaneously. To classify an instance, the algorithm dynamically projects the instance classifier and the training instance onto their shared feature subspace. The feature space classifier predicts the projection confidences for a given feature space. The instance classifier will be updated by following the empirical risk minimization principle and the strength of the constraints will be scaled by the projection confidences. Afterwards, a feature sparsity method is applied to reduce the model complexity. Experiments on 10 datasets with varying feature spaces have been conducted to demonstrate the performance of the proposed OLVF algorithm. Moreover, experiments with trapezoidal data streams on the same datasets have been conducted to show that OLVF performs better than the state-of-the-art learning algorithm (Zhang et al. 2016).},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Beyazit, Ege and Alagurajah, Jeevithan and Wu, Xindong},
	year = {2019},
	pages = {3232--3239},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5EG94M4Y\\Online Learning Variable Features.pdf:application/pdf},
}

@techreport{62,
	title = {6-2},
}

@misc{Dua:2019,
	title = {\{{UCI}\} {Machine} {Learning} {Repository}},
	url = {http://archive.ics.uci.edu/ml},
	publisher = {University of California, Irvine, School of Information and Computer Sciences},
	author = {Dua, Dheeru and Graff, Casey},
	year = {2017},
}

@article{Kasesniemi2002,
	title = {Mobile culture of children and teenagers in {Finland}},
	author = {Kasesniemi, Eija-Liisa and Rautianen, Pirjo},
	year = {2002},
	keywords = {Applications, Natural Language Processing},
}

@techreport{Qian,
	title = {On the momentum term in gradient descent learning algorithms},
	abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
	author = {Qian, Ning},
	keywords = {Critical damping, Damped harmonic oscillator, Gradient descent learning algorithm, Learning rate, Momentum, Speed of convergence},
}

@article{ChapterGradientbasedOptimization,
	title = {Chapter 3 {Gradient}-based optimization},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XGELSVLE\\Gradient-based optimization.pdf:application/pdf},
}

@article{Steele2009,
	title = {Performance of {Bayesian} {Model} {Selection} {Criteria} for {Gaussian} {Mixture} {Models}},
	abstract = {The authors are grateful to Matthew Stephens for helpful discussions. Abstract Bayesian methods are widely used for selecting the number of components in a mixture models, in part because frequentist methods have difficulty in addressing this problem in general. Here we compare some of the Bayesianly motivated or justifiable methods for choosing the number of components in a one-dimensional Gaussian mixture model: posterior probabilities for a well-known proper prior, BIC, ICL, DIC and AIC. We also introduce a new explicit unit-information prior for mixture models, analogous to the prior to which BIC corresponds in regular statistical models. We base the comparison on a simulation study, designed to reflect published estimates of mixture model parameters from the scientific literature across a range of disciplines. We found that BIC clearly outperformed the five other methods, with the maximum a posteriori estimate from the established proper prior second.},
	author = {Steele, Russell J and Raftery, Adrian E},
	year = {2009},
	keywords = {Algorithms, Bayesian},
}

@article{Brychcn2012,
	title = {Unsupervised {Methods} for {Language} {Modeling}},
	url = {http://www.kiv.zcu.cz/publications/},
	abstract = {Language models are crucial for many tasks in NLP 1 and N-grams are the best way to build them. Huge effort is being invested in improving n-gram language models. By introducing external information (morphology, syntax, partitioning into documents, etc.) into the models a significant improvement can be achieved. The models can however be improved with no external information and smoothing is an excellent example of such an improvement. Thesis summarizes the state-of-the-art approaches to unsupervised lan-guage modeling with emphases on the inflectional languages, which are par-ticularly hard to model. It is focused on methods that can discover hidden patterns that are already in a training corpora. These patterns can be very useful for enhancing the performance of language modeling, moreover they do not require additional information sources.},
	journal = {PhD Study Report},
	author = {Brychcn, Tomas},
	year = {2012},
	keywords = {Applications, Natural Language Processing},
}

@misc{Services2014,
	title = {The {Numbers}: {Where} {Data} and the {Movie} {Business} {Meet}},
	url = {http://www.the-numbers.com/},
	urldate = {2017-11-22},
	author = {Services, Nash Information},
	year = {2014},
	note = {Publication Title: Nash Information Services, LLC},
	keywords = {Algorithms, Bayesian},
}

@article{Biber2004,
	title = {Conversation text types: {A} multi-dimensional analysis},
	abstract = {Multi-dimensional (MD) analysis is a methodological approach that applies multivariate statistical techniques (especially factor analysis and cluster analysis) to the investigation of register variation in a language. The approach was originally developed to analyze the full range of spoken and written registers in a language. Early studies focused on English register variation (Biber 1985, 1986 and 1988), while later studies have applied the same approach to Somali, Korean, Tuvaluan, Taiwanese, and Spanish. Surprisingly, these studies have found some striking similarities in the underlying 'dimensions' that distinguish among spoken and written registers in these diverse languages. It is even more surprising that MD studies of restricted discourse domains have also uncovered dimensions that are similar in linguistic form and function to the more general studies of register variation. The present study presents an MD analysis of a single register: conversation. Three primary dimensions of variation are identified, and then cluster analysis is used to distinguish among six conversation text types. The dimensions and text types are interpreted in linguistic and functional terms. The author's expectations were that a unique set of dimensions would emerge to characterize the variation among conversational texts. Instead, the three dimensions identified here turn out to be closely related to dimensions identified in previous analyses of general register variation. Taken together with previous studies, the present study of conversation raises the possibility of universal dimensions of variation.},
	journal = {JADT},
	author = {Biber, Douglas},
	year = {2004},
	keywords = {Applications, Natural Language Processing},
}

@article{Johnson1944a,
	title = {Studies in {Language} {Behavior}: {I}. {A} program of research},
	volume = {56},
	number = {2},
	author = {Johnson, W},
	year = {1944},
	keywords = {Applications, Natural Language Processing},
	pages = {1--15},
}

@article{Sosa2018,
	title = {Data objects: {Design} principles for data physicalisation},
	volume = {4},
	issn = {18479073},
	doi = {10.21278/idc.2018.0125},
	abstract = {This paper describes the principles, methods and strategies for the design of everyday objects that embody data-or Data Objects. The work presented in the paper connects the fields of industrial design and data physicalisation to introduce the concept of using data as a design material. To support the creative synthesis of Data Objects the paper provides a literature review, methods and guidance on the creation of Data Objects alongside examples-and possible opportunities, challenges, and future scenarios-for the practice, use and the study of Data Objects.},
	journal = {Proceedings of International Design Conference, DESIGN},
	author = {Sosa, R and Gerrard, V and Esparza, A and Torres, R and Napper, R},
	year = {2018},
	note = {ISBN: 9789537738594},
	keywords = {Big data analysis, Design activities, Design methods},
	pages = {1685--1696},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BRSGIHUP\\Data Objects Design Principles.pdf:application/pdf},
}

@article{Mezei2012,
	title = {Effects of {Word} {Prediction} on {Writing} {Fluency} for {Students} {With} {Physical} {Disabilities}},
	volume = {31},
	abstract = {Many students with physical disabilities have difficulty with writing fluency due to motor limitations. One type of assistive technology that has been developed to improve writing speed and accuracy is word prediction software, although there is a paucity of research supporting its use for individuals with physical disabilities. This study used an alternating treatment design between word prediction versus word pro- cessing to examine fluency, accuracy, and passage length on writing draft papers by individuals who have physical disabilities. Results indicated that word prediction had little to no effectiveness in increasing writing speed for all of the students in this study, but it shows promise in decreasing spelling and typographical errors.},
	number = {1},
	journal = {Physical Disabilities: Education and Related Services},
	author = {Mezei, Peter J and Wolff Heller, Kathryn},
	year = {2012},
	note = {ISBN: 978-1-124-04823-9},
	keywords = {Applications, Natural Language Processing},
	pages = {3--26},
}

@misc{wiki:gradient,
	title = {Gradient --- \{{Wikipedia}\}\{,\} {The} {Free} {Encyclopedia}},
	url = {https://en.wikipedia.org/w/index.php?title=Gradient&oldid=864573538},
	author = {contributors, Wikipedia},
	year = {2018},
}

@article{Hacioglu,
	title = {On {Combining} {Language} {Models} : {Oracle} {Approach}},
	url = {http://www.aclweb.org/anthology/H01-1058},
	abstract = {In this paper, we address the problem of combining several lan-guage models (LMs). We find that simple interpolation methods, like log-linear and linear interpolation, improve the performance but fall short of the performance of an oracle. The oracle knows the reference word string and selects the word string with the best per-formance (typically, word or semantic error rate) from a list of word strings, where each word string has been obtained by using a dif-ferent LM. Actually, the oracle acts like a dynamic combiner with hard decisions using the reference. We provide experimental results that clearly show the need for a dynamic language model combina-tion to improve the performance further. We suggest a method that mimics the behavior of the oracle using a neural network or a de-cision tree. The method amounts to tagging LMs with confidence measures and picking the best hypothesis corresponding to the LM with the best confidence.},
	author = {Hacioglu, Kadri and Ward, Wayne},
	keywords = {Applications, Natural Language Processing},
}

@article{Gupta2014,
	title = {Applying data mining techniques in job recommender system for considering candidate job preferences},
	doi = {10.1109/ICACCI.2014.6968361},
	abstract = {Job recommender systems are desired to attain a high level of accuracy while making the predictions which are relevant to the customer, as it becomes a very tedious task to explore thousands of jobs, posted on the web, periodically. Although a lot of job recommender systems exist that uses different strategies , here efforts have been put to make the job recommendations on the basis of candidate's profile matching as well as preserving candidate's job behavior or preferences. Firstly, rules predicting the general preferences of the different user groups are mined. Then the job recommendations to the target candidate are made on the basis of content based matching as well as candidate preferences, which are preserved either in the form of mined rules or obtained by candidate's own applied jobs history. Through this technique a significant level of accuracy has been achieved over other basic methods of job recommendations.},
	journal = {Proceedings of the 2014 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2014},
	author = {Gupta, Anika and Garg, Deepak},
	year = {2014},
	note = {ISBN: 9781479930791},
	keywords = {Classification Rules, Content Based similarity, Data mining, Decision Tree, Job recommendations},
	pages = {1458--1465},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZNG9BSFQ\\Applying Data Mining Techniques in Job Recommender System for Considering Candidate Job Preferences.pdf:application/pdf},
}

@incollection{Jurafsky2019,
	title = {Logistic {Regression}},
	booktitle = {Speech and {Language} {Processing}},
	author = {Jurafsky, Daniel},
	year = {2019},
	doi = {10.4324/9780203461891_chapter_3},
}

@article{Bernicot,
	title = {Forms and {Functions} of {SMS} {Messages}: {A} {Study} of {Variations} in a {Corpus} {Written} by {Adolescents}},
	issn = {1701-1715},
	doi = {10.1016/j.pragma.2012.07.009},
	abstract = {The purpose of this research was to gain insights into SMS communication among French-speaking adolescents. We analyzed the effects of the writers' characteristics (age, gender, and SMS-messaging experience) on message length (number of characters and number of words), dialogue structure (with or without an opening and a closing), and message function (informative vs. relational). The SMS messages were produced in a real-world situation. We found differences across writers' characteristics for all the dependant variables. The commonly reported distinctions between girls and boys were mitigated. Moreover, for dialogical structure, the messages differed from those found in traditional oral and written interactions since 73\% of them did not have the conventional opening-message-closing format (the opening and/or the closing was missing). The results are discussed in terms of the specific characteristics that define the SMS register, and potentially relevant approaches to be taken in future research are addressed.},
	author = {Bernicot, Josie and Volckaert-Legrier, Olga and Goumi, Antonine and Bert-Erboul, Alain},
	keywords = {Applications, Natural Language Processing, adolescents, text-messaging, French language, gender, register, writing},
}

@article{Zou,
	title = {{CS273B} {Lecture} 6 : regularization and optimization for deep learning {Recap} : architectures},
	author = {Zou, James},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N2B9DBIV\\Regularization and Optimization for Deep Learning.pdf:application/pdf},
}

@article{Stephens2019,
	title = {Decision {Trees}},
	doi = {10.1002/9781119575955.ch12},
	journal = {Essential Algorithms},
	author = {Stephens, Rod},
	year = {2019},
	pages = {367--401},
}

@article{WeekDerivationConjugacy,
	title = {Week 2 {Derivation} of {Conjugacy} ( {Optional} ) {Beta}-{Binomial} {Conjugacy}},
	volume = {1},
	pages = {1--5},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PWBPB54V\\Bayesian Infernce - Week2_Conjugacy.pdf:application/pdf},
}

@article{Bahl1983,
	title = {A maximum likelihood approach to continuous speech recognition.},
	volume = {5},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.1983.4767370},
	abstract = {Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them.},
	number = {2},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Bahl, Lalit R and Jelinek, Frederick and Mercer, Robert L},
	year = {1983},
	pmid = {21869099},
	note = {ISBN: 1558601244},
	keywords = {Applications, Natural Language Processing, speech recognition, Index Terms-Markov models, maximum likelihood, parameter esti-mation, statistical models},
	pages = {179--190},
}

@book{Pecina2009,
	title = {Collocation {Extraction} {AND} {THEORETICAL} {LINGUISTICS}},
	isbn = {978-80-904175-5-7},
	author = {Pecina, Pavel},
	year = {2009},
	note = {Publication Title: Studies in computational and theoretical linguistics},
}

@incollection{Lopes2011,
	address = {New York, NY},
	title = {Kolmogorov-{Smirnov} {Test}},
	isbn = {978-1-4020-6753-2},
	url = {http://link.springer.com/10.1007/978-3-642-04898-2_326},
	abstract = {In statistics, the Kolmogorov–Smirnov test (K–S test) is a nonparametric test for the equality of continuous, one-dimensional probability distributions that can be used to compare a sample with a reference probability distribution (one-sample K–S test), or to compare two samples (two-sample K–S test). The Kolmogorov–Smirnov statistic quantifies a distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution, or between the empirical distribution functions of two samples. The null distribution of this statistic is calculated under the null hypothesis that the samples are drawn from the same distribution (in the two-sample case) or that the sample is drawn from the reference distribution (in the one-sample case). In each case, the distributions considered under the null hypothesis are continuous distributions but are otherwise unrestricted.},
	booktitle = {International {Encyclopedia} of {Statistical} {Science}},
	publisher = {Springer New York},
	author = {Lopes, Raul H C},
	year = {2011},
	doi = {10.1007/978-3-642-04898-2_326},
	note = {ISSN: 03784754},
	pages = {718--720},
}

@book{Bell:1990:TC:77753,
	address = {Upper Saddle River, NJ, USA},
	title = {Text {Compression}},
	isbn = {0-13-911991-4},
	publisher = {Prentice-Hall, Inc.},
	author = {Bell, Timothy C and Cleary, John G and Witten, Ian H},
	year = {1990},
	keywords = {Applications, Natural Language Processing},
}

@misc{tangent,
	title = {Tangent --- {Wikipedia} -{The} {Free} {Encyclopedia}},
	url = {https://en.wikipedia.org/w/index.php?title=Tangent&oldid=848826334},
	author = {contributors, Wikipedia},
	year = {2018},
}

@article{SMSLanguageBillion2015,
	title = {{SMS}: the language of 6 billion people {Megatrends} in consumer technology},
	url = {www.portioresearch.com},
	year = {2015},
	keywords = {Applications, Natural Language Processing},
}

@incollection{Carroll1967,
	address = {Providence},
	title = {On sampling from a lognormal model of frequency distribution},
	booktitle = {Computational {Analysis} of {Present}-{Day} {American} {English}},
	publisher = {Brown University Press},
	author = {Carroll, J B},
	editor = {Kucera, H and Francis, W N},
	year = {1967},
	keywords = {Applications, Natural Language Processing},
	pages = {406--424},
}

@inproceedings{sutton:problems,
	title = {Two {Problems} with {Backpropagation} and {Other} {Steepest}-{Descent} {Learning} {Procedures} for {Networks}},
	booktitle = {Proceedings of the {Eighth} {Annual} {Conference} of the {Cognitive} {Science} {Society}},
	publisher = {Hillsdale, NJ: Erlbaum},
	author = {Sutton, Richard S},
	year = {1986},
	keywords = {nn},
}

@book{hastie01statisticallearning,
	address = {New York, NY, USA},
	title = {The {Elements} of {Statistical} {Learning}},
	abstract = {The area's standard text revised and expanded. During the past decade has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book descibes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization and spectral clustering. There is also a chapter on methods for ``wide'' data ( p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit.},
	publisher = {Springer New York Inc.},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2001},
	note = {Series Title: Springer Series in Statistics},
	keywords = {psl},
	file = {The Elements of Statistical Learning-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\The Elements of Statistical Learning-metadata.md:text/plain},
}

@article{Jain2017,
	title = {Non-convex optimization for machine learning},
	volume = {10},
	issn = {19358245},
	doi = {10.1561/2200000058},
	abstract = {A vast majority of machine learning algorithms train their models and perform inference by solving optimization problems. In order to capture the learning and prediction problems accurately structural constraints such as sparsity or low rank are frequently imposed or else the objective itself is designed to be a non-convex function. This is especially true of algorithms that operate in high-dimensional spaces or that train non-linear models such as tensor models and deep networks. The freedom to express the learning problem as a non-convex optimization problem gives immense modeling power to the algorithm designer, but often such problems are NP-hard to solve. A popular workaround to this has been to relax non-convex problems to convex ones and use traditional methods to solve the (convex) relaxed optimization problems. However this approach may be lossy and nevertheless presents significant challenges for large scale optimization. On the other hand, direct approaches to non-convex optimization have met with resounding success in several domains and remain the methods of choice for the practitioner, as they frequently outperform relaxation-based techniques - popular heuristics include projected gradient descent and alternating minimization. However, these are often poorly understood in terms of their convergence and other properties. This monograph presents a selection of recent advances that bridge a long-standing gap in our understanding of these heuristics. We hope that an insight into the inner workings of these methods will allow the reader to appreciate the unique marriage of task structure and generative models that allow these heuristic techniques to (provably) succeed. The monograph will lead the reader through several widely used non-convex optimization techniques, as well as applications thereof. The goal of this monograph is to both, introduce the rich literature in this area, as well as equip the reader with the tools and techniques needed to analyze these simple procedures for non-convex problems.},
	number = {3-4},
	journal = {Foundations and Trends in Machine Learning},
	author = {Jain, Prateek and Kar, Purushottam},
	year = {2017},
	note = {arXiv: 1712.07897},
	pages = {142--336},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BCRUGG9Y\\Non-convex Optimization for Machine Learning.pdf:application/pdf},
}

@article{Fisman2008,
	title = {Racial preferences in dating},
	volume = {75},
	issn = {00346527},
	doi = {10.1111/j.1467-937X.2007.00465.x},
	abstract = {We examine racial preferences in dating. We employ a Speed Dating experiment that allows us to directly observe individual decisions and thus infer whose preferences lead to racial segregation in romantic relationships. Females exhibit stronger racial preferences than males. The richness of our data further allows us to identify many determinants of same-race preferences. Subjects' backgrounds, including the racial composition of the ZIP code where a subject grew up and the prevailing racial attitudes in a subject's state or country of origin, strongly influence same-race preferences. Older subjects and more physically attractive subjects exhibit weaker same-race preferences. ©2008 The Review of Economic Studies Limited.},
	number = {1},
	journal = {Review of Economic Studies},
	author = {Fisman, Raymond and Iyengar, Sheena S and Kamenica, Emir and Simonson, Itamar},
	year = {2008},
	pages = {117--132},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RKH4VDWV\\Racial Preferences in Dating.pdf:application/pdf},
}

@misc{PrinciplesProperValidation,
	title = {Principles\_of\_Proper\_Validation\_use\_and.pdf},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JWEUV75K\\Principles_of_Proper_Validation_use_and.pdf:application/pdf},
}

@article{Szczepaniak2016,
	title = {Predicting {Next} {Word} {Using} {Katz} {Back}-{Off}},
	volume = {2016},
	url = {https://rstudio-pubs-static.s3.amazonaws.com/271652_1525c0598da74774bfa4047803cee0d5.html},
	author = {Szczepaniak, Michael},
	year = {2016},
	pages = {1--10},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S3JU4V3E\\kbot_redacted_example.pdf:application/pdf},
}

@article{Processing2016,
	title = {Lecture 11 : {Oct} 5 {Empirical} {Risk} {Minimization}},
	author = {Processing, Information and Fall, Learning and Note, Aarti Singh and Eecs, U C Berkeley},
	year = {2016},
	pages = {1--7},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9RTBM8QD\\Empirical Risk Minimization Lecture Notes.pdf:application/pdf},
}

@article{Te,
	title = {{SpotIQ} {AI}-{Driven} {Analytics}},
	author = {Te, W H I and Er, P A P},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KPJ6VPIF\\AI-Driven-Analytics-White-Paper.pdf:application/pdf},
}

@misc{tensorflow2015-whitepaper,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Systems}},
	url = {https://www.tensorflow.org/},
	author = {{Martin{\textasciitilde}Abadi} and {Ashish{\textasciitilde}Agarwal} and {Paul{\textasciitilde}Barham} and {Eugene{\textasciitilde}Brevdo} and {Zhifeng{\textasciitilde}Chen} and {Craig{\textasciitilde}Citro} and {Greg{\textasciitilde}S.{\textasciitilde}Corrado} and {Andy{\textasciitilde}Davis} and {Jeffrey{\textasciitilde}Dean} and {Matthieu{\textasciitilde}Devin} and {Sanjay{\textasciitilde}Ghemawat} and {Ian{\textasciitilde}Goodfellow} and {Andrew{\textasciitilde}Harp} and {Geoffrey{\textasciitilde}Irving} and {Michael{\textasciitilde}Isard} and Jia, Yangqing and {Rafal{\textasciitilde}Jozefowicz} and {Lukasz{\textasciitilde}Kaiser} and {Manjunath{\textasciitilde}Kudlur} and {Josh{\textasciitilde}Levenberg} and {Dandelion{\textasciitilde}Mané} and {Rajat{\textasciitilde}Monga} and {Sherry{\textasciitilde}Moore} and {Derek{\textasciitilde}Murray} and {Chris{\textasciitilde}Olah} and {Mike{\textasciitilde}Schuster} and {Jonathon{\textasciitilde}Shlens} and {Benoit{\textasciitilde}Steiner} and {Ilya{\textasciitilde}Sutskever} and {Kunal{\textasciitilde}Talwar} and {Paul{\textasciitilde}Tucker} and {Vincent{\textasciitilde}Vanhoucke} and {Vijay{\textasciitilde}Vasudevan} and {Fernanda{\textasciitilde}Viégas} and {Oriol{\textasciitilde}Vinyals} and {Pete{\textasciitilde}Warden} and {Martin{\textasciitilde}Wattenberg} and {Martin{\textasciitilde}Wicke} and {Yuan{\textasciitilde}Yu} and {Xiaoqiang{\textasciitilde}Zheng}},
	year = {2015},
}

@article{Balcan2008,
	title = {New {Theoretical} {Frameworks} for {Machine} {Learning}},
	journal = {Finishing},
	author = {Balcan, Maria-florina and Blum, Manuel},
	year = {2008},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TIGRVZ9V\\New Theoretical Frameworks for Machine Learning.pdf:application/pdf},
}

@article{Dowle2016,
	title = {Package ‘data.table'},
	journal = {Cran},
	author = {Dowle, Matt},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{PredictiveMechanism1997,
	title = {The {Predictive} {Mechanism}},
	year = {1997},
	keywords = {Applications, Natural Language Processing},
}

@article{10.2307/2334208,
	title = {Bayes's {Theorem} and the {Use} of {Prior} {Knowledge} in {Regression} {Analysis}},
	volume = {51},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2334208},
	number = {1/2},
	journal = {Biometrika},
	author = {Tiao, George C and Zellner, Arnold},
	year = {1964},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {219--230},
}

@article{CBInsights2017,
	title = {Healthcare {Horizons} intelligence platform .},
	url = {-},
	author = {{CBInsights}},
	year = {2017},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4Q8YXN7D\\CB-Insights_Healthcare-Horizons.pdf:},
}

@article{Boender1987,
	title = {Bayesian stopping rules for multistart global optimization methods},
	volume = {37},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/BF02591684},
	doi = {10.1007/BF02591684},
	abstract = {By far the most efficient methods for global optimization are based on starting a local optimization routine from an appropriate subset of uniformly distributed starting points. As the number of local optima is frequently unknown in advance, it is a crucial problem when to stop the sequence of sampling and searching. By viewing a set of observed minima as a sample from a generalized multinomial distribution whose cells correspond to the local optima of the objective function, we obtain the posterior distribution of the number of local optima and of the relative size of their regions of attraction. This information is used to construct sequential Bayesian stopping rules which find the optimal trade off between reliability and computational effort.},
	number = {1},
	journal = {Mathematical Programming},
	author = {Boender, C G E and Rinnooy Kan, A H G},
	month = feb,
	year = {1987},
	pages = {59--80},
}

@article{Maclaurin,
	title = {{CS281} {Section} 2 : {Practical} {Optimization}},
	volume = {1},
	number = {3},
	author = {Maclaurin, Dougal},
	pages = {1--4},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8KGGTD35\\Practical Optimization - Harvard Canvas.pdf:application/pdf},
}

@article{Gregori-Signes2015,
	title = {Analysing {Lexical} {Density} and {Lexical} {Diversity} in {University} {Students}' {Written} {Discourse}},
	volume = {198},
	issn = {18770428},
	doi = {10.1016/j.sbspro.2015.07.477},
	abstract = {This study analyses both lexical density and lexical diversity in the written production of two groups of first year students at the Universitat de València at the beginning and end of one-semester teaching period. These results were compared with those obtained by a third group of students aiming at level C2. Lexical density was tested using Textalyser (http://textalyser.net) and lexical frequency used the software RANGE (Nation and Heatly, 1994). Our results prove that the students from both groups at level B1 show the same progression between writing tasks 1 and 3. Furthermore, we can claim that it is possible to obtain a reliable measure of lexical richness which is stable across two pieces of writing produced by the same learners.},
	number = {Cilc},
	journal = {Procedia - Social and Behavioral Sciences},
	author = {Gregori-Signes, Carmen and Clavel-Arroitia, Begoña},
	year = {2015},
	note = {Publisher: Elsevier B.V.},
	keywords = {english language teaching, learner corpus, lexical density, lexical diversity},
	pages = {546--556},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E9KLBUTX\\Analyzing lexical density and lexical diversity in university students' written discourse.pdf:application/pdf},
}

@book{Barton1994,
	title = {Literacy: {An} {Introduction} to the {Ecology} of {Written} {Language}},
	publisher = {Blackwell Publishing},
	author = {Barton, David},
	year = {1994},
	keywords = {Applications, Natural Language Processing},
}

@article{Papadimitriou2016,
	title = {Calculus {Review} {Univariate} functions},
	author = {Papadimitriou, Christos and Trevisan, Luca},
	year = {2016},
	pages = {1--13},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JA5GST6L\\Some notes on gradient descent.pdf:application/pdf},
}

@article{Gendron2015,
	title = {Natural {Language} {Processing} : {A} {Model} to {Predict} a {Sequence} of {Words}},
	number = {13},
	journal = {ModSim World 2015},
	author = {Gendron, Gerald R and Gendron, Gerald R},
	year = {2015},
	keywords = {n-gram, natural language processing, data, good-turing smoothing, katz back off, predictive model, predictive text analytics, product, text mining},
	pages = {1--10},
}

@misc{RstudioTeam2016a,
	title = {rmarkdown: {R} {Markdown} {Document} {Conversion}},
	url = {http://rmarkdown.rstudio.com/},
	publisher = {github.com/rstudio/rmarkdown},
	author = {{RStudio and Inc.}},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@techreport{Baia,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	url = {http://github.com/locuslab/TCN.},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional ar-chitectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convo-lutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolu-tional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN.},
	author = {Bai, Shaojie and Zico Kolter, J and Koltun, Vladlen},
	note = {arXiv: 1803.01271v2},
}

@article{Zeutschler2016,
	title = {{IT} {Applications} in {Business} {Analytics}},
	author = {Zeutschler, Thomas},
	year = {2016},
	pages = {32},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YNFVNRR7\\Lecture8_OutlierDetection.pdf:;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FVCB4VHS\\Lecture8_OutlierDetection.pdf:},
}

@article{Steel2011,
	title = {Bayesian model averaging and forecasting},
	volume = {44},
	url = {http://wrap.warwick.ac.uk/41090/},
	abstract = {This paper focuses on the problem of variable selection in\${\textbackslash}\$r\${\textbackslash}\$nlinear regression models. I briefy review the method of Bayesian model averaging, which has become an important tool in empirical settings with large numbers of potential regressors and relatively limited numbers of observations. Some of the literature is discussed with particular emphasis on forecasting in economics. The role of the prior assumptions in these procedures is highlighted, and some recommendations for applied users are given.},
	number = {0},
	journal = {Bulletin of E.U. and U.S. Inflation and Macroeconomic Analysis},
	author = {Steel, Mark F J},
	year = {2011},
	keywords = {HB Economic Theory, QA Mathematics},
	pages = {0--16},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WDNT5PRZ\\Bayesian Model Averaging and Forecasting.pdf:application/pdf},
}

@misc{Zadeh2016,
	title = {The hard thing about deep learning},
	url = {https://www.oreilly.com/radar/the-hard-thing-about-deep-learning/},
	urldate = {2020-07-03},
	author = {Zadeh, Reza},
	year = {2016},
	note = {Publication Title: O'Reilly Media},
}

@article{Prechelt1998,
	title = {Automatic {Early} {Stopping} {Using} {Cross} {Validation} : {Quantifying} the {Criteria} 1 {Training} for generalization 2 {Ideal} and real generalization curves},
	journal = {Neural Networks},
	author = {Prechelt, Lutz},
	year = {1998},
	pages = {1--12},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UBNALEQF\\Automatic Early Stopping Using Cross Validation Quantifying the Criteria.pdf:application/pdf},
}

@book{Crystal1995,
	address = {Cambridge},
	edition = {1},
	title = {The {Cambridge} encyclopedia of the {English} language},
	isbn = {0-521-40179-8},
	abstract = {Includes references to New Zealand. Азбучен показалец. Библиогр. An exploration of the history, structure, and use of the English language discusses its origins and evolution, its use in literature, and its structural features, and addresses grammar, pronunciation, and vocabulary. 1. Modelling English -- pt. I. The history of English -- 2. The origins of English -- 3. Old English -- Early borrowings -- Runes -- The Old English corpus -- Literary texts -- The Anglo-Saxon chronicle -- Spelling -- Sounds -- Grammar -- Vocabulary -- Late borrowings -- Dialects -- 4. Middle English -- French and English -- The transition from Old English -- The Middle English corpus -- Literary texts -- Chaucer -- Spelling -- Sounds -- Grammar -- Vocabulary -- Latin borrowings -- Dialects -- Middle Scots -- The origins of standard English -- 5. Early modern English -- Caxton -- Transitional texts -- Renaissance English -- The inkhorn controversy -- Shakespeare -- The King James bible -- Spelling and regularization -- Punctuation -- Sounds -- Grammar -- Vocabulary -- The academy debate -- Johnson -- 6. Modern English -- Transition -- Grammatical trends -- Prescriptivism -- American English -- Breaking the rules -- Variety awareness -- Scientific language -- Literary voices -- Dickens -- Recent trends -- 7. World English -- The new world -- American dialects -- Canada -- Black English vernacular -- Australia -- New Zealand -- South Africa -- South Asia -- West Africa -- East Africa -- South-Est Asia and the South Pacific -- A world language -- Numbers of speakers -- Standard English -- The future of English -- English threatened and as threat -- pt. II. English vocabulary -- 8. The nature of the lexicon -- Lexemes -- The size of the English lexicon -- Abbreviations -- Proper names -- The size of a person's lexicon -- 9. The sources of the lexicon -- Native vocabulary -- Foreign borrowings -- Word-formation -- Unusual structures -- Lexical creation -- Literary neologism -- 10. Etymology -- Lexical history -- Semantic change -- Folk etymology -- Place names -- Surnames -- First names -- Nicknames -- Object names -- Eponyms -- 11. The structure of the lexicon -- Semantic structure -- Semantic fields -- Dictionary and thesaurus -- Collocations -- Lexical predictability -- Idioms -- Synonyms -- Antonyms -- Hyponyms -- Incompatibility -- Other sense relations -- 12. Lexical dimensions -- Loaded vocabulary -- Taboo -- Swearing -- Jargon -- Doublespeak -- Political correctness -- Catch phrases -- Vogue words -- Slogans -- Graffiti -- Slang -- Quotations -- Proverbs -- Archaisms -- Clichés -- Last words. pt. III. English grammar -- 13. Grammatical mythology -- The nature of grammar -- Knowing vs knowing about -- Traditional grammar -- Prescriptive grammar -- The 20th-century legacy -- The main branches of grammar -- 14. The structure of words -- Morphology -- Suffixation -- Adjectives -- Nouns -- The apostrophe -- Pronouns -- Verbs -- 15. Word classes -- Parts of speech -- Traditional definitions -- New classes -- Nouns -- Pronouns -- Adjectives -- Adverbs -- Verbs -- Prepositions -- Conjunctions -- Interjections -- 16. The structure of sentences -- Spoken and written syntax -- Types of sentence -- Sentence structure -- Sentence functions -- Clause elements and types -- Phrases-- Noun phrases -- Verb phrases -- Multiple sentences -- Abbreviation -- Disjuncts and comment clauses -- Reporting speech -- Sentence information -- Beyond the sentence -- pt. IV. Spoken and written English -- 17. The sound system -- Phonetics and phonology -- Vocal organs -- Vowels -- Consonants -- Syllables -- Connected speech -- Prosody -- Sound symbolism -- Pronunciation in practice -- 18. The writing system -- Graphetics and graphology -- Typography -- The alphabet -- Properties of letters -- Letter frequency -- Letter distribution -- Letter symbolism -- Analysing handwriting -- Graphetic variety -- Spelling -- Sources of irregularity -- Spelling reform -- Punctuation -- The development of the writing system. pt. V. Using English -- 19. Varieties of discourse -- Structure vs use -- Pragmatic issues -- The nature of discourse -- Microlinguistic studies -- Texts and varieties -- Speech vs writing -- Mixed medium -- Monologue and dialogue -- 20. Regional variation -- Accent and dialect -- International and intranational -- A day in the life of the language -- American and British English -- American dialects -- British dialects -- Scotland -- Wales -- Ireland -- Canada -- Caribbean -- Pidgins and Creoles -- Australia -- New Zealand -- South Africa -- New Englishes -- 21. Social variation -- Sociolinguistic perspective -- Received pronunciation -- Prescriptive attitudes -- Gender -- Occupation -- Religion -- Science -- Law -- Plain English -- Politics -- News media -- Journalism -- Broadcasting -- Weather forecasting -- Sports commentary -- Advertising -- Restricted varieties -- New varieties -- 22. Personal variation -- Individual differences -- Deviance -- Word games -- Rule-breaking varieties -- The edges of language -- Jokes and puns -- Comic alphabets -- Variety humour -- Literary freedom -- Phonetics and phonology -- Graphetics and graphology -- Grammar and lexicon -- Discourse and variety -- Stylometry -- pt. VI. Learning about English -- 23. Learning English as a mother tongue -- Child language acquisition -- Literacy -- Grammatical development -- Early words and sounds -- Reading and writing -- Insufficient language -- Language disability -- 24. New ways of studying English -- Technological revolution -- Corpus studies -- National and international corpora -- Dictionaries -- Innovations -- Sources and resources -- Appendices -- I. Glossary -- II. Special symbols and abbreviations -- III. References -- IV. Further reading -- V. Index of linguistic items -- VI. Index of authors and personalities -- VII. Index of topics.},
	publisher = {Cambridge University Press},
	author = {Crystal, David},
	year = {1995},
}

@article{MixturesGpriorsBayesian2007,
	title = {Mixtures of g-priors for {Bayesian} {Variable} {Selection}},
	abstract = {Zellner's g-prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this paper, we study mixtures of g-priors as an alternative to default g-priors that resolve many of the problems with the original formulation, while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture priors and provide real and simulated examples to compare the mixture formulation with fixed g-priors, Empirical Bayes approaches and other default procedures.},
	year = {2007},
	keywords = {Algorithms, Bayesian, model selection, AIC, Bayesian Model Averaging, BIC, Cauchy, Empirical Bayes, Gaussian Hyperge-ometric functions, Zellner-Siow priors},
}

@article{Evert2005a,
	title = {Testing the extrapolation quality of word frequency models},
	issn = {1747-939},
	journal = {Proceedings of Corpus Linguistics 2005},
	author = {Evert, Stefan and Baroni, Marco},
	year = {2005},
	keywords = {Applications, Natural Language Processing},
	pages = {1747--1939},
}

@article{D.Jurafsky2018,
	title = {Speech and {Language} {Processing}. {Computer} {Science}, {Stanford} {University}},
	author = {D. Jurafsky, J-H. Martin},
	year = {2018},
	note = {ISBN: 0131873210},
}

@article{Aksakov2006,
	title = {Classical {Text} in {Translation} {On} a {Remarkable} {Case} of {Samples} {Connected} in a {Chain}. {Appendix} on the statistical investigation of a text},
	volume = {19},
	doi = {10.1017/S0269889706001086},
	abstract = {I have conducted a similar investigation on a text by a different author (S. T. Aksakov: " Childhood Years of Bagrov's Grandson "). The results of this investigation, which was performed on a text passage of 100,000 letters, 2 are presented in the following tables from which one can see how and to what extent the limit theorems of the calculus of probability actually become evident. The distribution of thousands of letters (hundreds of groups of ten) according to the groups of ten, which contain the same number of vowels. The number of vowels in the first group of ten is shown in the first column and the number of the groups of ten in the first row. The tables provide the corresponding numbers for hundreds of groups of ten. From this, one can examine the probability that the number of vowels in the group of ten corresponds to the numbers 2, 3, 4, 5, 6, and 7 (other numbers were not found). These probabilities are entered in the penultimate column; the last column shows the values of their dispersion coefficients. 3 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Prob. D.c.1 The text was originally published in Markov 1924, 577–581. Translated into German by Christiane Büchner, Lioudmila Voropai, and David Link; translated into English by Gloria Custance and David Link. 2 [Footnote by Markov]: For the investigation I used my own handwritten copy of the text, which differs slightly from the original because of some mistakes I made. However, as these mistakes are only very minor, they should not gravely affect the results. In my first investigation, I spent much time and effort on excluding such errors. The calculations were done in both cases with the same exactitude. 3 Prob. = probability; D. c. = dispersion coefficient.},
	number = {4},
	journal = {Science in Context},
	author = {{by Aksakov} and Markov, A A},
	year = {2006},
	note = {Publisher: Cambridge University Press},
	keywords = {Applications, Natural Language Processing},
	pages = {601--604},
}

@article{COMBINATIONNGRAMSSTOCHASTIC,
	title = {{COMBINATION} {OF} {N}-{GRAMS} {AND} {STOCHASTIC} {CONTEXT}-{FREE} {GRAMMARS} {FOR} {LANGUAGE} {MODELING}},
	keywords = {Applications, Natural Language Processing},
}

@article{2016AnnualRelationship,
	title = {2016 {Annual} {Relationship}, {Marriage}, and {Divorce} {Survey} {Final} {Report} {The} {Study}},
	abstract = {These findings are based on a non-probability online sample of data collected using the Research Now panel. Data was collected in March 2016. A sample of over 2000 U.S. adults age 18 and over were surveyed online. For this poll, the credibility interval was calculated to be plus or minus 2.5 percentage points for all respondents. The data were weighted to the U.S. current population data by gender, age, and region based on Census data. Avvo conducts periodic studies of topics at the intersection of lifestyle and the law to better understand the issues facing individuals engaging with attorneys and the legal system. Given that divorce and family law are two of the largest and most routine legal actions taken in the United States, understanding the relationship dynamics that lead to marriage and divorce is beneficial to the legal consumers and attorneys Avvo serves. 2 Contents},
}

@article{Gatys2016,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	volume = {16},
	issn = {1534-7362},
	doi = {10.1167/16.12.326},
	abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
	number = {12},
	journal = {Journal of Vision},
	author = {Gatys, Leon and Ecker, Alexander and Bethge, Matthias},
	year = {2016},
	note = {arXiv: 1508.06576},
	pages = {326},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2SU4ZC2B\\A Neural Algorithm of Artistic Style.pdf:application/pdf},
}

@article{CS474NaturalLanguagea,
	title = {{CS474} {Natural} {Language} {Processing}  {Last} class},
	keywords = {Applications, Natural Language Processing},
}

@book{Richert,
	title = {Building {Machine} {Learning} {Systems} with {Python}},
	isbn = {978-1-78439-277-2},
	abstract = {This is a tutorial-driven and practical, but well-grounded book showcasing good Machine Learning practices. There will be an emphasis on using existing technologies instead of showing how to write your own implementations of algorithms. This book is a scenario-based, example-driven tutorial. By the end of the book you will have learnt critical aspects of Machine Learning Python projects and experienced the power of ML-based systems by actually working on them.This book primarily targets Python developers who want to learn about and build Machine Learning into their projects, or who want to provide Machine Learning support to their existing projects, and see them get implemented effectively .Computer science researchers, data scientists, Artificial Intelligence programmers, and statistical programmers would equally gain from this book and would learn about effective implementation through lots of the practical examples discussed.Readers need no prior experience with Machine Learning or statistical processing. Python development experience is assumed.},
	author = {Richert, Willi and Coelho, Luis Pedro},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YXMNEPCA\\Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf:application/pdf},
}

@article{IqpRtsvuxwTy2EEfXgxh,
	title = {iqp {rtsvuxwTy2}  h   p  {Y}  {Y}  {Y}  d ¦ {eEfXgxh}  ei  jflkt  tm  h  n  {uS}  l  so  p \&  {qrSrtsjrtsun} v £{wY}  pd  x  {X}  {ytrtpzwY}  ux  {D} \{ p  {\textbar} t  {Xw}   \%\} sjrt {\textbar} {xuS} {\textasciitilde} {\textasciitilde} w  x  \&  x t  ¨},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y9GVKM2T\\Approximate Statistical Tests for Comparing Classification Learning Algorithms.pdf:application/pdf},
}

@incollection{McCain2016,
	address = {Cham},
	title = {Explanation and {Understanding}},
	isbn = {978-3-319-33405-9},
	url = {https://doi.org/10.1007/978-3-319-33405-9_9},
	abstract = {This chapter begins the transition from talking about knowledge in general to talking about scientific knowledge in particular. Although scientific knowledge is itself something that falls under the general account of knowledge, it has special features that are worth exploring. In this chapter the importance of explanation to scientific knowledge is brought to the forefront of the discussion. The nature of scientific explanation itself as well as its relation to understanding is explored in this chapter. It is made clear that good explanations are those which provide understanding of particular phenomena. In addition to examining the relationship between explanation and understanding, this chapter also examines what can make one explanation superior to another. This examination of explanatory virtues is very important because it is common in scientific practice to adopt a particular theory as a result of its being more virtuous than its competitors. Not only is such a practice common in science, it is something that we do routinely in our everyday life.},
	booktitle = {The {Nature} of {Scientific} {Knowledge}: {An} {Explanatory} {Approach}},
	publisher = {Springer International Publishing},
	author = {McCain, Kevin},
	year = {2016},
	doi = {10.1007/978-3-319-33405-9_9},
	pages = {133--154},
}

@techreport{Mcallester2000,
	title = {On the {Convergence} {Rate} of {Good}-{Turing} {Estimators}},
	abstract = {Good-Turing adjustments of word frequencies are an important tool in natural language modeling. In particular, for any sample of words, there is a set of words not occuring in that sample. The total probability mass of the words not in the sample is the so-called missing mass. Good showed that the fraction of the sample consisting of words that occur only once in the sample is a nearly unbiased estimate of the missing mass. Here, we give a high-probability confidence interval for the actual missing mass. More generally, for k 0, we give a confidence interval for the true probability mass of the set of words occuring k times in the sample.},
	author = {Mcallester, David and Schapire, Robert E},
	year = {2000},
}

@inproceedings{Akaike1998,
	address = {Budapest},
	title = {Information theory and an extension of the maximum likelihood principle},
	booktitle = {Second {International} {Symposium} on {Information} {Theory}},
	publisher = {Akadémiai Kiado},
	author = {Akaike, H},
	editor = {Petrov, B N and Csaki, F},
	year = {1973},
	keywords = {modelselection},
	pages = {267--281},
}

@article{DerPhilosophisch-Historischen2004,
	title = {The {Statistics} of {Word} {Cooccurrences} {Word} {Pairs} and {Collocations}},
	author = {der Philosophisch-Historischen, Von and Evert aus Ludwigsburg Hauptberichter, Stefan and Rohrer Mitberichter, C and Kahnert Mitberichter, Apl D and Heid, H D U},
	year = {2004},
	keywords = {Applications, Natural Language Processing},
}

@article{McCarthy2010,
	title = {{MTLD}, vocd-{D}, and {HD}-{D}: a validation study of sophisticated approaches to lexical diversity assessment.},
	volume = {42},
	issn = {1554-3528},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/20479170},
	doi = {10.3758/BRM.42.2.381},
	abstract = {The main purpose of this study was to examine the validity of the approach to lexical diversity assessment known as the measure of textual lexical diversity (MTLD). The index for this approach is calculated as the mean length of word strings that maintain a criterion level of lexical variation. To validate the MTLD approach, we compared it against the performances of the primary competing indices in the field, which include vocd-D, TTR, Maas, Yule's K, and an HD-D index derived directly from the hypergeometric distribution function. The comparisons involved assessments of convergent validity, divergent validity, internal validity, and incremental validity. The results of our assessments of these indices across two separate corpora suggest three major findings. First, MTLD performs well with respect to all four types of validity and is, in fact, the only index not found to vary as a function of text length. Second, HD-D is a viable alternative to the vocd-D standard. And third, three of the indices--MTLD, vocd-D (or HD-D), and Maas--appear to capture unique lexical information. We conclude by advising researchers to consider using MTLD, vocd-D (or HD-D), and Maas in their studies, rather than any single index, noting that lexical diversity can be assessed in many ways and each approach may be informative as to the construct under investigation.},
	number = {2},
	journal = {Behavior research methods},
	author = {McCarthy, Philip M and Jarvis, Scott},
	year = {2010},
	pmid = {20479170},
	note = {arXiv: 1011.1669v3
ISBN: 9788578110796},
	keywords = {Experimental, Experimental: methods, Humans, Language Tests, Language Tests: statistics \& numerical data, Psychology, Reproducibility of Results},
	pages = {381--392},
}

@article{Wakita,
	title = {{AN} {EVALUATION} {OF} {STATISTICAL} {LANGUAGE} {MODELING} {FOR} {SPEECH} {RECOGNITION} {USING} {A} {MIXED} {CATEGORY} {OF} {BOTH} {WORDS} {AND} {PARTS}-{OF}-{SPEECH}},
	abstract = {In our previous paper, we proposed a mixed category of words and parts-of-speech names the MWP category based on class N-gram modeling [1]. However, we had not con-rmed the eciency of MWP category. In this paper, we evaluate the proposed MWP category. At rst we use \${\textbackslash}\$cov-erage of words and category sequences to open data" and \${\textbackslash}\$perplexity to training data" for the evaluation and we con-rmed the characteristics of parts-of-speech are useful to for generating a suitable class N-gram modeling. As a result of the speech recognition experimentation, we also conrmed that the class N-gram modeling using MWP category is ef-fective in improving the recognition rate for open data that shows a low coverage of words and category sequences, with-out decreasing the recognition rate much for closed data.},
	author = {Wakita, Yumi and Kawai, Jun and Iida, Hitoshi},
	keywords = {Applications, Natural Language Processing},
}

@article{Goodfellow2013,
	title = {Pylearn2: a machine learning research library},
	url = {http://arxiv.org/abs/1308.4214},
	abstract = {Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.},
	author = {Goodfellow, Ian J and Warde-Farley, David and Lamblin, Pascal and Dumoulin, Vincent and Mirza, Mehdi and Pascanu, Razvan and Bergstra, James and Bastien, Frédéric and Bengio, Yoshua},
	year = {2013},
	note = {arXiv: 1308.4214},
	pages = {1--9},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CQ29H639\\Pylearn2 Machine Learning Research Library.pdf:application/pdf},
}

@article{Nemeth2001,
	title = {Word {Unit} {Based} {Multilingual} {Comparative} {Analysis} of {Text} {Corpora}},
	abstract = {Parallel study of three very different languages -Hungarian. German and English -using text corpora of a similar size gives a possibility for the exploration of both similarities and differences. Corpora of publicly available Internet sources was used. The corpus size was the same (app. 20Mbytes, 2.5-3.5 million word forms) for all languages. Besides traditional corpus coverage, word length and occurence statistics, some new features about prosodic boundaries (sentence beginning and final positions, preceding and following a comma) were also computed. Among others, it was found, that the coverage of corpora by the most frequent words follows a parallel logarithmic rule for all languages in the 40-85\% coverage range. The functions are much nearer for English and German than for Hungarian. The results can be applied in such diverse domains as predictive text input, word hyphenation, language modeling in speech recognition, corpus-based speech synthesis, etc.},
	journal = {Eurospeech},
	author = {Németh, Géza and Zainkó, Csaba},
	year = {2001},
	keywords = {Applications, Natural Language Processing, language modeling, corpus analysis, corpus-based speech synthesis, multilinguality, text corpora, word length, unit based analysis},
}

@article{Chipman2010,
	title = {{BART}: {Bayesian} additive regression trees},
	volume = {4},
	doi = {10.1214/09-AOAS285},
	abstract = {We develop a Bayesian " sum-of-trees " model where each tree is constrained by a regularization prior to be a weak learner, and fitting and inference are accomplished via an iterative Bayesian backfitting MCMC algorithm that generates samples from a posterior. Effec-tively, BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a likelihood. This approach enables full posterior inference including point and interval estimates of the unknown regression function as well as the marginal effects of potential predictors. By keeping track of predictor inclusion frequencies, BART can also be used for model-free variable selection. BART's many features are illustrated with a bake-off against compet-ing methods on 42 different data sets, with a simulation experiment and on a drug discovery classification problem.},
	number = {1},
	journal = {The Annals of Applied Statistics},
	author = {Chipman, Hugh A and George, Edward I and Mcculloch, Robert E},
	year = {2010},
	note = {arXiv: 0806.3286v2},
	keywords = {Algorithms, Bayesian, classification, ensemble, variable selection, boosting, Bayesian backfitting, CART, MCMC, nonparametric regression, probit model, random basis, regularizatio, sum-of-trees model, weak learner},
	pages = {266--298},
}

@article{Smith2019,
	title = {Super-convergence: very fast training of neural networks using large learning rates},
	issn = {1996756X},
	doi = {10.1117/12.2520589},
	abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
	author = {Smith, Leslie N and Topin, Nicholay},
	year = {2019},
	note = {arXiv: 1708.07120v3
ISBN: 9781510626775},
	pages = {36},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L47NNJP2\\Super-Convergence Very Fast Training of Neural Networks Using Large Learning Rates.pdf:application/pdf},
}

@article{Cavalieri2015,
	title = {On {Combining} {Language} {Models} to {Improve} a {Text}-based {Human}-machine {Interface}},
	doi = {10.5772/61753},
	abstract = {This paper concentrates on improving a text-based human-machine interface integrated into a robotic wheelchair. Since word prediction is one of the most common methods used in such systems, the goal of this work is to improve the results using this specific module. For this, an expo‐ nential interpolation language model (LM) is considered. First, a model based on partial differential equations is proposed; with the appropriate initial conditions, we are able to design a interpolation language model that merges a word-based n-gram language model and a part-of-speech-based language model. Improvements in keystroke saving (KSS) and perplexity (PP) over the word-based n-gram language model and two other traditional interpola‐ tion models are obtained, considering two different task domains and three different languages. The proposed interpolation model also provides additional improve‐ ments over the hit rate (HR) parameter.},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Cavalieri, D C and Bastos-Filho, Teodiano and Palazuelos-Cagigas, S E},
	year = {2015},
	keywords = {Applications, Natural Language Processing, Communication Aid, Human-machine Interfaces, Language Modelling, Word Prediction Systems},
}

@article{Lukacs2003,
	title = {A {Non}-{Designer} s {Guide} to {Creating} {Memorable} {Visual} {Slides}},
	issn = {0196-6553},
	doi = {10.1002/ejoc.201200111},
	abstract = {If you're like most people, you've probably created dozens of presentations in your lifetime, and many of these in just under a few hours. But ask yourself: Do you really know how to design a memorable presentation that will stick in your viewers' minds for months, even years to come? The answer is probably no. Most of us have never actually learned the design principles necessary to impact audiences through visual storytelling. Perhaps the closest we have ever come to crafting a visual message is a PowerPoint presentation full of bullet points, overused stock photos and bland color schemes. But these kinds of presentations rarely inspire real change, especially in this new age of visual communication. A good public speaker with a boring slide deck may be able to maintain the attention of an audience for a few minutes, but a good public speaker with a wellplanned and well-designed visual presentation can truly mesmerize an audience.In this book, we'll cover basic design principles and tools you can apply right awayto take your slide decks from mediocre to stunning.},
	number = {November},
	journal = {Semikron},
	author = {Lukacs, Michael and Bhadra, Dipasis},
	year = {2003},
	pmid = {26840611},
	note = {arXiv: 1011.1669v3
ISBN: 9780132478663},
	keywords = {keywords},
	pages = {2004},
}

@article{Wagacha,
	title = {Adaptive and {Optimization} {Predictive} {Text} {Entry} for {Short} {Message} {Service} ({Sms})},
	journal = {Review Literature And Arts Of The Americas},
	author = {Wagacha, Peter Waiganjo and Chege, Dennis},
	keywords = {Applications, Natural Language Processing},
}

@article{Hillard2001,
	title = {N-gram {Language} {Modeling} {Tutorial}},
	number = {Lm},
	author = {Hillard, Dustin and Petersen, Sarah},
	year = {2001},
	pages = {1--19},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ULTUPVM5\\N-Gram Language Modeling Tutorial.pdf:application/pdf},
}

@inproceedings{Bertin-Mahieux2011,
	title = {The {Million} {Song} {Dataset}},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Music} {Information} {Retrieval} (\{{ISMIR}\} 2011)},
	author = {Bertin-Mahieux, Thierry and Ellis, Daniel P W and Whitman, Brian and Lamere, Paul},
	year = {2011},
}

@misc{James2017,
	title = {Data for an {Introduction} to {Statistical} {Learning} with {Applications} in {R}},
	url = {http://www.liacs.nl/%7B~%7Dputten/library/cc2000/},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Maintainer, Rob Tibshirani},
	year = {2017},
}

@article{Baron,
	title = {{EMERGING} {PATTERNS} {OF} {AMERICAN} {MOBILE} {PHONE} {USE}: {Electronically}-mediated communication in transition},
	abstract = {Mobile telephony in the United States is gaining ground against high adoption rates in other parts of the world as a medium for both talking and sending text messages. While there is research on the use of written forms of computer-mediated communication in the US using full keyboards (e.g., chat, email, instant messaging), we know relatively little about mobile telephony as an American form of electronically-mediated communication. To address this lacuna, we administered questionnaires using convenience sampling to American college students on two campuses regarding their use of mobile phones for both talking and texting. The results suggest that the mobile phone platform is still a medium in transition but that some usage patterns may be gender-driven or economically-based, and that others may be distinctive to American culture.},
	author = {Baron, Naomi S and Ling Telenor, Rich},
	keywords = {cell phones, computer-mediated communication, electronically-mediated communication, mobile phones, SMS, texting},
}

@article{Anderson2017,
	title = {3 – {Way} {Tables} {Edpsy} / {Psych} / {Soc} 589},
	author = {Anderson, Carolyn J},
	year = {2017},
	keywords = {()},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LFBYPE3K\\Analysis of 3-Way Tables.pdf:application/pdf},
}

@article{Kothari2013,
	title = {Histological image classification using biologically interpretable shape-based features},
	volume = {13},
	issn = {14712342},
	url = {BMC},
	doi = {10.1186/1471-2342-13-9},
	abstract = {Background: Automatic cancer diagnostic systems based on histological image classification are important for improving therapeutic decisions. Previous studies propose textural and morphological features for such systems. These features capture patterns in histological images that are useful for both cancer grading and subtyping. However, because many of these features lack a clear biological interpretation, pathologists may be reluctant to adopt these features for clinical diagnosis.Methods: We examine the utility of biologically interpretable shape-based features for classification of histological renal tumor images. Using Fourier shape descriptors, we extract shape-based features that capture the distribution of stain-enhanced cellular and tissue structures in each image and evaluate these features using a multi-class prediction model. We compare the predictive performance of the shape-based diagnostic model to that of traditional models, i.e., using textural, morphological and topological features.Results: The shape-based model, with an average accuracy of 77\%, outperforms or complements traditional models. We identify the most informative shapes for each renal tumor subtype from the top-selected features. Results suggest that these shapes are not only accurate diagnostic features, but also correlate with known biological characteristics of renal tumors.Conclusions: Shape-based analysis of histological renal tumor images accurately classifies disease subtypes and reveals biologically insightful discriminatory features. This method for shape-based analysis can be extended to other histological datasets to aid pathologists in diagnostic and therapeutic decisions. ©2013 Kothari et al.; licensee BioMed Central Ltd.},
	number = {1},
	journal = {BMC Medical Imaging},
	author = {Kothari, Sonal and Phan, John H and Young, Andrew N and Wang, May D},
	year = {2013},
	note = {Publisher: BMC Medical Imaging},
	pages = {1},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C53NC85K\\Histological_image_classification_using_biological.pdf:application/pdf},
}

@article{Raftery1997,
	title = {Bayesian {Model} {Averaging} for {Linear} {Regression} {Models}},
	volume = {92},
	number = {437},
	journal = {Journal of American Statistical Association},
	author = {Raftery, Adrian E and Madigan, David and Hoeting, Jennifer A},
	year = {1997},
	keywords = {Algorithms, Bayesian, model uncertainty, bayes factor, markov chain monte carlo, model composition, occam, posterior, s window},
	pages = {179--191},
}

@misc{Leek,
	title = {Coursera {\textbar} {Practical} {Machine} {Learning} {\textbar} {Data} {Science} {Specialization} by {Johns} {Hopkins} {University}},
	url = {https://www.coursera.org/learn/practical-machine-learning},
	abstract = {One of the most common tasks performed by data scientists and data analysts are prediction and machine learning. This course will cover the basic components of building and applying prediction functions with an emphasis on practical applications. The course will provide basic grounding in concepts such as training and tests sets, overfitting, and error rates. The course will also introduce a range of model based and algorithmic machine learning methods including regression, classification trees, Naive Bayes, and random forests. The course will cover the complete process of building prediction functions including data collection, feature creation, algorithms, and evaluation.},
	urldate = {2017-09-08},
	author = {Leek, Jeff PhD and Peng, Roger D PhD and Caffo, Brian, PhD},
}

@misc{Baayen2013,
	title = {Package '{languageR}' : {Data} sets and functions for {Analyzing} {Linguistic} {Data}: {A} practical introduction to statistics''},
	author = {Baayen, R H},
	year = {2013},
	keywords = {Applications, Natural Language Processing},
}

@article{Khot2015,
	title = {Gradient-based boosting for statistical relational learning: the {Markov} logic network and missing data cases},
	volume = {100},
	issn = {15730565},
	doi = {10.1007/s10994-015-5481-4},
	abstract = {Recent years have seen a surge of interest in Statistical Relational Learning (SRL) models that combine logic with probabilities. One prominent and highly expressive SRL model is Markov Logic Networks (MLNs), but the expressivity comes at the cost of learning complexity. Most of the current methods for learning MLN structure follow a two-step approach where first they search through the space of possible clauses (i.e. structures) and then learn weights via gradient descent for these clauses. We present a functional-gradient boosting algorithm to learn both the weights (in closed form) and the structure of the MLN simultaneously. Moreover most of the learning approaches for SRL apply the closed-world assumption, i.e., whatever is not observed is assumed to be false in the world. We attempt to open this assumption. We extend our algorithm for MLN structure learning to handle missing data by using an EM-based approach and show this algorithm can also be used to learn Relational Dependency Networks and relational policies. Our results in many domains demonstrate that our approach can effectively learn MLNs even in the presence of missing data.},
	number = {1},
	journal = {Machine Learning},
	author = {Khot, Tushar and Natarajan, Sriraam and Kersting, Kristian and Shavlik, Jude},
	year = {2015},
	keywords = {Statistical relational learning, Expectation maximization, Markov logic networks, Missing data},
	pages = {75--100},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S8SBLTG5\\Gradient-based Boosting for Statistical Relational Learning.pdf:application/pdf},
}

@phdthesis{McCarthy2005,
	title = {An assessment of the range and usefulness of lexical diversity measures and the potential of the measure of textual, lexical diversity.},
	author = {McCarthy, Philip M},
	year = {2005},
}

@article{Heafielda,
	title = {Scalable {Modified} {Kneser}-{Ney} {Language} {Model} {Estimation}},
	abstract = {We present an efficient algorithm to es-timate large modified Kneser-Ney mod-els including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7\% of the RAM and 14.0\% of the wall time taken by SRILM. The code is open source as part of KenLM.},
	author = {Heafield, Kenneth and Pouzyrevsky, Ivan and Clark, Jonathan H and Koehn, Philipp},
}

@book{goodstein1999feynman,
	title = {Feynman's {Lost} {Lecture}: {The} {Motion} of {Planets} {Around} the {Sun}},
	isbn = {978-0-393-31995-8},
	url = {https://books.google.com/books?id=ysZI5NcksUcC},
	publisher = {Norton},
	author = {Goodstein, D L and Goodstein, J R},
	year = {1999},
}

@misc{TheRFoundation2016,
	title = {The {Comprehensive} {R} {Archive} {Network}},
	url = {https://cran.r-project.org/},
	author = {{The R Foundation}},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Liu2006,
	title = {Web {Text} {Corpus} for {Natural} {Language} {Processing}},
	abstract = {Web text has been successfully used as training data for many NLP applications. While most previous work accesses web text through search engine hit counts, we created a Web Corpus by downloading web pages to create a topic-diverse collec- tion of 10 billion words of English. We show that for context-sensitive spelling correction theWeb Corpus results are bet- ter than using a search engine. For the- saurus extraction, it achieved similar over- all results to a corpus of newspaper text. With many more words available on the web, better results can be obtained by col- lecting much larger web corpora.},
	journal = {Eacl},
	author = {Liu, Vinci and Curran, James R},
	year = {2006},
	note = {ISBN: 1932432590},
	keywords = {Applications, Natural Language Processing},
	pages = {233--240},
}

@article{Hutchinson,
	title = {Rank and {Sparsity} in {Language} {Processing}},
	author = {Hutchinson, Brian},
	keywords = {Applications, Natural Language Processing},
}

@inproceedings{atakulreka2007,
	address = {Berlin, Heidelberg},
	title = {Avoiding {Local} {Minima} in {Feedforward} {Neural} {Networks} by {Simultaneous} {Learning}},
	isbn = {978-3-540-76928-6},
	abstract = {Feedforward neural networks are particularly useful in learning a training dataset without prior knowledge. However, weight adjusting with a gradient descent may result in the local minimum problem. Repeated training with random starting weights is among the popular methods to avoid this problem, but it requires extensive computational time. This paper proposes a simultaneous training method with removal criteria to eliminate less promising neural networks, which can decrease the probability of achieving a local minimum while efficiently utilizing resources. The experimental results demonstrate the effectiveness and efficiency of the proposed training method in comparison with conventional training.},
	booktitle = {{AI} 2007: {Advances} in {Artificial} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Atakulreka, Akarachai and Sutivong, Daricha},
	editor = {Orgun, Mehmet A and Thornton, John},
	year = {2007},
	keywords = {Feedforward Neural Networks, Local Minima, Removal Criteria, Simultaneous Learning},
	pages = {100--109},
}

@article{Amidi2018,
	title = {super {VIP} {Cheatsheet} : {Machine} {Learning}},
	url = {https://stanford.edu/%7B~%7Dshervine},
	journal = {CS 229 – Machine Learning https://stanford.edu/{\textasciitilde}shervine},
	author = {Amidi, Afshine and Amidi, Shervine},
	year = {2018},
	pages = {2--3},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\56RU6ELZ\\super-cheatsheet-machine-learning.pdf:application/pdf},
}

@book{Doherty2015,
	title = {Building {Data} {Pipelines}},
	isbn = {978-1-4919-3547-7},
	author = {Doherty, Conor and Orenstein, Gary and Camiña, Steven and White, Kevin},
	year = {2015},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\J8GMQLZ4\\building-real-time-data-pipeline-MEMSQL_ebook.pdf:application/pdf},
}

@article{scikit-learn,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F and Varoquaux, G and Gramfort, A and Michel, V and Thirion, B and Grisel, O and Blondel, M and Prettenhofer, P and Weiss, R and Dubourg, V and Vanderplas, J and Passos, A and Cournapeau, D and Brucher, M and Perrot, M and Duchesnay, E},
	year = {2011},
	pages = {2825--2830},
}

@misc{Google,
	title = {Google {Books} {Ngram} {Datasets}},
	url = {http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html},
	urldate = {2017-03-17},
	author = {{Google}},
	year = {2013},
	keywords = {Applications, Natural Language Processing},
}

@article{Sundermeyer2011,
	title = {On the estimation of discount parameters for language model smoothing},
	volume = {1},
	issn = {19909772},
	abstract = {The goal of statistical language modeling is to find probability estimates for arbitrary word sequences. To obtain non-zero values, the probability distributions found in the training data need to be smoothed. In the widely-used Kneser-Ney family of smoothing algorithms, this is achieved by absolute discounting. The discount parameters can be computed directly using some approximation formulas minimizing the leaving-one-out log-likelihood of the training data. In this work, we outline several shortcomings of the standard estimators for the discount parameters. We propose an efficient method for computing the discount values on heldout data and analyze the resulting parameter estimates. Experiments on large English and French corpora show consistent improvements in perplexity and word error rate over the baseline method. At the same time, this approach can be used for language model pruning, leading to slightly better results than standard pruning algorithms. Copyright ©2011 ISCA.},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	author = {Sundermeyer, Martin and Schlüter, Ralf and Ney, Hermann},
	year = {2011},
	keywords = {Absolute discounting, Kneser-Ney method, Language model pruning, Language model smoothing},
	pages = {1433--1436},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z948LDSB\\On the Estimation of Discount Parameters for Language Model Smoothing.pdf:application/pdf},
}

@article{Caffo2015,
	title = {Regression {Models} for {Data} {Science} in {R}},
	abstract = {Ordinary Least Squares (OLS) - Linear Regression Models - Residuals - Regression inference - Multivariable Regression Analysis - Generalized Linear Models (GLM) - Count data},
	author = {Caffo, Brian},
	year = {2015},
	pages = {137},
}

@article{Nirme2020,
	title = {Audio-visual speech comprehension in noise with real and virtual speakers},
	volume = {116},
	issn = {01676393},
	url = {https://doi.org/10.1016/j.specom.2019.11.005},
	doi = {10.1016/j.specom.2019.11.005},
	abstract = {This paper presents a study where a 3D motion-capture animated ‘virtual speaker' is compared to a video of a real speaker with regards to how it facilitates children's speech comprehension of narratives in background multitalker babble noise. As secondary measures, children self-assess the listening- and attentional effort demanded by the task, and associates words describing positive or negative social traits to the speaker. The results show that the virtual speaker, despite being associated with more negative social traits, facilitates speech comprehension in babble noise compared to a voice-only presentation but that the effect requires some adaptation. We also found the virtual speaker to be at least as facilitating as the video. We interpret these results to suggest that audiovisual integration supports speech comprehension independently of children's social perception of the speaker, and discuss virtual speakers' potential in research and pedagogical applications.},
	number = {November 2019},
	journal = {Speech Communication},
	author = {Nirme, Jens and Sahlén, Birgitta and Lyberg Åhlander, Viveka and Brännström, Jonas and Haake, Magnus},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	pages = {44--55},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IGJ7CSBH\\Audio-visual-speech-comprehension-in-noise-with-real-an_2020_Speech-Communic.pdf:application/pdf},
}

@book{Akinkunmi2019,
	title = {Introduction to {Statistics} {Using} {R}},
	volume = {11},
	isbn = {978-1-68173-505-4},
	abstract = {Introduction to Statistics Using R is organized into 13 major chapters. Each chapter is broken down into many digestible subsections in order to explore the objectives of the book. There are many real-life practical examples in this book and each of the examples is written in R codes to acquaint the readers with some statistical methods while simultaneously learning R scripts.},
	author = {Akinkunmi, Mustapha},
	year = {2019},
	doi = {10.2200/S00899ED1V01Y201902MAS024},
	note = {Publication Title: Synthesis Lectures on Mathematics and Statistics
Issue: 4
ISSN: 19381751},
	keywords = {regression analysis, confidence interval, correlation analysis, descriptive statistics, hypothesis testing, probability distributions, sampling distribution},
}

@article{Eisenstein2015,
	title = {Systematic patterning in phonologically-motivated orthographic variation *},
	abstract = {Social media features a wide range of nonstandard spellings, many of which appear inspired by phonological variation. However, the nature of the connection between variation across the spoken and written modalities remains poorly understood. Are phono-logical variables transferred to writing on the level of graphemes, or is the larger system of contextual patterning also transferred? This paper considers orthographic coda deletions corresponding to the phonological variables of (ing) and (t,d). In both cases, orthography mirrors speech: reduction of the -ing suffix depends on the word's syntactic category, and reduction of the -t,-d suffix depends on the succeeding phonological context. These spellings are more frequently used in informal conversational contexts, and in areas with high propor-tions of African Americans, again mirroring the patterning of the associated phonological variables. This suggests a deep connection between variation in the two modalities, necessi-tating a new account of the production of cross-modal variation. (150 words)},
	author = {Eisenstein, Jacob},
	year = {2015},
	keywords = {Applications, Natural Language Processing, computer-mediated communication Word count 7687, orthography, social media, variation},
}

@article{Belloni2015,
	title = {Escaping the local minima via simulated annealing: {Optimization} of approximately convex functions},
	volume = {40},
	issn = {15337928},
	abstract = {We consider the problem of optimizing an approximately convex function over a bounded convex set in Rn using only function evaluations. The problem is reduced to sampling from an approximately log-concave distribution using the Hit-and-Run method, which is shown to have the same \$Ω\$∗ complexity as sampling from log-concave distributions. In addition to extend the analysis for log-concave distributions to approximate log-concave distributions, the implementation of the 1- dimensional sampler of the Hit-and-Run walk requires new methods and analysis. The algorithm then is based on simulated annealing which does not relies on first order conditions which makes it essentially immune to local minima. We then apply the method to different motivating problems. In the context of zeroth order stochastic convex optimization, the proposed method produces an ϵ-minimizer after \$Ω\$∗ (n7.5ϵ-2) noisy function evaluations by inducing a \$Ω\$(ϵ=n)-approximately log concave distribution. We also consider in detail the case when the "amount of non-convexity" decays towards the optimum of the function. Other applications of the method discussed in this work include private computation of empirical risk minimizers, two-stage stochastic programming, and approximate dynamic programming for online learning.},
	number = {2015},
	journal = {Journal of Machine Learning Research},
	author = {Belloni, Alexandre and Liang, Tengyuan and Narayanan, Hariharan and Rakhlin, Alexander},
	year = {2015},
	note = {arXiv: 1501.07242},
	pages = {1--26},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MBVR93AU\\Escaping the Local Minima via Simulated Annealing.pdf:application/pdf},
}

@article{Bottolo2010,
	title = {Evolutionary {Stochastic} {Search} for {Bayesian} {Model} {Exploration}},
	volume = {5},
	doi = {10.1214/10-BA523},
	abstract = {Implementing Bayesian variable selection for linear Gaussian regression models for analysing high dimensional data sets is of current interest in many fields. In order to make such analysis operational, we propose a new sampling algorithm based upon Evolutionary Monte Carlo and designed to work under the " large p, small n " paradigm, thus making fully Bayesian multivariate analysis feasible, for example, in genetics/genomics experiments. Two real data examples in genomics are presented, demonstrating the performance of the algorithm in a space of up to 10, 000 covariates. Finally the methodology is compared with a recently proposed search algorithms in an extensive simulation study.},
	number = {3},
	journal = {Bayesian Analysis},
	author = {Bottolo, Leonard and Richardson, Sylvia},
	year = {2010},
	keywords = {Algorithms, Bayesian, variable selection, Evolutionary Monte Carlo, Fast Scan Metropolis-Hastings scheme, lin-ear Gaussian regression models},
	pages = {583--618},
}

@article{Yuecheng,
	title = {Improving a statistical language model by modulating the effects of context words},
	abstract = {We show how to improve a state-of-the-art neural network language model that converts the previous " context " words into feature vectors and combines these feature vectors to predict the feature vector of the next word. Significant improvements in predictive accuracy are achieved by using higher-level features to modulate the effects of the con-text words. This is more effective than using the higher-level features to directly predict the feature vector of the next word, but it is also possible to combine both methods.},
	author = {Yuecheng, Zhang and Mnih, Andriy and Hinton, Geoffrey},
}

@article{Package2020,
	title = {Package ‘ {SmartEDA} '},
	author = {Package, Type and Summarize, Title},
	year = {2020},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YDEBK3WS\\SmartEDA.pdf:application/pdf},
}

@article{Fu2020,
	title = {Automatic assessment of {English} proficiency for {Japanese} learners without reference sentences based on deep neural network acoustic models},
	volume = {116},
	issn = {01676393},
	doi = {10.1016/j.specom.2019.12.002},
	abstract = {Speech-based computer-assisted language learning (CALL) systems should recognize the utterances of the learner with high accuracy and evaluate the language proficiency of the specific speaker with appropriate methods. In this paper, we discuss the automatic assessment of the second language (L2) for non-native speakers. There are many existing works on pronunciation evaluation by applying the goodness of pronunciation (GOP) method. This paper introduces an automatic proficiency evaluation system that combines various kinds of non-native acoustic models and native ones, such as Gaussian mixture model (GMM)-hidden Markov model (HMM) and deep neural network (DNN)-HMM. Most of existing works assume that we know the transcription of an utterance (the reference sentence) when evaluating the utterance, especially in reading and repeating tasks. To realize a reference-free proficiency evaluation, we propose a novel machine score named as the reference-free error rate (RER) to evaluate English proficiency. In our experiments, the DNN-based non-native acoustic models outperformed the traditional acoustic models on non-native speech recognition. Thus, we calculated the RER by regarding the recognition result from the DNN-based non-native acoustic model as “reference” and the result from the native acoustic model as “recognition result”. The proposed RER has high correlation with human proficiency scores, which indicates the effectiveness of RER for automatically estimating the proficiency. By combining the RER with other machine scores such as the log-likelihood scores, we obtained high correlation (reading aloud task: [Formula presented]; constrained interactive dialogue task: [Formula presented]; spontaneous English conversation task: [Formula presented]) to the human scores.},
	number = {December 2019},
	journal = {Speech Communication},
	author = {Fu, Jiang and Chiba, Yuya and Nose, Takashi and Ito, Akinori},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Speech recognition, Acoustic models, Automatic proficiency assessment, Computer-assisted language learning (CALL), Deep neural network (DNN), Japanese learners, Non-native speech},
	pages = {86--97},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D4ERCDRB\\Automatic-assessment-of-English-proficiency-for-Japanese-learn_2020_Speech-C.pdf:application/pdf},
}

@article{Diaby2013,
	title = {Toward the next generation of recruitment tools: {An} online social network-based job recommender system},
	doi = {10.1145/2492517.2500266},
	abstract = {This paper presents a content-based recommender system which proposes jobs to Facebook and LinkedIn users. A variant of this recommender system is currently used by Work4, a San Francisco-based software company that offers Facebook recruitment solutions. Work4 is the world leader in social recruitment technology; to use its applications, Facebook or LinkedIn users explicitly grant access to some parts of their data, and they are presented with the jobs whose descriptions are matching their profiles the most. The profile of a user contains two types of data: interactions data (user's own data) and social connections data (user's friends data). Furthermore the users profiles and the description of jobs are divided into several parts called fields. Our experiments suggest that to predict the users interests for jobs, using basic similarity measures together with their interactions data collected by Work4 can be improved upon. The second part of this study presents a method to estimate the importance of each field of users and jobs in the task of job recommendation. Finally, the third part is devoted to the use of a machine learning algorithm in order to improve the results obtained with similarity measures: we trained a linear SVM (Support Vector Machines). Our results show that using this supervised learning procedure increases the performance of our contentbased recommender system. Copyright 2013 ACM.},
	journal = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2013},
	author = {Diaby, Mamadou and Viennet, Emmanuel and Launay, Tristan},
	year = {2013},
	note = {ISBN: 9781450322409},
	keywords = {Support vector machines, Content-based recommender system, Social networks, Social recruiting},
	pages = {821--828},
}

@article{Vergyri2004,
	title = {Morphology-{Based} {Language} {Modeling} for {Arabic} {Speech} {Recognition}},
	abstract = {Language modeling is a difficult problem for languages with rich morphology. In this paper we investigate the use of morphology-based language models at different stages in a speech recognition system for conversational Arabic. Class-based and single-stream factored language models using morphological word representations are applied within an N-best list rescoring framework. In addition, we explore the use of factored language models in first-pass recognition, which is facilitated by two novel procedures: the data-driven optimization of a multi-stream language model structure, and the conversion of a factored language model to a standard word-based model. We evaluate these techniques on a large-vocabulary recognition task and demonstrate that they lead to perplexity and word error rate reductions.},
	journal = {Interspeech},
	author = {Vergyri, Dimitra and Kirchhoff, Katrin and Duh, Kevin and Stolcke, Andreas and Park, Menlo},
	year = {2004},
	keywords = {Applications, Natural Language Processing, language modeling, speech recognition, Arabic, morphology},
	pages = {4--7},
}

@article{Elster2015,
	title = {A {Guide} to {Bayesian} {Inference} for {Regression} {Problems}},
	abstract = {C. Elster, K. Klauenberg, M. Walzel, G. W ̈ubbeler, P. Harris, M. Cox, C. Matthews, I. Smith, L. Wright, A. Allard, N. Fischer, S. Cowen, S. Ellison, P. Wilson, F. Pennecchi, G. Kok, A. van der Veen, L. Pendrill, A Guide to Bayesian Inference for Regression Problems , Deliverable of EMRP project NEW04 “Novel math- ematical and statistical approaches to uncertainty evaluation”, 2015.},
	journal = {European Metrology Research Programme (EMRP)},
	author = {Elster, Clemens and Klauenberg, Katy and Walzel, Monika},
	year = {2015},
	pages = {1--75},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MMGYPEEZ\\A Guide to Bayesian Inference for Regression Problems.pdf:},
}

@article{Ormel1994,
	title = {Common mental disorders and disability across cultures. {Results} from the {WHO} {Collaborative} {Study} on {Psychological} {Problems} in {General} {Health} {Care}},
	volume = {272},
	issn = {00987484},
	url = {http://jama.ama-assn.org/cgi/doi/10.1001/jama.272.22.1741},
	doi = {10.1001/jama.272.22.1741},
	number = {22},
	journal = {JAMA: The Journal of the American Medical Association},
	author = {Ormel, J},
	year = {1994},
	pages = {1741--1748},
}

@book{Plag2003,
	address = {Cambridge},
	title = {Word-formation in {English}},
	isbn = {0-521-52563-2},
	abstract = {This textbook provides an accessible introduction to the study of word-formation, focusing specifically on English. Assuming no prior linguistic knowledge, it explains the fundamentals of word-formation, encouraging students to undertake their own morphological analyses of English words, and familiarising them with the methodological tools to obtain and analyse relevant data. Cover; Half-title; Series-title; Title; Copyright; Contents; Preface; Abbreviations and notational conventions; Introduction: what this book is about and how it can be used; 1 Basic concepts; 2 Studying complex words; 3 Productivity and the mental lexicon; 4 Affixation; 5 Derivation without affixation; 6 Compounding; 7 Theoretical issues: modeling word-formation; Answer key to exercises; References; Subject index; Affix index; Author index.},
	publisher = {Cambridge University Press},
	author = {Plag, Ingo.},
	year = {2003},
	keywords = {Applications, Natural Language Processing},
}

@article{TomasBrychcin,
	title = {Semantic {Spaces} for {Improving} {Language} {Modeling}},
	abstract = {Language models are crucial for many tasks in NLP (Natural Language Processing) and n-grams are the best way to build them. Huge effort is being invested in improving n-gram language models. By introducing external information (morphology, syntax, partitioning into documents, etc.) into the models a significant improvement can be achieved. The models can however be improved with no external information and smoothing is an excellent example of such an improvement. In this article we show another way of improving the models that also requires no external information. We examine patterns that can be found in large corpora by building semantic spaces (HAL, COALS, BEAGLE and others described in this article). These semantic spaces have never been tested in language modeling before. Our method uses semantic spaces and clustering to build classes for a class-based language model. The class-based model is then coupled with a standard n-gram model to create a very effective language model. Our experiments show that our models reduce the perplexity and improve the accuracy of n-gram lan-guage models with no external information added. Training of our models is fully unsupervised. Our models are very effective for inflectional languages, which are particularly hard to model. We show results for five different semantic spaces with different settings and different number of classes. The perplexity tests are ac-companied with machine translation tests that prove the ability of proposed models to improve performance of a real-world application.},
	author = {Tomáš Brychcín, \$ and Konopík, Miloslav},
	keywords = {Applications, Natural Language Processing, Clustering, BEAGLE, Class-based language models, COALS, HAL, Inflectional languages, Machine translation, Purandare\&Pedersen, Random Indexing, Semantic spaces},
}

@misc{Kottmann2016,
	title = {{openNLP}: {Apache} {OpenNLP} {Tools} {Interface}},
	url = {https://opennlp.apache.org/},
	author = {Kottmann, Jörn and Ingersoll, Grant and Drost, Isabel and Kosin, James and Baldridge, Jason and Morton, Thomas and Silva, William and Agerri, Rodrigo and Autayeu, Aliaksandr and Galitsky, Boris and Giaconia, Mark and Teofili, Tommaso and Khuc, Vinh and Beylerian, Anthony and Bouazizi, Mondher and Mattmann, Chris and Mensikova, Anastasija},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Meyer2013,
	title = {Package ‘ vcd '},
	abstract = {Lista de funciones del package vcd de R.},
	author = {Meyer, David and Zeileis, Achim and Hornik, Kurt and Gerber, Florian and Friendly, Michael},
	year = {2013},
	keywords = {R STAT, Software, Software Manual, STATISTICAL},
	pages = {123},
}

@article{Federico2008,
	title = {{IRSTLM}: {An} open source toolkit for handling large scale language models},
	issn = {19909772},
	abstract = {Research in speech recognition and machine translation is boosting the use of large scale n-gram language models. We present an open source toolkit that permits to efficiently handle language models with billions of n-grams on conventional machines. The IRSTLM toolkit supports distribution of n-gram collection and smoothing over a computer cluster, language model compression through probability quantization, lazy-loading of huge language models from disk. IRSTLM has been so far successfully deployed with the Moses toolkit for statistical machine translation and with the FBK-irst speech recognition system. Efficiency of the tool is reported on a speech transcription task of Italian political speeches using a language model of 1.1 billion four-grams.},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	author = {Federico, Marcello and Bertoldi, Nicola and Cettolo, Mauro},
	year = {2008},
	keywords = {Language modeling, Automatic speech recognition, Statistical machine translation},
	pages = {1618--1621},
}

@article{Rcpp2016,
	title = {R {Package} ‘reshape2'},
	author = {Rcpp, Linkingto},
	year = {2016},
}

@article{Wu2018,
	title = {{WNGrad}: {Learn} the {Learning} {Rate} in {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1803.02865},
	abstract = {Adjusting the learning rate schedule in stochastic gradient methods is an important unresolved problem which requires tuning in practice. If certain parameters of the loss function such as smoothness or strong convexity constants are known, theoretical learning rate schedules can be applied. However, in practice, such parameters are not known, and the loss function of interest is not convex in any case. The recently proposed batch normalization reparametrization is widely adopted in most neural network architectures today because, among other advantages, it is robust to the choice of Lipschitz constant of the gradient in loss function, allowing one to set a large learning rate without worry. Inspired by batch normalization, we propose a general nonlinear update rule for the learning rate in batch and stochastic gradient descent so that the learning rate can be initialized at a high value, and is subsequently decreased according to gradient observations along the way. The proposed method is shown to achieve robustness to the relationship between the learning rate and the Lipschitz constant, and near-optimal convergence rates in both the batch and stochastic settings (\$O(1/T)\$ for smooth loss in the batch setting, and \$O(1/{\textbackslash}backslashsqrt\{{\textbackslash}\{\}T\{{\textbackslash}\}\})\$ for convex loss in the stochastic setting). We also show through numerical evidence that such robustness of the proposed method extends to highly nonconvex and possibly non-smooth loss function in deep learning problems.Our analysis establishes some first theoretical understanding into the observed robustness for batch normalization and weight normalization.},
	author = {Wu, Xiaoxia and Ward, Rachel and Bottou, Léon},
	year = {2018},
	note = {arXiv: 1803.02865},
	pages = {1--16},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZBJARK7I\\Learn the Learning Rate in Gradient Descent.pdf:application/pdf},
}

@article{TextureInternet,
	title = {The {Texture} of {Internet}},
	keywords = {Applications, Natural Language Processing},
}

@book{Wickham2015,
	title = {Advanced {R}},
	volume = {53},
	isbn = {978-1-4665-8697-0},
	abstract = {applicability for this approach.},
	author = {Wickham, Hadley},
	year = {2015},
	pmid = {25246403},
	doi = {10.1017/CBO9781107415324.004},
	note = {arXiv: 1011.1669v3
Publication Title: Journal of Chemical Information and Modeling
Issue: 9
ISSN: 1098-6596},
}

@misc{Norvig2015,
	title = {What is the average number of letters for an {English} word? - {Quora}},
	url = {https://www.quora.com/What-is-the-average-number-of-letters-for-an-English-word},
	author = {Norvig, Peter},
	year = {2015},
	note = {Publication Title: Quora},
	keywords = {Applications, Natural Language Processing},
}

@article{PRACEBcTOMASMIKOLOVAUTHORVEDOUCIPRACEDocRNDrPAVELSMRZ2007,
	title = {{MODELOVÁNÍ} {JAZYKA} {V} {ROZPOZNÁVÁNÍ} Č {EŠ} {TINY} {LANGUAGE} {MODELING} {FOR} {SPEECH} {RECOGNITION} {IN} {CZECH} {DIPLOMOVÁ} {PRÁCE}},
	url = {https://www.vutbr.cz/www_base/zav_prace_soubor_verejne.php?file_id=115583},
	author = {PRÁCE Bc TOMÁŠ MIKOLOV AUTHOR VEDOUCÍ PRÁCE Doc RNDr PAVEL SMRŽ, Autor},
	year = {2007},
	keywords = {Applications, Natural Language Processing},
}

@article{Arora2007,
	title = {Introduction to optimization},
	doi = {10.1142/9789812779670_0001},
	abstract = {Basic concepts of optimization are described in this chapter. Optimization models for engineering and other applications are described and discussed. These include continuous variable and discrete variable problems. Optimality conditions for the continuous unconstrained and constrained problems are presented. Basic concepts of algorithms for continuous and discrete variable problems are described. An introduction to the topics of multiobjective and global optimization is also presented.},
	journal = {Optimization of Structural and Mechanical Systems},
	author = {Arora, Jasbir S},
	year = {2007},
	note = {ISBN: 9789812779670},
	pages = {1--34},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EE6ZPY5U\\Lecture-Optimization.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HBP728U9\\Lecture-Optimization.pdf:application/pdf},
}

@article{Tripathy2020,
	title = {Recommendation systems},
	volume = {26},
	issn = {1528-4972},
	doi = {10.1145/3383396},
	number = {3},
	journal = {XRDS: Crossroads, The ACM Magazine for Students},
	author = {Tripathy, Chittaranjan and Pavlidis, Yannis},
	year = {2020},
	pages = {54--56},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V6HS5K6V\\Recommendation Systems (Stanford).pdf:application/pdf},
}

@article{REVISITINGHIDDENMARKOV2019,
	title = {{REVISITING} {HIDDEN} {MARKOV} {MODELS} {FOR} {SPEECH} {EMOTION} {RECOGNITION} {Shuiyang} {Mao} , {Dehua} {Tao} , {Guangyan} {Zhang} , {P} . {C} . {Ching} and {Tan} {Lee}},
	year = {2019},
	note = {ISBN: 9781538646588},
	pages = {6715--6719},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8Q8SG6CR\\REVISITING HIDDEN MARKOV MODELS FOR SPEECH EMOTION RECOGNITION.pdf:application/pdf},
}

@article{Kelly2012,
	title = {“{It}'s the {American} {Lifestyle}!”: {An} {Investigation} of {Text} {Messaging} by {College} {Students}},
	volume = {13},
	issn = {1745-9435},
	url = {http://www.tandfonline.com/doi/abs/10.1080/17459435.2012.719203},
	doi = {10.1080/17459435.2012.719203},
	number = {1},
	journal = {Qualitative Research Reports in Communication},
	author = {Kelly, Lynne and Keaten, James A and Becker, Bonnie and Cole, Jodi and Littleford, Lea and Rothe, Barrett},
	month = jan,
	year = {2012},
	keywords = {Applications, Natural Language Processing},
	pages = {1--9},
}

@misc{Koohafkan2015,
	title = {Package 'kfigr' {\textbar} {Integrated} {Code} {Chunk} {Anchoring} and {Referencing} for {R} {Markdown}},
	author = {Koohafkan, Michael C},
	year = {2015},
	keywords = {Applications, Natural Language Processing},
}

@article{Gr8TxtpectationsCreativity2007,
	title = {Gr8 {Txtpectations} {The} {Creativity} of {Text} {Spelling} {The} vernacular spelling tradition},
	abstract = {Tim Shortis argues that new vernacular forms of spelling are the latest in a creative tradition of rule-based, non-standard orthography which poses little threat to standard spelling but challenges accepted ideas about the function of standardisation. The term Txt is used to refer to the text used in SMS text messaging, instant messaging, internet chat, informal emails and social software. A manifesto for Txt spelling Popular media concerns about Txt spelling and the associated allegations of 'dumbing down' in youth text messaging are erroneous. There is considerable creativity and diversity on the part of the users in the ways they deploy the vernacular resources of Txt and there is a longstanding historical basis for such practices; both in 'untutored' domestic contexts, and in popular culture. The logical basis of non-standard orthography, as found in Txt, is also at the root of the intelligibility of some literary verbal art which includes text respelled in non-standard forms. e.e. cummings, James Joyce and William Faulkner come to mind. In these examples too, the non-standard spelling is a source of creativity and vividness and enables a simulation of spoken mode. The growth of informal writing enabled by new text forms such as SMS and MSN has de-regulated what counts as English spelling rather than altered spelling itself. It has opened up tolerance of a wider range of spelling choices available in day-to-day use and has allowed users new flexibility, economy and means of inflecting nuances of meaning. Seven years after its mass adoption in the UK, Txt is no longer the domain of the 'yoof' who first popularised it: users are now from all age ranges and social profiles. The traditional discourse around codified standard English spelling and its associated binary evaluations of competence and incompetence has given way to criteria based on appropriateness and the pragmatic issue of what works for the user in a given context. Spelling is now a more flexible friend used for functional economy and identity performance as well as to show credible mastery of standard conventions. In effect, the less defined, determinate spaces of what counts as literacy in new text forms have created a context in which there has been an extension of the orthographic palette of meaning-making potential beyond the standard forms listed in dictionaries. Viral spelling reform Underneath the excited media coverage of Txt as a youth argot and the purported evidence of moral and linguistic decline, the spelling of Txt can be seen as a mass iteration of a sort of informalised spelling reform but without the official framings of that movement. These framings, as set out in Masha Bell's article in this issue of EDM, include the organised project to unpick the standard English conventions in which print has been conducted for four hundred years and replace them with codified alternative spellings in a new standard orthography. In the case of Txt, there is no codification and no supplanting of standard forms: the standard and non-standard co-exist, and the non-standard is not unitary or prescriptive but may include several variations in the ways to spell a single word. It does not follow from this that all writers of new text forms such as SMS make use of the extended orthographic palette, or that any one user will be consistent in her or his approach irrespective of the situation. People routinely respell in some contexts and expect and provide standard forms in others. These 'people' are not homogeneous and all individuals exercise their choices and positions heterogeneously and in response to their sense of identity, social affiliations and their perception of the exigencies of the particular situation. So Txt spelling, unlike standard English spelling, is heterogeneous in its practices, with varied idiolectal profiles relating to the individual user's choices, habits, and sense of identity.},
	year = {2007},
	keywords = {Applications, Natural Language Processing},
}

@article{Clarke2016,
	title = {Improving {Health} {Outcomes} for {Patients} with {Depression}: {A} {Population} {Health} {Imperative}. {Report} on an {Expert} {Panel} {Meeting}},
	volume = {19},
	issn = {1942-7891},
	doi = {10.1089/pop.2016.0114},
	abstract = {Improving Health Outcomes for Patients with Depression: A Population Health Imperative. Report on an Expert Panel Meeting Janice L. Clarke, RN, Alexis Skoufalos, EdD, Alice Medalia, PhD, and A. Mark Fendrick, MD Editorial: A Call to Action: David B. Nash, MD, MBA S-2 Overview: Depression and the Population Health Imperative S-3 Promoting Awareness of the Issues and Opportunities for Improvement S-5 Cognitive Dysfunction in Affective Disorders S-5 Critical Role of Employers in Improving Health Outcomes for Employees with Depression S-6 Closing the Behavioral Health Professional and Process Gaps S-6 Achieving the Triple Aim for Patients with Depressive Disorders S-6 Improving the Experience of Care for Patients with Depression S-6 Improving Quality of Care and Health Outcomes for Patients with Depression S-7 Changing the Cost of Care Discussion from How Much to How Well S-8 Panel Insights and Recommendations S-9 Conclusion S-10},
	number = {S2},
	journal = {Population Health Management},
	author = {Clarke, Janice L and Skoufalos, Alexis and Medalia, Alice and Fendrick, A Mark},
	year = {2016},
	pages = {S--1--S--12},
}

@article{Raftery1994,
	title = {Model {Selection} and {Accounting} for {Model} {Uncertainty} in {Linear} {Regression} {Models}},
	volume = {89},
	abstract = {We consider the problems of variable selection and accounting for model uncertainty in linear regression models. Conditioning on a single selected model ignores model uncertainty, and thus leads to the underestimation of uncertainty when making inferences about quantities of interest. The complete Bayesian solution to this problem involves averaging over all possible models when making inferences about quantities of interest. This approach is often not practical. In this paper we offer two alternative approaches. First we describe a Bayesian model selection algorithm called "Occam's "Window" which involves averaging over a reduced set of models. Second, we describe a Markov chain Monte Carlo approach which directly approximates the exact solution. Both these model averaging procedures provide better predictive performance than any single model which might reasonably have been selected. In the extreme case where there are many candidate predictors but there is no relationship between any of them and the response, standard variable selection procedures often choose some subset of variables that yields a high RÂ² and a highly significant overall F value. We refer to this unfortunate phenomenon as "Freedman's Paradox" (Freedman, 1983). In this situation, Occam's \{vVindow\} usually indicates the null model as the only one to be considered, or else a small number of models including the null model, thus largely resolving the paradox.},
	number = {428},
	journal = {Journal of the American Statistical Association},
	author = {Raftery, Adrian and Madigan, David and Hoeting, Jennifer},
	year = {1994},
	keywords = {Algorithms, Bayesian, model uncertainty, bayes factor, markov chain monte carlo, occam, s window, composition, freedman, model, posterior model probability, s paradox},
	pages = {1535--1546},
}

@book{feynman2011six,
	title = {Six {Easy} {Pieces}: {Essentials} of {Physics} {Explained} by {Its} {Most} {Brilliant} {Teacher}},
	isbn = {978-0-465-02529-9},
	url = {https://books.google.com/books?id=2OCKrF6YNKEC},
	publisher = {Basic Books},
	author = {Feynman, R P and Leighton, R B and Sands, M},
	year = {2011},
	note = {Series Title: Helix Books},
}

@article{Clark,
	title = {Pre-processing very noisy text},
	abstract = {Existing techniques for tokenisation and sentence boundary identification are extremely accurate when the data is perfectly clean (Mikheev, 2002), and have been applied successfully to corpora of news feeds and other post-edited corpora. Informal written texts are readily available, and with the growth of other informal text modalities (IRC, ICQ, SMS etc.) are becoming an interesting alternative, perhaps better suited as a source for lexical resources and language models for studies of dialogue and spontaneous speech. However, the high degree of spelling errors, irregularities and idiosyncrasies in the use of punctuation, white space and capitalisation require specialised tools. In this paper we study the design and implementation of a tool for pre-processing and normalisation of noisy corpora. We argue that rather than having separate tools for tokenisation, segmentation and spelling correction organised in a pipeline, a unified tool is appropriate because of certain specific sorts of errors. We describe how a noisy channel model can be used at the character level to perform this. We describe how the sequence of tokens needs to be divided into various types depending on their characteristics, and also how the modelling of white-space needs to be conditioned on the type of the preceding and following tokens. We use trainable stochastic transducers to model typographical errors, and other orthographic changes and a variety of sequence models for white space and the different sorts of tokens. We discuss the training of the models and various efficiency issues related to the decoding algorithm, and illustrate this with examples from a 100 million word corpus of Usenet news.},
	author = {Clark, Alexander and Tim, Issco /},
	keywords = {Applications, Natural Language Processing},
}

@article{Arnold2017,
	title = {A {Tidy} data model for natural language processing using {cleanNLP}},
	volume = {9},
	issn = {20734859},
	doi = {10.32614/rj-2017-035},
	abstract = {Recent advances in natural language processing have produced libraries that extract lowlevel features from a collection of raw texts. These features, known as annotations, are usually stored internally in hierarchical, tree-based data structures. This paper proposes a data model to represent annotations as a collection of normalized relational data tables optimized for exploratory data analysis and predictive modeling. The R package cleanNLP, which calls one of two state of the art NLP libraries (CoreNLP or spaCy), is presented as an implementation of this data model. It takes raw text as an input and returns a list of normalized tables. Specific annotations provided include tokenization, part of speech tagging, named entity recognition, sentiment analysis, dependency parsing, coreference resolution, and word embeddings. The package currently supports input text in English, German, French, and Spanish.},
	number = {2},
	journal = {R Journal},
	author = {Arnold, Taylor},
	year = {2017},
	note = {arXiv: 1703.09570},
	pages = {248--267},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\K3MJLXST\\A Tidy Data Model for Natural Language Processing using cleanNLP.pdf:application/pdf},
}

@misc{RCoreTeam2016,
	title = {The {R} project for statistical computing},
	url = {https://www.r-project.org/},
	author = {{The R Foundation}},
	year = {2016},
	note = {Volume: 8/1},
}

@article{Lech2020,
	title = {Real-{Time} {Speech} {Emotion} {Recognition} {Using} a {Pre}-trained {Image} {Classification} {Network}: {Effects} of {Bandwidth} {Reduction} and {Companding}},
	volume = {2},
	doi = {10.3389/fcomp.2020.00014},
	abstract = {This paper provides a step by step introduction to real-time speech emotion recognition (SER) using a pre-trained image classification network. The procedure has low computational requirements and can be implemented on different voice-based communication platforms such as mobile phones, call centers, and online communication facilities. Effects of reduced speech bandwidth and the mu-low companding procedure used in transmission systems on the SER accuracy are examined. The results showed that the baseline approach achieved an average accuracy of 82\% when trained on the Berlin Emotional Speech (EMO-DB) data with seven categorical emotions. Reduction of the sampling frequency from the baseline 16 kHz to 8 kHz (i.e., bandwidth reduction from 8 kHz to 4 kHz respectively) led to a decrease of SER accuracy by about 3.3\%. The companding procedure reduced the result by 3.8\%, and the combined effect of both factors further reduced the average accuracy by about 7\% compared to the baseline results. The SER was implemented in real-time with emotional labels generated every 1.033 to 1.026 seconds. Real-time implementation timelines are presented.},
	number = {May},
	journal = {Frontiers in Computer Science},
	author = {Lech, Margaret and Stolar, Melissa and Best, Christopher and Bolia, Robert},
	year = {2020},
	keywords = {bandwidth reduction, companding, real-time speech classification, speech emotions, transfer learning},
	pages = {1--14},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NP3YJZUY\\Real-Time Speech Emotion Recognition Using a Pre-trained Image Classification Network Effects of Bandwidth Reduction and Companding.pdf:application/pdf},
}

@article{Copestake1997,
	title = {Augmented and alternative {NLP} techniques for augmentative and alternative communication},
	abstract = {Current communication devices designed\${\textbackslash}\$nfor non-speaking users are inadequate to\${\textbackslash}\$nsupport conversation because the speed\${\textbackslash}\$nwith which a user can input information is typically very limited. We describe some practical work on word prediction, and discuss its limitations as a technique for speeding up free text entry. We then outline an alternative approach, currently under development, which combines prediction with a constrained technique for natural language generation.},
	journal = {Proceedings of the ACL workshop on Natural Language Processing for Communication Aids},
	author = {Copestake, Ann},
	year = {1997},
	keywords = {Applications, Natural Language Processing},
	pages = {37--42},
}

@article{Link2006,
	title = {{MODEL} {WEIGHTS} {AND} {THE} {FOUNDATIONS} {OF} {MULTIMODEL} {INFERENCE}},
	volume = {87},
	abstract = {Statistical thinking in wildlife biology and ecology has been profoundly influenced by the introduction of AIC (Akaike's information criterion) as a tool for model selection and as a basis for model averaging. In this paper, we advocate the Bayesian paradigm as a broader framework for multimodel inference, one in which model averaging and model selection are naturally linked, and in which the performance of AIC-based tools is naturally evaluated. Prior model weights implicitly associated with the use of AIC are seen to highly favor complex models: in some cases, all but the most highly parameterized models in the model set are virtually ignored a priori. We suggest the usefulness of the weighted BIC (Bayesian information criterion) as a computationally simple alternative to AIC, based on explicit selection of prior model probabilities rather than acceptance of default priors associated with AIC. We note, however, that both procedures are only approximate to the use of exact Bayes factors. We discuss and illustrate technical difficulties associated with Bayes factors, and suggest approaches to avoiding these difficulties in the context of model selection for a logistic regression. Our example highlights the predisposition of AIC weighting to favor complex models and suggests a need for caution in using the BIC for computing approximate posterior model weights.},
	number = {10},
	journal = {Ecology},
	author = {Link, William A and Barker, Richard J},
	year = {2006},
	keywords = {Algorithms, Bayesian, Bayesian inference, model selection, AIC, BIC, Akaike's information criterion, Bayes factors, Bayesian information criterion, model averaging, Salmo trutta},
	pages = {2626--2635},
}

@techreport{Buza2014,
	title = {Feedback {Prediction} for {Blogs}},
	url = {http://www.cs.bme.hu/},
	abstract = {The last decade lead to an unbelievable growth of the importance of social media. Due to the huge amounts of documents appearing in social media, there is an enormous need for the automatic analysis of such documents. In this work, we focus on the analysis of documents appearing in blogs. We present a proof-of-concept industrial application, developed in cooperation with Capgemini Magyaroszág Kft. The most interesting component of this software prototype allows to predict the number of feedbacks that a blog document is expected to receive. For the prediction, we used various predictions algorithms in our experiments. For these experiments, we crawled blog documents from the internet. As an additional contribution, we published our dataset in order to motivate research in this field of growing interest.},
	institution = {Springer International Publishing},
	author = {Buza, Krisztian},
	year = {2014},
	pages = {145--152},
}

@article{Workshop2003,
	title = {{USING} {CONTINUOUS} {SPACE} {LANGUAGE} {MODELS} {Holger} {Schwenk} and {Jean}-{Luc} {Gauvain}},
	author = {Workshop, Ieee and Processing, Spontaneous Speech},
	year = {2003},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SNAKPAF6\\USING CONTINUOUS SPACE LANGUAGE MODELS.pdf:application/pdf},
}

@article{EMPIRICALSTUDYLEARNING2013,
	title = {{AN} {EMPIRICAL} {STUDY} {OF} {LEARNING} {RATES} {IN} {DEEP} {NEURAL} {NETWORKS} {FOR} {SPEECH} {RECOGNITION} {Andrew} {Senior} , {Georg} {Heigold} , {Marc} ' {Aurelio} {Ranzato} , {Ke} {Yang} {New} {York}},
	journal = {New York},
	year = {2013},
	note = {ISBN: 9781479903566},
	pages = {6724--6728},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C9R3R59H\\An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recognition.pdf:application/pdf},
}

@book{Cramer1946,
	title = {Mathematical {Methods} of {Statistics}.},
	volume = {42},
	isbn = {978-0-691-00547-8},
	url = {http://www.jstor.org/stable/2280199?origin=crossref},
	abstract = {In this classic of statistical mathematical theory, Harald Cram\{é\}r joins the two major lines of development in the field: while British and American statisticians were developing the science of statistical inference, French and Russian probabilitists transformed the classical calculus of probability into a rigorous and pure mathematical theory. The result of Cram\{é\}r's work is a masterly exposition of the mathematical methods of modern statistics that set the standard that others have since sought to follow. For anyone with a working knowledge of undergraduate mathematics the book is self contained. The first part is an introduction to the fundamental concept of a distribution and of integration with respect to a distribution. The second part contains the general theory of random variables and probability distributions while the third is devoted to the theory of sampling, statistical estimation, and tests of significance.},
	author = {Hoel, Paul G and Wolfowitz, J and Cramer, Harald},
	year = {1947},
	doi = {10.2307/2280199},
	note = {Publication Title: Journal of the American Statistical Association
Issue: 237
ISSN: 01621459},
}

@article{Bauman2013,
	title = {Optimization of click-through rate prediction in the {Yandex} search engine},
	volume = {47},
	issn = {0005-1055},
	doi = {10.3103/s0005105513020040},
	number = {2},
	journal = {Automatic Documentation and Mathematical Linguistics},
	author = {Bauman, K E and Kornetova, A N and Topinskii, V A and Khakimova, D A},
	year = {2013},
	keywords = {click through rate estimation, experiment plan, off line prediction, prediction quality metrics},
	pages = {52--58},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QNVDRD95\\2012_OptimizationCTRPrediction.pdf:application/pdf},
}

@misc{Hlavac2015,
	title = {Well-{Formatted} {Regression} and {Summary} {Statistics} {Tables}},
	abstract = {Produces LaTeX code for well-formatted tables that hold regression analysis results from several models side-by-side,as well as summary statistics},
	author = {Hlavac, Marek and Package, Type and Regression, Title Well-formatted and Tables, Summary Statistics},
	year = {2015},
	note = {Pages: 1-11},
	file = {Well-Formatted Regression and Summary Statistics Tables.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Well-Formatted Regression and Summary Statistics Tables.pdf:application/pdf},
}

@misc{techdown,
	title = {Tech hiring slowdown bad sign ahead of jobs report},
	url = {https://www.cnbc.com/2020/08/06/tech-hiring-slowdown-bad-sign-ahead-of-jobs-report.html},
	urldate = {2020-08-13},
}

@misc{Ng,
	title = {Machine {Learning} by {Stanford} {University}},
	url = {https://www.coursera.org/learn/machine-learning/home/welcome},
	abstract = {Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI. This course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include: (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI). The course will also draw from numerous case studies and applications, so that you'll also learn how to apply learning algorithms to building smart robots (perception, control), text understanding (web search, anti-spam), computer vision, medical informatics, audio, database mining, and other areas.},
	urldate = {2017-09-08},
	author = {Ng, Andrew},
	year = {2015},
}

@article{Eda2006,
	title = {1 . {Exploratory} {Data} {Analysis} 1 . {Exploratory} {Data} {Analysis} - {Detailed} {Table} of {Contents} [ 1 .]},
	volume = {1},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21097328},
	abstract = {The cardiac magnetic resonance (CMR) images from a group of patients with myocardial scars and implanted cardioverter-defibrillator (ICD) are divided into a group with low risk of arrhythmias (late incidents) and a group with high risk of arrhythmias (early incidents). Several hundred quantitative features describing sizes, statistics and textures of the segmented and defined areas of the images are computed from manually segmented images in an exploratory analysis. The method used to determine decision regions to discriminate the patients with low risk of arrhythmias from the patient with high risk of arrhythmias is a maximum likelihood estimation based Bayes classifiers described in 1. The results presented can be interpreted as hypothesis of which features, and combinations of features, that might have discriminative power. A major hypothesis that arises is that there are important textural information in the scarred and non-scarred areas.},
	number = {4},
	journal = {Analysis},
	author = {Eda, What},
	year = {2006},
	pages = {5728--5731},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SYMD3LB5\\Exploratory Data Analysis.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T23MM4RS\\Exploratory Data Analysis.pdf:application/pdf},
}

@article{Keskar2016,
	title = {On {Large}-{Batch} {Training} for {Deep} {Learning}: {Generalization} {Gap} and {Sharp} {Minima}},
	issn = {0148396X},
	doi = {10.1227/01.NEU.0000255452.20602.C9},
	abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	year = {2016},
	pmid = {17460516},
	note = {arXiv: 1609.04836
ISBN: 9781405161251},
}

@article{Hornik2009,
	title = {Open-{Source} {Machine} {Learning}: \{{R}\} {Meets} \{{Weka}\}},
	volume = {24},
	doi = {10.1007/s00180-008-0119-7},
	number = {2},
	journal = {Computational Statistics},
	author = {Hornik, Kurt and Buchta, Christian and Zeileis, Achim},
	year = {2009},
	keywords = {Applications, Natural Language Processing},
	pages = {225--232},
}

@article{Jelinek1999,
	title = {Putting {Language} into {Language} {Modeling}},
	volume = {1},
	number = {June 1999},
	author = {Jelinek, Frederick and Helba, Ciprian},
	year = {1999},
}

@book{Doherty2016,
	title = {The {Path} to {Predictive} {Analytics} and {Machine} {Learning}},
	isbn = {978-1-4919-6966-3},
	abstract = {This chapter provides a broad introduction to applied machine learning with emphasis on resolving these tradeoffs with business objectives in mind. We present a conceptual overview of the theory underpinning machine learning. Later chapters will expand the discussion to include system design considerations and practical advice for implementing predictive analytics applications. Given the experimental nature of applied data science, the theme of flexibility will show up many times. In addition to the theoretical, computational, and mathematical features of machine learning techniques, the reality of running a business with limited resources, especially limited time, affects how you should choose and deploy strategies.},
	author = {Doherty, Conor and Camiña, Steven and White, Kevin and Orenstein, Gary},
	year = {2016},
	keywords = {bigdata, machine\_learning, memsql, predictive analytics},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F8D8DT72\\Path-to-Predictive-Analytics-and-Machine-Learning_eBook.pdf:application/pdf},
}

@inproceedings{Katon2002,
	title = {Impact of major depression on chronic medical illness},
	volume = {53},
	isbn = {0022-3999},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0022399902003136},
	doi = {10.1016/S0022-3999(02)00313-6},
	abstract = {Due to the aging of the U.S. population, healthcare providers will be called upon to diagnose and treat patients with chronic medical illness with increasing frequency. This paper will review the epidemiology of depression in patients with chronic illness and the impact of comorbid depression on increased use of medical resources and costs, amplifica- tion of physical symptoms, additive functional impairment, decreased ability to adhere to medications, important life- style changes (i.e., increasing exercise, changing dietary patterns, quitting smoking), and increased mortality},
	booktitle = {Journal of {Psychosomatic} {Research}},
	author = {Katon, Wayne and Ciechanowski, Paul},
	month = oct,
	year = {2002},
	pmid = {12377294},
	note = {Issue: 4
ISSN: 00223999},
	pages = {859--863},
}

@article{Ghayoomi2008,
	title = {A {POS}-{Based} {Word} {Prediction} {System} for},
	author = {Ghayoomi, Masood},
	year = {2008},
	keywords = {Applications, Natural Language Processing, word prediction, statistical language modeling, pos tagging},
	pages = {5221},
}

@article{Saini2019,
	title = {{PrivateJobMatch}: {A} privacy-oriented deferred multi-match recommender system for stable employment},
	doi = {10.1145/3298689.3346983},
	abstract = {Coordination failure reduces match quality among employers and candidates in the job market, resulting in a large number of unflled positions and/or unstable, short-term employment. Centralized job search engines provide a platform that connects directly employers with job-seekers. However, they require users to disclose a signifcant amount of personal data, i.e., build a user profle, in order to provide meaningful recommendations. In this paper, we present PrivateJobMatch - a privacy-oriented deferred multi-match recommender system - which generates stable pairings while requiring users to provide only a partial ranking of their preferences. PrivateJobMatch explores a series of adaptations of the game-theoretic Gale-Shapley deferred acceptance algorithm which combine the fexibility of decentralized markets with the intelligence of centralized matching. We identify the shortcomings of the original algorithm when applied to a job market and propose novel solutions that rely on machine learning techniques. Experimental results on real and synthetic data confrm the benefts of the proposed algorithms across several quality measures. Over the past year, we have implemented a PrivateJobMatch prototype and deployed it in an active job market economy. Using the gathered real-user preference data, we fnd that the match recommendations are superior to a typical decentralized job market-while requiring only a partial ranking of the user preferences.},
	journal = {RecSys 2019 - 13th ACM Conference on Recommender Systems},
	author = {Saini, Amar and Rusu, Florin and Johnston, Andrew},
	year = {2019},
	note = {arXiv: 1905.04564
ISBN: 9781450362436},
	keywords = {Applications, Recommender Systems, DAA algorithm, Gale-Shapley algorithm, Job matching, Low-rank matrix factorization, MMDAA algorithm, Reciprocal matching},
	pages = {87--95},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FCVRSSWJ\\PrivateJobMatch  A Privacy-Oriented Deferred Multi-Match.pdf:application/pdf},
}

@book{Fulltext,
	title = {full-text},
	keywords = {Applications, Natural Language Processing},
}

@article{Wickham2016a,
	title = {Package ‘reshape'},
	url = {http://had.co.nz/reshape},
	author = {Wickham, Hadley and Maintainer, ]},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{JamesHastie2013,
	title = {An {Introduction} to {Statistical} {Learning}},
	volume = {8},
	issn = {01621459},
	url = {https://www-bcf.usc.edu/%7B~%7Dgareth/ISL/},
	doi = {10.1016/j.peva.2007.06.006},
	abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\textasciitilde}without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
	author = {Gareth, James and Daniela, Witten and Trevor, Hastie and Tibshirani, Robert},
	year = {2013},
	pmid = {10911016},
	note = {arXiv: 1011.1669v3
ISBN: 9780387781884},
	keywords = {psl},
}

@article{Raginsky2017,
	title = {Non-convex learning via {Stochastic} {Gradient} {Langevin} {Dynamics}: a nonasymptotic analysis},
	url = {http://arxiv.org/abs/1702.03849},
	abstract = {Stochastic Gradient Langevin Dynamics (SGLD) is a popular variant of Stochastic Gradient Descent, where properly scaled isotropic Gaussian noise is added to an unbiased estimate of the gradient at each iteration. This modest change allows SGLD to escape local minima and suffices to guarantee asymptotic convergence to global minimizers for sufficiently regular non-convex objectives (Gelfand and Mitter, 1991). The present work provides a nonasymptotic analysis in the context of non-convex learning problems, giving finite-time guarantees for SGLD to find approximate minimizers of both empirical and population risks. As in the asymptotic setting, our analysis relates the discrete-time SGLD Markov chain to a continuous-time diffusion process. A new tool that drives the results is the use of weighted transportation cost inequalities to quantify the rate of convergence of SGLD to a stationary distribution in the Euclidean \$2\$-Wasserstein distance.},
	author = {Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
	year = {2017},
	note = {arXiv: 1702.03849},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XNXIK6RM\\Non-Convex Learning via Stochastic Gradient Langevin.pdf:application/pdf},
}

@article{Bai,
	title = {A {Sample}-gradient-based {Algorithm} for a {Multiple}-{OR} and {PACU} {Surgery} {Scheduling} {Problem} {ISE} {Technical} {Report} {16T}-004 {A} {Sample}-gradient-based {Algorithm} for a {Multiple}-{OR} and {PACU} {Surgery} {Scheduling} {Problem}},
	author = {Bai, Miao and Storer, Robert H and Tonkay, Gregory L},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XTZAS3KP\\A Sample-gradient-based Algorithm for a Multiple-OR and PACU Surgery Scheduling Problem.pdf:application/pdf},
}

@incollection{Prechelt2012,
	title = {Early {Stopping} — {But} {When}?},
	isbn = {978-3-642-35288-1},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8_5},
	abstract = {Validation can be used to detect when overrtting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overrtting early stopping". The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization , whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoo between training time and generalization: From the given mix of 1296 training runs using diierent 12 problems and 24 diierent network architectures I conclude slower stopping criteria allow for small improvements in generalization here: about 4 on average, but cost much more training time here: about factor 4 longer on average.},
	author = {Prechelt, Lutz},
	year = {2012},
	pmid = {8514134},
	doi = {10.1007/978-3-642-35289-8_5},
	note = {arXiv: 1412.6830
ISSN: 01628828},
	keywords = {Algorithms, Regularization, Early Stopping},
	pages = {53--67},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9XDVIBN5\\Early Stopping  but when.pdf:},
}

@article{Lecun1998,
	title = {A {B7CEDGF} {HIB7PRQTSUDGQICWVYX} {HIB} {edCdSISIXvg5r} ` {CdQTw} {XvefCdS}},
	url = {http://ieeexplore.ieee.org/document/726791/#full-text-section},
	abstract = {r£a© ¨ £¡£¢ª9¥a«£¡£a"¢ip¤\{{\textasciicircum}¬iª\${\textbackslash}\$¦Y­®" ¥¥­  ¯  r "§m°w\{\&±N ¨  ¥£¢³² i´ ¯ ³µr:5¦¦ ¦£r  i\{w: ¥£¢"d¥  ¶¯  ³d§\}°{\textasciicircum}·±N£ ¨  ¨ £¢ª\${\textbackslash}\$ ¥£p¤w ¸ ¹¢id"³º¥£aª\${\textbackslash}\$¦¬ ¥"³ap"  ­»¥£k£¢ir ¥ ¥³ ­»®¢ ¨ ¢a§\}ªSr"³air¦r £" ¥£¢®B¢i£ d¸¥£¢i§¥ £¶ ¢ ª¼ªS ¦ ¦ ¥£"" ¨ ´U½¹¢³U¦¦dg"µ³YBµ"§³a ªS£¢iiY¦¦³"1¢iY£¡""r¥£¢i ¥  d¥£ ¨ ³a 1¥aª\${\textbackslash}\$¦"d£¢idª(a¾S\{"1¢iY£¡""d¾ ¨ Y"d¥£ ¨ §³a"´U¿waµa a³airpd  p\{wa¶£¢ir; w"¦¥­§¥£³©  ¨ iS"  ¡£¢ £¢iYµ£³¤¡\{®­ À Á6 ¢i¦d£¶" "¢ir ¼ a a¦d"­»aªÂ ¢id"d¥¢² i£´ ÃY§\}­»Wi¥ ªr Ä ¥£ ¨ ¡£aLi\{"rªSh W¥£iªS¦\{d ­ªS ¡£³¦³OªSi ³:¥  ¨ÆÅdH¬a  ¥£a¶ ¨ ªSr  r§³a¶B d¥£ ¨ ³a¶;Æ ¨ i ¨ ¼ªSd ¨ ´1ÇLi(³£ ¨ ¦  ¨ ª¾¶¥£d ¯  ¦¢·½  \{­»£ªSp\{{\textasciicircum}iSÈ ¯ ½{\textasciicircum}®É¶¡§³dYU" ¥£¢\${\textbackslash}\$ª §\}ªSi ³{\textasciicircum}i\{"dª\${\textbackslash}\$;"®¤"i ¨ a¤³®  ¨ ¯  r "§m°w\{·ªS¢ip1":ª¼ª¼³º \&rµd ;¦"§­»£ªS¥£¸ªS"  ´ ½{\textasciicircum}¾i\{"rªS­»¸aa§\}i\${\textbackslash}\$¢iY£¡£ ¨  ¥ ¨ ³a"\${\textbackslash}\$§¥¤dN´Ê;¬¦d£ªSd dªSa« r" £¢iµd   ¨ ­ ¨ ¤    ¨ ¶¸¢iwË¬¤¡\{­ ¯ "¦¢¸½  \{­»aªSd;p\{{\textasciicircum}a i£´ Ç ¯  ¦¢¾½  \{­»aªSdwp\{w­»" ¨ ¤¼¥¢id¥¾³ \{d¥£¤N´Ìm \{d¿waµa a³aipd  N\{{\textasciicircum}\${\textbackslash}\$¥£¢i§¥ ¸"d¥£ ¨ ³ºd ¥£aª©¤i ¡£¢ ¨ a¤N   ¨ "d¥£¢³² i " ¦ rµ³d "d¥£¸¥¥  ¥£ i®¤ "i ¦ai ¥£¢i¥i£´ Ìmwg¦³d\${\textbackslash}\$¥£aª¼ªS¥³³\${\textbackslash}\$ g\{µ ª¼i®¥¢id¥a ¦dd ´\}\}\}\}\}\}\}\}\}\}\}\}\}\}\}\}\}},
	journal = {proc. OF THE IEEE},
	author = {Lecun, Yann and Bottou, Le'on and Bengio, Yoshua and Haffner, Parick},
	year = {1998},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LCHFJF9P\\Gradient-Based Learning Applied to Document Recognition.pdf:application/pdf},
}

@article{Dunlop2000,
	title = {Predictive text entry methods for mobile phones},
	volume = {4},
	issn = {0949-2054},
	url = {http://link.springer.com/article/10.1007/BF01324120},
	doi = {10.1007/BF01324120},
	abstract = {Mobile phone networks are increasingly supporting the transmission of textual messages between mobile phones and between mobile phones and other services. This paper describes the current text entry method on mobile phones and describes a new text entry method using a single key-press per letter together with a large dictionary of words for disambiguation. This approach, which is similar to technology recently licensed, independently, to several phone companies, is then extended with automatic word completion. The paper reports the results of initial user tests comparing the text entry methods, analysis of word clashes with the dictionary-based methods and keystroke level modelling of the different input methods.},
	number = {2-3},
	journal = {Personal Technologies},
	author = {Dunlop, M D and Crossan, Andrew},
	year = {2000},
	keywords = {Applications, Natural Language Processing},
	pages = {134--143},
}

@article{Steyvers,
	title = {Probabilistic {Topic} {Models}},
	author = {Steyvers, Mark and Griffiths, Tom},
}

@article{Gray2017,
	title = {{GPU} {Kernels} for {Block}-{Sparse} {Weights}},
	url = {https://github.com/openai/blocksparse},
	doi = {10.1523/JNEUROSCI.1088-12.2012.},
	abstract = {We're releasing highly optimized GPU kernels for an underexplored class of neural network architectures: networks with block-sparse weights. The kernels allow for efficient evaluation and differentiation of linear layers, including convolutional layers, with flexibly configurable block-sparsity patterns in the weight matrix. We find that depending on the sparsity, these kernels can run orders of magnitude faster than the best available alternatives such as cuBLAS. Using the kernels we improve upon the state-of-the-art in text sentiment analysis and generative modeling of text and images. By releasing our kernels in the open we aim to spur further advancement in model and algorithm design.},
	number = {1},
	journal = {Technical Report},
	author = {Gray, Scott and Radford, Alec and Kingma, Diederik P},
	year = {2017},
	pages = {1--12},
}

@article{Ustimenko2020,
	title = {{SGLB}: {Stochastic} {Gradient} {Langevin} {Boosting}},
	volume = {5},
	url = {http://arxiv.org/abs/2001.07248},
	abstract = {In this paper, we introduce Stochastic Gradient Langevin Boosting (SGLB) - a powerful and efficient machine learning framework, which may deal with a wide range of loss functions and has provable generalization guarantees. The method is based on a special form of Langevin Diffusion equation specifically designed for gradient boosting. This allows us to guarantee the global convergence, while standard gradient boosting algorithms can guarantee only local optima, which is a problem for multimodal loss functions. To illustrate the advantages of SGLB, we apply it to a classification task with 0-1 loss function, which is known to be multimodal, and to a standard Logistic regression task that is convex. The algorithm is implemented as a part of the CatBoost gradient boosting library and outperforms classic gradient boosting methods.},
	author = {Ustimenko, Aleksei and Prokhorenkova, Liudmila},
	year = {2020},
	note = {arXiv: 2001.07248},
	pages = {1--14},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3YFBK68F\\SGLB Stochastic Gradient Langevin Boosting.pdf:application/pdf},
}

@article{Hasan2012,
	title = {A {Comparative} {Study} of {Witten} {Bell} and {Kneser}-{Ney} {Smoothing} {Methods} for {Statistical} {Machine} {Translation}},
	volume = {1},
	abstract = {-Smoothing techniques is the utmost possibility estimate of probabilities to produce more precise probabilities. Smoothing is one of the most significant techniques while constructing a language model with a limited number of training data. In this paper, our main aim to analyze the performance of different smoothing techniques on n-grams. For language modeling, we considered two most widely-used smoothing algorithms: Witten-Bell smoothing and Kneser-Ney smoothing. For the evaluation we use BLEU (Bilingual Evaluation Understudy) and NIST (National Institute of Standards and Technology) scoring techniques. We have done the evaluation of these models is performed by comparing the automatically produced word alignment. We use Moses Statistical Machine Translation System for our work. Our machine translation approach has been tested on German to English and English to German task. The obtain results are considerably better than those obtained with alternative approaches to machine translation. This paper addresses several aspects of Statistical Machine Translation (SMT).},
	number = {June},
	journal = {JU Journal of Information Technology},
	author = {Hasan, A S M Mahmudul and Islam, Saria and Rahman, M Arifur},
	year = {2012},
	keywords = {Applications, Natural Language Processing},
}

@misc{TxtDropComText,
	title = {{txtDrop}.com - {Text} {Message} {Abbreviations} {List}},
	url = {http://www.txtdrop.com/abbreviations.php},
}

@article{Ley2011,
	title = {Mixtures of g-priors for {Bayesian} {Model} {Averaging} with {Economic} {Applications}},
	abstract = {We examine the issue of variable selection in linear regression modeling, where we have a potentially large amount of possible covari-ates and economic theory offers insufficient guidance on how to select the appropriate subset. In this context, Bayesian Model Averaging presents a formal Bayesian solution to dealing with model uncertainty. Our main interest here is the effect of the prior on the results, such as posterior inclusion probabilities of regressors and predictive performance. We com-bine a Binomial-Beta prior on model size with a g-prior on the coefficients of each model. In addition, we assign a hyperprior to g, as the choice of g has been found to have a large impact on the results. For the prior on g, we examine the Zellner-Siow prior and a class of Beta shrinkage priors, which covers most choices in the recent literature. We propose a benchmark Beta prior, inspired by earlier findings with fixed g, and show it leads to consistent model selection. Inference is conducted through a Markov chain Monte Carlo sampler over model space and g. We exam-ine the performance of the various priors in the context of simulated and real data. For the latter, we consider two important applications in eco-nomics, namely cross-country growth regression and returns to schooling. Recommendations to applied users are provided.},
	author = {Ley, Eduardo and Steel, Mark F J and Mark, Address and Steel, F J},
	year = {2011},
	keywords = {Algorithms, Bayesian, Model uncertainty, O47, Posterior odds, Prediction, Robustness JEL Classification System C11, Consistency},
}

@book{Bernhardt,
	title = {Reactive {Data} {Handling}},
	isbn = {978-1-61729-419-8},
	author = {Bernhardt, Manuel},
	keywords = {copyright 2016 manning publications, ctive data handling, manning author picks, selections by manuel bernhardt},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PX7KSF4W\\Reactive_Data_Handling.pdf:application/pdf},
}

@article{Covington2010b,
	title = {Cutting the {Gordian} {Knot}: {The} {Moving}-{Average} {Type}-{Token} {Ratio} ({MATTR}).},
	volume = {17},
	number = {2},
	journal = {Journal of Quantitative Linguistics},
	author = {Covington, M A and McFall, J D},
	year = {2010},
	keywords = {Applications, Natural Language Processing},
	pages = {94--100},
}

@misc{PDFTrendsProblems,
	title = {(1) ({PDF}) {Trends}, problems and solutions of recommender system},
	url = {https://www.researchgate.net/publication/307862659_Trends_problems_and_solutions_of_recommender_system},
	urldate = {2020-08-09},
	keywords = {Applications, Recommender Systems},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EAJJ5UFC\\Trends, Problems and Solutions of Recommender Systems.pdf:application/pdf},
}

@article{ThGries2008,
	title = {John {Benjamins} {Publishing} {Company} {Dispersions} and adjusted frequencies in corpora},
	volume = {134},
	issn = {1569–9811},
	doi = {10.1075/ijcl.13.4.02gri},
	abstract = {The most frequent statistics in corpus linguistics are frequencies of occurrence and frequencies of co-occurrence of two or more linguistic variables. However, such frequencies in isolation may sometimes be misleading since they do not take into consideration the degree of dispersion of the relevant linguistic vari-able. Many dispersion measures and adjusted frequency measures have been suggested but are neither widely known nor applied. Another unfortunate aspect of such measures is that many also come with a variety of problems. I pursue three objectives with this article. First, I want to raise awareness of this issue and make the available measures more widely known, so I present an overview of many measures of dispersion and adjusted frequencies. Second, I propose a conceptually simple alternative measure, DP, explain and exemplify it, and compare it to previously discussed measures. Third and most importantly, I urge corpus linguists to explore the notion of dispersion in more detail and outline a few proposals which steps to take next.},
	journal = {International Journal of Corpus Linguistics},
	author = {Th Gries, Stefan},
	year = {2008},
	keywords = {Applications, Natural Language Processing, collocations, collostructions, constructions/patterns, dispersion, frequency of co-occurrence, frequency of occurrence},
	pages = {403--437},
}

@article{Perov,
	title = {{ADVANCED} {TEXT} {PROCESSING} {WORKFLOWS} {IN} {A} {WEB}-{BASED} {TEXT} {MINING} {PLATFORM} {Matic} {Perov}²ek {Doctoral} {Dissertation} {Joºef} {Stefan} {International} {Postgraduate} {School} {Ljubljana} , {Slovenia} {Supervisor} : {Co}-{Supervisor} : {Evaluation} {Board} :},
	author = {Perov, Matic},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FGH9HSNA\\Advanced Text Processing Workflows in a Web-Based Text Mining Platform.pdf:application/pdf},
}

@article{Ding2018,
	title = {Model {Selection} {Techniques}: {An} {Overview}},
	volume = {35},
	issn = {15580792},
	doi = {10.1109/MSP.2018.2867638},
	abstract = {In the era of big data, analysts usually explore various statistical models or machine-learning methods for observed data to facilitate scientific discoveries or gain predictive power. Whatever data and fitting procedures are employed, a crucial step is to select the most appropriate model or method from a set of candidates. Model selection is a key ingredient in data analysis for reliable and reproducible statistical inference or prediction, and thus it is central to scientific studies in such fields as ecology, economics, engineering, finance, political science, biology, and epidemiology. There has been a long history of model selection techniques that arise from researches in statistics, information theory, and signal processing. A considerable number of methods has been proposed, following different philosophies and exhibiting varying performances. The purpose of this article is to provide a comprehensive overview of them, in terms of their motivation, large sample performance, and applicability. We provide integrated and practically relevant discussions on theoretical properties of state-of-the-art model selection approaches. We also share our thoughts on some controversial views on the practice of model selection.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Ding, Jie and Tarokh, Vahid and Yang, Yuhong},
	year = {2018},
	note = {arXiv: 1810.09583},
	pages = {16--34},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KGMQBTSL\\Model Selection Techniques - An Overview.pdf:application/pdf},
}

@article{WeekNonConjugate,
	title = {Week 2 {Non} {Conjugate} {Example} ( {Optional} )},
	volume = {2},
	pages = {3--5},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AWU4LBKD\\Bayesian Infernce - Week2_NonConjugateExample.pdf:application/pdf},
}

@article{Sun2020a,
	title = {A {Survey} of {Optimization} {Methods} from a {Machine} {Learning} {Perspective}},
	volume = {50},
	issn = {21682275},
	doi = {10.1109/TCYB.2019.2950779},
	abstract = {Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this article, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Finally, we explore and give some challenges and open problems for the optimization in machine learning.},
	number = {8},
	journal = {IEEE Transactions on Cybernetics},
	author = {Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing},
	year = {2020},
	pmid = {31751262},
	note = {arXiv: 1906.06821},
	keywords = {machine learning, Approximate Bayesian inference, deep neural network (DNN), optimization method, reinforcement learning (RL)},
	pages = {3668--3681},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7D4ZMD5P\\A Survey of Optimization Methods from a Machine Learning Perspective.pdf:application/pdf},
}

@misc{Von2016,
	title = {Carnegie {Mellon} {University} - {List} of {Profane} {Words}},
	url = {https://www.cs.cmu.edu/%7B~%7Dbiglou/resources/bad-words.txt},
	author = {von Ahn, Luis},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Li2018,
	title = {Visualizing the loss landscape of neural nets},
	volume = {2018-Decem},
	issn = {10495258},
	abstract = {Neural network training relies on our ability to find “good” minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple “filter normalization” method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	number = {NeurIPS 2018},
	journal = {Advances in Neural Information Processing Systems},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	year = {2018},
	note = {arXiv: 1712.09913},
	pages = {6389--6399},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QHZSG7TH\\Visualizing the Loss Landscape of Neural Nets.pdf:application/pdf},
}

@misc{Xie2014,
	title = {The printr {Package} {\textbar} {Automatically} {Print} {R} {Objects} {According} to knitr {Output} {Format}},
	url = {http://yihui.name/printr/},
	author = {Xie, Yihui},
	year = {2014},
	keywords = {Applications, Natural Language Processing},
}

@inproceedings{Powers1998,
	title = {Applications and {Explanations} of {Zipf}'s {Law}},
	isbn = {0-7258-0634-6},
	doi = {10.3115/1603899.1603924},
	abstract = {Recently I have been intrigued by the reappearance of an old friend, George Kingsley Zipf, in a number of not entirely expected places. The law named for him is ubiquitous, but Zipf did not actually discover the law so much as provide a plausible explanation. Others have proposed modifications to Zipf's Law, and closer examination uncovers systematic deviations from its normative form. We demonstrate how Zipf's analysis can be extended to include some of these phenomena.},
	booktitle = {{NeMLaP3}/{CoNLL} '98 {Proceedings} of the {Joint} {Conferences} on {New} {Methods} in {Language} {Processing} and {Computational} {Natural} {Language} {Learning}},
	author = {Powers, David M W},
	year = {1998},
	keywords = {Applications, Natural Language Processing},
	pages = {151--160},
}

@article{Bilmes2003,
	title = {Factored {Language} {Models} and {Generalized} {Parallel} {Backoff}},
	volume = {2},
	url = {http://www.aclweb.org/anthology/N03-2002},
	doi = {10.3115/1073483.1073485},
	abstract = {We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words. GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed. These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit. This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles. Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams. In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant.},
	journal = {Naacl-2003},
	author = {Bilmes, Jeff A and Kirchhoff, Katrin},
	year = {2003},
	keywords = {Applications, Natural Language Processing},
	pages = {4--6},
}

@article{Deloitte2017,
	title = {Business impacts of machine learning {Sponsored} by {Google} {Cloud}},
	url = {https://www2.deloitte.com/content/dam/Deloitte/tr/Documents/process-and-operations/TG_Google},
	author = {{Deloitte}},
	year = {2017},
	keywords = {Applications, Recommender Systems},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\57YMLCSI\\Deloitte Machine Learning report_Digital Final.pdf:},
}

@article{LanguageModelingIntroduction,
	title = {Language {Modeling} 5.1 {Introduction}},
	abstract = {A language model is simply a model of what strings (of words) are more or less likely to be generated by a speaker of English (or some other language). More specifically, it's a model that predicts what the next word will be, given all the words so far. 5.1.1 Applications Predicting the next word has some direct applications – for example, your phone tries to predict what you will type, in order to make your typing easier. Most importantly, however, language models are used in many applications whose output is English text: for example, in automatic speech recognition or machine translation, we need a language model in order to make the system prefer to output sentences that are well-formed English. We'll see how this is done in the next chapter. Finally, we can use a language model as a fancier replacement for the bag-of-words model in text classification: if we build a different language model for each class, then we can classify documents using P (k {\textbar} d) ∝ P (k)P (d {\textbar} k) where P (d {\textbar} k) is the class-specific language model. 5.1.2 Evaluation Language models are best evaluated extrinsically, that is, how much they help the application (e.g., speech recognition or machine translation) in which they are embedded. But for intrinsic evaluation, the standard evaluation metric is (per-word) perplexity: perplexity = 2 cross-entropy (5.1) cross-entropy = − 1 N log 2 likelihood (5.2) likelihood = P (w 1 ···w N) (5.3) CSE 40/60657: Natural Language Processing Fall 2016 (version of September 29, 2016) Chapter 5. Language Modeling 33 A lower perplexity is better. Perplexity should be computed on held-out data, that is, data that is different from the training data. But held-out data is always going to have unknown words (words not seen in the training data), which require some special care. For if a language model assigns zero probability to unknown words, then it will have a perplexity of infinity. But if it assigns a nonzero probability to unknown words, how can it sum to one if it doesn't know how many unknown word types there are? If we compare two language models, we should ensure that they have exactly the same vocabulary. Then, when calculating perplexity, we can either skip unknown words, or we can merge them all into a single unknown word type, usually written {\textless}unk{\textgreater}. But if two language models have different},
	keywords = {Applications, Natural Language Processing},
}

@article{Bennett2010,
	title = {An {IntroductIon} to corpus {LInguIstIcs} {Using} {Corpora} in the {Language} {Learning} {Classroom}: {Corpus} {Linguistics} for {Teachers} 2 {Using} {Corpora} in the {Language} {Learning} {Classroom}},
	url = {http://www.press.umich.edu/titleDetailDesc.do?id=371534},
	author = {Bennett, Gena R},
	year = {2010},
	keywords = {Applications, Natural Language Processing},
}

@article{Zeugner2011,
	title = {Bayesian {Model} {Averaging} with {BMS}},
	abstract = {This manual is a brief introduction to applied Bayesian Model Averaging with the R package BMS. The manual is structured as a hands-on tutorial for readers with few experience with BMA. Readers from a more technical background are advised to consult the table of contents for formal representations of the concepts used in BMS. For other tutorials and more information, please refer to http://bms.zeugner.eu.},
	author = {Zeugner, Stefan},
	year = {2011},
	keywords = {Algorithms, Bayesian},
}

@article{VanDenBosch2005,
	title = {Scalable classification-based word prediction and confusible correction},
	volume = {46},
	abstract = {We present a classification-based word prediction model based on IGTREE, a decision-tree induction algorithm with favorable scaling abilities. Through a first series of experiments we demonstrate that the system exhibits log-linear increases in prediction accu-racy and decreases in discrete perplexity, a new evaluation metric, with increasing numbers of training examples. The induced trees grow linearly with the amount of training examples. Trained on 30 million words of newswire text, prediction accuracies reach 42.2\% on the same type of text. In a second series of experiments we show that this generic approach to word pre-diction can be specialized to confusible prediction, yielding high accuracies on nine example confusible sets in all genres of text. The confusible-specific approach outperforms the generic word-prediction approach, but with more data the difference decreases. RÉSUMÉ. Cet article présente un modèle pour la tâche de prédiction de mots, considérée ici comme une tâche de classification. Ce modèle repose sur l'utilisation de IGTREE, un algo-rithme d'inférence d'arbre de décision capable de traiter à la fois un grand nombre de classes et d'exemples d'apprentissage. À travers une première série d'expérimentations nous montrons que la capacité de prédiction du modèle augmente log-linéairement avec le nombre d'exemples d'entraînement. Le même comportement est obtenu avec la perplexité discrète, une nouvelle métrique introduite pour la tâche de prédiction de mots ; la taille des arbres inférés croît, elle, linéairement. Lorsque notre modèle est entraîné sur un corpus journalistique de 30 millions de mots, le nombre de mots correctement prédits est de 42.2 \% sur des textes journalistiques. Une seconde série d'expérimentations démontre que ce prédicteur générique peut être spécialisé pour traiter des configurations dans lesquelles l'ensemble des mots à prédire se restreint à un petit ensemble. Le modèle spécialisé atteint des meilleurs résultats que le classifieur générique. KEYWORDS: word prediction, language modeling, induction of decision trees, perplexity. MOTS-CLÉS : prédiction de mots, modèles de langage, inférence d'arbres de décision, perplexité.},
	number = {2},
	author = {Van Den Bosch, Antal},
	year = {2005},
	pages = {39--63},
}

@misc{Benoit2016b,
	title = {quanteda: {Quantitative} {Analysis} of {Textual} {Data}},
	url = {https://github.com/kbenoit/quanteda},
	author = {Benoit, Kenneth and Nulty, Paul and Watanabe, Kohei and Lauderdale, Benjamin and Obeng, Adam and Barberá, Pablo and Lowe, Will},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Betrò1987,
	title = {Sequential stopping rules for the multistart algorithm in global optimisation},
	volume = {38},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/BF02592015},
	doi = {10.1007/BF02592015},
	abstract = {In this paper a sequential stopping rule is developed for the Multistart algorithm. A statistical model for the values of the observed local maxima of an objective function is introduced in the framework of Bayesian non-parametric statistics. A suitablea-priori distribution is proposed which is general enough and which leads to computationally manageable expressions for thea-posteriori distribution. Sequential stopping rules of thek-step look-ahead kind are then explicitly derived, and their numerical effectiveness compared.},
	number = {3},
	journal = {Mathematical Programming},
	author = {Betrò, Bruno and Schoen, Fabio},
	month = oct,
	year = {1987},
	pages = {271--286},
}

@article{Bentz,
	title = {Towards a computational model of grammaticalization and lexical diversity},
	abstract = {Languages use different lexical inven-tories to encode information, ranging from small sets of simplex words to large sets of morphologically complex words. Grammaticalization theories argue that this variation arises as the outcome of diachronic processes whereby co-occurring words merge to one word and build up complex morphology. To model these pro-cesses we present a) a quantitative measure of lexical diversity and b) a preliminary computational model of changes in lexical diversity over several generations of merging higly frequent collocates.},
	author = {Bentz, Christian and Buttery, Paula},
	keywords = {Applications, Natural Language Processing},
	pages = {38--42},
}

@article{Patel2020,
	title = {Stopping {Criteria} for, and {Strong} {Convergence} of, {Stochastic} {Gradient} {Descent} on {Bottou}-{Curtis}-{Nocedal} {Functions}},
	url = {http://arxiv.org/abs/2004.00475},
	abstract = {While Stochastic Gradient Descent (SGD) is a rather efficient algorithm for data-driven problems, it is an incomplete optimization algorithm as it lacks stopping criteria, which has limited its adoption in situations where such criteria are necessary. Unlike stopping criteria for deterministic methods, stopping criteria for SGD require a detailed understanding of (A) strong convergence, (B) whether the criteria will be triggered, (C) how false negatives are controlled, and (D) how false positives are controlled. In order to address these issues, we first prove strong global convergence (i.e., convergence with probability one) of SGD on a popular and general class of convex and nonconvex functions that are specified by, what we call, the Bottou-Curtis-Nocedal structure. Our proof of strong global convergence refines many techniques currently in the literature and employs new ones that are of independent interest. With strong convergence established, we then present several stopping criteria and rigorously explore whether they will be triggered in finite time and supply bounds on false negative probabilities. Ultimately, we lay a foundation for rigorously developing stopping criteria for SGD methods for a broad class of functions, in hopes of making SGD a more complete optimization algorithm with greater adoption for data-driven problems.},
	author = {Patel, Vivak},
	year = {2020},
	note = {arXiv: 2004.00475},
	keywords = {stochastic gradient descent, nonconvex, stopping criteria},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D3WTEKU3\\Stopping Criteria for Stochastic Gradient Descent.pdf:application/pdf},
}

@article{dunning-kruger,
	title = {Unskilled and {Unaware} of {It}: {How} {Difficulties} in {Recognizing} {One}'s {Own} {Incompetence} {Lead} to {Inflated} {Self}-{Assessments}},
	volume = {77},
	doi = {10.1037//0022-3514.77.6.1121},
	journal = {Journal of Personality and Social Psychology},
	author = {Kruger, Justin and Dunning, David},
	year = {2000},
	pages = {1121--1134},
}

@article{CS497LearningNLP2005,
	title = {{CS497}:{Learning} and {NLP} {Lec} 2: {Natural} {Language} and {Statistics} {Fall} 2005},
	volume = {8},
	abstract = {In this lecture we examine some of the philosophical themes and leading ideas that motivate statistical approaches to linguistics and natural language and begin exploring what can be learned by looking at statistics of texts. We will use some terminology that will be introduced later. 1 The study of natural Language The study of language is concerned with two basic questions: • What kinds of things do people say? • What do these things say/ ask/ request about the world? The first point covers aspects of the structure of language. The second pertains to semantics, pragmatics and discourse – how to connect utterances to the world. Most of corpus linguistics is about the first point. But, patterns of use can also imply deep under-standing, and therefore corpus based techniques may also be used to address the second question. In some sense, if one wants to make significant progress in NLP, one should hope that the fact that something can be said, statistically, about the first point, would facilitate progress on the second. Even strongly, these statistical regularities could be the only reason that natural language is such a powerful communication channel. Wittgenstein: The meaning of the word is defined by the circumstances of its use. However, most of the statistical discoveries are done in the context of the first question, so we will discuss it in the rest of this lecture. Traditional Linguistics view of language is that " People produce grammatical sentences " . As a conse-quence of this basic view the theory developed cares about whether the sentence is structurally well formed and less about whether this is the kind of thing people say, or whether this is semantically strange. This document (as well as the lecture...) exemplifies that this distinction is not sufficient. People can use ill-formed language and be perfectly clear and vice versa. In many cases you can hear non native speakers of a language use sentences that are grammatically correct but are not what people typically say. (E.g. " Open the Radio " .)},
	journal = {Septemeber},
	year = {2005},
	keywords = {Applications, Natural Language Processing},
}

@techreport{Buttorff2017,
	title = {Multiple {Chronic} {Conditions} in the {United} {States}},
	abstract = {This chartbook updates previous versions with more recent data on the prevalence of multiple chronic conditions (2008–2014) and associated health care utilization and spending. It also analyzes functional and other limitations for those with multiple chronic conditions. In 2014, 60 percent of Americans had at least one chronic condition, and 42 percent had multiple chronic conditions. These proportions have held steady since 2008. Americans with chronic conditions utilize more — and spend more on — health care services and may have reduced physical and social functioning.},
	author = {Buttorff, Christine and Ruder, Teague and Bauman, Melissa},
	year = {2017},
	doi = {10.7249/TL221},
	note = {ISBN: 9780833097378},
}

@article{Grosse,
	title = {Lecture 8 : {Optimization}},
	author = {Grosse, Roger},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U98E7XQG\\Lecture 8- Optimization.pdf:application/pdf},
}

@article{Rosenberg2018,
	title = {Stochastic {Gradient} {Descent} {Review} : {Statistical} {Learning} {Theory} {Framework}},
	author = {Rosenberg, David S},
	year = {2018},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Q43PUBVH\\Stochastic Gradient Descent (NYU).pdf:application/pdf},
}

@article{Stolcke2002,
	title = {Srilm — an {Extensible} {Language} {Modeling} {Toolkit}},
	volume = {2},
	url = {http://www.speech.sri.com/},
	doi = {10.1.1.157.2429},
	abstract = {SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools.},
	number = {Denver, Colorado},
	journal = {Interspeech},
	author = {Stolcke, Andreas},
	year = {2002},
	keywords = {Applications, Natural Language Processing},
	pages = {901--904},
}

@misc{Johnson2016,
	title = {The combination of human and artificial intelligence will define humanity's future {\textbar} {TechCrunch}},
	url = {https://techcrunch.com/2016/10/12/the-combination-of-human-and-artificial-intelligence-will-define-humanitys-future/},
	urldate = {2016-12-31},
	author = {Johnson, Bryan},
	year = {2016},
	note = {Publication Title: TechCrunch},
	keywords = {Applications, Natural Language Processing},
}

@article{Seymore,
	title = {{SCALABLE} {BACKOFF} {LANGUAGE} {MODELS}},
	abstract = {When a trigram backoff language model is created from a large body of text, trigrams and bigrams that occur few times in the training text are often excluded from the model in order to de-crease the model size. Generally, the elimination of n-grams with very low counts is believed to not significantly affect model perfor-mance. This project investigates the degradation of a trigram back-off model's perplexity and word error rates as bigram and trigram cutoffs are increased. The advantage of reduction in model size is compared to the increase in word error rate and perplexity scores. More importantly, this project also investigates alternative ways of excluding bigrams and trigrams from a backoff language model, us-ing criteria other than the number of times an n-gram occurs in the training text. Specifically, a difference method has been investi-gated where the difference in the logs of the original and backed off trigram and bigram probabilities is used as a basis for n-gram exclusion from the model. We show that excluding trigrams and bi-grams based on a weighted version of this difference method results in better perplexity and word error rate performance than excluding trigrams and bigrams based on counts alone.},
	author = {Seymore, Kristie and Rosenfeld, Ronald},
	keywords = {Applications, Natural Language Processing},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AQ9CE56Q\\SCALABLE BACKOFF LANGUAGE MODELS.pdf:application/pdf},
}

@book{Boyd2018,
	title = {Introduction to {Applied} {Linear} {Algebra}},
	isbn = {978-1-316-51896-0},
	abstract = {This groundbreaking textbook combines straightforward explanations with a wealth of practical examples to offer an innovative approach to teaching linear algebra. Requiring no prior knowledge of the subject, it covers the aspects of linear algebra - vectors, matrices, and least squares - that are needed for engineering applications, discussing examples across data science, machine learning and artificial intelligence, signal and image processing, tomography, navigation, control, and finance. The numerous practical exercises throughout allow students to test their understanding and translate their knowledge into solving real-world problems, with lecture slides, additional computational exercises in Julia and MATLAB, and data sets accompanying the book online. It is suitable for both one-semester and one-quarter courses, as well as self-study, this self-contained text provides beginning students with the foundation they need to progress to more advanced study.},
	author = {Boyd, Stephen and Vandenberghe, Lieven},
	year = {2018},
	doi = {10.1017/9781108583664},
	note = {Publication Title: Introduction to Applied Linear Algebra},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FBXYIESG\\Introduction to Applied Linear Algebra.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5YU8KEDE\\Introduction to Applied Linear Algebra.pdf:application/pdf},
}

@misc{FBI2018,
	title = {Internet {Crime} {Complaint} {Center} ({IC3}) {\textbar} {Business} {E}-mail {Compromise} {The} 12 {Billion} {Dollar} {Scam}.},
	url = {https://www.ic3.gov/media/2018/180712.aspx},
	urldate = {2020-05-10},
	author = {{FBI}},
	year = {2018},
	note = {Publication Title: https://www.ic3.gov/media/2018/180712.aspx},
}

@article{Gale,
	title = {Good-{Turing} {Smoothing} {Without} {Tears}},
	abstract = {The performance of statistically based techniques for many tasks such as spelling correction, sense disambiguation, and translation is improved if one can estimate a probability for an object of interest which has not been seen before. Good-Turing methods are one means of estimating these probabilities for previously unseen objects. However, the use of Good-Turing methods requires a smoothing step which must smooth in regions of vastly different accuracy. Such smoothers are difficult to use, and may have hindered the use of Good-Turing methods in computational linguistics. This paper presents a method which uses the simplest possible smooth, a straight line, together with a rule for switching from Turing estimates which are more accurate at low frequencies. We call this method the Simple Good-Turing (SGT) method. Two examples, one from prosody, the other from morphology, are used to illustrate the SGT.},
	author = {Gale, William A},
	keywords = {Applications, Natural Language Processing},
}

@article{Zhang2017b,
	title = {An up-to-date comparison of state-of-the-art classification algorithms},
	volume = {82},
	issn = {09574174},
	url = {http://dx.doi.org/10.1016/j.eswa.2017.04.003},
	doi = {10.1016/j.eswa.2017.04.003},
	abstract = {Current benchmark reports of classification algorithms generally concern common classifiers and their variants but do not include many algorithms that have been introduced in recent years. Moreover, important properties such as the dependency on number of classes and features and CPU running time are typically not examined. In this paper, we carry out a comparative empirical study on both established classifiers and more recently proposed ones on 71 data sets originating from different domains, publicly available at UCI and KEEL repositories. The list of 11 algorithms studied includes Extreme Learning Machine (ELM), Sparse Representation based Classification (SRC), and Deep Learning (DL), which have not been thoroughly investigated in existing comparative studies. It is found that Stochastic Gradient Boosting Trees (GBDT) matches or exceeds the prediction performance of Support Vector Machines (SVM) and Random Forests (RF), while being the fastest algorithm in terms of prediction efficiency. ELM also yields good accuracy results, ranking in the top-5, alongside GBDT, RF, SVM, and C4.5 but this performance varies widely across all data sets. Unsurprisingly, top accuracy performers have average or slow training time efficiency. DL is the worst performer in terms of accuracy but second fastest in prediction efficiency. SRC shows good accuracy performance but it is the slowest classifier in both training and testing.},
	journal = {Expert Systems with Applications},
	author = {Zhang, Chongsheng and Liu, Changchang and Zhang, Xiangliang and Almpanidis, George},
	year = {2017},
	note = {Publisher: Elsevier Ltd},
	keywords = {Classification benchmarking, Classifier comparison, Classifier evaluation, psl},
	pages = {128--150},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\32S6EZI6\\An up-to-date comparison of state-of-the-art classification algorithms.pdf:application/pdf},
}

@article{Thesis2012,
	title = {Bptx\_2011\_1\_\_0\_283621\_0\_117041},
	author = {Thesis, Bachelor},
	year = {2012},
}

@article{Begleiter2004,
	title = {On prediction using variable order {Markov} models},
	volume = {22},
	issn = {10769757},
	doi = {10.1613/jair.1491},
	abstract = {This paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models. The class of such algorithms is large and in principle includes any lossless compression algorithm. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real life sequences from three domains: proteins, English text and music pieces. The comparison is made with respect to prediction quality as measured by the average log-loss. We also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks. Our results indicate that a “decomposed ” CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in sequence prediction tasks. Somewhat surprisingly, a different algorithm, which is a modification of the Lempel-Ziv compression algorithm, significantly outperforms all algorithms on the protein classification problems. 1.},
	journal = {Journal of Artificial Intelligence Research},
	author = {Begleiter, Ron and El-Yaniv, Ran and Yona, Golan},
	year = {2004},
	keywords = {Applications, Natural Language Processing},
	pages = {385--421},
}

@misc{Beal2016,
	title = {Huge {List} of {Text} {Message} \& {Chat} {Abbreviations} - {Webopedia}},
	url = {http://www.webopedia.com/quick_ref/textmessageabbreviations.asp},
	author = {Beal, Vangie},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Suster,
	title = {An investigation into language complexity of {World}-of-{Warcraft} game-external texts},
	abstract = {We present a language complexity analysis of World of Warcraft (WoW) community texts, which we compare to texts from a general corpus of web English. Results from several complexity types are presented, including lexical diversity, density, readability and syntactic complexity. The language of WoW texts is found to be comparable to the general corpus on some complexity measures, yet more specialized on other measures. Our findings can be used by educators willing to include game-related activities into school curricula.},
	author = {Suster, Simoň},
	keywords = {Applications, Natural Language Processing},
}

@book{stewart12,
	address = {Belmont, Cal.},
	title = {Calculus : early transcendentals},
	isbn = {0-538-49790-4 978-0-538-49790-9 0-8400-5885-3 978-0-8400-5885-0 0-538-49871-4 978-0-538-49871-5 0-538-49887-0 978-0-538-49887-6 0-8400-4825-4 978-0-8400-4825-7},
	publisher = {Brooks/Cole, Cengage Learning},
	author = {Stewart, James},
	year = {2012},
	keywords = {Calculus textbook},
}

@article{Savoy2015,
	title = {Word {Distributions} and {Zipf}'s {Law}},
	journal = {Word Distributions and Zipf's Law},
	author = {Savoy, J},
	year = {2015},
	pages = {1--38},
}

@article{Garay-Vitoria1997,
	title = {Intelligent word-prediction to enhance text input rate (a syntactic analysis-based word-prediction aid for people with severe motor and speech disability)},
	url = {http://portal.acm.org/citation.cfm?doid=238218.238333},
	doi = {10.1145/238218.238333},
	number = {December},
	journal = {Proceedings of the 2nd international conference on Intelligent user interfaces - IUI '97},
	author = {Garay-Vitoria, Nestor and González-Abascal, Julio},
	year = {1997},
	note = {ISBN: 0897918398},
	keywords = {Applications, Natural Language Processing},
	pages = {241--244},
}

@article{Greenberg2015,
	title = {The {Economic} {Burden} of {Adults} {With} {Major} {Depressive} {Disorder} in the {United} {States} (2005 and 2010)},
	issn = {0160-6689},
	url = {http://www.psychiatrist.com/jcp/article/pages/2015/v76n02/v76n0204.aspx},
	doi = {10.4088/JCP.14m09298},
	abstract = {BACKGROUND: The economic burden of depression in the United States--including major depressive disorder (MDD), bipolar disorder, and dysthymia--was estimated at \$83.1 billion in 2000. We update these findings using recent data, focusing on MDD alone and accounting for comorbid physical and psychiatric disorders.{\textbackslash}backslashn{\textbackslash}backslashnMETHOD: Using national survey (DSM-IV criteria) and administrative claims data (ICD-9 codes), we estimate the incremental economic burden of individuals with MDD as well as the share of these costs attributable to MDD, with attention to any changes that occurred between 2005 and 2010.{\textbackslash}backslashn{\textbackslash}backslashnRESULTS: The incremental economic burden of individuals with MDD increased by 21.5\% (from \$173.2 billion to \$210.5 billion, inflation-adjusted dollars). The composition of these costs remained stable, with approximately 45\% attributable to direct costs, 5\% to suicide-related costs, and 50\% to workplace costs. Only 38\% of the total costs were due to MDD itself as opposed to comorbid conditions.{\textbackslash}backslashn{\textbackslash}backslashnCONCLUSIONS: Comorbid conditions account for the largest portion of the growing economic burden of MDD. Future research should analyze further these comorbidities as well as the relative importance of factors contributing to that growing burden. These include population growth, increase in MDD prevalence, increase in treatment cost per individual with MDD, changes in employment and treatment rates, as well as changes in the composition and quality of MDD treatment services.},
	journal = {The Journal of Clinical Psychiatry},
	author = {Greenberg, Paul E and Fournier, Andree-Anne and Sisitsky, Tammy and Pike, Crystal T and Kessler, Ronald C},
	month = feb,
	year = {2015},
	pmid = {25742202},
	note = {ISBN: 0160-6689},
	pages = {155--162},
}

@article{Principle2014,
	title = {{\textless}{Pyramid}-principle\_consulting-methodology.pdf{\textgreater}},
	author = {Principle, The Pyramid and Minto, Barbara and Consultant, Mckinsey and Methodology, Consulting and Principle, Minto Pyramid and Consulting, Issue Based and Consulting, Issue Based and Principle, Pyramid and Principle, The Pyramid},
	year = {2014},
	pages = {1--5},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9K8Z79EW\\The_Pyramid_Principle.pdf:},
}

@techreport{Andra2019,
	title = {300+ {Terrifying} {Cybercrime} \& {Cybersecurity} {Statistics}},
	url = {https://www.comparitech.com/vpn/cybersecurity-cyber-crime-statistics-facts-trends/},
	author = {Andra, Zaharia},
	year = {2019},
	note = {Publication Title: Comparitech.com
Volume: 2021},
	pages = {1--63},
}

@article{Brin1998,
	title = {The anatomy of a large scale hypertextual {Web} search engine},
	volume = {30},
	issn = {01697552},
	doi = {10.1.1.109.4049},
	abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from 3 years ago. This paper provides an in-depth description of our large-scale web search engine - the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections, where anyone can publish anything they want. ?? 2012 Elsevier B.V. All rights reserved.},
	number = {1/7},
	journal = {Computer Networks and ISDN Systems},
	author = {Brin, S and Page, L},
	year = {1998},
	pmid = {726238533241861100},
	note = {arXiv: 1111.6189v1
ISBN: 0169-7552},
	keywords = {world wide web, information retrieval, pagerank, search engines},
	pages = {107--117},
}

@misc{ModernregressionmethodswileyseriesinprobabilPdf,
	title = {modern-regression-methods-wiley-series-in-probabil.pdf},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UDVUJRNG\\modern-regression-methods-wiley-series-in-probabil.pdf:application/pdf},
}

@article{Ratliff2014,
	title = {Optimization {I} : {Theory} and {Analytical} {Methods} {Optimization} in {Intelligent} {Systems}},
	author = {Ratliff, Nathan},
	year = {2014},
	pages = {1--20},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YHSWATMD\\mathematics_for_intelligent_systems_lecture7_notes.pdf:application/pdf},
}

@misc{Wolpert2012,
	title = {What the {No} {Free} {Lunch} {Theorems} {Really} {Mean}; {How} to {Improve} {Search} {Algorithms}},
	author = {Wolpert, David H and Wolpert, David H},
	year = {2012},
}

@article{Tukey1977,
	title = {Exploratory {Data} {Analysis} ({Book} {Section})},
	issn = {1557170X},
	url = {http://www.springerlink.com/index/10.1007/978-1-4419-7976-6},
	doi = {10.1007/978-1-4419-7976-6},
	abstract = {Scratching down numbers (stem-and-leaf); Schematic summaries (pictures and numbers); Easy re-expression; Effective comparison (including well-chosen expresion); Plots of relationship; Straightening out plots (using three points); Smoothing sequences; Optional sections for chapter 7; Parallel and wandering schematic plots; Delineations of batches of points; Using two-way analyses; Making two-way analyses; Advances fits; Three-way fits; Looking in two or more ways at batches of points; Counted fractions; Better smoothing; Counts in bin after bin; Product-ratio plots; Shapes of distribution; Mathematical distributions; Postscript.},
	journal = {Exploratory Data Analysis},
	author = {Tukey, J W},
	year = {1977},
	pmid = {21097328},
	note = {ISBN: 0201076160},
	pages = {61--100},
}

@article{Eicher2011,
	title = {{JOURNAL} {OF} {APPLIED} {ECONOMETRICS} {DEFAULT} {PRIORS} {AND} {PREDICTIVE} {PERFORMANCE} {IN} {BAYESIAN} {MODEL} {AVERAGING}, {WITH} {APPLICATION} {TO} {GROWTH} {DETERMINANTS}},
	volume = {26},
	doi = {10.1002/jae.1112},
	abstract = {SUMMARY Bayesian model averaging (BMA) has become widely accepted as a way of accounting for model uncertainty, notably in regression models for identifying the determinants of economic growth. To implement BMA the user must specify a prior distribution in two parts: a prior for the regression parameters and a prior over the model space. Here we address the issue of which default prior to use for BMA in linear regression. We compare 12 candidate parameter priors: the unit information prior (UIP) corresponding to the BIC or Schwarz approximation to the integrated likelihood, a proper data-dependent prior, and 10 priors considered by Fernández et al. (Journal of Econometrics 2001; 100: 381–427). We also compare two model priors: the uniform model prior and a prior with prior expected model size 7. We compare them on the basis of cross-validated predictive performance on a well-known growth dataset and on two simulated examples from the literature. We found that the UIP with uniform model prior generally outperformed the other priors considered. It also identified the largest set of growth determinants.},
	journal = {J. Appl. Econ},
	author = {Eicher, Theo S and Papageorgiou, Chris and Raftery, Adrian E},
	year = {2011},
	keywords = {Algorithms, Bayesian},
	pages = {30--55},
}

@article{Staniak2019,
	title = {The landscape of {R} packages for automated exploratory data analysis},
	volume = {11},
	issn = {20734859},
	doi = {10.32614/RJ-2019-033},
	abstract = {The increasing availability of large but noisy data sets with a large number of heterogeneous variables leads to the increasing interest in the automation of common tasks for data analysis. The most time-consuming part of this process is the Exploratory Data Analysis, crucial for better domain understanding, data cleaning, data validation, and feature engineering. There is a growing number of libraries that attempt to automate some of the typical Exploratory Data Analysis tasks to make the search for new insights easier and faster. In this paper, we present a systematic review of existing tools for Automated Exploratory Data Analysis (autoEDA). We explore the features of fifteen popular R packages to identify the parts of analysis that can be effectively automated with the current tools and to point out new directions for further autoEDA development.},
	number = {2},
	journal = {R Journal},
	author = {Staniak, Mateusz and Biecek, Przemysław},
	year = {2019},
	note = {arXiv: 1904.02101},
	pages = {347--369},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QHKKQPNM\\The Landscape of R Packages for Automated Exploratory Data Analysis.pdf:application/pdf},
}

@techreport{PonemonInstitute2019,
	title = {2019 {Cost} of a {Data} {Breach} {Report} {\textbar} {IBM} {Security}},
	url = {https://databreachcalculator.mybluemix.net/?_ga=2.214403223.188415805.1589149233-80792300.1579803330&cm_mc_uid=73278236154515798033293&cm_mc_sid_50200000=24053711589149232727&cm_mc_sid_52640000=71590571589149232767},
	abstract = {IBM Security and Ponemon Institute are pleased to release the 2019 Cost of a Data Breach Report1. Based on in-depth interviews with more than 500 companies around the world who have experienced a data breach between July 2018 and April 2019, the analysis in this research study takes into account hundreds of cost factors, from legal, regulatory and technical activities, to loss of brand equity, customer turnover, and the drain on employee productivity. Now in the 14th year of the Cost of a Data Breach Report, we included historical data showing trends for a range of metrics over a period of several years. The research continues to evolve, with consideration for the changing nature of information technology, data regulation, and security tools and processes. Above all, this report shows IT professionals, business leaders, researchers and the broader security community that, although the consequences of data breaches are severe, there are concrete ways organizations can mitigate costs and improve overall security posture},
	author = {{Ponemon Institute}},
	year = {2019},
	pages = {76},
}

@article{Alom2019,
	title = {A state-of-the-art survey on deep learning theory and architectures},
	volume = {8},
	issn = {20799292},
	doi = {10.3390/electronics8030292},
	abstract = {In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bioinformatics, natural language processing, cybersecurity, and many others. This survey presents a brief survey on the advances that have occurred in the area of Deep Learning (DL), starting with the Deep Neural Network (DNN). The survey goes on to cover Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we have discussed recent developments, such as advanced variant DL techniques based on these DL approaches. This work considers most of the papers published after 2012 from when the history of deep learning began. Furthermore, DL approaches that have been explored and evaluated in different application domains are also included in this survey. We also included recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys that have been published on DL using neural networks and a survey on Reinforcement Learning (RL). However, those papers have not discussed individual advanced techniques for training large-scale deep learning models and the recently developed method of generative models.},
	number = {3},
	journal = {Electronics (Switzerland)},
	author = {Alom, Md Zahangir and Taha, Tarek M and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Hasan, Mahmudul and Van Essen, Brian C and Awwal, Abdul A S and Asari, Vijayan K},
	year = {2019},
	keywords = {Deep learning, Auto-encoder (AE), Convolutional neural network (CNN), Deep belief network (DBN), Deep reinforcement learning (DRL), Generative adversarial network (GAN), Recurrent neural network (RNN), Restricted Boltzmann machine (RBM), Transfer learning},
	pages = {1--67},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7FVF4IEX\\State of the Art Survey of Deep Learning Theory.pdf:application/pdf},
}

@article{Jozefowicz,
	title = {Exploring the {Limits} of {Language} {Modeling}},
	abstract = {In this work we explore recent advances in Re-current Neural Networks for large scale Lan-guage Modeling, a task central to language un-derstanding. We extend current models to deal with two key challenges present in this task: cor-pora and vocabulary sizes, and complex, long term structure of language. We perform an ex-haustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Bench-mark. Our best single model significantly im-proves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of param-eters by a factor of 20), while an ensemble of models sets a new record by improving perplex-ity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.},
	author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
	keywords = {Applications, Natural Language Processing},
}

@article{Suomeksi2005,
	title = {{PERSPECTIVES} {ON} {THE} {UTILITY} {OF} {LINGUISTIC} {KNOWLEDGE} {IN} {ENGLISH} {WORD} {PREDICTION}},
	author = {Suomeksi, Tiivistelmä and Tti, Per and Ynen, Väyr},
	year = {2005},
	keywords = {Applications, Natural Language Processing, word prediction, word completion, computational linguistics, englannin kieli, ennakointi, kieliteknologia, language technology, leksikologia, linguistic knowledge, sananennakointi, tietokonelingvistiikka},
}

@misc{Davies2015,
	title = {Corpus of {Contemporary} {American} {English} ({COCA})},
	author = {Davies, Mark},
	year = {2015},
}

@article{Langsari2018,
	title = {Measuring {Performance} {Efficiency} of {Application} applying {Design} {Patterns} and {Refactoring} {Method}},
	volume = {4},
	issn = {2354-6026},
	doi = {10.12962/j23546026.y2018i1.3527},
	abstract = {Design patterns are always useful concept using in designing and developing a software application. Performance play essential role in the quality attribute of an enterprise application. It is useful to measure and examine how design patterns influence and affect the performance of an application. In this study, we investigate the impact of selected design pattern through refactoring processes for performance efficiency. The systematic study phases included; analyzing, refactoring and performance measuring with implemented in case study SIA system. The performance measuring measure with different test cases and round for the results comparison of each differences test cases and round for design pattern indicate an influence on the performance of an application.},
	number = {1},
	journal = {IPTEK Journal of Proceedings Series},
	author = {Langsari, Kholed and Rochimah, Siti and Akbar, Rizky Januar},
	year = {2018},
	keywords = {design},
	pages = {149},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WTBT24FL\\Measuring_Performance_Efficiency_of_Application_ap.pdf:application/pdf},
}

@article{Shiffrin2008,
	title = {A survey of model evaluation approaches with a tutorial on hierarchical bayesian methods},
	volume = {32},
	issn = {03640213},
	doi = {10.1080/03640210802414826},
	abstract = {This article reviews current methods for evaluating models in the cognitive sciences, including theoretically based approaches, such as Bayes factors and minimum description length measures; simulation approaches, including model mimicry evaluations; and practical approaches, such as validation and generalization measures. This article argues that, although often useful in specific settings, most of these approaches are limited in their ability to give a general assessment of models. This article argues that hierarchical methods, generally, and hierarchical Bayesian methods, specifically, can provide a more thorough evaluation of models in the cognitive sciences. This article presents two worked examples of hierarchical Bayesian analyses to demonstrate how the approach addresses key questions of descriptive adequacy, parameter interference, prediction, and generalization in principled and coherent ways.},
	number = {8},
	journal = {Cognitive Science},
	author = {Shiffrin, Richard M and Lee, Michael D and Kim, Woojae and Wagenmakers, Eric Jan},
	year = {2008},
	keywords = {Model selection, Bayesian model selection, Hierarchical Bayesian modeling, Minimum description length, Model evaluation, Model mimicry, Prequential analysis},
	pages = {1248--1284},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D62NY6BX\\A Survey of Model Evaluation Approaches.pdf:application/pdf},
}

@article{REFACTORINGSOFTWAREDEVELOPMENT,
	title = {{REFACTORING}, {SOFTWARE} {DEVELOPMENT} {AND} {DESIGN} {PATTERNS}},
	keywords = {Object Oriented Programming, Software Engineering},
}

@misc{InsideAirbnb2016,
	title = {Inside {Airbnb}: {Adding} data to the debate.},
	url = {http://insideairbnb.com/},
	abstract = {Inside Airbnb is a set of independent tools and open data that allows you to explore how Airbnb is REALLY being used in cities around the world.},
	urldate = {2020-03-14},
	author = {{Inside Airbnb}},
	year = {2016},
	note = {Publication Title: InsideAirbnb},
}

@misc{wiki:vectornote,
	title = {Vector notation --- {Wikipedia} {The} {Free} {Encyclopedia}},
	url = {https://en.wikipedia.org/w/index.php?title=Vector_notation&oldid=954856065},
	author = {contributors, Wikipedia},
	year = {2020},
}

@article{Fan2012,
	title = {Term {Paper}: {How} and {Why} to {Use} {Stochastic} {Gradient} {Descent}?},
	abstract = {Stochastic gradient descent is a simple but efficient numerical optimization method. It has been widely used in solving large-scale machine learning problems. This paper first shows how to implement stochastic gradient descent, particularly for ridge regression and regularized logistic regression. Then the pros and cons of the method are demonstrated through two simulated datasets. The comparison of stochastic gradient descent with a state-of-the-art method L-BFGS is also done.},
	author = {Fan, Minjie},
	year = {2012},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\92HBL4YQ\\How and Why to Use Stochastic Gradient Descent.pdf:},
}

@article{Dulek2013,
	title = {Properties of the {Hypothesis} {Space} and their {Effect} on {Machine} {Learning}},
	abstract = {The best method to use for machine learning depends on the problem. This thesis considers one aspect of machine learning problems: How do the properties of the hypothesis space affect machine learning? It collects academic advances on how dimensionality and representational capacity of the space and the presence of local optima affect machine learning. Useful additions to generic machine learning methods are listed that deal with these properties. The result is a collective overview on how to design a machine learning process that uses these properties of the hypothesis space.},
	author = {Dulek, Ruben},
	year = {2013},
	pages = {1--23},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\X2A942LU\\dulek_properties_of__the_hypothesis_space_and_their_effect_on_machine_learning.pdf:application/pdf},
}

@article{Economy,
	title = {On {Demand} {Economy} {Business} {Model} 101},
	author = {Economy, On Demand and Model, Business},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9QN566K4\\On-Demand-Economy-Business-Model.pdf:application/pdf},
}

@misc{PennTreebankTags,
	title = {Penn {Treebank} {P}.{O}.{S}. {Tags}},
	url = {https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html},
}

@article{Hsu2008,
	title = {Iterative language model estimation: efficient data structure \& algorithms},
	volume = {8},
	issn = {19909772},
	abstract = {Despite the availability of better performing techniques, most language models are trained using popular toolkits that do not support perplexity optimization. In this work, we present an efficient data structure and optimized algorithms specifically designed for iterative parameter tuning. With the resulting implementation, we demonstrate the feasibility and effectiveness of such iterative techniques in language model estimation. Index Terms: language modeling, smoothing, interpolation 1.},
	journal = {Proceedings of Interspeech},
	author = {Hsu, Bo-June and Glass, James},
	year = {2008},
	keywords = {Applications, Natural Language Processing, smoothing, language modeling, interpolation, Index Terms},
	pages = {1--4},
}

@misc{ImplementationModifiedKneserNey,
	title = {Implementation of {Modified} {Kneser}-{Ney} {S}...pdf},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N2XZ6NMC\\Implementation of Modified Kneser-Ney S...pdf:application/pdf},
}

@book{Bertsekas/99,
	title = {Nonlinear {Programming}},
	publisher = {Athena Scientific},
	author = {Bertsekas, D P},
	year = {1999},
	keywords = {imported},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I3H8WK2P\\nonlinear-programming.pdf:application/pdf},
}

@article{Gandrud2018,
	title = {Reproducible {Research} with {R} and {RStudio}},
	doi = {10.1201/9781315382548},
	abstract = {Bringing together computational research tools in one accessible source, Reproducible Research with R and RStudio guides you in creating dynamic and highly reproducible research. Suitable for researchers in any quantitative empirical discipline, it presents practical tools for data collection, data analysis, and the presentation of results. With straightforward examples, the book takes you through a reproducible research workflow, showing you how to use: R for dynamic data gathering and automated results presentation knitr for combining statistical analysis and results into one document LaTeX for creating PDF articles and slide shows, and Markdown and HTML for presenting results on the web Cloud storage and versioning services that can store data, code, and presentation files; save previous versions of the files; and make the information widely available Unix-like shell programs for compiling large projects and converting documents from one markup language to another RStudio to tightly integrate reproducible research tools in one place Whether you're an advanced user or just getting started with tools such as R and LaTeX, this book saves you time searching for information and helps you successfully carry out computational research. It provides a practical reproducible research workflow that you can use to gather and analyze data as well as dynamically present results in print and on the web. Supplementary files used for the examples and a reproducible research project are available on the author's website.},
	journal = {Reproducible Research with R and RStudio},
	author = {Gandrud, Christopher},
	year = {2018},
}

@article{Examples,
	title = {How to {Design} {Best}-in-{Class} {Dashboards} :},
	author = {Examples, Four Must-see},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LBLA7IYR\\eb-how-to-design-best-in-class-dashboards-en.pdf:application/pdf},
}

@article{Halper2017,
	title = {Advanced {Analytics}: {Moving} {Toward} {AI}, {Machine} {Learning}, and {Natural} {Language} {Processing} {BEST} {PRACTICES} {REPORT}},
	author = {Halper, Fern and 2017, Q3},
	year = {2017},
	keywords = {machine learning, natural language processing, advanced analytics, artificial intelligence, Fern Halper, SAS, TDWI, ThoughtSpot, Vertica},
}

@article{Roughgarden2016,
	title = {{CS168} : {The} {Modern} {Algorithmic} {Toolbox} {Lecture} \# 6 : {Stochastic} {Gradient} {Descent} and {Regularization}},
	author = {Roughgarden, Tim and Valiant, Gregory},
	year = {2016},
	pages = {1--12},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TCGEGENK\\CS168  The Modern Algorithmic Toolbox - Gradient Descent.pdf:application/pdf},
}

@book{R.H.Baayen2008,
	title = {Analyzing {Linguistic} {Data}: {A} {Practical} {Introduction} to {Statistics} using {R}},
	volume = {2},
	isbn = {978-0-521-88259-0},
	abstract = {Statistical analysis is a useful skill for linguists and psycholinguists, allowing them to understand the quantitative structure of their data. This textbook provides a straightforward introduction to the statistical analysis of language. Designed for linguists with a non-mathematical background, it clearly introduces the basic principles and methods of statistical analysis, using 'R', the leading computational statistics programme. The reader is guided step-by-step through a range of real data sets, allowing them to analyse acoustic data, construct grammatical trees for a variety of languages, quantify register variation in corpus linguistics, and measure experimental data using state-of-the-art models. The visualization of data plays a key role, both in the initial stages of data exploration and later on when the reader is encouraged to criticize various models. Containing over 40 exercises with model answers, this book will be welcomed by all linguists wishing to learn more about working with and presenting quantitative data.},
	publisher = {Cambridge University Press},
	author = {Baayen, R H and {R. H. Baayen}},
	year = {2008},
	pmid = {25246403},
	doi = {10.1558/sols.v2i3.471},
	note = {arXiv: 1011.1669v3
Issue: 3
ISSN: 17508657},
	keywords = {Applications, Natural Language Processing, R, statistics, linguistics, psycholinguistics},
}

@incollection{Baayen2001,
	address = {Dordrecht},
	title = {Examples of {Applications}},
	isbn = {978-94-010-0844-0},
	url = {http://dx.doi.org/10.1007/978-94-010-0844-0_6},
	booktitle = {Word {Frequency} {Distributions}},
	publisher = {Springer Netherlands},
	author = {Baayen, R Harald},
	year = {2001},
	doi = {10.1007/978-94-010-0844-0_6},
	keywords = {Applications, Natural Language Processing},
	pages = {195--236},
}

@article{STATISTICALLANGUAGEMODELS,
	title = {{STATISTICAL} {LANGUAGE} {MODELS} {BASED} {ON} {NEURAL} {NETWORKS}},
	keywords = {Applications, Natural Language Processing},
}

@book{dalton20202,
	title = {The 2-{Hour} {Job} {Search}, {Second} {Edition}: {Using} {Technology} to {Get} the {Right} {Job} {Faster}},
	isbn = {978-1-984857-29-3},
	url = {https://books.google.com/books?id=V4GlDwAAQBAJ},
	publisher = {Potter/Ten Speed/Harmony/Rodale},
	author = {Dalton, S},
	year = {2020},
}

@misc{ScienceDirectArticles11Aug2020,
	title = {{ScienceDirect}\_articles\_11Aug2020\_09-51-47},
}

@article{Blockeel2012,
	title = {Hypothesis {Space}},
	doi = {10.1007/springerreference_179085},
	journal = {SpringerReference},
	author = {Blockeel, Hendrik},
	year = {2012},
	pages = {1--4},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U3MII2RL\\hyp-space.pdf:application/pdf},
}

@misc{wiki:hopfield,
	title = {Hopfield network --- {Wikipedia}, {The} {Free} {Encyclopedia}},
	url = {https://en.wikipedia.org/w/index.php?title=Hopfield_network&oldid=865570487},
	author = {contributors, Wikipedia},
	year = {2018},
}

@article{momentum,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	doi = {10.1038/323533a0},
	journal = {\${\textbackslash}\$nat},
	author = {Rumelhart, D.{\textasciitilde}E. and Hinton, G.{\textasciitilde}E. and Williams, R.{\textasciitilde}J.},
	month = oct,
	year = {1986},
	pages = {533--536},
	file = {Learning representations by back-propagating errors.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\Learning representations by back-propagating errors.md:text/plain;Learning Rrepresentations by Back-Propagating Errors.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8PJL2NNK\\Learning Rrepresentations by Back-Propagating Errors.pdf:application/pdf},
}

@article{Du2006,
	title = {Neural networks in a softcomputing framework},
	doi = {10.1007/1-84628-303-5},
	abstract = {Conventional model-based data processing methods are computationally expensive and require experts' knowledge for the modelling of a system; neural networks provide a model-free, adaptive, parallel-processing solution. Neural Networks in a Softcomputing Framework presents a thorough review of the most popular neural-network methods and their associated techniques. This concise but comprehensive textbook provides a powerful and universal paradigm for information processing. Each chapter provides state-of-the-art descriptions of the important major research results of the respective neural-network methods. A range of relevant computational intelligence topics, such as fuzzy logic and evolutionary algorithms, are introduced. These are powerful tools for neural-network learning. Array signal processing problems are discussed in order to illustrate the applications of each neural-network model. Neural Networks in a Softcomputing Framework is an ideal textbook for graduate students and researchers in this field because in addition to grasping the fundamentals, they can discover the most recent advances in each of the popular models. The systematic survey of each neural-network model and the exhaustive list of references will enable researchers and students to find suitable topics for future research. The important algorithms outlined also make this textbook a valuable reference for scientists and practitioners working in pattern recognition, signal processing, speech and image processing, data analysis and artificial intelligence. ©Springer-Verlag London Limited 2006.},
	journal = {Neural Networks in a Softcomputing Framework},
	author = {Du, K L and Swamy, M N S},
	year = {2006},
	note = {ISBN: 1846283027},
	pages = {1--566},
}

@article{Norviga,
	title = {Natural {Language} {Corpus} {Data}},
	abstract = {MOST OF THIS BOOK DEALS WITH DATA THAT IS BEAUTIFUL IN THE SENSE OF BAUDELAIRE: " ALL WHICH IS beautiful and noble is the result of reason and calculation. " This chapter's data is beautiful in Thoreau's sense: " All men are really most attracted by the beauty of plain speech. " The data we will examine is the plainest of speech: a trillion words of English, taken from pub-licly available web pages. All the banality of the Web—the spelling and grammatical errors, the LOL cats, the Rickrolling—but also the collected works of Twain, Dickens, Austen, and millions of other authors. The trillion-word data set was published by Thorsten Brants and Alex Franz of Google in 2006 and is available through the Linguistic Data Consortium (http://tinyurl.com/ngrams). The data set summarizes the original texts by counting the number of appearances of each word, and of each two-, three-, four-, and five-word sequence. For example, " the " appears 23 billion times (2.2\% of the trillion words), making it the most common word.},
	author = {Norvig, Peter},
	keywords = {Applications, Natural Language Processing},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\92SWQPSI\\Natural Language Corpus Data.pdf:application/pdf},
}

@article{Pesme2020,
	title = {On {Convergence}-{Diagnostic} based {Step} {Sizes} for {Stochastic} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/2007.00534},
	abstract = {Constant step-size Stochastic Gradient Descent exhibits two phases: a transient phase during which iterates make fast progress towards the optimum, followed by a stationary phase during which iterates oscillate around the optimal point. In this paper, we show that efficiently detecting this transition and appropriately decreasing the step size can lead to fast convergence rates. We analyse the classical statistical test proposed by Pflug (1983), based on the inner product between consecutive stochastic gradients. Even in the simple case where the objective function is quadratic we show that this test cannot lead to an adequate convergence diagnostic. We then propose a novel and simple statistical procedure that accurately detects stationarity and we provide experimental results showing state-of-the-art performance on synthetic and real-world datasets.},
	author = {Pesme, Scott and Dieuleveut, Aymeric and Flammarion, Nicolas},
	year = {2020},
	note = {arXiv: 2007.00534},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IZ636PKN\\On Convergence-Diagnostic based Step Sizes for Stochastic Gradient Descent.pdf:application/pdf},
}

@techreport{Ge2015,
	title = {Escaping {From} {Saddle} {Points}-{Online} {Stochastic} {Gradient} for {Tensor} {Decomposition}},
	abstract = {We analyze stochastic gradient descent for optimizing non-convex functions. In many cases for non-convex functions the goal is to find a reasonable local minimum, and the main concern is that gradient updates are trapped in saddle points. In this paper we identify strict saddle property for non-convex problem that allows for efficient optimization. Using this property we show that stochastic gradient descent converges to a local minimum in a polynomial number of iterations. To the best of our knowledge this is the first work that gives global convergence guarantees for stochastic gradient descent on non-convex functions with exponentially many local minima and saddle points. Our analysis can be applied to orthogonal tensor decomposition, which is widely used in learning a rich class of latent variable models. We propose a new optimization formulation for the tensor decomposition problem that has strict saddle property. As a result we get the first online algorithm for orthogonal tensor decomposition with global convergence guarantee.},
	author = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
	year = {2015},
	note = {arXiv: 1503.02101v1},
	keywords = {()},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6N7KE3KD\\Escaping From Saddle Points – Online Stochastic Gradient for Tensor Decomposition.pdf:},
}

@article{Tian2012,
	title = {Recent advances on support vector machines research},
	volume = {18},
	issn = {20294921},
	doi = {10.3846/20294913.2012.661205},
	abstract = {Support vector machines (SVMs), with their roots in Statistical Learning Theory (SLT) and optimization methods, have become powerful tools for problem solution in machine learning. SVMs reduce most machine learning problems to optimization problems and optimization lies at the heart of SVMs. Lots of SVM algorithms involve solving not only convex problems, such as linear programming, quadratic programming, second order cone programming, semi-definite programming, but also non-convex and more general optimization problems, such as integer programming, semi-infinite programming, bi-level programming and so on. The purpose of this paper is to understand SVM from the optimization point of view, review several representative optimization models in SVMs, their applications in economics, in order to promote the research interests in both optimization-based SVMs theory and economics applications. This paper starts with summarizing and explaining the nature of SVMs. It then proceeds to discuss optimization models for SVM following three major themes. First, least squares SVM, twin SVM, AUC Maximizing SVM, and fuzzy SVM are discussed for standard problems. Second, support vector ordinal machine, semisupervised SVM, Universum SVM, robust SVM, knowledge based SVM and multi-instance SVM are then presented for nonstandard problems. Third, we explore other important issues such as lp-norm SVM for feature selection, LOOSVM based on minimizing LOO error bound, probabilistic outputs for SVM, and rule extraction from SVM. At last, several applications of SVMs to financial forecasting, bankruptcy prediction, credit risk analysis are introduced. ©2012 Copyright Vilnius Gediminas Technical University (VGTU) Press Technika.},
	number = {1},
	journal = {Technological and Economic Development of Economy},
	author = {Tian, Yingjie and Shi, Yong and Liu, Xiaohui},
	year = {2012},
	keywords = {optimization, bankruptcy prediction, credit risk analysis, data mining (DM), financial forecasting, machine learning (ML), support vector machines (SVMs)},
	pages = {5--33},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TT3IQU6I\\Recent advances in support vector machines.pdf:application/pdf},
}

@misc{Benoit2016,
	title = {Package 'quanteda': {Quantitative} {Analysis} of {Textual} {Data}},
	author = {Benoit, Kenneth and Nulty, Paul and Watanabe, Kohei and Lauderdale, Benjamin and Obeng, Adam and Barberá, Pablo and Lowe, Will},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@book{Guiraud1954a,
	title = {Les caractères statistiques du vocabulaire},
	author = {Guiraud, Pierre},
	year = {1954},
	keywords = {Applications, Natural Language Processing},
}

@article{Smith-Miles2008,
	title = {Cross-disciplinary perspectives on meta-learning for algorithm selection},
	volume = {41},
	issn = {03600300},
	doi = {10.1145/1456650.1456656},
	abstract = {The algorithm selection problem [Rice 1976] seeks to answer the question: Which algorithm is likely to perform best for my problem? Recognizing the problem as a learning task in the early 1990's, the machine learning community has developed the field of meta-learning, focused on learning about learning algorithm performance on classification problems. But there has been only limited generalization of these ideas beyond classification, and many related attempts have been made in other disciplines (such as AI and operations research) to tackle the algorithm selection problem in different ways, introducing different terminology, and overlooking the similarities of approaches. In this sense, there is much to be gained from a greater awareness of developments in meta-learning, and how these ideas can be generalized to learn about the behaviors of other (nonlearning) algorithms. In this article we present a unified framework for considering the algorithm selection problem as a learning problem, and use this frmework to tie together the crossdisciplinary developments in tackling the algorithm selection problem. We discuss the generalization of meta-learning concepts to algorithms focused on tasks including sorting, forecasting, constraint satisfaction, and optimization, and the extension of these ideas to bioinformatics, cryptography, and other fields.},
	number = {1},
	journal = {ACM Computing Surveys},
	author = {Smith-Miles, Kate A},
	year = {2008},
	keywords = {Model selection, Classification, Algorithm selection, Combinatorial optimization, Constraint satisfaction, Dataset characterization, Empirical hardness, Forecasting, Landscape analysis, Meta-learning, Sorting},
	pages = {1--25},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MDWZ9HM9\\Cross-Disciplinary Perspectives on Meta-Learning for Algorithm Selection.pdf:application/pdf},
}

@misc{Ggplot2,
	title = {ggplot2},
	url = {http://docs.ggplot2.org/current/},
}

@article{Sahu2018,
	title = {Lecture 3 : 20 {September} 2018 {Taylor} series approximation {Gradient} descent},
	number = {September},
	author = {Sahu, Pritish and Zhang, Wenjia},
	year = {2018},
	pages = {1--6},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EEJNPBLY\\Gradient Descent Taylor Approximation.pdf:application/pdf},
}

@article{Williams2010,
	title = {The role of explanation in discovery and generalization: evidence from category learning.},
	volume = {34},
	issn = {1551-6709 (Electronic)},
	doi = {10.1111/j.1551-6709.2010.01113.x},
	abstract = {Research in education and cognitive development suggests that explaining plays a key role in learning and generalization: When learners provide explanations-even to themselves-they learn more effectively and generalize more readily to novel situations. This paper proposes and tests a subsumptive constraints account of this effect. Motivated by philosophical theories of explanation, this account predicts that explaining guides learners to interpret what they are learning in terms of unifying patterns or regularities, which promotes the discovery of broad generalizations. Three experiments provide evidence for the subsumptive constraints account: prompting participants to explain while learning artificial categories promotes the induction of a broad generalization underlying category membership, relative to describing items (Exp. 1), thinking aloud (Exp. 2), or free study (Exp. 3). Although explaining facilitates discovery, Experiment 1 finds that description is more beneficial for learning item details. Experiment 2 additionally suggests that explaining anomalous observations may play a special role in belief revision. The findings provide insight into explanation's role in discovery and generalization.},
	number = {5},
	journal = {Cognitive science},
	author = {Williams, Joseph J and Lombrozo, Tania},
	month = jul,
	year = {2010},
	pmid = {21564236},
	pages = {776--806},
}

@article{Shakshuki2006,
	title = {Explaining {Algorithms} : {A} {New} {Perspective} . {Explaining} {Algorithms} : {A} {New} {Perspective} {Tomasz} {Müldner} {Jodrey} {School} of {Computer} {Science} {Tomasz}.{Muldner}@acadiau.ca {Jodrey} {School} of {Computer} {Science}},
	number = {October 2014},
	author = {Shakshuki, Elhadi},
	year = {2006},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FJEHATBU\\Explaining Algorithms - A New Perspective.pdf:application/pdf},
}

@article{Landauer1997,
	title = {A {Solution} to {Plato}'s {Problem}: {The} {Latent} {Semantic} {Analysis} {Theory} of {Acquisition}, {Induction}, and {Representation} of {Knowledge}},
	volume = {1},
	abstract = {Bellcore How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by ex-tracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched. Prologue "How much do we know at any time? Much more, or so I believe, than we know we know!" —Agatha Christie, The Moving Finger A typical American seventh grader knows the meaning of 10-15 words today that she did not know yesterday. She must have acquired most of them as a result of reading because (a) the majority of English words are used only in print, (b) she already knew well almost all the words she would have encoun-tered in speech, and (c) she learned less than one word by direct instruction. Studies of children reading grade-school text find that about one word in every 20 paragraphs goes from wrong to right on a vocabulary test. The typical seventh grader would have read less than 50 paragraphs since yesterday, from which she should have learned less than three new words. Apparently, she mastered the meanings of many words that she did not encounter. Evidence for all these assertions is given in detail later. This phenomenon offers an ideal case in which to study a problem that has plagued philosophy and science since Plato We thank Karen Lochbaum for valuable help in analysis; George Furnas for early ideas and inspiration; Peter Foltz, Walter Kintsch, and Ernie Mross for unpublished data; and for helpful comments on the ideas and drafts, we thank, in alphabetic order, 24 centuries ago, the fact that people have much more knowl-edge than appears to be present in the information to which they have been exposed. Plato's solution, of course, was that people must come equipped with most of their knowledge and need only hints and contemplation to complete it. In this article we suggest a very different hypothesis to explain the mystery of excessive learning. It rests on the simple notion that some domains of knowledge contain vast numbers of weak interrelations that, if properly exploited, can greatly amplify learning by a process of inference. We have discovered that a very simple mechanism of induction, the choice of the correct dimensionality in which to represent similarity between objects and events, can sometimes, in particular in learning about the similarity of the meanings of words, produce sufficient enhance-ment of knowledge to bridge the gap between the information available in local contiguity and what people know after large amounts of experience. Overview},
	number = {2},
	journal = {Psychological Review},
	author = {Landauer, Thomas K and Dutnais, Susan T and Anderson, Richard and Carroll, Doug and Fbltz, Peter and Pumas, George and Kintsch, Walter and Menn, Lise and Streeter, Lynn},
	year = {1997},
	keywords = {Applications, Natural Language Processing},
	pages = {211--240},
}

@article{Jurafsky,
	title = {Language {Modeling} {Probabilis1c} {Language} {Models}},
	author = {Jurafsky, Dan},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y3DLUNZ9\\Intro to Language Modeling - Ngrams.pdf:application/pdf},
}

@article{Tagg2009,
	title = {by},
	number = {March},
	author = {Tagg, Caroline},
	year = {2009},
}

@article{Peng2017,
	title = {Mastering {Software} {Development} in {R}},
	abstract = {This book is designed to be used in conjunction with the course sequence Mastering Software Development in R, available on Coursera. The book covers R software development for building data science tools. As the field of data science evolves, it has become clear that software development skills are es- sential for producing useful data science results and products. You will obtain rigorous training in the R language, including the skills for handling complex data, building R packages and developing custom data visualizations. You will learn modern software development practices to build tools that are highly reusable, modular, and suitable for use in a team-based environment or a community of developers.},
	journal = {Mastering Software Development in R},
	author = {Peng, Roger D and Kross, Sean and Anderson, Brooke},
	year = {2017},
	pages = {1--479},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C57P7EXW\\Mastering Software Development in R.pdf:},
}

@techreport{10ProvenLead,
	title = {10 {Proven} {Lead} {Generators} for {Medical} {Device} {Companies}},
}

@article{UnitOutline,
	title = {Unit 4 {Outline}},
	pages = {1--44},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LKM2IEES\\Unit 04 - Maximum Likelihood Estimation - 1 per page.pdf:application/pdf},
}

@article{GloriaCorpasPastor2010,
	title = {Size matters: a quantative approach to corpus representativeness},
	volume = {1},
	author = {{Gloria Corpas Pastor} and {Miriam Seghiri}},
	year = {2010},
	keywords = {Applications, Natural Language Processing},
	pages = {111--145},
}

@misc{MacFarlane2006,
	title = {About {Pandoc}},
	url = {http://pandoc.org/},
	author = {MacFarlane, John},
	year = {2006},
	keywords = {Applications, Natural Language Processing},
}

@article{Stanford2012,
	title = {Gradient-{Free} {Optimization}},
	url = {http://adl.stanford.edu/aa222/Home.html},
	abstract = {6.1 Introduction Using optimization in the solution of practical applications we often encounter one or more of the following challenges: • non-differentiable functions and/or constraints • disconnected and/or non-convex feasible space • discrete feasible space • mixed variables (discrete, continuous, permutation) • large dimensionality • multiple local minima (multi-modal) • multiple objectives Gradient-based optimizers are efficient at finding local minima for high-dimensional, nonlinearly-constrained, convex problems; however, most gradient-based optimizers have problems dealing with noisy and discontinuous functions, and they are not designed to handle multi-modal problems or discrete and mixed discrete-continuous design variables. Consider, for example, the Griewank function: f (x) = n i=1 x 2 Figure 6.1: Graphs illustrating the various types of functions that are problematic for gradient-based optimization algorithms AA222: MDO 134 Monday 30 th April, 2012 at 16:25 Figure 6.2: The Griewank function looks deceptively smooth when plotted in a large domain (left), but when you zoom in, you can see that the design space has multiple local minima (center) although the function is still smooth (right) How we could find the best solution for this example? • Multiple point restarts of gradient (local) based optimizer • Systematically search the design space • Use gradient-free optimizers Many gradient-free methods mimic mechanisms observed in nature or use heuristics. Unlike gradient-based methods in a convex search space, gradient-free methods are not necessarily guar-anteed to find the true global optimal solutions, but they are able to find many good solutions (the mathematician's answer vs. the engineer's answer). The key strength of gradient-free methods is their ability to solve problems that are difficult to solve using gradient-based methods. Furthermore, many of them are designed as global optimizers and thus are able to find multiple local optima while searching for the global optimum. Various gradient-free methods have been developed. We are going to look at some of the most commonly used algorithms: • Nelder–Mead Simplex (Nonlinear Simplex)},
	journal = {AA222 - Introduction to Multidisciplinary Design Optimization},
	author = {Stanford, University},
	year = {2012},
	pages = {133--161},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\946TR5T5\\Gradient-Free Optimization.pdf:application/pdf},
}

@article{Haverkort2013,
	title = {Writing a report on experiments with algorithms},
	author = {Haverkort, Herman},
	year = {2013},
	pages = {1--28},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SYXXR8C5\\Writing a Report on Experiments with Algorithms.pdf:application/pdf},
}

@article{Sherstinsky2020,
	title = {Fundamentals of {Recurrent} {Neural} {Network} ({RNN}) and {Long} {Short}-{Term} {Memory} ({LSTM}) network},
	volume = {404},
	issn = {01672789},
	doi = {10.1016/j.physd.2019.132306},
	abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of “unrolling” an RNN is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the “Vanilla LSTM”1 network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Sherstinsky, Alex},
	year = {2020},
	note = {arXiv: 1808.03314},
	keywords = {LSTM, RNN, Convolutional input context windows, External input gate, RNN unfolding/unrolling},
	pages = {1--39},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BIPMQ325\\Fundamentals of Recurrent Neural Network (RNN).pdf:application/pdf},
}

@article{UnsupervisedMethodsLanguage2012,
	title = {Unsupervised {Methods} for {Language} {Modeling}},
	url = {http://www.kiv.zcu.cz/publications/},
	abstract = {Language models are crucial for many tasks in NLP 1 and N-grams are the best way to build them. Huge effort is being invested in improving n-gram language models. By introducing external information (morphology, syntax, partitioning into documents, etc.) into the models a significant improvement can be achieved. The models can however be improved with no external information and smoothing is an excellent example of such an improvement. Thesis summarizes the state-of-the-art approaches to unsupervised lan-guage modeling with emphases on the inflectional languages, which are par-ticularly hard to model. It is focused on methods that can discover hidden patterns that are already in a training corpora. These patterns can be very useful for enhancing the performance of language modeling, moreover they do not require additional information sources.},
	year = {2012},
	keywords = {Applications, Natural Language Processing},
}

@article{Devol2007,
	title = {An {Unhealthy} {America} : {The} {Economic} {Burden} of {Chronic} {Disease}},
	url = {www.milkeninstitute.org},
	abstract = {Good information on trends and costs on national and state levels. Implications and impact of chronic disease in the workplace are presented.},
	number = {October},
	journal = {Milken Institute},
	author = {Devol, Ross and Bedroussian, Armen},
	year = {2007},
	pages = {1--252},
}

@incollection{Bottoua,
	title = {On-line {Learning} and {Stochastic} {Approximations}},
	isbn = {978-0-521-11791-3},
	url = {https://www.cambridge.org/core/product/identifier/CBO9780511569920A009/type/book_part},
	abstract = {An abstract is not available.},
	booktitle = {On-{Line} {Learning} in {Neural} {Networks}},
	author = {Bottou, Léon},
	doi = {10.1017/CBO9780511569920.003},
	note = {ISSN: 0009-2673},
	pages = {9--42},
}

@incollection{DeMori1996,
	title = {– {A} {Cache}-{Based} {Natural} {Language} {Model} for {Speech} {Recognition}},
	volume = {12},
	isbn = {978-0-444-81607-8},
	abstract = {This chapter discusses cache-based language model for speech recognition. Automatic Speech Recognition (ASR) consists of two components: an acoustic component, which matches the acoustic input to words in its vocabulary, producing a set of the most plausible word candidates together with a probability for each, and the second component, which incorporates a language model, estimating the probability for each word in the vocabulary that it will occur, given a list of previously hypothesized words. The chapter focuses on the language model incorporated in the second component. A hypothesis is adopted in the chapter that a word used in the recent past is much more likely to be used sooner, from either its overall frequency in the language or that a 3g-gram model would suggest. The cache component of the combined model estimates the probability of a word from its recent frequency of use. The model uses a weighted average of the 3g-gram and cache components in calculating word probabilities, where the relative weights assigned to each component depend on the Part of Speech (POS). For purpose of comparison, a pure 3g-gram model has been created in the chapter, consisting of only the 3g-gram component of the combined model.},
	booktitle = {Recent {Research} {Towards} {Advanced} {Man}-{Machine} {Interface} {Through} {Spoken} {Language}},
	author = {De Mori, Renato and Kuhn, Roland},
	year = {1996},
	doi = {10.1016/B978-044481607-8/50065-7},
	note = {Issue: 6},
	keywords = {Applications, Natural Language Processing},
	pages = {219--228},
}

@article{hart1988,
	title = {Sequential {Stopping} {Rules} for {Random} {Optimization} {Methods} with {Applications} to {Multistart} {Local} {Search}},
	volume = {9},
	url = {https://doi.org/10.1137/S1052623494277317},
	doi = {10.1137/S1052623494277317},
	number = {1},
	journal = {SIAM Journal on Optimization},
	author = {Hart, W},
	year = {1998},
	pages = {270--290},
}

@article{Trnka2008b,
	title = {Word {Prediction} {Techniques} for {User} {Adaptation} and {Sparse} {Data} {Mitigation} {Ph} . {D} . {Thesis} {Proposal}},
	journal = {Information Sciences},
	author = {Trnka, Keith},
	year = {2008},
	keywords = {Applications, Natural Language Processing, proposal},
}

@misc{DeCock,
	title = {House {Prices}: {Advanced} {Regression} {Techniques} {\textbar} {Kaggle}},
	url = {https://www.kaggle.com/c/house-prices-advanced-regression-techniques#description},
	urldate = {2017-09-16},
	author = {De Cock, Dean},
}

@article{Egede2007,
	title = {Major depression in individuals with chronic medical disorders: prevalence, correlates and association with health resource utilization, lost productivity and functional disability},
	volume = {29},
	issn = {01638343},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0163834307001260},
	doi = {10.1016/j.genhosppsych.2007.06.002},
	abstract = {Objective: The objective of this study was to determine the prevalence and odds of major depression and the incremental effect of major depression on utilization, lost productivity and functional disability in individuals with common chronic medical disorders. Method: Data on 30,801 adults from the 1999 National Health Interview Survey were analyzed. The 12-month prevalence and age/sex-adjusted odds of major depression were calculated for adults with hypertension (HTN), diabetes mellitus (DM), coronary artery disease (CAD), congestive heart failure (CHF), stroke or cerebrovascular accident (CVA), chronic obstructive pulmonary disease (COPD) and end-stage renal disease (ESRD). The association between chronic condition status (with and without major depression) and utilization, lost productivity and functional disability was determined by controlling for covariates. Results: The 12-month prevalence and age/sex-adjusted odds of major depression by chronic conditions were as follows: CHF, 7.9\% [odds ratio (OR)=1.96]; HTN, 8.0\% (OR=2.00); DM, 9.3\% (OR=1.96); CAD, 9.3\% (OR=2.30); CVA, 11.4\% (OR=3.15); COPD, 15.4\% (OR=3.21); ESRD, 17.0\% (OR=3.56); any chronic condition, 8.8\% (OR=2.61). Compared to adults without chronic conditions, those with chronic conditions plus major depression had greater odds of ???1 ambulatory visit [OR=1.50; 95\% confidence interval (95\% CI)=1.28, 1.77]; ???1 emergency room visit (OR=1.94; 95\% CI=1.55, 2.45); and ???1 day in bed due to illness (OR=1.60; 95\% CI=1.28, 2.00); and functional disability (OR=2.48; 95\% CI=1.96, 3.15). Conclusion: The 12-month prevalence and odds of major depression are high in individuals with chronic medical conditions, and major depression is associated with significant increases in utilization, lost productivity and functional disability. ?? 2007 Elsevier Inc. All rights reserved.},
	number = {5},
	journal = {General Hospital Psychiatry},
	author = {Egede, Leonard E},
	month = sep,
	year = {2007},
	pmid = {17888807},
	note = {ISBN: 0163-8343 (Print)\${\textbackslash}\$r0163-8343 (Linking)},
	keywords = {Chronic medical disorders, Major depression, National Health Interview Survey},
	pages = {409--416},
}

@article{DavidCrystaltxtngGr8,
	title = {david\_crystal-txtng\_\_the\_gr8\_db8-oxford\_university\_press,\_usa\_(2008)},
}

@book{Management2003,
	title = {Executive {Guide}},
	isbn = {978-3-319-63819-5},
	abstract = {Aim. During a nursing conference of the Northeaster Piedmont Neonatal\${\textbackslash}\$nIntensive and Subintensive Neonatal Units the error in pediatrics and\${\textbackslash}\$nneonatology was discussed and a follow-up work was proposed with the aim\${\textbackslash}\$nto understand how many, what type of errors and what kind of adverse\${\textbackslash}\$nevent they cause in our clinical practice.\${\textbackslash}\$nMethods. Through an anonymous ``detection sheet\{''\} we detected the\${\textbackslash}\$nerrors made between March 1 and April 30, 2010 in a NICU and 2\${\textbackslash}\$nSubintensive therapies. The total number of patients was 166 for 2398\${\textbackslash}\$ndays of hospitalization.\${\textbackslash}\$nResults. The total number of errors was 72, with a error of\${\textbackslash}\$n0.43/patient. Forty-six patients had experienced at least 1 error (28\%\${\textbackslash}\$nof patients) and more than a 16 (10\% of our patients). There is a\${\textbackslash}\$nstatistically significant correlation between days of hospitalization\${\textbackslash}\$nand the number of errors occurred (r=0.63 Sperman's correlation,\${\textbackslash}\$nP{\textless}0.01); 48\% and 53\% of the errors in the NICU and Subintensive CU\${\textbackslash}\$nwere related to medication administration.\${\textbackslash}\$nConclusion. The severe damage in the NICU was caused by errors more\${\textbackslash}\$nfrequently related to vascular access while the only mistake that led to\${\textbackslash}\$na serious incident in subintensive CU was determined by a monitoring\${\textbackslash}\$nerror. Errors were most frequently attributed to\${\textbackslash}\$ninattention-distraction, less frequently have been attributed to a lack\${\textbackslash}\$nof experience or a state of excessive fatigue. The data of our study\${\textbackslash}\$nwere made available to all staff in order to make operators more aware\${\textbackslash}\$nof the importance of working safely.},
	author = {Management, Texas Department of Emergency},
	year = {2003},
	note = {Publication Title: Project Management Institute
ISSN: 0026-4946},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YR8SF89D\\The-Executive-Guide-to-Artificial-Intelligence-How-to-identify-and-implement-applications-for-AI-in-your-organization-2018.pdf:application/pdf},
}

@article{Draper1995,
	title = {Assessment and {Propagation} of {Model} {Uncertainty}},
	volume = {57},
	number = {109},
	journal = {J.R. Statist. Soc. B},
	author = {Draper, David},
	year = {1995},
	keywords = {Algorithms, Bayesian, multidimensional register contrast, yi},
	pages = {45--97},
}

@article{Jensen2020,
	title = {Harmonic beamformers for speech enhancement and dereverberation in the time domain},
	volume = {116},
	issn = {01676393},
	url = {https://doi.org/10.1016/j.specom.2019.11.003},
	doi = {10.1016/j.specom.2019.11.003},
	abstract = {This paper presents a framework for parametric broadband beamforming that exploits the frequency-domain sparsity of voiced speech to achieve more noise reduction than traditional nonparametric broadband beamforming without introducing additional distortion. In this framework, the harmonic model is used to parametrize the signal of interest by a single parameter, the fundamental frequency, whereby both speech enhancement and derevereration can be performed. This framework thus exploits both the spatial and temporal properties of speech signals simultaneously and includes both fixed and adaptive beamformers, such as (1) delay-and-sum, (2) null forming, (3) Wiener, (4) minimum variance distortionless response (MVDR), and (5) linearly constrained minimum variance beamformers. Moreover, the framework contains standard broadband beamforming as a special case, whereby the proposed beamformers can also handle unvoiced speech. The reported experimental results demonstrate the capabilities of the proposed framework to perform both speech enhancement and dereverberation simultaneously. The proposed beamformers are evaluated in terms of speech distortion and objective measures for speech quality and speech intelligibility, and are compared to nonparametric broadband beamformers. The results show that the proposed beamformers perform well compared to traditional methods, including a state-of-the-art dereverberation method, particularly in adverse conditions with high amounts of noise and reverberation.},
	number = {November 2019},
	journal = {Speech Communication},
	author = {Jensen, J R and Karimian-Azari, S and Christensen, M G and Benesty, J},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Beamforming, Dereverberation, Enhancement, Microphone arrays, Noise reduction, Time domain},
	pages = {1--11},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C22YFPBT\\Harmonic-beamformers-for-speech-enhancement-and-dereverbe_2020_Speech-Commun.pdf:application/pdf},
}

@article{G.L.xiongJ.S.2017,
	title = {Statistics tutorials},
	journal = {Zhejiang University Press Hangzhou},
	author = {G. L. xiong, J.S., Juan Wang},
	year = {2017},
}

@article{Downey2011,
	title = {Think {Stats}: {Exploratory} {Data} {Analysis} in {Python}, 2nd {Ed}.},
	url = {http://books.google.com/books?hl=en&lr=&id=TCfZ7d6skT4C&oi=fnd&pg=PR5&dq=Think+Stats&ots=LxYK9ntvRZ&sig=UchH878Uf04hPLGPmOwnNHBGmdk%5Cnhttp://greenteapress.com/thinkstats2/index.html},
	abstract = {ISBN},
	journal = {Book},
	author = {Downey, Allen B},
	year = {2011},
	note = {ISBN: 9781449307110},
	pages = {136},
}

@article{Lindley1957,
	title = {A {Statistical} {Paradox}},
	volume = {44},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2333251},
	number = {1/2},
	journal = {Biometrika},
	author = {Lindley, D V},
	year = {1957},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	keywords = {Algorithms, Bayesian},
	pages = {187--192},
}

@inproceedings{j.2018on,
	title = {On the {Convergence} of {Adam} and {Beyond}},
	url = {https://openreview.net/forum?id=ryQu7f-RZ},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
	year = {2018},
}

@techreport{PortioResearch2015,
	title = {{SMS}: {The} language of 6 billion people},
	url = {www.portioresearch.com%5Cnportioresearch.com},
	abstract = {This report explains how three key trends are shaping the future of communications and technology: ? A rapid shift to a mobile first mindset ? The unprecedented reach of SMS ? The emphasis on content personalisation These three trends have already fundamentally changed how people purchase goods and services, and they will continue to change the way consumers interact with companies and services. Throughout this report, we bring your attention to the ubiquity of SMS. 6.1 billion people, out of a total human population of 7.3 billion, use an SMS-capable mobile phone. SMS can be used to reach 84\% of the human race alive today. SMS is a true mobile first technology, and SMS offers enterprises an opportunity to communicate with their customers, staff and suppliers in a highly personalised and extremely responsive way. The future of the telecoms, media, technology and consumer electronics industries are merging and changing faster now than ever before, and we believe the trends discussed in this report will continue to be profoundly important over the next decade.},
	author = {{PortioResearch}},
	year = {2015},
	note = {Publication Title: Megatrends in consumer technology},
	keywords = {Applications, Natural Language Processing},
	pages = {1--49},
}

@article{Wasserman1996,
	title = {The {Selection} of {Prior} {Distributions} by {Formal} {Rules}},
	volume = {91},
	number = {435},
	journal = {Journal of the American Statistical Association},
	author = {Wasserman, Larry and Kass, Robert E},
	year = {1996},
	keywords = {Algorithms, Bayesian},
	pages = {1343--1370},
}

@incollection{Jurafsky2016,
	title = {Language {Modeling} with {N}- grams},
	isbn = {0-13-187321-0},
	abstract = {CHAPTER 4 Language Modeling with N-grams " You are uniformly charming! " cried he, with a smile of associating and now and then I bowed and they perceived a chaise and four to wish for. Random sentence generated from a Jane Austen trigram model Being able to predict the future is not always a good thing. Cassandra of Troy had the gift of foreseeing but was cursed by Apollo that her predictions would never be believed. Her warnings of the destruction of Troy were ignored and to simplify, let's just say that things just didn't go well for her later. In this chapter we take up the somewhat less fraught topic of predicting words. What word, for example, is likely to follow Please turn your homework ... Hopefully, most of you concluded that a very likely word is in, or possibly over, but probably not refrigerator or the. In the following sections we will formalize this intuition by introducing models that assign a probability to each possible next word. The same models will also serve to assign a probability to an entire sentence. Such a model, for example, could predict that the following sequence has a much higher probability of appearing in a text: all of a sudden I notice three guys standing on the sidewalk},
	author = {Jurafsky, Daniel and Martin, James R},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Schweinberger2016,
	title = {Part-{Of}-{Speech} {Tagging} with {R}},
	author = {Schweinberger, Martin and Schweinberger, Martin},
	year = {2016},
	pages = {1--10},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NL6V9F46\\PosTagR.pdf:application/pdf},
}

@book{Rota,
	title = {Encyclopedia of {Mathematics} and its {Applications}},
	isbn = {978-0-521-85155-8},
	author = {Rota, Founding Editor G C and Mora, Teo and Bichteler, Klaus and Lothaire, M and Ivanov, A A and Shpectorov, S V and Mcmullen, Peter and Schulte, Egon and Gierz, G and Finch, Steven R and Jabri, Youssef and Gasper, George and Rahman, Mizan and Pedicchio, Maria Cristina and Tholen, Walter and Olivieri, Enzo and Vares, Maria Eulalia and Wilson, R J and Beineke, L},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\29ISS7L6\\dynamic-data-assimilation-a-least-squares-approach.pdf:application/pdf},
}

@article{Clyde1985,
	title = {Model averaging},
	doi = {10.1002/9780470317105},
	abstract = {13.1 INTRODUCTION In Chapter 12, we considered inference in a normal linear regression model with q predictors. In many instances, the set of predictor variables X can be quite large, as one considers many potential variables and possibly transfor-mations and interactions of these variables that may be relevant to modelling the response Y. One may start with a large set to reduce chances that an im-portant predictor has been omitted, but employ variable selection to eliminate variables do not appear to be necessary and avoid over-fitting. Historically, variable selection methods, such as forwards, backwards, and stepwise selec-tion, maximum adjusted R 2 , AIC, Cp, etc., have been used, and, as is well known, these can each lead to selection of a different final model (Weisberg 1985). Other modeling decisions that may arise in practice include specify-ing the structural form of the model, including choice of transformation of the response, error distribution, or choice of functional form that relates the mean to the predictors. Decisions on how to handle " outliers " may involve multiple tests with a somewhat arbitrary cut-off for p-values or the use of " ro-bust " outlier resistant methods. Many of the modeling decisions are made conditional on previous choices, and final measures of " significance " may be questionable. While one may not be surprised that approaches for selection of a model reach different conclusions, a major problem with such analyses is that often only a " best " model and its associated summaries are presented, giving a},
	author = {Clyde, Merlise},
	year = {1985},
	note = {ISBN: 9780470317105},
	keywords = {Algorithms, Bayesian},
	pages = {1--25},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\38SUEZV4\\Bayesian Model Averaging.pdf:application/pdf},
}

@misc{Google2012,
	title = {Offensive {Words} from {Google}'s '{What} {Do} {You} {Love}' {Project}},
	url = {https://gist.github.com/jamiew/1112488},
	author = {{Google}},
	year = {2012},
	keywords = {Applications, Natural Language Processing},
}

@article{Zhu,
	title = {{CS769} {Spring} 2010 {Advanced} {Natural} {Language} {Processing} {Basic} {Text} {Process}},
	author = {Zhu, Xiaojin},
	keywords = {Applications, Natural Language Processing},
}

@book{LockeECHUBOOK,
	title = {Locke {ECHU} {BOOK} {III} {Chapter} {II} {Of} the {Signification} of {Words}},
	url = {http://www.rbjones.com/rbjpub/philos/classics/locke/ctb3c02.htm},
	publisher = {Rbjones.com},
	keywords = {Applications, Natural Language Processing},
}

@book{Minto,
	title = {Barbara {Minto} .•},
	isbn = {978-0-273-71051-6},
	author = {Minto, Barbara and Hall, Prentice},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NH8U9FZ8\\Barbara Minto - Pyramid principle HQ.pdf:application/pdf},
}

@misc{TheMathworksInc.2016,
	title = {{MATLAB} - {MathWorks}},
	url = {https://www.mathworks.com/products/matlab.html},
	abstract = {The MATLAB platform is optimized for solving engineering and scientific problems. The matrix-based MATLAB language is the world's most natural way to express computational mathematics.},
	urldate = {2017-09-16},
	author = {{The Mathworks Inc.}},
	year = {2016},
	doi = {2016-11-26},
	note = {Publication Title: www.mathworks.com/products/matlab},
}

@article{Ring2019,
	title = {A survey of network-based intrusion detection data sets},
	volume = {86},
	issn = {01674048},
	doi = {10.1016/j.cose.2019.06.005},
	abstract = {Labeled data sets are necessary to train and evaluate anomaly-based network intrusion detection systems. This work provides a focused literature survey of data sets for network-based intrusion detection and describes the underlying packet- and flow-based network data in detail. The paper identifies 15 different properties to assess the suitability of individual data sets for specific evaluation scenarios. These properties cover a wide range of criteria and are grouped into five categories such as data volume or recording environment for offering a structured search. Based on these properties, a comprehensive overview of existing data sets is given. This overview also highlights the peculiarities of each data set. Furthermore, this work briefly touches upon other sources for network-based data such as traffic generators and data repositories. Finally, we discuss our observations and provide some recommendations for the use and the creation of network-based data sets.},
	journal = {Computers and Security},
	author = {Ring, Markus and Wunderlich, Sarah and Scheuring, Deniz and Landes, Dieter and Hotho, Andreas},
	year = {2019},
	note = {arXiv: 1903.02460v2},
	keywords = {Data mining, Data sets, Evaluation, IDS, Intrusion detection, NIDS},
	pages = {147--167},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AX3SV678\\Survey of Available Network Intrusion Detection Datasets.pdf:application/pdf},
}

@misc{HunspellDictionaryDownload,
	title = {Hunspell {Dictionary} download {\textbar} {SourceForge}.net},
	url = {https://sourceforge.net/projects/hunspell/},
}

@inproceedings{Talbot2007,
	title = {Smoothed {Bloom} filter language models: {Tera}-scale {LMs} on the cheap},
	abstract = {A Bloom filter (BF) is a randomised data structure for set membership queries. Its space requirements fall significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability. Here we present a general framework for deriving smoothed language model probabilities from BFs. We investigate how a BF containing n-gram statistics can be used as a direct replacement for a conventional n-gram model. Recent work has demonstrated that corpus statistics can be stored efficiently within a BF, here we consider how smoothed language model probabilities can be derived efficiently from this randomised representation. Our proposal takes advantage of the one-sided error guarantees of the BF and simple inequalities that hold between related n-gram statistics in order to further reduce the BF storage requirements and the error rate of the derived probabilities. We use these models as replacements for a conventional language model in machine translation experiments.},
	booktitle = {{EMNLP}-2007},
	author = {Talbot, David and Osborne, Miles},
	year = {2007},
	note = {Issue: June},
	keywords = {Applications, Natural Language Processing},
	pages = {468--476},
}

@article{Tsamardinos2015,
	title = {Performance-{Estimation} {Properties} of {Cross}-{Validation}-{Based} {Protocols} with {Simultaneous} {Hyper}-{Parameter} {Optimization}},
	volume = {24},
	issn = {17936349},
	doi = {10.1142/S0218213015400230},
	abstract = {In a typical supervised data analysis task, one needs to perform the following two tasks: (a) select an optimal combination of learning methods (e.g., for variable selection and classifier) and tune their hyper-parameters (e.g., K in K-NN), also called model selection, and (b) provide an estimate of the performance of the final, reported model. Combining the two tasks is not trivial because when one selects the set of hyper-parameters that seem to provide the best estimated performance, this estimation is optimistic (biased/overfitted) due to performing multiple statistical comparisons. In this paper, we discuss the theoretical properties of performance estimation when model selection is present and we confirm that the simple Cross-Validation with model selection is indeed optimistic (overestimates performance) in small sample scenarios and should be avoided. We present in detail and investigate the theoretical properties of the Nested Cross Validation and a method by Tibshirani and Tibshirani for removing the estimation bias. In computational experiments with real datasets both protocols provide conservative estimation of performance and should be preferred. These statements hold true even if feature selection is performed as preprocessing.},
	number = {5},
	journal = {International Journal on Artificial Intelligence Tools},
	author = {Tsamardinos, Ioannis and Rakhshani, Amin and Lagani, Vincenzo},
	year = {2015},
	keywords = {model selection, comparative evaluation, cross validation, Performance estimation, stratification},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S75F5WS9\\Performance-Estimation Properties of Cross-Validation-Based Protocols with Simultaneous Hyper-Parameter Optimization.pdf:application/pdf},
}

@article{BROUGHTYOUPARTNERSHIP,
	title = {{BROUGHT} {TO} {YOU} {IN} {PARTNERSHIP} {WITH} {Dear} {Reader} , {Table} of {Contents}},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JTTV6EVY\\AI (DZone).pdf:application/pdf},
}

@misc{NORCa,
	title = {{GSS} {Data} {Explorer} {\textbar} {NORC} at the {University} of {Chicago}},
	url = {https://gssdataexplorer.norc.org/pages/show?page=gss%2Fabout},
	abstract = {General Social Survey(GSS) monitors societal change/ opinion.},
	urldate = {2017-09-29},
	author = {of Chicago, NORC at the University},
}

@article{Showcase2000,
	title = {Incorporating {Linguistic} {Structure} into {Statistical} {Language} {Models}},
	url = {http://repository.cmu.edu/compsci},
	author = {Showcase, Research and Rosenfeld, Roni and ª, Y},
	year = {2000},
	keywords = {Applications, Natural Language Processing},
}

@book{Skiena2017,
	title = {The data science design manual},
	isbn = {978-3-319-55443-3},
	url = {http://link.springer.com/10.1007/978-3-319-55444-0},
	abstract = {This engaging and clearly written textbook/reference provides a must-have introduction to the rapidly emerging interdisciplinary field of data science. It focuses on the principles fundamental to becoming a good data scientist and the key skills needed to build systems for collecting, analyzing, and interpreting data. The Data Science Design Manual is a source of practical insights that highlights what really matters in analyzing data, and provides an intuitive understanding of how these core concepts can be used. The book does not emphasize any particular programming language or suite of data-analysis tools, focusing instead on high-level discussion of important design principles. This easy-to-read text ideally serves the needs of undergraduate and early graduate students embarking on an "Introduction to Data Science" course. It reveals how this discipline sits at the intersection of statistics, computer science, and machine learning, with a distinct heft and character of its own. Practitioners in these and related fields will find this book perfect for self-study as well. Additional learning tools: Contains "War Stories," offering perspectives on how data science applies in the real world Includes "Homework Problems," providing a wide range of exercises and projects for self-study Provides a complete set of lecture slides and online video lectures at www.data-manual.com Provides "Take-Home Lessons," emphasizing the big-picture concepts to learn from each chapter Recommends exciting "Kaggle Challenges" from the online platform Kaggle Highlights "False Starts," revealing the subtle reasons why certain approaches fail Offers examples taken from the data science television show "The Quant Shop" (www.quant-shop.com). What is Data Science? -- Mathematical Preliminaries -- Data Munging -- Scores and Rankings -- Statistical Analysis -- Visualizing Data -- Mathematical Models -- Linear Algebra -- Linear and Logistic Regression -- Distance and Network Methods -- Machine Learning -- Big Data: Achieving Scale.},
	author = {Skiena, Steven S},
	year = {2017},
	doi = {10.1007/978-3-319-55444-0},
	note = {Publication Title: Springer},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NM8RMS29\\The Data Science Design Manual.pdf:application/pdf},
}

@book{Grosan2011,
	title = {Machine {Learning}},
	volume = {17},
	isbn = {978-3-642-21003-7},
	abstract = {Introduction: Machine Learning[6][8][12] is concerned with the study of building computer programs that automatically improve and/or adapt their performance through experience. Machine learning can be thought of as "programming by example" [11]. Machine learning has many common things with other domains such as statistics and probability theory (understanding the phenomena that have generated the data), data mining (finding patterns in the data that are understandable by people) and cognitive sciences (human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people such as concept learning, skill acquisition, strategy change, etc.) [1]. ©Springer-Verlag Berlin Heidelberg 2011.},
	author = {Grosan, Crina and Abraham, Ajith},
	year = {2011},
	doi = {10.1007/978-3-642-21004-4_10},
	note = {Publication Title: Intelligent Systems Reference Library
ISSN: 18684394},
}

@article{Ward2014,
	title = {Multiple {Chronic} {Conditions} {Among} {US} {Adults}: {A} 2012 {Update}},
	volume = {11},
	issn = {1545-1151},
	url = {http://www.cdc.gov/pcd/issues/2014/13_0389.htm},
	doi = {10.5888/pcd11.130389},
	journal = {Preventing Chronic Disease},
	author = {Ward, Brian W and Schiller, Jeannine S and Goodman, Richard A},
	month = apr,
	year = {2014},
	pages = {130389},
}

@article{Samuelsson,
	title = {A {CLASS}-{BASED} {LANGUAGE} {MODEL} {FOR} {LARGE}-{VOCABULARY} {SPEECH} {RECOGNITION} {EXTRACTED} {FROM} {PART}-{OF}-{SPEECH} {STATISTICS}},
	abstract = {A novel approach is presented to class-based language modeling based on part-of-speech statistics. It uses a deter-ministic word-to-class mapping, which handles words with alternative part-of-speech assignments through the use of ambiguity classes. The predictive power of word-based lan-guage models and the generalization capability of class-based language models are combined using both linear interpola-tion and word-to-class backoff, and both methods are eval-uated. Since each word belongs to one precisely ambigu-ity class, an exact word-to-class backoff model can easily be constructed. Empirical evaluations on large-vocabulary speech-recognition tasks show perplexity improvements and significant reductions in word error-rate.},
	author = {Samuelsson, Christer and Reichl, Wolfgang},
	keywords = {Applications, Natural Language Processing},
}

@article{Dauphin2019,
	title = {{MetaInit}: {Initializing} learning by learning to initialize},
	abstract = {Deep learning models frequently trade handcrafted features for deep features learned with much less human intervention using gradient descent. While this paradigm has been enormously successful, deep networks are often difficult to train and performance can depend crucially on the initial choice of parameters. In this work, we introduce an algorithm called MetaInit as a step towards automating the search for good initializations using meta-learning. Our approach is based on a hypothesis that good initializations make gradient descent easier by starting in regions that look locally linear with minimal second order effects. We formalize this notion via a quantity that we call the gradient quotient, which can be computed with any architecture or dataset. MetaInit minimizes this quantity efficiently by using gradient descent to tune the norms of the initial weight matrices. We conduct experiments on plain and residual networks and show that the algorithm can automatically recover from a class of bad initializations. MetaInit allows us to train networks and achieve performance competitive with the state-of-the-art without batch normalization or residual connections. In particular, we find that this approach outperforms normalization for networks without skip connections on CIFAR-10 and can scale to Resnet-50 models on Imagenet.},
	number = {NeurIPS},
	journal = {Advances in Neural Information Processing Systems 32},
	author = {Dauphin, Yann N and Ai, Google and Schoenholz, Samuel S},
	year = {2019},
	pages = {12624--12636},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NYBK8W3D\\metainit_initializing_learning_by_learning_to_initialize.pdf:application/pdf},
}

@article{Shah2000,
	title = {Transforming an {Imperative} {Design} into an {Object}-{Oriented} {Design}},
	volume = {12},
	doi = {10.1016/S1319-1578(00)80001-4},
	abstract = {Most of the traditional and legacy systems were designed using traditional methodologies such as Structured Analysis/Structured Design (SA/SD) methodology. Design of such a system is called an imperative design. After the introduction of the object-oriented technology, there are compelling reasons to redevelop those systems using this new technology to benefit from its merits. To redevelop them, there are two possible choices: either develop them from scratch using some object-oriented methodology, or use the available design documents (i.e., imperative design) of those systems and transform their designs into object-oriented designs. The second choice clearly results in saving both the development cost and time. This paper reports on an effort to build support for the second choice mentioned above. We started our effort in 1992 and proposed a framework of a redesign methodology. Our proposed redesign methodology, i.e., imperative design to object-oriented design (ID-OOD), transforms a given imperative design of an already implemented system into an object-oriented design using the design documents of the system. The methodology works in four phases and they are presented formally. We also illustrate the methodology with a case study.},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Shah, Abad and Mathkour, Hassan},
	year = {2000},
	keywords = {Object Oriented Programming, Software Engineering, Data flow diagram, Entity relationship, ID-OOD methodology, Imperative design, Object-oriented design, Redesign methodology},
	pages = {1--44},
}

@book{simmons1995calculus,
	title = {Calculus with {Analytic} {Geometry}},
	isbn = {978-0-07-114716-3},
	url = {https://books.google.com/books?id=m1Q8AAAACAAJ},
	publisher = {McGraw-Hill},
	author = {Simmons, G F},
	year = {1995},
	note = {Series Title: Schaum's outline series in mathematics and statistics},
}

@article{Wickham2015a,
	title = {Package ‘dplyr'},
	abstract = {A Grammar of Data Manipulation},
	author = {Wickham, Hadley and Francois, Romain and {RStudio}},
	year = {2015},
	pages = {71},
}

@incollection{jack2009,
	title = {State of the {Art} {Recommender} {System}},
	isbn = {978-1-60566-306-7},
	author = {Candillier, Laurent and Jack, Kris and Fessant, Françoise and Meyer, Frank},
	year = {2009},
	doi = {10.4018/978-1-60566-306-7.ch001},
	pages = {1--22},
}

@article{Sadasivan2020,
	title = {Speech {Enhancement} {Using} a {Risk} {Estimation} {Approach}},
	volume = {116},
	issn = {01676393},
	url = {https://doi.org/10.1016/j.specom.2019.11.001},
	doi = {10.1016/j.specom.2019.11.001},
	abstract = {The goal in speech enhancement is to obtain an estimate of clean speech starting from the noisy signal by minimizing a chosen distortion measure (risk). Often, this results in an estimate that depends on the unknown clean signal or its statistics. Since access to such priors is limited or impractical, one has to rely on an estimate of the clean signal statistics. In this paper, we develop a risk estimation framework for speech enhancement, in which one optimizes an unbiased estimate of the risk instead of the actual risk. The estimated risk is expressed solely as a function of the noisy observations and the noise statistics. Hence, the corresponding denoiser does not require the clean speech prior. We consider several speech-specific perceptually relevant distortion measures and develop corresponding unbiased estimates. Minimizing the risk estimates gives rise to denoisers, which are nonlinear functions of the a posteriori SNR. Listening tests show that, within the risk estimation framework, Itakura-Saito and weighted hyperbolic cosine distortions are superior than the other measures. Comparisons in terms of perceptual evaluation of speech quality (PESQ), segmental SNR (SSNR), source-to-distortion ratio (SDR), and short-time objective intelligibility (STOI) also indicate a superior performance for these two distortion measures. For SNRs greater than 5 dB, the proposed approach results in better denoising performance — both in terms of objective and subjective assessment — than techniques based on the Wiener filter, log-MSE minimization, and Bayesian nonnegative matrix factorization.},
	number = {November 2019},
	journal = {Speech Communication},
	author = {Sadasivan, Jishnu and Seelamantula, Chandra Sekhar and Muraka, Nagarjuna Reddy},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Objective and subjective assessment, Perceptual distortion measure, Speech enhancement, Stein's lemma, Unbiased risk estimation},
	pages = {12--29},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WYZ98CHZ\\Speech-Enhancement-Using-a-Risk-Estimation-Approach_2020_Speech-Communicatio.pdf:application/pdf},
}

@article{Dick2013,
	title = {How many random restarts are enough ?},
	abstract = {Many machine learning problems, such as K-means, are non-convex optimization problems. Usually they are solved by performing several local searches with ran-dom initializations. How many searches should be done? Typically a fixed num-ber is performed, but how do we know it was enough? We present a new stopping rule with non-asymptotic frequentist guarantees, which, to our knowledge, no ex-isting rule has. By comparing all stopping rules on various benchmarks, we shed light on their effectiveness in machine-learning problems, including K-means and maximum marginal likelihood parameter selection.},
	author = {Dick, Travis and Wong, Eric and Dann, Christoph},
	year = {2013},
}

@article{Gagolewski2016,
	title = {Package 'stringi': {Character} {String} {Processing} {Facilities}},
	abstract = {Allows for fast, correct, consistent, portable, as well as convenient character string/text processing in every locale and any native encoding. Owing to the use of the ICU library, the package provides R users with platform-independent functions known to Java, Perl, Python, PHP, and Ruby programmers. Among available
features there are: pattern searching (e.g., with ICU Java-like regular expressions or the Unicode Collation Algorithm), random string generation, case mapping, string transliteration, concatenation,
Unicode normalization, date-time formatting and parsing, etc.},
	journal = {CRAN},
	author = {Gagolewski, Marek},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Report2017,
	title = {Best practices report q3 2017},
	author = {Report, Best Practices},
	year = {2017},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CVWDDWD7\\TDWI_BPReport_Q317_Advanced_Analytics.pdf:application/pdf},
}

@article{Gandrud,
	title = {Reproducible {Research} with {R} and {RStudio} {Second} {Edition}},
	author = {Gandrud, Christopher},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8U6ZUHYP\\reproducible-research-with-r-and-rstudio---second-edition.pdf:application/pdf},
}

@article{Thisb,
	title = {Rules of {Machine} {Learning} : {Best} {Practices} for {ML} {Engineering}},
	author = {This, Martin Zinkevich and Guide, Style},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3HVCRJTA\\Rules of Machine Learning.pdf:application/pdf},
}

@techreport{Valverde-Rebaza,
	title = {Job {Recommendation} based on {Job} {Seeker} {Skills}: {An} {Empirical} {Study}},
	url = {https://www.linkedin.com},
	abstract = {In the last years, job recommender systems have become popular since they successfully reduce information overload by generating personal-ized job suggestions. Although in the literature exists a variety of techniques and strategies used as part of job recommender systems, most of them fail to recommending job vacancies that fit properly to the job seekers profiles. Thus, the contributions of this work are threefold, we: i) made publicly available a new dataset formed by a set of job seekers profiles and a set of job vacancies collected from different job search engine sites; ii) put forward the proposal of a framework for job recommendation based on professional skills of job seekers; and iii) carried out an evaluation to quantify empirically the recommendation abilities of two state-of-the-art methods, considering different configurations, within the proposed framework. We thus present a general panorama of job recommendation task aiming to facilitate research and real-world application design regarding this important issue.},
	urldate = {2020-08-15},
	author = {Valverde-Rebaza, Jorge and Puma, Ricardo and Bustios, Paul and Silva, Nathalia C},
	keywords = {person-job fit, Job matching, job recommender systems, job search, job seeking, LinkedIn, word embedding},
}

@misc{WatsonAssistant,
	title = {About {Watson} {Assistant}},
	url = {https://cloud.ibm.com/docs/assistant?topic=assistant-index},
	urldate = {2020-08-15},
}

@book{Vo.T.H2017,
	title = {Python: {End}-to-end {Data} {Analysis}},
	isbn = {978-1-78839-469-7},
	url = {https://www.safaribooksonline.com/library/view/python-end-to-end-data/9781788394697/},
	abstract = {Authors: Phuong Vo. T.H. [and four others]. Cf. Credits page.},
	author = {Vo. T. H, Phuong},
	year = {2017},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\R2FJIABS\\Python End-to-End data Analysis Learning Path.pdf:application/pdf},
}

@article{Zellner1980,
	title = {Posterior odds ratios for selected regression hypotheses},
	volume = {31},
	issn = {0041-0241},
	url = {https://doi.org/10.1007/BF02888369},
	doi = {10.1007/BF02888369},
	abstract = {Bayesian posterior odds ratios for frequently encountered hypotheses about parameters of the normal linear multiple regression model are derived and discussed. For the particular prior distributions utilized, it is found that the posterior odds ratios can be well approximated by functions that are monotonic in usual sampling theoryF statistics. Some implications of this finding and the relation of our work to the pioneering work of Jeffreys and others are considered. Tabulations of odds ratios are provided and discussed.},
	number = {1},
	journal = {Trabajos de Estadistica Y de Investigacion Operativa},
	author = {Zellner, A and Siow, A},
	month = feb,
	year = {1980},
	keywords = {Algorithms, Bayesian},
	pages = {585--603},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AKK9ETHJ\\Posterior Odds Ratios for Selected Regression Hypotheses.pdf:application/pdf},
}

@book{cart84-2,
	address = {Belmont, California, U.S.A.},
	title = {Classification and {Regression} {Trees}},
	publisher = {Wadsworth Publishing Company},
	author = {Breiman, Leo and Friedman, J H and Olshen, R A and Stone, C J},
	year = {1984},
	note = {Series Title: Statistics/Probability Series},
}

@article{Learning2016,
	title = {Introduction to {Probably} {Approximately} {Correct} ( {PAC} ) {Learning} {Empirical} {Risk} {Minimization} ( {ERM} ) as a {Universal} {Learn}- ing {Algorithm} {Additional} {Readings}},
	author = {Learning, Theoretical Machine},
	year = {2016},
	note = {ISBN: 0262111934},
	pages = {2--3},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PA262ZC6\\introduction to pac and erm.pdf:application/pdf},
}

@incollection{LeCunBOM12,
	title = {Efficient {BackProp}.},
	volume = {7700},
	isbn = {978-3-642-35288-1},
	url = {http://dblp.uni-trier.de/db/series/lncs/lncs7700.html#LeCunBOM12},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade} (2nd ed.)},
	publisher = {Springer},
	author = {LeCun, Yann and Bottou, Léon and Orr, Genevieve B and Müller, Klaus-Robert},
	editor = {Montavon, Grégoire and Orr, Genevieve B and Müller, Klaus-Robert},
	year = {2012},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {BackProp Efficient},
	pages = {9--48},
}

@misc{wiki:Differentiable,
	title = {Differentiable {Function}},
	author = {{Wikipedia Contributors}},
}

@article{Leamer1978,
	title = {Specification {Searches}: {Ad} {Hoc} {Inference} with {Nonexperimental} {Data}},
	issn = {00222437},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Specification+Searches#7%5Cnhttp://www.anderson.ucla.edu/faculty/edward.leamer/books/specification_searches/specification_searches.htm},
	doi = {doi: 10.2307/2287437},
	abstract = {Section 4.3 p. 100-110, Testing a Point-Null Hypothesis Against\${\textbackslash}\$na Composite Alternative. 4.4 Weighted Likelihoods: conjugate\${\textbackslash}\$npriors. Bayes factors. Diffuse priors.},
	journal = {SERBIULA (sistema Librum 2.0)},
	author = {Leamer, Edward E},
	year = {1978},
	pmid = {3126},
	note = {ISBN: 0471015202},
	keywords = {Algorithms, Bayesian},
	pages = {370},
}

@article{Yang2017,
	title = {{VISTopic}: {A} visual analytics system for making sense of large document collections using hierarchical topic modeling},
	volume = {1},
	issn = {2468502X},
	url = {http://dx.doi.org/10.1016/j.visinf.2017.01.005},
	doi = {10.1016/j.visinf.2017.01.005},
	abstract = {Effective analysis of large text collections remains a challenging problem given the growing volume of available text data. Recently, text mining techniques have been rapidly developed for automatically extracting key information from massive text data. Topic modeling, as one of the novel techniques that extracts a thematic structure from documents, is widely used to generate text summarization and foster an overall understanding of the corpus content. Although powerful, this technique may not be directly applicable for general analytics scenarios since the topics and topic–document relationship are often presented probabilistically in models. Moreover, information that plays an important role in knowledge discovery, for example, times and authors, is hardly reflected in topic modeling for comprehensive analysis. In this paper, we address this issue by presenting a visual analytics system, VISTopic, to help users make sense of large document collections based on topic modeling. VISTopic first extracts a set of hierarchical topics using a novel hierarchical latent tree model (HLTM) (Liu et al., 2014). In specific, a topic view accounting for the model features is designed for overall understanding and interactive exploration of the topic organization. To leverage multi-perspective information for visual analytics, VISTopic further provides an evolution view to reveal the trend of topics and a document view to show details of topical documents. Three case studies based on the dataset of IEEE VIS conference demonstrate the effectiveness of our system in gaining insights from large document collections.},
	number = {1},
	journal = {Visual Informatics},
	author = {Yang, Yi and Yao, Quanming and Qu, Huamin},
	year = {2017},
	note = {Publisher: Elsevier B.V.},
	keywords = {Text visualization, Topic-modeling, Visual analytics},
	pages = {40--47},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DWR99J5Z\\VISTopic--A-visual-analytics-system-for-making-sense-of-large_2017_Visual-In.pdf:application/pdf},
}

@article{Domingos2012,
	title = {A few useful things to know about machine learning},
	volume = {55},
	issn = {00010782},
	doi = {10.1145/2347736.2347755},
	abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. 15 Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell 16 and Witten et al. 24). However, much of the "folk knowledge" that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article. ©2012 ACM.},
	number = {10},
	journal = {Communications of the ACM},
	author = {Domingos, Pedro},
	year = {2012},
	pages = {78--87},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3HS4F5VF\\A Few Useful Things to Know about Machine Learning.pdf:application/pdf},
}

@book{Xie2013,
	title = {knitr: {Elegant}, flexible and fast dynamic report generation with {R} {\textbar} knitr},
	isbn = {1-4822-0353-7},
	url = {http://yihui.name/knitr/},
	abstract = {Suitable for both beginners and advanced users, this book shows you how to write reports in simple languages such as Markdown. The reports range from homework, projects, exams, books, blogs, and web pages to any documents related to statistical graphics, computing, and data analysis. While familiarity with LaTeX and HTML is helpful, the book requires no prior experience with advanced programs or languages. For beginners, the text provides enough features to get started on basic applications. For power users, the last several chapters enable an understanding of the extensibility of the knitr package.},
	author = {Xie, Yihui},
	year = {2013},
	keywords = {Applications, Natural Language Processing},
}

@incollection{Mandelbrot61,
	address = {New York},
	title = {On the theory of word frequencies and on related {Markovian} models of discourse},
	booktitle = {Structures of {Language} and its {Mathematical} {Aspects}},
	publisher = {American Mathematical Society},
	author = {Mandelbrot, B B},
	editor = {Jacobsen, R},
	year = {1961},
	keywords = {Applications, Natural Language Processing, bibtex-import},
}

@article{doi:10.1080/0092623X.2016.1178675,
	title = {Prevalence of {Experiences} {With} {Consensual} {Nonmonogamous} {Relationships}: {Findings} {From} {Two} {National} {Samples} of {Single} {Americans}},
	volume = {43},
	url = {http://dx.doi.org/10.1080/0092623X.2016.1178675},
	doi = {10.1080/0092623X.2016.1178675},
	number = {5},
	journal = {Journal of Sex \& Marital Therapy},
	author = {Haupert, M L and Gesselman, Amanda N and Moors, Amy C and Fisher, Helen E and Garcia, Justin R},
	year = {2017},
	note = {Publisher: Routledge},
	pages = {424--440},
}

@techreport{Lee2016,
	title = {Gradient {Descent} {Converges} to {Minimizers}},
	abstract = {We show that gradient descent converges to a local minimizer, almost surely with random initializa-tion. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.},
	author = {Lee, Jason D and Simchowitz, Max and Jordan ♯ †, Michael I and Recht, Benjamin},
	year = {2016},
	note = {arXiv: 1602.04915v2},
	keywords = {()},
}

@article{Shannon,
	title = {A {Mathematical} {Theory} of {Communication}},
	volume = {27},
	author = {Shannon, C E},
	keywords = {Applications, Natural Language Processing},
	pages = {379--423},
}

@article{Wu2019,
	title = {Demystifying {Learning} {Rate} {Policies} for {High} {Accuracy} {Training} of {Deep} {Neural} {Networks}},
	doi = {10.1109/BigData47090.2019.9006104},
	abstract = {Learning Rate (LR) is an important hyper-parameter to tune for effective training of deep neural networks (DNNs). Even for the baseline of a constant learning rate, it is non-trivial to choose a good constant value for training a DNN. Dynamic learning rates involve multi-step tuning of LR values at various stages of the training process and offer high accuracy and fast convergence. However, they are much harder to tune. In this paper, we present a comprehensive study of 13 learning rate functions and their associated LR policies by examining their range parameters, step parameters, and value update parameters. We propose a set of metrics for evaluating and selecting LR policies, including the classification confidence, variance, cost, and robustness, and implement them in LRBench, an LR benchmarking system. LRBench can assist end-users and DNN developers to select good LR policies and avoid bad LR policies for training their DNNs. We tested LRBench on Caffe, an open source deep learning framework, to showcase the tuning optimization of LR policies. Evaluated through extensive experiments, we attempt to demystify the tuning of LR policies by identifying good LR policies with effective LR value ranges and step sizes for LR update schedules.},
	journal = {Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019},
	author = {Wu, Yanzhao and Liu, Ling and Bae, Juhyun and Chow, Ka Ho and Iyengar, Arun and Pu, Calton and Wei, Wenqi and Yu, Lei and Zhang, Qi},
	year = {2019},
	note = {arXiv: 1908.06477
ISBN: 9781728108582},
	keywords = {Training, Neural Networks, Deep Learning, Learning Rates},
	pages = {1971--1980},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5TK5C277\\Demystifying Learning Rate Policies for High Accuracy Training of Deep Neural Networks.pdf:application/pdf},
}

@article{Drucker,
	title = {Trailers tease {Hollywood}'s upcoming blockbusters and {Oscar}-season favorites},
	url = {http://www.tuftsdaily.com/trailers-tease-hollywood-s-upcoming-blockbusters-and-oscar-season-favorites-1.2383918},
	journal = {Tufts Daily},
	author = {Drucker, Zach},
	keywords = {Algorithms, Bayesian},
}

@article{Felice2012,
	title = {Linguistic {Indicators} for {Quality} {Estimation} of {Machine} {Translations}},
	number = {May},
	journal = {Social Sciences},
	author = {Felice, Mariano},
	year = {2012},
}

@article{Jarabo2017,
	title = {Recent advances in transient imaging: {A} computer graphics and vision perspective},
	volume = {1},
	issn = {2468502X},
	url = {http://dx.doi.org/10.1016/j.visinf.2017.01.008},
	doi = {10.1016/j.visinf.2017.01.008},
	abstract = {Transient imaging has recently made a huge impact in the computer graphics and computer vision fields. By capturing, reconstructing, or simulating light transport at extreme temporal resolutions, researchers have proposed novel techniques to show movies of light in motion, see around corners, detect objects in highly-scattering media, or infer material properties from a distance, to name a few. The key idea is to leverage the wealth of information in the temporal domain at the pico or nanosecond resolution, information usually lost during the capture-time temporal integration. This paper presents recent advances in this field of transient imaging from a graphics and vision perspective, including capture techniques, analysis, applications and simulation.},
	number = {1},
	journal = {Visual Informatics},
	author = {Jarabo, Adrian and Masia, Belen and Marco, Julio and Gutierrez, Diego},
	year = {2017},
	note = {arXiv: 1611.00939
Publisher: Elsevier B.V.},
	keywords = {Time-of-flight, Transient imaging, Ultrafast imaging},
	pages = {65--79},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SVJQJ5TF\\Recent-advances-in-transient-imaging--A-computer-graphics-_2017_Visual-Infor.pdf:application/pdf},
}

@article{Information,
	title = {Ways to get ready for the future of machine learning},
	author = {Information, T H E and For, Source and Data, T H E},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QUVNCXL7\\5 Ways To Get Ready For The Future Of Machine Learning.pdf:application/pdf},
}

@article{Porter1993,
	title = {Introduction to {Linear} {Regression} {Analysis}},
	volume = {35},
	issn = {00401706},
	url = {http://people.duke.edu/%7B~%7Drnau/regintro.htm},
	doi = {10.2307/1269673},
	number = {2},
	journal = {Technometrics},
	author = {Porter, Donald R and Montgomery, Douglas C and Peck, Elizabeth A},
	year = {1993},
	keywords = {Algorithms, Regression, Linear Regression},
	pages = {224},
}

@article{HowNormalizeControl2016,
	title = {How to normalize to control when control value is 0 ?},
	volume = {5},
	year = {2016},
	pages = {3--6},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\68RUSCAG\\Howtonormalizetocontrolwhencontrolvalueis0.pdf:application/pdf},
}

@article{Business2011,
	title = {Marketing {Strategy} for {Medical} {Devices}},
	author = {Business, Faculty O F},
	year = {2011},
}

@article{Roughgarden2017,
	title = {Algorithms {Illuminated} {Part} 1 : {The} {Basics}},
	volume = {53},
	abstract = {First edition. Includes index. "Algorithms are the heart and soul of computer science. Their applications range from network routing and computational genomics to publick-key cryptography and database system implementation. Studying algorithms can make make you a better programmer, a clearer thinker, and a master of technical interviews. Algorithms Illuminated is an accessible introduction to the subject--a transcript of of one-on-one lessons. Part 1 covers asymptotic analysis and big-O notation, divide-and-conquer algorithms and the master method, randomized algorithms, and several famous algorithms for sorting and selection."--Back cover. Part 1 The basics.},
	number = {95},
	journal = {Soundlikeyourself Publishing, LLC (September 25, 2017)},
	author = {Roughgarden, Tim},
	year = {2017},
	note = {ISBN: 978-0-9992829-1-5},
	pages = {216},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FQQCJBRM\\Algorithms Illuminated.pdf:application/pdf},
}

@article{Toutanova2003,
	title = {Feature-rich part-of-speech tagging with a cyclic dependency network},
	url = {http://dl.acm.org/citation.cfm?id=1073478},
	doi = {10.3115/1073445.1073478},
	abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) ﬁne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24\% accuracy on the Penn Treebank WSJ, an error reduction of 4.4\% on the best previous single automatically learned tagging result},
	journal = {In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL '03),},
	author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D},
	year = {2003},
	keywords = {Applications, Natural Language Processing},
	pages = {252--259},
}

@inproceedings{Li2014,
	title = {Efficient mini-batch training for stochastic optimization},
	isbn = {978-1-4503-2956-9},
	url = {http://dx.doi.org/10.1145/2623330.2623612},
	doi = {10.1145/2623330.2623612},
	abstract = {Stochastic gradient descent (SGD) is a popular technique for large-scale optimization problems in machine learning. In order to parallelize SGD, minibatch training needs to be employed to reduce the communication cost. However, an increase in minibatch size typically decreases the rate of convergence. This paper introduces a technique based on approximate optimization of a conservatively regular-ized objective function within each minibatch. We prove that the convergence rate does not decrease with increasing minibatch size. Experiments demonstrate that with suitable implementations of approximate optimization, the resulting algorithm can outperform standard SGD in many scenarios.},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} '14},
	author = {Li, Mu and Zhang, Tong and Chen, Yuqiang and Smola, Alexander J},
	year = {2014},
	pmid = {25497547},
	note = {arXiv: 1206.5533
ISSN: 03029743},
	pages = {661--670},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SVZLPF8V\\Efficient Mini-batch Training for Stochastic Optimization.pdf:application/pdf},
}

@article{Klakow1998a,
	title = {Log-linear interpolation of language models},
	url = {http://www.mirlab.org/conference_papers/International_Conference/ICSLP},
	abstract = {Building probabilistic models of language is a central task in natural language and speech processing allowing to integrate the syntactic and/or semantic (and recently pragmatic) constraints of the language into the systems. Probabilistic language models are an attractive alternative to the more traditional rule-based systems, such as context free grammars, because of the recent availability of massive amount of text corpora which can be used to efficiently train the models and because instead of binary grammaticality judgement offered by the rule-based systems, likelihood of any sequence of lexical units can be obtained, which is a crucial factor in such tasks as speech recognition. Probabilistic language models also find their application in part-of-speech tagging, machine translation, semantic disambiguation and numerous other fields. The most widely used language models are based on the estimation of the proba-bility of observing a given lexical unit conditioned on the observations of n−1 preced-ing lexical units, and are known as n-gram models. When the n-gram estimates are poor, whatever the reason for that may be, a technique called smoothing is applied to adjust the estimates and hopefully produce more accurate model. Smoothing techniques may be roughly divided into the backing-off and interpolation. In the first case, the best n-gram model in the current context is selected, whereas in the second case all the n-gram models of different specificities are combined together to form a better predictor. In this thesis, a recently proposed novel interpolation scheme is investigated, namely, the log-linear interpolation. Unlike the original publication, however, which dealt with combining the models of unrelated nature, the aim of this thesis is to formulate the theoretical framework for smoothing the n-gram probability estimates obtained from similar language models with different levels of specificity on the same corpus, which will be called log-linear n-gram smoothing, and compare it to the well-established linear interpolation and back-off methods. The framework being proposed includes probability combination, parameter optimisation, dealing with data sparsity and parameter clustering. The resulting technique is shown to outperform the conventional linear interpo-lation and back-off techniques when applied to the n-gram smoothing tasks. ii},
	number = {January},
	journal = {Proc. ICSLP},
	author = {Klakow, Dietrich},
	year = {1998},
	keywords = {Applications, Natural Language Processing},
	pages = {1--4},
}

@article{TransformerDesignPattern2007,
	title = {The {Transformer} {Design} {Pattern}},
	volume = {370},
	abstract = {Last time, we looked in detail at the Visitor design pattern. Recall that the idea there was to figure out a way to traverse a structured object, and " do something " at every element of the structured object that we traverse. We did this by decoupling the traversal from the action to be performed at every element. The idea being that we only need to implement the traversal code once, in the classes representing the objects to be traversed, and we then only need to parameterized the traversal by an object that captured what actions should be performed at each element. One limitation we saw last time was that the traversal did not return any value. Indeed, the accept method implementing the traversal has a void return type. This means that the only thing really we can during a traversal is perform a side-effecting operation. For example, we saw that this works perfectly well for printing out something for every element we traverse. If we actually need to return a value from the traversal, however, things were a bit uglier. The one example we had was returning the minimum value in a binary tree. We did so by defining an instance variable in the visitor that would hold the minimal value we had seen until now in the traversal, and updating that instance variable everytime we encountered a node with a smaller value. Once the traversal was done, we could access the instance variable through the method getMin of the visitor and extract the minimum value that way. The purpose of this lecture is to show how the Visitor pattern we saw last time could be generalized to return a value from the traversal.},
	journal = {CSU},
	year = {2007},
	keywords = {Object Oriented Programming, Software Engineering},
}

@techreport{DattaGupta2019,
	title = {A {Survey} on {Recommender} {System}},
	url = {http://www.ripublication.com},
	abstract = {Recommender systems have gained its importance because of the availability of enormous online information. In current time, deep learning has gained appreciable attention in many researches such as natural language processing, artificial intelligence due to high performance and great learning feature representations. The effect of deep learning is also persistent, lately showing its usefulness when put to retrieval of information and recommenders work which eventually have resulted in the flourish of deep learning approaches in recommender system. Hybrid approaches for designing recommender models have been gaining popularity in recent years. The paper aims in giving a comprehensive insight of recent research works on recommender systems.},
	author = {Datta Gupta, Koyel},
	year = {2019},
	note = {Publication Title: International Journal of Applied Engineering Research
Volume: 14},
	keywords = {collaborative, hybrid technique, Recommender system, content-based},
	pages = {3274--3277},
	file = {A Survey on Recommender System.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\A Survey on Recommender System.pdf:application/pdf},
}

@misc{Wiktionary2016,
	title = {Appendix:{English} internet slang - {Wiktionary}},
	url = {https://en.wiktionary.org/wiki/Appendix:English_internet_slang},
	author = {{Wiktionary}},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Shortis2007,
	title = {The {Creativity} of {Text} {Spelling}},
	number = {June},
	author = {Shortis, Tim},
	year = {2007},
	pages = {21--26},
}

@article{Evert2005,
	title = {The {Statistics} of {Word} {Cooccurrences} {Word} {Pairs} and {Collocations}},
	volume = {98},
	issn = {00278424},
	url = {http://en.scientificcommons.org/19948039},
	doi = {10.1073/pnas.141413598},
	abstract = {You shall know a word by the company it keeps! With this slogan, Firth (1957) drew attention to a fact that language scholars had intuitively known for a long time: In natural language, words are not combined randomly into phrases and sentences, con- strained only by the rules of syntax. The particular ways in which they go together are a rich and important source of information both about language and about the world we live in. In the 1930s, J. R. Firth coined the term collocations for such char- acteristic, or habitual word combinations (as he called them). While Firth used to be lamentably vague about his precise understanding of this concept (cf. Lehr 1996, 21), the term itself and the general idea behind it that collocations correspond to some conventional way of saying things (Manning and Schütze 1999, 151) were eagerly taken up by researchers in various fields, leading to the serious terminolog- ical confusion that surrounds the concept of collocations today. As Choueka puts it: even though any two lexicographers would agree that once upon a time, hit the road and similar idioms are collocations, they would most certainly disagree on al- most anything else (Choueka 1988, 4). Feel free to replace lexicographers with any profession that is concerned with language data.},
	number = {August 2004},
	journal = {Unpublished doctoral dissertation Institut fur maschinelle Sprachverarbeitung Universitat Stuttgart},
	author = {Evert, Stefan},
	year = {2005},
	pmid = {11447261},
	note = {ISBN: 0027-8424 (Print)},
	keywords = {Applications, Natural Language Processing},
	pages = {353},
}

@misc{Guta2019,
	title = {Phishing {Statistics}: {What} an {Attack} {Costs} {Your} {Business} [{INFOGRAPHIC}] - {Small} {Business} {Trends}},
	url = {https://www.inky.com/blog/what-a-phishing-attack-costs-your-business},
	urldate = {2020-05-10},
	author = {Guta, Michael},
	year = {2019},
	note = {Pages: 1
Publication Title: July 12 2019},
}

@article{Package2020a,
	title = {Package ‘ dlookr '},
	author = {Package, Type},
	year = {2020},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HZ7GQNJ3\\dlookr.pdf:application/pdf},
}

@article{Werbos1990,
	title = {Backpropagation {Through} {Time}: {What} {It} {Does} and {How} to {Do} {It}},
	volume = {78},
	issn = {15582256},
	doi = {10.1109/5.58337},
	abstract = {Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for back-propagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, it describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule forordered derivatives—the theorem which underlies backpropagation—is briefly discussed. ©1990, IEEE},
	number = {10},
	journal = {Proceedings of the IEEE},
	author = {Werbos, Paul J},
	year = {1990},
	pages = {1550--1560},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TLDR4WL9\\Backpropagation Through Time.pdf:application/pdf},
}

@inproceedings{Caruana,
	title = {An empirical evaluation of supervised learning in high dimensions},
	isbn = {978-1-60558-205-4},
	url = {http://yann.lecun.com/exdb/mnist/},
	doi = {10.1145/1390156.1390169},
	abstract = {In this paper we perform an empirical evaluation of supervised learning on high-dimensional data. We evaluate performance on three metrics: accuracy, AUC, and squared loss and study the effect of increasing dimensionality on the performance of the learning algorithms. Our findings are consistent with previous studies for problems of relatively low dimension, but suggest that as dimensionality increases the relative performance of the learning algorithms changes. To our surprise, the method that performs consistently well across all dimensions is random forests, followed by neural nets, boosted trees, and SVMs.},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning - {ICML} '08},
	author = {Caruana, Rich and Karampatziakis, Nikos and Yessenalina, Ainur},
	year = {2008},
	pmid = {17255001},
	note = {arXiv: cs/9605103
ISSN: 9781605582054},
	pages = {96--103},
}

@article{Cavalieri2015a,
	title = {Combination of {Language} {Models} for {Word} {Prediction}: {An} {Exponential} {Approach}},
	volume = {0},
	url = {http://www.ieee.org/publications_standards/publications/rights/index.html},
	doi = {10.1109/TASLP.2016.2547743},
	abstract = {—This paper proposes an exponential interpolation to merge a part-of-speech-based language model and a word-based n-gram language model to accomplish word prediction tasks. In order to find a set of mathematical equations to properly describe the language modeling, a model based on partial differential equations is proposed. With the appropriate initial conditions, it was found an interpolation model similar to the traditional maximum entropy language model. Improvements in keystroke saved and perplexity over the word-based n-gram language model and two other traditional interpolation models is obtained, considering three different languages. The proposed interpolation model also provides additional improvement in hit rate parameter.},
	number = {0},
	journal = {Transactions on Audio, Speech, and Language Processing IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
	author = {Cavalieri, Daniel C and Palazuelos-Cagigas, Sira E and Bastos-Filho, Teodiano F and Sarcinelli-Filho, Mário},
	year = {2015},
	keywords = {Applications, Natural Language Processing, word prediction, Index Terms—Natural language processing, combination of language models},
}

@article{Caffo2015b,
	title = {Developing {Data} {Products} in {R}},
	abstract = {This book introduces the topic of Developing Data Products in R. A data product is the ideal output of a Data Science experiment. This book is based on the Coursera Class "Developing Data Products" as part of the Data Science Specialization. Particular emphasis is paid to developing Shiny apps and interactive graphics.},
	journal = {R Software},
	author = {Caffo, Brian},
	year = {2015},
	pages = {52},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZRZL9IBH\\ddp.pdf:application/pdf},
}

@misc{ListEnglishContractions,
	title = {List of {English} {Contractions}},
	url = {http://www.oxforddictionaries.com/us/definition/english/amn't},
	urldate = {2016-11-25},
	note = {Publication Title: Oxford Dictionaries},
}

@article{Whittaker2001,
	title = {Quantization-based language model compression.},
	url = {http://www.merl.com},
	abstract = {This paper describes two techniques for reducing the size of statistical back-off -gram language models in computer memory. Language model compression is achieved through a combination of quantizing language model probabilities and back-off weights and the pruning of parameters that are determined to be unnecessary after quantization. The recognition performance of the original and compressed language models is eval-uated across three different language models and two different recognition tasks. The results show that the language models can be compressed by up to 60\% of their original size with no significant loss in recognition performance. Moreover, the techniques that are described provide a principled method with which to compress language models further while minimising degradation in recognition performance. Abstract This paper describes two techniques for reducing the size of statistical back-off ¤ -gram language models in computer mem-ory. Language model compression is achieved through a combi-nation of quantizing language model probabilities and back-off weights and the pruning of parameters that are determined to be unnecessary after quantization. The recognition performance of the original and compressed language models is evaluated across three different language models and two different recog-nition tasks. The results show that the language models can be compressed by up to 60\% of their original size with no signifi-cant loss in recognition performance. Moreover, the techniques that are described provide a principled method with which to compress language models further while minimising degrada-tion in recognition performance.},
	journal = {Interspeech},
	author = {Whittaker, E W D and Raj, Bhiksha},
	year = {2001},
	note = {ISBN: 8790834100},
	keywords = {Applications, Natural Language Processing},
	pages = {2--5},
}

@article{Bunnell2015,
	title = {Intentions to smoke cigarettes among never-smoking {US} middle and high school electronic cigarette users: {National} youth tobacco survey, 2011-2013},
	volume = {17},
	issn = {1469994X},
	url = {https://academic.oup.com/ntr/article-lookup/doi/10.1093/ntr/ntu166},
	doi = {10.1093/ntr/ntu166},
	abstract = {INTRODUCTION: Electronic cigarette (e-cigarette) use is increasing rapidly, and the impact on youth is unknown. We assessed associations between e-cigarette use and smoking intentions among US youth who had never smoked conventional cigarettes.\${\textbackslash}\$n\${\textbackslash}\$nMETHODS: We analyzed data from the nationally representative 2011, 2012, and 2013 National Youth Tobacco Surveys of students in grades 6-12. Youth reporting they would definitely not smoke in the next year or if offered a cigarette by a friend were defined as not having an intention to smoke; all others were classified as having positive intention to smoke conventional cigarettes. Demographics, pro-tobacco advertisement exposure, ever use of e-cigarettes, and ever use of other combustibles (cigars, hookah, bidis, kreteks, and pipes) and noncombustibles (chewing tobacco, snuff, dip, snus, and dissolvables) were included in multivariate analyses that assessed associations with smoking intentions among never-cigarette-smoking youth.\${\textbackslash}\$n\${\textbackslash}\$nRESULTS: Between 2011 and 2013, the number of never-smoking youth who used e-cigarettes increased 3-fold, from 79,000 to more than 263,000. Intention to smoke conventional cigarettes was 43.9\% among ever e-cigarette users and 21.5\% among never users. Ever e-cigarette users had higher adjusted odds for having smoking intentions than never users (adjusted odds ratio = 1.70, 95\% confidence interval = 1.24-2.32). Those who ever used other combustibles, ever used noncombustibles, or reported pro-tobacco advertisement exposure also had increased odds for smoking intentions.\${\textbackslash}\$n\${\textbackslash}\$nCONCLUSION: In 2013, more than a quarter million never-smoking youth used e-cigarettes. E-cigarette use is associated with increased intentions to smoke cigarettes, and enhanced prevention efforts for youth are important for all forms of tobacco, including e-cigarettes.},
	number = {2},
	journal = {Nicotine and Tobacco Research},
	author = {Bunnell, Rebecca E and Agaku, Israel T and Arrazola, René A and Apelberg, Benjamin J and Caraballo, Ralph S and Corey, Catherine G and Coleman, Blair N and Dube, Shanta R and King, Brian A},
	month = feb,
	year = {2015},
	pmid = {25143298},
	note = {ISBN: 1462-2203},
	pages = {228--235},
}

@article{BanditMachinePlay2017,
	title = {Bandit {Machine} 2 {Play} {Posterior} {Probability} {Calculation}},
	year = {2017},
	pages = {1--2},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KBQWRSKD\\Bayesian Infernce - Week1Lab_BanditMachineExample.pdf:application/pdf},
}

@article{Jimarkon,
	title = {Using {Quantitative} {Methods} as a {Framework} for {Qualitative} {Analyses}},
	abstract = {Much of the literature on research in applied linguistics views quantitative and qualitative research as distinct entities embodying contrasting philosophies. In this paper, however, we present an example of how preliminary quantitative analyses of data can inform a qualitative discourse analysis study. Data from an online discussion forum concerning the Thai political crisis of 2010 were initially analysed quantitatively to identify keywords, word clusters, length of postings and user ratings of postings for the contributions from the opposing political factions. Each posting was also rated for level of antagonism and credibility of argumentation. These quantitative data provide an overview of the discussion forum and the patterns of discussion within it which was then used as a framework to guide the main qualitative analysis ensuring that the key revealed meanings and functions were covered in the analysis and reducing potential bias in data analysis and presentation. Introduction Mixed methods research has a long history in disciplines that attempt to explain behavior and social phenomena (Dörnyei, 2007). The practice includes a mix or qualitative and quantitative methods, a mix of quantitative methods or a mix of qualitative methods. The type of mixed methods approach that is most popular and is increasingly employed is the first, the mix of the two, which are often based on different research paradigms. Single method research is normally criticised by their opposition as inferior and insufficient. In a pure quantitative study, with the focus on theory or hypothesis testing, the researcher may not be sensitive on contextual details. Moreover, it requires a large amount of data to be able to give an effective ground. A qualitative method, on the other hand, is prone to high subjectivity of the researcher and is unlikely to be generalisable. It can only deal with a small size of data, which makes decision making of the overview and conclusive deduction an ordeal. Four models of mixed methods design are proposed including concurrent design, explanatory sequential design, exploratory sequential design and embedded sequential/concurrent designs (Creswell \& Zhang, 2009). The first model, the concurrent design compares and contrasts the results between the two methods to present evidence. Second, the explanatory sequential design utilises the explanation of one set of results to support the other's. In an exploratory sequential design, one method's results are followed by the other's to strengthen the claims made, in the name of generalisability, for instance. In the last design, embedded sequential/concurrent design, a small database is made part of the big database and is used to experiment or enhance the major findings. Typically, a mixed method research deals with different types of data but it may also mean applying different methods to investigate the same data. Two dimensions of advancements that mixed methods data analyses (MMDA) have to offer can be considered as 1) the design virtue and b) research expertise. The research virtue obtained from mixed methods may refer to the strategies that are used to display trustworthiness of the research such as triangulation, complementarity, development, initiation and expansion (Green et al, 1989). In greater details, four major advantages of MMDA have been put forward (Dörnyei, 2007). First, oversimplification, decontexualisation and reduction of the quantitative analysis can be disputed by in depth meaningful qualitative analysis, while content-specificity and unrepresentativeness can be overcome by the generalisable quantitative analysis. Second, for multi-level analyses, MMDA can add more meaning by converging numbers into words and vice versa. Third, triangulation through multi-methods analyses means increasing validity of the study of the results. Fourth, a study with mixed methods analyses tends to attract more attention from the},
	author = {Jimarkon, Pattamawan and Todd, Richard Watson},
	keywords = {Applications, Natural Language Processing},
}

@article{Griffiths2011,
	title = {Predicting the future as {Bayesian} inference: {People} combine prior knowledge with observations when estimating duration and extent},
	volume = {140},
	issn = {00963445},
	doi = {10.1037/a0024899},
	abstract = {Predicting the future is a basic problem that people have to solve every day and a component of planning, decision making, memory, and causal reasoning. In this article, we present 5 experiments testing a Bayesian model of predicting the duration or extent of phenomena from their current state. This Bayesian model indicates how people should combine prior knowledge with observed data. Comparing this model with human judgments provides constraints on possible algorithms that people might use to predict the future. In the experiments, we examine the effects of multiple observations, the effects of prior knowledge, and the difference between independent and dependent observations, using both descriptions and direct experience of prediction problems. The results indicate that people integrate prior knowledge and observed data in a way that is consistent with our Bayesian model, ruling out some simple heuristics for predicting the future. We suggest some mechanisms that might lead to more complete algorithmic-level accounts. ©2011 American Psychological Association.},
	number = {4},
	journal = {Journal of Experimental Psychology: General},
	author = {Griffiths, Thomas L and Tenenbaum, Joshua B},
	year = {2011},
	pmid = {21875247},
	keywords = {Bayesian inference, Heuristics, Mathematical modeling, Predicting the future},
	pages = {725--743},
}

@article{Raftery1988,
	title = {{INFERENCE} {FOR} {THE} {BINOMIAL} {N}-{PARAMETER} - {A} {HIERARCHICAL} {BAYES} {APPROACH}},
	volume = {75},
	doi = {10.1093/biomet/75.2.223},
	number = {2},
	journal = {Biometrika},
	author = {Raftery, Adrian E},
	year = {1988},
	note = {ISBN: 0006-3444},
	keywords = {Algorithms, Bayesian},
	pages = {223--228},
}

@article{Strang2005,
	title = {Linear algbera and its applications},
	abstract = {Renowned professor and author Gilbert Strang demonstrates that linear algebra is a fascinating subject by showing both its beauty and value. While the mathematics is there, the effort is not all concentrated on proofs. Strang's emphasis is on understanding. He explains concepts, rather than deduces. This book is written in an informal and personal style and teaches real mathematics. The gears change in Chapter 2 as students reach the introduction of vector spaces. Throughout the book, the theory is motivated and reinforced by genuine applications, allowing pure mathematicians to teach applied mathematics.},
	author = {Strang, Gilbert},
	year = {2005},
	note = {ISBN: 0030105676},
	pages = {544},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I9NGETJ2\\Linear Algebra and its Applications.pdf:application/pdf},
}

@book{Zipf:36,
	address = {London},
	title = {The {Psychobiology} of {Language}: {An} {Introduction} to {Dynamic} {Philology}},
	publisher = {Routledge},
	author = {Zipf, George K},
	year = {1936},
	keywords = {Applications, Natural Language Processing},
}

@article{Connor2014,
	title = {Lecture 22 {Exploratory} {Text} {Analysis} \& {Topic} {Models}},
	author = {Connor, Brendan O},
	year = {2014},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F3ZH9UQK\\Exploratory Text Analysis & Topic Models.pdf:application/pdf},
}

@article{Ouyang2018,
	title = {Intelligent {Straggler} {Mitigation} in {Massive}-{Scale} {Computing} {Systems}},
	abstract = {In order to satisfy increasing demands for Cloud services, modern computing systems are often massive in scale, typically consisting of hundreds to thousands of heterogeneous machine nodes. Parallel computing frameworks such as MapReduce are widely deployed over such cluster infrastructure to provide reliable yet prompt services to customers. How- ever, complex characteristics of Cloud workloads, including multi-dimensional resource requirements and highly changeable system environments, e.g. dynamic node perfor- mance, are introducing new challenges to service providers in terms of both customer ex- perience and system efficiency. One primary challenge is the straggler problem, whereby a small subset of the parallelized tasks take abnormally longer execution time in compar- ison with the siblings, leading to extended job response and potential late-timing failure. The state-of-the-art approach to straggler mitigation is speculative execution. Although it has been deployed in several real-world systems with a variety of implementation op- timizations, the analysis from this thesis has shown that speculative execution is often inefficient. According to various production tracelogs of data centers, the failure rate of speculative execution could be as high as 71\%. Straggler mitigation is a complicated problem in its own nature: 1) stragglers may lead to different consequences to parallel job execution, possibly with different degrees of severity, 2) whether a task should be regarded as a straggler is highly subjective, depending upon different application and sys- tem conditions, 3) the efficiency of speculative execution would be improved if dynamic node performance could be modelled and predicted appropriately, and 4) there are other types of stragglers, e.g. those caused by data skews, that are beyond the capability of speculative execution. This thesis starts with a quantitative and rigorous analysis of issues with stragglers, in- cluding their root-causes and impacts, the execution environment running them, and the limitations to their mitigation. Scientific principles of straggler mitigation are investi- gated and new algorithms are developed. An intelligent system for straggler mitigation is then designed and developed, being compatible with the majority of current parallel com- puting frameworks. Combined with historical data analysis and online adaptation, the system is capable of mitigating stragglers intelligently, dynamically judging a task as a straggler and handling it, avoiding current weak nodes, and dealing with data skew, a spe- cial type of straggler, with a dedicated method. Comprehensive analysis and evaluation of the system show that it is able to reduce job response time by up to 55\%, as compared with the speculator used in the default YARN system, while the optimal improvement a speculative-based method may achieve is around 66\% in theory. The system also achieves a much higher success rate of speculation than other production systems, up to 89\%.},
	number = {March},
	author = {Ouyang, Xue},
	year = {2018},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V7NLB8WX\\Intelligent Straggler Mitigation in Massive-Scale Computing Systems.pdf:application/pdf},
}

@article{Chui2018,
	title = {Most of {AI}'s {Business} {Uses} {Will} {Be} in {Two} {Areas}},
	url = {https://www.mckinsey.com/%7B~%7D/media/McKinsey/Business},
	abstract = {This document is authorized for use only by Roberto Donat (rdonat@westpac.com.au). Copying or posting is an infringement of copyright. Please contact customerservice@harvardbusiness.org or 800-988-0886 for additional copies.},
	journal = {Harvard Business Review},
	author = {Chui, Michael and Henke, Nicolaus and Miremadi, Mehdi},
	year = {2018},
	pages = {3--7},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MKJRVEDC\\Most of AI’s Business Uses Will Be in Two Areas.pdf:},
}

@article{Macs2007,
	title = {{DOCTORS} {D} {ON} ' {T} {T} {ELL} {Y} {OU} {The} {Medical} {Detective} :},
	number = {2017},
	author = {Macs, Healthier Big},
	year = {2007},
	keywords = {dodd frank!!},
	pages = {1--11},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XH8X9KIJ\\don_t_decay_the_learning_rate_increase_the_batch_size.pdf:application/pdf},
}

@article{Wilson2003,
	title = {The general inefficiency of batch training for gradient descent learning},
	volume = {16},
	issn = {08936080},
	doi = {10.1016/S0893-6080(03)00138-2},
	abstract = {Gradient descent training of neural networks can be done in either a batch or on-line manner. A widely held myth in the neural network community is that batch training is as fast or faster and/or more 'correct' than on-line training because it supposedly uses a better approximation of the true gradient for its weight updates. This paper explains why batch training is almost always slower than on-line training - often orders of magnitude slower - especially on large training sets. The main reason is due to the ability of on-line training to follow curves in the error surface throughout each epoch, which allows it to safely use a larger learning rate and thus converge with less iterations through the training data. Empirical results on a large (20,000-instance) speech recognition task and on 26 other learning tasks demonstrate that convergence can be reached significantly faster using on-line training than batch training, with no apparent difference in accuracy. ©2003 Elsevier Ltd. All rights reserved.},
	number = {10},
	journal = {Neural Networks},
	author = {Wilson, D Randall and Martinez, Tony R},
	year = {2003},
	keywords = {Optimization, Learning rate, Backpropagation, Batch training, Generalization, Gradient descent, On-line training, Stochastic approximation},
	pages = {1429--1451},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2DF5WWDD\\The general inefficiency of batch training for gradient descent.pdf:application/pdf},
}

@misc{gradient,
	title = {Gradient - {Wikipedia}},
	url = {https://en.wikipedia.org/w/index.php?title=Camera_obscura&oldid=862875479},
	author = {{Wikipedia}},
	year = {2018},
}

@article{Volkswirtschaft2005,
	title = {A {Software} {Framework} for {Data} {Based} {Analysis} {DISSERTATION}},
	author = {Volkswirtschaft, Fach and Fakult, Wirtschaftswissenschaftlichen and Kr, Markus and Pr, Berlin and Dekan, Mlynek and Gutachter, Joachim Schwalbach and Berendt, Bettina},
	year = {2005},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WY5Q44SJ\\A Software Framework for Data Based Analysis.pdf:application/pdf},
}

@techreport{BLS2016,
	title = {Employed persons by detailed occupation and sex , 2015 annual averages},
	url = {https://www.bls.gov/cps/cpsaat11b.htm},
	author = {{Bureau of Labor Statistics}},
	year = {2016},
	keywords = {Applications, Natural Language Processing},
}

@article{Al-Mubaid2007,
	title = {A {Learning}-{Classification} {Based} {Approach} for {Word} {Prediction}},
	volume = {4},
	abstract = {Word prediction is an important NLP problem in which we want to predict the correct word in a given context. Word completion utilities, predictive text entry systems, writing aids, and language translation are some of common word prediction applications. This paper presents a new word prediction approach based on context features and machine learning. The proposed method casts the problem as a learning-classification task by training word predictors with highly discriminating features selected by various feature selection techniques. The contribution of this work lies in the new way of presenting this problem, and the unique combination of a top performer in machine learning, svm, with various feature selection techniques MI, X 2 , and more. The method is implemented and evaluated using several datasets. The experimental results show clearly that the method is effective in predicting the correct words by utilizing small contexts. The system achieved impressive results, compared with similar work; the accuracy in some experiments approaches 91\% correct predictions.},
	number = {3},
	journal = {The International Arab Journal of Information Technology},
	author = {Al-Mubaid, Hisham},
	year = {2007},
	keywords = {Applications, Natural Language Processing, machine learning, word completion, natural language processing, Word prediction},
}

@misc{UnknownUnknown1993Biometrika,
	title = {Unknown - {Unknown} - {1993Biometrika}\_guttman.pdf.pdf},
}

@article{Lepine2011,
	title = {The increasing burden of depression.},
	volume = {7},
	issn = {1178-2021},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21750622},
	doi = {10.2147/NDT.S19617},
	abstract = {Recent epidemiological surveys conducted in general populations have found that the lifetime prevalence of depression is in the range of 10\% to 15\%. Mood disorders, as defined by the World Mental Health and the Diagnostic and Statistical Manual of Mental Disorders, 4th edition, have a 12-month prevalence which varies from 3\% in Japan to over 9\% in the US. A recent American survey found the prevalence of current depression to be 9\% and the rate of current major depression to be 3.4\%. All studies of depressive disorders have stressed the importance of the mortality and morbidity associated with depression. The mortality risk for suicide in depressed patients is more than 20-fold greater than in the general population. Recent studies have also shown the importance of depression as a risk factor for cardiovascular death. The risk of cardiac mortality after an initial myocardial infarction is greater in patients with depression and related to the severity of the depressive episode. Greater severity of depressive symptoms has been found to be associated with significantly higher risk of all-cause mortality including cardiovascular death and stroke. In addition to mortality, functional impairment and disability associated with depression have been consistently reported. Depression increases the risk of decreased workplace productivity and absenteeism resulting in lowered income or unemployment. Absenteeism and presenteeism (being physically present at work but functioning suboptimally) have been estimated to result in a loss of \$36.6 billion per year in the US. Worldwide projections by the World Health Organization for the year 2030 identify unipolar major depression as the leading cause of disease burden. This article is a brief overview of how depression affects the quality of life of the subject and is also a huge burden for both the family of the depressed patient and for society at large.},
	number = {Suppl 1},
	journal = {Neuropsychiatric disease and treatment},
	author = {Lépine, Jean-Pierre and Briley, Mike},
	year = {2011},
	pmid = {21750622},
	note = {Publisher: Dove Press},
	keywords = {DALY, depression, economic burden, epidemiology, family burden, mortality risk},
	pages = {3--7},
}

@article{Niesler,
	title = {{COMBINATION} {OF} {WORD}-{BASED} {AND} {CATEGORY}-{BASED} {LANGUAGE} {MODELS}},
	abstract = {A language model combining word-based and category-based n-grams within a backoff framework is presented. Word n-grams conveniently capture sequential relations between particular words, while the category-model, which is based on part-of-speech classific ations and allows ambiguous category membership, is able to gen-eralise to unseen word sequences and therefore appropriate in back-off situations. Experiments on the LOB, Switchboard and WSJ0 corpora demonstrate that the technique greatly improves language model perplexities for sparse training sets, and offers significantly improved complexity versus performance tradeoffs when compared with standard trigram models.},
	journal = {Appears},
	author = {Niesler, T R and Woodland, P C},
	keywords = {Applications, Natural Language Processing},
}

@misc{Thurlow,
	title = {Generation {Txt}? {The} socialinguistics of youn people's text-messaging},
	abstract = {The so called 'net generation' is popularly assumed to be naturally media literate and to be necessarily reinventing conventional linguistic and communicative practices. With this in mind, this essay centres around discursive analyses of qualitative data arising from an investigation of 159 older teenagers' use of mobile telephone text-messaging - or SMS (i.e. short-messaging services). In particular, against a backdrop of media commentaries, we examine the linguistic forms and communicative functions in a corpus of 544 participants' actual text-messages. While young people are surely using their mobile phones as a novel, creative means of enhancing and supporting intimate relationships and existing social networks, popular discourses about the linguistic exclusivity and impenetrability of this particular technologically-mediated discourse appear greatly exaggerated. Serving the sociolinguistic 'maxims' of (a) brevity and speed, (b) paralinguistic restitution and (c) phonological approximation, young people's messages are both linguistically unremarkable and communicatively adept.},
	author = {Thurlow, Crispin},
}

@article{Evert2004,
	title = {A simple {LNRE} model for random character sequences},
	abstract = {This paper describes a population model for word frequency distributions based on the Zipf-Mandelbrot law, corresponding to the word frequency distribution induced by a random character sequence. The model, which has convenient analytical and numerical properties, is shown to be adequate for the description of language data extracted by automatic means from large text corpora. It can thus be used to study the problems faced by the statistical analysis of such data in the field of natural-language processing.},
	journal = {Proceedings of JADT},
	author = {Evert, Stefan},
	year = {2004},
	note = {ISBN: 2-930344-49-0},
	keywords = {Applications, Natural Language Processing, \$π\$ s, 1 introduction to lexical, 1 this, area of lexical statistics, cooccurrence statistics, is based on random, lexical statistics, lnre models, model assumes a population, most work in the, of types w 1, probabilities \$π\$ 1, random text, s, sampling with replacement, statistics and lnre models, w s with occurrence, zipf-mandelbrot law},
	pages = {1--12},
}

@misc{Turing1939,
	title = {Scientific {Commons}: {Systems} of {Logic} {Based} on {Ordinals}},
	url = {https://pure.mpg.de/rest/items/item_2403325/component/file_2403324/content},
	author = {Turing, A M},
	year = {1939},
}

@article{Ling2005,
	title = {The {Length} of {Text} {Messages} and {Use} of {Predictive} {Texting}: {Who} {Uses} it and {How} {Much} {Do} {They} {Have} to {Say}?},
	volume = {4},
	doi = {10.13140/RG.2.1.1922.6089},
	abstract = {Text messaging–or texting–via mobile telephones has become a fixture in many parts of the world. The ability to cheaply send text messages on a mobile asynchronous basis was adopted first by teens and is now spreading to other parts of the population. This said, texting is not an intuitive process. The interface is difficult to master, and the technology is being pressed into areas for which it was not necessarily intended. It is into this arena that systems of predictive texting have been introduced.},
	number = {l},
	journal = {Association of Internet Researchers},
	author = {Ling, Rich},
	year = {2005},
	keywords = {Applications, Natural Language Processing},
	pages = {1--18},
}

@misc{Ponset,
	title = {Package 'modeest'},
	author = {Ponset, P},
}

@book{Manecki2000,
	title = {Kinetics of aqueous {Pb} reaction with apatites},
	volume = {165},
	isbn = {978-0-262-03384-8},
	abstract = {Apatite has been Used to remediate Pb contamination; apatite dissolution releases phosphate, which combines with Pb to form highly insoluble Pb-phosphate minerals. This research focused on the effects of aqueous Pb (initial [Pbaq] = 0.185 mM) on the kenetics of apatite dissolution. Synthetic microcrystalline hydroxylapatite (HAP) and natural chlorapatite (CAP) and fluorapatite (FAP) were used in batch experiments at 22°C, with pH within the range of 4.2-7.0, and in the presence of aqueous Cl. In these batch experiments, apatites followed linear (zeroth-order) dissolution kinetics. Dissolution experiments were performed using 1 g apatite/L for all three apatites. When dissolution rate constants (kAP) are adjusted for particle specific surface area (As), kCAP {\textgreater} kFAP {\textgreater} kHAP. In the presence of Pbaq and Cl, all three apatites reacted to form pyromorphite (PY; Pb10(PO4)6Cl2). Rates of Pbaq uptake by the apatites decreased in the same order as the apparent (not normalized for As) dissolution rate constants of apatite (kAp°): HAP {\textgreater} CAP {\textgreater} FAP, suggesting that Pbaq uptake is controlled by the total amount of dissolved phosphate in the system. While HAP and CAP removed more than 98\% of Pbaq during 2 weeks of the experiment, FAP decreased the initial [Pbaq] by ∼30\%. Pb uptake rates calculated on a molar basis correlated with Ca release rates. Concentration of dissolved phosphate during the reaction with Pbaq was below the detection limit of 10-7 mol/L. Phosphate concentration was probably controlled by solute equilibrium with precipitating PY, which has very low solubility (log Ksp = -167). This indicates that the rate-controlling step was apatite dissolution. The presence of Pbaq increased apatite batch dissolution rates, most probably because formation of PY acted as a sink for dissolved phosphate, hence increasing the thermodynamic drive for dissolution. Although PY formed heterogeneously on the surfaces of apatite particles, the PY did not prevent continued apatite dissolution.},
	author = {Manecki, Maciej and Maurice, Patricia A. and Traina, Samuel J.},
	year = {2000},
	doi = {10.1097/00010694-200012000-00002},
	note = {Publication Title: Soil Science
Issue: 12
ISSN: 0038075X},
	keywords = {Apatite, Dissolution, Hydrochemistry, Kenetics, Lead, Pyromorphite},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6AFGE23A\\Introduction to Algorithms.pdf:application/pdf},
}

@article{Raschka2018b,
	title = {Model {Evaluation}, {Model} {Selection}, and {Algorithm} {Selection} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1811.12808},
	abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
	author = {Raschka, Sebastian},
	year = {2018},
	note = {arXiv: 1811.12808},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\X8P6QJXL\\Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning.pdf:application/pdf},
}

@article{Marshall,
	title = {The {Statistics} {Tutor} ’ s {Quick} {Guide} to {Commonly} {Used} {Statistical} {Tests}},
	author = {Marshall, Ellen},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WY8CVCII\\tutorsquickguidetostatistics.pdf:application/pdf},
}

@article{Concepts,
	title = {Classification : {Basic} {Concepts} , {Decision} {Trees} , and},
	author = {Concepts, Basic and Trees, Decision and Evaluation, Model},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L977HERE\\Classification, Basic Concepts, Decision Trees & Model Evaluation.pdf:application/pdf},
}

@article{Brownlee,
	title = {Basics of {Linear} {Algebra} for {Machine} {Learning} {Discover} the {Mathematical} {Language} of {Data} in {Python}},
	author = {Brownlee, Jason},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YCS6I39S\\Basics for Linear Algebra for Machine Learning - Discover the Mathematical Language of Data in Python (2018).pdf:application/pdf},
}

@article{Dec,
	title = {Non-{Convex} {Distributed} {Optimization}},
	author = {Dec, O C},
	note = {arXiv: 1512.00895v2},
	pages = {1--27},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\B6PWKCCC\\Non-Convex Distributed Optimization.pdf:application/pdf},
}

@article{Gilbert,
	title = {Numerical {Optimization}},
	author = {Gilbert, J Charles and Lemar, Claude and Sagastiz, Claudia A},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\82EWJ2QA\\Numerical optimization. Theoretical and practical aspects.pdf:application/pdf},
}

@misc{PythonDataVisualization,
	title = {Python {Data} {Visualization} {Cookbook} [{Milovanović} 2013-11-25].pdf},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9IJE2T53\\Python Data Visualization Cookbook [Milovanović 2013-11-25].pdf:application/pdf},
}

@book{Halmos,
	title = {{LINEAR} {ALGEBRA}},
	isbn = {978-0-88385-322-1},
	author = {Halmos, Paul R},
	keywords = {bases, canonical forms, duality, inner product spaces, normality, scalars, similarity, transformations, vectors},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VYVHDVNH\\Paul R. Halmos-Linear algebra Problem Book-Mathematical Association of America.pdf:application/pdf},
}

@book{Gentleman,
	title = {Use {R} !},
	isbn = {978-1-4614-6867-7},
	author = {Gentleman, Robert and Hornik, Kurt and Parmigiani, Giovanni G},
}

@article{Strang,
	title = {Fourth {Edition}},
	author = {Strang, Gilbert},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CPTW6P8Q\\Linear_Algebra_and_Its_Applications__4ed_.pdf:application/pdf},
}

@article{Kruschke2011,
	title = {Doing {Bayesian} {Data} {Analysis} : {A} {Tutorial} with {R} and {BUGS}},
	author = {Kruschke, John K},
	year = {2011},
	file = {kruschkejk bayesian data analysis.ch01-06.Pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2Y7JEFGC\\kruschkejk bayesian data analysis.ch01-06.Pdf:application/pdf},
}

@article{Roth2016,
	title = {Decision {Trees}},
	author = {Roth, Dan},
	year = {2016},
	pages = {1--15},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UWWR33QI\\Decision Trees.pdf:application/pdf},
}

@article{Hagan,
	title = {Neural {Network} {Design}},
	author = {Hagan, Martin T and Beale, Mark Hudson},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XCYMBIKT\\Neural Network Design.pdf:application/pdf},
}

@article{ExploratoryMultivariateAnalysis2001,
	title = {Exploratory {Multivariate} {Analysis}},
	year = {2001},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\63MIZERM\\Multivariate Exploratory Analysis.pdf:application/pdf},
}

@article{Prediction2007,
	title = {But it must be recognized that the notion “probability of a sen- tence” is an entirely useless one, under any known interpretation of this term.},
	author = {Prediction, Word and Models, Language},
	year = {2007},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6TUM2P6K\\Counting Words in Corpora.pdf:application/pdf},
}

@article{Grouin2017,
	title = {Certification and {Cleaning} up of a {Text} {Corpus} : {Towards} an {Evaluation} of the " {Grammatical} " {Quality} of a {Corpus} . towards an evaluation of the “ grammatical ” quality of a corpus},
	number = {January},
	author = {Grouin, Cyril and Grouin, Cyril},
	year = {2017},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9SJFHTX7\\Certification_and_Cleaning_up_of_a_Text_Corpus_Tow.pdf:application/pdf},
}

@misc{ComparingProbabilisticMethods,
	title = {Comparing {Probabilistic} {Methods} for {Outlier} {Detection} in {Linear} {Models}.pdf},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DZ5M6B29\\Comparing Probabilistic Methods for Outlier Detection in Linear Models.pdf:application/pdf},
}

@article{Guide,
	title = {The {Practical} {Guide} to {Managing} {Data} {Science} at {Scale}},
	author = {Guide, The Practical and Science, Managing Data},
	pages = {1--25},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D2BMQ84D\\domino-managing-ds.pdf:application/pdf},
}

@article{Byrne,
	title = {Development {Work} ows for {Data} {Scientists}},
	author = {Byrne, Ciara},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PJUI5WSB\\development-workflows-data-scientists.pdf:application/pdf},
}

@article{Samuelssona,
	title = {{RECOGNITION} {EXTRACTED} {FROM} {PART}-{OF}-{SPEECH} {STATISTICS}},
	author = {Samuelsson, Christer and Reichl, Wolfgang and Laboratories, Bell and Technologies, Lucent and Ave, Mountain and Hill, Murray},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NXSNB8G6\\A CLASS-BASED LANGUAGE MODEL FOR LARGE-VOCABULARY SPEECH.pdf:application/pdf},
}

@article{Ncr2000,
	title = {Crisp-dm 1.0},
	author = {Ncr, Pete Chapman and Spss, Julian Clinton and Ncr, Randy Kerber and Spss, Thomas Khabaza and Daimlerchrysler, Thomas Reinartz and Spss, Colin Shearer and Daimlerchrysler, Rüdiger Wirth},
	year = {2000},
}

@article{Translation,
	title = {Chapter 7 {Language} models {Language} models},
	author = {Translation, Statistical Machine},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\W58LJ36A\\07-language-models.pdf:application/pdf},
}

@article{Thesis2012a,
	title = {Michal {Koutný}},
	author = {Thesis, Bachelor},
	year = {2012},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C2JP6E8A\\Word prediction using language models.pdf:application/pdf},
}

@article{Analysisa,
	title = {Think {Stats} {Exploratory} {Data} {Analysis} in {Python}},
	author = {Analysis, Exploratory Data},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JZNZUX8B\\Think Stats - Exploratory Data Analysis in Python.pdf:application/pdf},
}

@article{With,
	title = {Table of {Contents}},
	author = {With, Created},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DWKBUU5N\\A Non-Designer s Guide to Creating Memorable Visual Slides by Visme.pdf:application/pdf},
}

@article{James2015,
	title = {Modified {Kneser}-{Ney} {Smoothing} of n-gram {Models} {Modified} {Kneser}-{Ney} {Smoothing} of n-gram {Models}},
	number = {January 2000},
	author = {James, Frankie and Motors, General and James, Frankie},
	year = {2015},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TPSTDZAP\\Modified_Kneser-Ney_Smoothing_of_n-gram_Models.pdf:application/pdf},
}

@article{Caffo2017,
	title = {Advanced linear models for data science},
	author = {Caffo, Brian},
	year = {2017},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7HJZD2SC\\lm.pdf:application/pdf},
}

@article{Collins,
	title = {Language {Modeling}},
	author = {Collins, Michael},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PITNUDZU\\lm-spring2013.pdf:application/pdf},
}

@article{Caffo,
	title = {Statistical inference for data science {A} companion to the {Coursera} {Statistical} {Inference} {Course}},
	author = {Caffo, Brian},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XK3PEJ4W\\LittleInferenceBook.pdf:application/pdf},
}

@article{Wachsmuth2015a,
	title = {A -},
	number = {January},
	author = {Wachsmuth, Henning and Science, Computer and Engineering, Electrical},
	year = {2015},
}

@article{ChapterExploratoryData,
	title = {Chapter 4 {Exploratory} {Data} {Analysis}},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5FERM9V9\\Exploratory Data Analysis (CMU).pdf:application/pdf},
}

@article{Mining,
	title = {Springer {Series} in {Statistics} {The} {Elements} of},
	author = {Mining, Data},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MSBH4Q23\\Elements of Statistical Learning.pdf:application/pdf},
}

@book{Casellaa,
	title = {Springer {Texts} in {Statistics}},
	isbn = {978-1-4614-7137-0},
	author = {Casella, G and Fienberg, S and Olkin, I},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KVLULYCJ\\Introduction to Statistical Learning.pdf:application/pdf},
}

@article{Nemeth2001a,
	title = {Word {Unit} {Based} {Multilingual} {Comparative} {Analysis} of {Text} {Corpora} : {RUG} {OHQJWK} {LQ} {FKDUDFWHUV}},
	author = {Németh, Géza and Zainkó, Csaba},
	year = {2001},
	keywords = {language modeling, corpus analysis, multilinguality, text corpora, word length, unit based analysis, asterisks, based speech synthesis, corpus-, dashes, round and square, slashes, tried to filter out},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z6Y9N68X\\Word Unit Based Multilingual Comparative Analysis of Text Corpora.pdf:application/pdf},
}

@article{Sundarkantham2007,
	title = {{WORD} {PREDICTOR} {USING} {NATURAL} {LANGUAGE} {GRAMMAR} {INDUCTION} {TECHNIQUE}},
	author = {Sundarkantham, K and Shalinie, S Mercy},
	year = {2007},
	keywords = {word prediction, checker may benefit from, correction by a spelling, k-means clustering, natural language grammatical inference, require, support vector machines, word prediction does not},
	pages = {1--8},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9FB9L9QK\\WORD PREDICTOR USING NATURAL LANGUAGE GRAMMAR INDUCTION TECHNIQUE.pdf:application/pdf},
}

@article{Statistics,
	title = {Think {Bayes}},
	author = {Statistics, Bayesian and Simple, Made},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TL8NSNM9\\thinkbayes.pdf:application/pdf},
}

@article{Savoy,
	title = {Word {Distributions} and {Zipf} ’ s {Law} {What} is a word ?},
	author = {Savoy, J and Neuchâtel, Université De},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9BACFWNT\\ZipfCL.pdf:application/pdf},
}

@article{Stern2006,
	title = {E. {F}. {Allan}, {S}. {Abeyasekera} \& {R}. {D}. {Stern} {January} 2006},
	number = {January},
	author = {Stern, R D},
	year = {2006},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\43JA3TC5\\WritingUpResearchAStatisticalPerspective.pdf:application/pdf},
}

@article{ReportingResultsCommon2010,
	title = {Reporting {Results} of {Common} {Statistical} {Tests} in {APA} {Format}},
	number = {33},
	year = {2010},
	pages = {3--5},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8THWJPPV\\Reporting statistical results in APA format.pdf:application/pdf},
}

@article{Link2018,
	title = {Predicting the {Future} as {Bayesian} {Inference} : {People} {Combine} {Prior} {Knowledge} {With} {Observations} {When} {Estimating} {Duration} and {Extent} {Accessed}},
	author = {Link, Citable},
	year = {2018},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\J7IHQSBM\\Predicting the future as Bayesian inference.pdf:application/pdf},
}

@article{Company,
	title = {Sample size and power calculations},
	author = {Company, Electric},
	pages = {437--456},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PAC33V2U\\Power Calculations for Continuous Outcomes.pdf:application/pdf},
}

@article{Caffoa,
	title = {Regression {Models} for {Data} {Science} in {R} {A} companion book for the {Coursera} {Regression} {Models} class},
	author = {Caffo, Brian},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NTGKEVFR\\regmods.pdf:application/pdf},
}

@article{Prechelt2012a,
	title = {Early stopping - {But} when?},
	volume = {7700 LECTU},
	issn = {03029743},
	doi = {10.1007/978-3-642-35289-8-5},
	abstract = {Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting ("early stopping"). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoff between training time and generalization: From the given mix of 1296 training runs using different 12 problems and 24 different network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: About 4\% on average), but cost much more training time (here: About factor 4 longer on average). © Springer-Verlag Berlin Heidelberg 2012.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Prechelt, Lutz},
	year = {2012},
	note = {ISBN: 9783642352881},
	pages = {53--67},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2PWW5PNM\\Early Stopping  but when.pdf:application/pdf},
}

@article{Smith2018a,
	title = {Don’t decay the learning rate, increase the batch size},
	abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate and scaling the batch size B ∝ . Finally, one can increase the momentum coefficient m and scale B ∝ 1/(1 − m), although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to 76.1\% validation accuracy in under 30 minutes.},
	number = {2017},
	journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
	author = {Smith, Samuel L. and Kindermans, Pieter Jan and Ying, Chris and Le, Quoc V.},
	year = {2018},
	note = {arXiv: 1711.00489v2},
	pages = {1--11},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZGRTPPXY\\DON’T DECAY THE LEARNING RATE, INCREASE BATCH SIZE.pdf:application/pdf},
}

@article{Macs2007a,
	title = {{DOCTORS} {D} {ON} ’ {T} {T} {ELL} {Y} {OU} {The} {Medical} {Detective} :},
	number = {2017},
	author = {Macs, Healthier Big},
	year = {2007},
	keywords = {dodd frank!!},
	pages = {1--11},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WAFB77RV\\don_t_decay_the_learning_rate_increase_the_batch_size.pdf:application/pdf},
}

@techreport{Ito,
	title = {Large-{Scale} {Price} {Optimization} via {Network} {Flow}},
	abstract = {This paper deals with price optimization, which is to find the best pricing strategy that maximizes revenue or profit, on the basis of demand forecasting models. Though recent advances in regression technologies have made it possible to reveal price-demand relationship of a large number of products, most existing price optimization methods, such as mixed integer programming formulation, cannot handle tens or hundreds of products because of their high computational costs. To cope with this problem, this paper proposes a novel approach based on network flow algorithms. We reveal a connection between supermodularity of the revenue and cross elasticity of demand. On the basis of this connection, we propose an efficient algorithm that employs network flow algorithms. The proposed algorithm can handle hundreds or thousands of products, and returns an exact optimal solution under an assumption regarding cross elasticity of demand. Even if the assumption does not hold, the proposed algorithm can efficiently find approximate solutions as good as other state-of-the-art methods, as empirical results show.},
	urldate = {2020-08-17},
	author = {Ito, Shinji and Fujimaki, Ryohei},
	keywords = {Price Optimization},
}

@techreport{Spedicato,
	title = {Machine {Learning} {Methods} to {Perform} {Pricing} {Optimization}. {A} {Comparison} with {Standard} {GLMs}},
	abstract = {1 Abstract As the level of competition increases, pricing optimization is gaining a central role in most mature insurance markets, forcing insurers to optimise their rating and consider customer behaviour; the modeling scene for the latter is one currently dominated by frameworks based on Generalised Linear Models (GLMs). In this paper, we explore the applicability of novel machine learning techniques such as tree boosted models to optimise the proposed premium on prospective policyholders. Given their predictive gain over GLMs, we carefully analyse both the advantages and disadvatanges induced by their use.},
	urldate = {2020-08-17},
	author = {Spedicato, Giorgio Alfredo and Dutang, Christophe and Petrini, Leonardo},
	keywords = {Price Optimization, Boosted Trees, Conversion, Customer Behaviour, Machine Learning, Pricing Optimization},
}

@techreport{Ito2016,
	title = {Optimization {Beyond} {Prediction}: {Prescriptive} {Price} {Optimization}},
	abstract = {This paper addresses a novel data science problem , prescriptive price optimization, which derives the optimal price strategy to maximize future profit/revenue on the basis of massive predictive formulas produced by machine learning. The prescriptive price optimization first builds sales forecast formulas of multiple products, on the basis of historical data, which reveal complex relationships between sales and prices, such as price elasticity of demand and cannibalization. Then, it constructs a mathematical optimization problem on the basis of those predictive formulas. We present that the optimization problem can be formulated as an instance of binary quadratic programming (BQP). Although BQP problems are NP-hard in general and computation-ally intractable, we propose a fast approximation algorithm using a semi-definite programming (SDP) relaxation, which is closely related to the Goemans-Williamson's Max-Cut approximation. Our experiments on simulation and real retail datasets show that our prescriptive price optimization simultaneously derives the optimal prices of tens/hundreds products with practical computational time, that potentially improve 8.2 \% of gross profit of those products .},
	urldate = {2020-08-17},
	author = {Ito, Shinji and Fujimaki, Ryohei},
	year = {2016},
	note = {arXiv: 1605.05422v2},
	keywords = {Price Optimization},
}

@misc{Intellipaat,
	title = {10 {Data} {Scientist} {Skills} {You} {Must} {Have} in 2020 – {Intellipaat} {Blog}},
	url = {https://intellipaat.com/blog/data-scientist-skills/},
	urldate = {2020-08-17},
	author = {{Intellipaat}},
}

@article{Nuggets2019,
	title = {The {Most} {In} {Demand} {Tech} {Skills} for {Data} {Scientists}},
	url = {https://www.kdnuggets.com/2019/12/most-demand-tech-skills-data-scientists.html},
	number = {September},
	urldate = {2020-08-17},
	author = {Nuggets, K D and Scientist, Data and States, United and Python, Beautiful Soup and Linkedin, Scraping and States, United and Court, Supreme},
	year = {2019},
	pages = {1--10},
}

@techreport{MMCVentures2019,
	title = {The {State} of {AI}: {Divergence} 2019. {MMC} {Ventures}},
	url = {https://mmc.vc/research/the-state-of-ai-2019-divergence-a-quick-read-deck},
	urldate = {2020-08-17},
	author = {{MMC Ventures}},
	year = {2019},
	pages = {1--151},
}

@article{LouisColumbus2018,
	title = {Roundup {Of} {Machine} {Learning} {Forecasts} {And} {Market} {Estimates}, 2018},
	url = {https://www.forbes.com/sites/forbes-personal-shopper/2020/08/14/best-sales-right-now/#3ac7b89241cd},
	urldate = {2020-08-17},
	journal = {Forbes},
	author = {{Louis Columbus}},
	year = {2018},
	pages = {1--21},
}

@techreport{MichaelChui2018,
	title = {Sizing the potential value of {AI} and advanced analytics},
	url = {https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and-value-of-deep-learning},
	abstract = {McKinsey - stands out as a transformational technology of our digital age—and its practical application throughout the economy is growing apace. For this briefing, Notes from the AI frontier: Insights from hundreds of use cases (PDF–446KB), we mapped both traditional analytics and newer “deep learning” techniques and the problems they can solve to more than 400 specific use cases in companies and organizations. Drawing on McKinsey Global Institute research and the applied experience with AI of McKinsey Analytics, we assess both the practical applications and the economic potential of advanced AI techniques across industries and business functions. Our findings highlight the substantial potential of applying deep learning techniques to use cases across the economy, but we also see some continuing limitations and obstacles—along with future opportunities as the technologies continue their advance. Ultimately, the value of AI is not to be found in the models themselves, but in companies’ abilities to harness them.},
	urldate = {2020-08-17},
	author = {Michael Chui, James Manyika},
	year = {2018},
}

@article{Dong2015,
	title = {Question answering over freebase with multi-column convolutional neural networks},
	volume = {1},
	doi = {10.3115/v1/p15-1026},
	abstract = {Answering natural language questions over a knowledge base is an important and challenging task. Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context, and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-Answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use FREEBASE as the knowledge base and conduct extensive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn.},
	journal = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
	author = {Dong, Li and Wei, Furu and Zhou, Ming and Xu, Ke},
	year = {2015},
	note = {ISBN: 9781941643723},
	keywords = {Applications, Virtual Assistant},
	pages = {260--269},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WR78IR6X\\Question Answering over Freebase with Multi-Column Convolutional Neural Networks.pdf:application/pdf},
}

@article{IqpRtsvuxwTy2EEfXgxha,
	title = {iqp {rtsvuxwTy2}  h   p  {Y}  {Y}  {Y}  d ¦ {eEfXgxh}  ei  jflkt  tm  h  n  {uS}  l  so  p \&  {qrSrtsjrtsun} v £ {wY}  pd  x  {X}  {ytrtpzwY}  ux  {D} \{ p  {\textbar} t  {Xw}   \%\} sjrt {\textbar} {xuS} {\textasciitilde} {\textasciitilde} w  x  \&  x t  ¨},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TT39XUNS\\Approximate Statistical Tests for Comparing Classification Learning Algorithms.pdf:application/pdf},
}

@misc{IEEEXploreFullText,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8566156},
	urldate = {2020-08-23},
	keywords = {ctr},
}

@misc{EMarketer20,
	title = {Global {Digital} {Ad} {Spending} {Update} {Q2} 2020 - {eMarketer} {Trends}, {Forecasts} \& {Statistics}},
	url = {https://www.emarketer.com/content/global-digital-ad-spending-update-q2-2020},
	urldate = {2020-08-23},
	author = {{EMarketer}},
	year = {2019},
	keywords = {ctr},
}

@misc{EMarketer2019,
	title = {{eMarketer}'s {Facebook}-{Google} {Duopoly} {Digital} {Ad} {Spending} {Forecast} {Estimates} 2019 - {eMarketer} {Trends}, {Forecasts} \& {Statistics}},
	url = {https://www.emarketer.com/content/facebook-google-duopoly-won-t-crack-this-year},
	urldate = {2020-08-23},
	author = {{EMarketer}},
	year = {2019},
	keywords = {ctr},
}

@misc{Consultant2014,
	title = {Digitizing the consumer decision journey {\textbar} {McKinsey}},
	url = {https://www.mckinsey.com/business-functions/marketing-and-sales/our-insights/digitizing-the-consumer-decision-journey},
	urldate = {2020-08-23},
	author = {{Mckinsey}},
	year = {2014},
	keywords = {ctr},
}

@misc{MorderIntelligence2020,
	title = {Online {Advertising} {Market} {\textbar} {Growth}, {Trends}, and {Forecast} (2020 - 2025)},
	url = {https://www.mordorintelligence.com/industry-reports/online-advertising-market},
	urldate = {2020-08-23},
	author = {{Mordor Intelligence}},
	year = {2019},
	keywords = {ctr},
}

@article{Karatzoglou2017,
	title = {Deep learning for recommender systems},
	doi = {10.1145/3109859.3109933},
	abstract = {Deep Learning is one of the next big things in Recommendation Systems technology. The past few years have seen the tremendous success of deep neural networks in a number of complex machine learning tasks such as computer vision, natural language processing and speech recognition. After its relatively slow uptake by the recommender systems community, deep learning for recommender systems became widely popular in 2016. We believe that a tutorial on the topic of deep learning will do its share to further popularize the topic. Notable recent application areas are music recommendation, news recommendation, and session-based recommendation. The aim of the tutorial is to encourage the application of Deep Learning techniques in Recommender Systems, to further promote research in deep learning methods for Recommender Systems.},
	journal = {RecSys 2017 - Proceedings of the 11th ACM Conference on Recommender Systems},
	author = {Karatzoglou, Alexandros and Hidasi, Balázs},
	year = {2017},
	note = {arXiv: 1606.07792v1
ISBN: 9781450346528},
	keywords = {Deep learning, Recommender systems},
	pages = {396--397},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U5NP3F9N\\Wide & Deep Learning for Recommender Systems.pdf:application/pdf},
}

@article{Salakhutdinov2008,
	title = {Bpmf.{Pdf}},
	journal = {25th International Conference on Machine Learning (ICML- 2008)},
	author = {Salakhutdinov, Ruslan},
	year = {2008},
}

@article{Rendle2010,
	title = {Factorization machines},
	issn = {15504786},
	doi = {10.1109/ICDM.2010.127},
	abstract = {In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models. © 2010 IEEE.},
	journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
	author = {Rendle, Steffen},
	year = {2010},
	note = {ISBN: 9780769542560},
	keywords = {Factorization machine, Sparse data, Support vector machine, Tensor factorization},
	pages = {995--1000},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QMJ7P3L9\\Factorization Machines.pdf:application/pdf},
}

@article{Salje2013,
	title = {Domains within {Domains}\_SI},
	issn = {01616412},
	url = {http://dx.doi.org/10.1007/b97729},
	doi = {10.1017/CCOL0521824737.025},
	abstract = {Factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition. We extend these models to the multi-output setting, i.e., for learning vector-valued functions, with application to multi-class or multi-task problems. We cast this as the problem of learning a 3-way tensor whose slices share a common basis and propose a convex formulation of that problem. We then develop an efficient conditional gradient algorithm and prove its global convergence, despite the fact that it involves a non-convex basis selection step. On classification tasks, we show that our algorithm achieves excellent accuracy with much sparser models than existing methods. On recommendation system tasks, we show how to combine our algorithm with a reduction from ordinal regression to multi-output classification and show that the resulting algorithm outperforms simple baselines in terms of ranking accuracy.},
	number = {Nips},
	journal = {Physical Review Letters},
	author = {Salje, E.K.H. and Aktas, O. and Carpenter, M.A. and Laguta, V.V. and Scott, J.F.},
	year = {2013},
	pmid = {17636916},
	note = {arXiv: 1401.4290v2
ISBN: 978-1-907643-19-4},
	pages = {203--213},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JIQUL6ZU\\Higher-Order Factorization Machines.pdf:application/pdf},
}

@techreport{Blondel,
	title = {Higher-{Order} {Factorization} {Machines}},
	abstract = {Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks.},
	urldate = {2020-08-23},
	author = {Blondel, Mathieu and Fujino, Akinori and Ueda, Naonori},
	keywords = {ctr},
}

@techreport{Salakhutdinov,
	title = {Bayesian {Probabilistic} {Matrix} {Factorization} using {Markov} {Chain} {Monte} {Carlo}},
	abstract = {Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be performed efficiently even on very large datasets. However, unless the regularization parameters are tuned carefully, this approach is prone to overfit-ting because it finds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be efficiently trained using Markov chain Monte Carlo methods by applying them to the Netflix dataset, which consists of over 100 million movie ratings. The resulting models achieve significantly higher prediction accuracy than PMF models trained using MAP estimation.},
	urldate = {2020-08-23},
	author = {Salakhutdinov, Ruslan and Mnih, Andriy},
	keywords = {ctr},
}

@article{Google2001,
	title = {Things you should know about {Ads} {Quality} - {Google} {Ads}},
	issn = {07446616},
	url = {https://support.google.com/google-ads/answer/156066#QSvAR},
	number = {JUN.},
	urldate = {2020-08-23},
	journal = {American Printer},
	author = {{Google}},
	year = {2001},
	keywords = {ctr},
}

@article{PricewaterhouseCoopers2019,
	title = {Internet {Advertising} {Revenue} {Report} 2019},
	url = {www.iab.comwww.pwc.com/e&m},
	abstract = {2015 first six months results October 2015. An industry survey conducted by PwC and sponsored by the IAB},
	number = {April},
	urldate = {2020-08-23},
	journal = {Interactive Advertising Bureau},
	author = {{PricewaterhouseCoopers}},
	year = {2019},
	keywords = {ctr},
	pages = {4--24},
}

@techreport{LondonAuthority2019,
	title = {Housing in {London} 2019},
	url = {www.london.gov.uk},
	urldate = {2020-08-22},
	author = {London Authority, Greater},
	year = {2019},
}

@misc{NumberDwellingsLondon,
	title = {Number of dwellings in {London} 2019 {\textbar} {Statista}},
	url = {https://www.statista.com/statistics/585272/number-of-dwellings-london-uk/},
	urldate = {2020-08-22},
}

@misc{PeepSays,
	title = {Peep says…},
	url = {https://cxl.com/guides/click-through-rate/},
	urldate = {2020-08-22},
	keywords = {ctr},
}

@misc{Statistica2019,
	title = {U.{S}. online advertising revenue 2019 {\textbar} {Statista}},
	url = {https://www.statista.com/statistics/183816/us-online-advertising-revenue-since-2000/},
	urldate = {2020-08-22},
	author = {{Statistica}},
	year = {2019},
	keywords = {ctr},
}

@misc{AmazonMarketingSpending2020,
	title = {Amazon {Marketing} {Spending} {Worldwide} 2019},
	url = {https://www.statista.com/statistics/506535/amazon-marketing-spending/},
	urldate = {2020-08-22},
	journal = {Statista},
	year = {2020},
}

@misc{ABC,
	title = {Flynn prepared to testify that {Trump} directed him to contact {Russians} about {ISIS}, confidant says - {ABC} {News}},
	url = {https://abcnews.go.com/Politics/michael-flynn-charged-making-false-statements-fbi-documents/story?id=50849354},
	urldate = {2020-08-21},
	author = {{ABC}},
}

@misc{snp,
	title = {The {Cost} {Of} {Fake} {News} {For} {The} {S}\&{P} 500 {\textbar} {Seeking} {Alpha}},
	url = {https://seekingalpha.com/article/4129355-cost-of-fake-news-for-s-and-p-500},
	urldate = {2020-08-21},
}

@techreport{cheq,
	title = {The {Economic} {Cost} of {Bad} {Actors} on the {Internet}},
	abstract = {The internet has heralded an economic revolution. The internet economy of the G20 countries alone is worth more than \$4.2 trillion representing 5.3\% of their total GDP. However, as Tim Berners Lee, the father of the internet has put it: ’’While the web has created opportunity, given marginalized groups a voice, and made our daily lives easier, it has also created opportunity for scammers, given a voice to those who spread hatred, and made all kinds of crime easier to commit.’’ In a series of reports, we reveal the monetary cost caused by bad actors on the internet. CHEQ has commissioned economist, Professor Roberto Cavazos at the University of Baltimore, to undertake the first ever in-depth economic analysis of the full scale of internet harm. For the first time, using economic analysis, alongside statistical and data analysis, we measure the global economic price paid by businesses and society due to problems including ad fraud, online bullying, and fake news.},
	institution = {University of Baltimore},
	author = {CHEQ, University of Baltimore},
	year = {2019},
	keywords = {fake news},
	pages = {17},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5ESICTF4\\THE_ECONOMIC_COST_Fake_News_final.pdf:application/pdf},
}

@techreport{commons,
	title = {Disinformation and 'fake news': {Final} {Report} {Eighth} {Report} of {Session} 2017-19 {Report}, together with formal minutes relating to the report},
	url = {www.parliament.uk.},
	abstract = {This is the Final Report in an inquiry on disinformation that has spanned over 18 months, covering individuals’ rights over their privacy, how their political choices might be affected and influenced by online information, and interference in political elections both in this country and across the world—carried out by malign forces intent on causing disruption and confusion.},
	urldate = {2020-08-21},
	author = {{Digital Culture Media and Sport Committee}},
	year = {2019},
	note = {Issue: February},
}

@article{Ito2017,
	title = {Optimization {Beyond} {Prediction}},
	doi = {10.1145/3097983.3098188},
	abstract = {This paper addresses a novel data science problem, prescriptive price optimization, which derives the optimal price strategy to maximize future profit/revenue on the basis of massive predictive formulas produced by machine learning. The prescriptive price optimization first builds sales forecast formulas of multiple products, on the basis of historical data, which reveal complex relationships between sales and prices, such as price elasticity of demand and cannibalization. Then, it constructs a mathematical optimization problem on the basis of those predictive formulas.We present that the optimization problem can be formulated as an instance of binary quadratic programming (BQP). Although BQP problems are NP-hard in general and computationally intractable, we propose a fast approximation algorithm using a semi-definite programming (SDP) relaxation. Our experiments on simulation and real retail datasets show that our prescriptive price optimization simultaneously derives the optimal prices of tens/hundreds products with practical computational time, that potentially improve approximately 30\% of gross profit of those products.},
	author = {Ito, Shinji and Fujimaki, Ryohei},
	year = {2017},
	note = {arXiv: 1605.05422v2},
	pages = {1833--1841},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KGLTTGH9\\Optimization Beyond Prediction Prescriptive Price Optimization.pdf:application/pdf},
}

@article{Sockets1998,
	title = {Beej ' s {Guide} to {Network} {Programming} {Contents} :},
	volume = {4},
	author = {Sockets, Using Internet},
	year = {1998},
	pages = {1--26},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TC69TD3L\\Beej' s Guide to Network Programming.pdf:application/pdf},
}

@article{Deng2020a,
	title = {{DeepLight}: {Deep} {Lightweight} {Feature} {Interactions} for {Accelerating} {CTR} {Predictions} in {Ad} {Serving}},
	url = {http://arxiv.org/abs/2002.06987},
	abstract = {Click-through rate (CTR) prediction is a crucial task in online display advertising and the key part is to learn important feature interactions. The mainstream models are embedding-based neural networks that provide end-to-end training by incorporating hybrid components to model both low-order and high-order feature interactions. These models, however, slow down the prediction inference by at least hundreds of times due to the deep neural network (DNN) component. Considering the challenge of deploying embedding-based neural networks for online advertising, we propose to prune the redundant parameters for the first time to accelerate the inference and reduce the run-time memory usage. Most notably, we can accelerate the inference by 46X on Criteo dataset and 27X on Avazu dataset without loss on the prediction accuracy. In addition, the deep model acceleration makes an efficient model ensemble possible with low latency and significant gains on the performance.},
	author = {Deng, Wei and Pan, Junwei and Zhou, Tian and Flores, Aaron and Lin, Guang},
	year = {2020},
	note = {arXiv: 2002.06987},
	keywords = {structural pruning, ctr, ad serving, deep acceleration, fast inference, lightweight models, low memory, preconditioner},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IBRQFP2K\\DeepLight - Deep Lightweight Feature Interactions for Accelerating CTR Predictions in Ad Serving.pdf:application/pdf},
}

@article{Kaliyar2020,
	title = {{FNDNet} – {A} deep convolutional neural network for fake news detection},
	volume = {61},
	issn = {13890417},
	url = {https://doi.org/10.1016/j.cogsys.2019.12.005},
	doi = {10.1016/j.cogsys.2019.12.005},
	abstract = {With the increasing popularity of social media and web-based forums, the distribution of fake news has become a major threat to various sectors and agencies. This has abated trust in the media, leaving readers in a state of perplexity. There exists an enormous assemblage of research on the theme of Artificial Intelligence (AI) strategies for fake news detection. In the past, much of the focus has been given on classifying online reviews and freely accessible online social networking-based posts. In this work, we propose a deep convolutional neural network (FNDNet) for fake news detection. Instead of relying on hand-crafted features, our model (FNDNet) is designed to automatically learn the discriminatory features for fake news classification through multiple hidden layers built in the deep neural network. We create a deep Convolutional Neural Network (CNN) to extract several features at each layer. We compare the performance of the proposed approach with several baseline models. Benchmarked datasets were used to train and test the model, and the proposed model achieved state-of-the-art results with an accuracy of 98.36\% on the test data. Various performance evaluation parameters such as Wilcoxon, false positive, true negative, precision, recall, F1, and accuracy, etc. were used to validate the results. These results demonstrate significant improvements in the area of fake news detection as compared to existing state-of-the-art results and affirm the potential of our approach for classifying fake news on social media. This research will assist researchers in broadening the understanding of the applicability of CNN-based deep models for fake news detection.},
	journal = {Cognitive Systems Research},
	author = {Kaliyar, Rohit Kumar and Goswami, Anurag and Narang, Pratik and Sinha, Soumendu},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Deep learning, Machine learning, Neural network, fake news, Fake news, Social media},
	pages = {32--44},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2EIAJQR2\\FNDNet - A Deep Convolutional Neural Network for Fake News Detection.pdf:application/pdf},
}

@article{Horne2019,
	title = {Robust fake news detection over time and attack},
	volume = {11},
	issn = {21576912},
	doi = {10.1145/3363818},
	abstract = {In this study, we examine the impact of time on state-of-the-art news veracity classifiers. We show that, as time progresses, classification performance for both unreliable and hyper-partisan news classification slowly degrade. While this degradation does happen, it happens slower than expected, illustrating that hand-crafted, content-based features, such as style of writing, are fairly robust to changes in the news cycle.We show that this small degradation can bemitigated using online learning. Last, we examine the impact of adversarial content manipulation by malicious news producers. Specifically, we test three types of attack based on changes in the input space and data availability. We show that static models are susceptible to content manipulation attacks, but online models can recover from such attacks.},
	number = {1},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Horne, Benjamin D. and NØrregaard, Jeppe and Adali, Sibel},
	year = {2019},
	keywords = {Fake news, Adversarial machine learning, Biased news, Concept drift, Disinformation, Fake news detection, Misinformation, Misleading news, Robust machine learning},
	pages = {1--23},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I4LLMVBS\\Robust Fake News Detection Over Time and Attack.pdf:application/pdf},
}

@article{Praseetha2018,
	title = {Deep learning models for speech emotion recognition},
	volume = {14},
	issn = {15493636},
	doi = {10.3844/jcssp.2018.1577.1587},
	abstract = {Emotions play a vital role in the efficient and natural human computer interaction. Recognizing human emotions from their speech is truly a challenging task when accuracy, robustness and latency are considered. With the recent advancements in deep learning now it is possible to get better accuracy, robustness and low latency for solving complex functions. In our experiment we have developed two deep learning models for emotion recognition from speech. We compare the performance of a feed forward Deep Neural Network (DNN) with the recently developed Recurrent Neural Network (RNN) which is known as Gated Recurrent Unit (GRU) for speech emotion recognition. GRUs are currently not explored for classifying emotions from speech. The DNN model gives an accuracy of 89.96\% and the GRU model gives an accuracy of 95.82\%. Our experiments show that GRU model performs very well on emotion classification compared to the DNN model.},
	number = {11},
	journal = {Journal of Computer Science},
	author = {Praseetha, V. M. and Vadivel, Sangil},
	year = {2018},
	keywords = {Deep learning, Neural network, Deep neural network, Gated recurrent unit, Recurrent neural network},
	pages = {1577--1587},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V6L6D6BG\\Deep_Learning_Models_for_Speech_Emotion_Recognitio.pdf:application/pdf},
}

@article{Rojas-Barahona2016,
	title = {Deep learning for sentiment analysis},
	volume = {10},
	issn = {1749818X},
	doi = {10.1111/lnc3.12228},
	abstract = {Research and industry are becoming more and more interested in finding automatically the polarised opinion of the general public regarding a specific subject. The advent of social networks has opened the possibility of having access to massive blogs, recommendations, and reviews. The challenge is to extract the polarity from these data, which is a task of opinion mining or sentiment analysis. The specific difficulties inherent in this task include issues related to subjective interpretation and linguistic phenomena that affect the polarity of words. Recently, deep learning has become a popular method of addressing this task. However, different approaches have been proposed in the literature. This article provides an overview of deep learning for sentiment analysis in order to place these approaches in context.},
	number = {12},
	journal = {Language and Linguistics Compass},
	author = {Rojas-Barahona, Lina Maria},
	year = {2016},
	pages = {701--719},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QBHGV9WB\\Deep Learning for Sentiment Analysis A Survey.pdf:application/pdf},
}

@techreport{Mahdavi,
	title = {Towards {Automated} {Data} {Cleaning} {Workflows}},
	abstract = {The success of AI-based technologies depends crucially on trustful and clean data. Research in data cleaning has provided a variety of approaches to address different data quality problems. Most of them require some prior knowledge about the dataset in order to select and configure the approach correctly. We argue that for unknown datasets, it is unrealistic to know the data quality problems upfront and to formulate all necessary quality constraints in one shot. Pragmatically, the user solves data quality problems by implementing an iterative cleaning process. This incremental approach poses the challenge of identifying the right sequence of cleaning routines and their configurations. In this paper, we highlight our work in progress towards building a cleaning workflow orchestrator that learns from cleaning tasks in the past and proposes promising cleaning workflows for a new dataset. To this end, we highlight new approaches for selecting the most promising error detection routines, aggregating their outputs, and explaining the final results.},
	urldate = {2020-08-20},
	author = {Mahdavi, Mohammad and Neutatz, Felix and Visengeriyeva, Larysa and Abedjan, Ziawasch},
	keywords = {Cleaning, Data, Learning ·, Machine, Pro-filing, Workflows ·},
}

@article{Dingli2017,
	title = {Comparison of {Deep} {Learning} {Algorithms} to {Predict} {Customer} {Churn} within a {Local} {Retail} {Industry}},
	volume = {7},
	doi = {10.18178/ijmlc.2017.7.5.634},
	number = {5},
	author = {Dingli, Alexiei and Marmara, Vincent and Fournier, Nicole Sant},
	year = {2017},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UKV6H77R\\Comparison of Deep Learning Algorithms to Predict Customer Churn within a Local Retail Industry.pdf:application/pdf},
}

@article{Yan2018,
	title = {Coupled context modeling for deep chit-chat: {Towards} conversations between human and computer},
	doi = {10.1145/3219819.3220045},
	abstract = {To have automatic conversations between human and computer is regarded as one of the most hardcore problems in computer science. Conversational systems are of growing importance due to their promising potentials and commercial values as virtual assistants and chatbots. To build such systems with adequate intelligence is challenging, and requires abundant resources including an acquisition of big conversational data and interdisciplinary techniques, such as content analysis, text mining, and retrieval. The arrival of big data era reveals the feasibility to create a conversational system empowered by data-driven approaches. Now we are able to collect an extremely large number of human-human conversations on Web, and organize them to launch human-computer conversational systems. Given a human issued utterance, i.e., a query, a conversational system will search for appropriate responses, conduct relevance ranking using contexts information, and then output the highly relevant result. In this paper, we propose a novel context modeling framework with end-to-end neural networks for human-computer conversational systems. The proposed model is general and unified. In the experiments, we demonstrate the effectiveness of the proposed model for human-computer conversations using p@1, MAP, nDCG, and MRR metrics.},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Yan, Rui and Zhao, Dongyan},
	year = {2018},
	note = {ISBN: 9781450355520},
	keywords = {Context modeling, Conversational system, Retrieval model},
	pages = {2574--2583},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BCEGNJWE\\Coupled Context Modeling for Deep Chit-Chat.pdf:application/pdf},
}

@article{Wang2018,
	title = {Click-through {Rate} {Estimates} based on {Deep} {Learning}},
	doi = {10.1145/3234804.3234811},
	abstract = {Internet advertising has become a major source of income for Internet companies, among which the prediction of ad click-through rate is the most important task. The accuracy of ad click-through rate can directly generate revenue for the company. At present, the mainstream methods such as Baidu and Google are linear models with a lot of artificial features, which are more and more unsustainable. Because a lot of manual features consume a lot of manpower, their benefits are declining. Linear models cannot learn the nonlinear relationship between features. In this paper, we propose a method for forecasting click rate of advertisements based on deep learning, which can make full use of large-scale sparse data and learn non-linear features, and further analyze the role of different features in predicting ad click through rate. The experimental results on the KDD Cup 2012 Track2 validate that the proposed method can improve the predictive performance of search ads, with an AUC value of 0.771.},
	journal = {ACM International Conference Proceeding Series},
	author = {Wang, Wentao and He, Dongzhi},
	year = {2018},
	note = {ISBN: 9781450364737},
	keywords = {Deep learning, CTR, DNN, FM},
	pages = {12--15},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZZA4R2ZX\\Click-through Rate Estimates based on Deep Learning.pdf:application/pdf},
}

@article{Ouyang2019,
	title = {Click-through rate prediction with the user memory network},
	doi = {10.1145/3326937.3341258},
	abstract = {Click-through rate (CTR) prediction is a critical task in online advertising systems. Models like Deep Neural Networks (DNNs) are simple but stateless. They consider each target ad independently and cannot directly extract useful information contained in users' historical ad impressions and clicks. In contrast, models like Recurrent Neural Networks (RNNs) are stateful but complex. They model temporal dependency between users' sequential behaviors and can achieve improved prediction performance than DNNs. However, both the offline training and online prediction process of RNNs are much more complex and time-consuming. In this paper, we propose Memory Augmented DNN (MA-DNN) for practical CTR prediction services. In particular, we create two external memory vectors for each user, memorizing high-level abstractions of what a user possibly likes and dislikes. The proposed MA-DNN achieves a good compromise between DNN and RNN. It is as simple as DNN, but has certain ability to exploit useful information contained in users' historical behaviors as RNN. Both offline and online experiments demonstrate the effectiveness of MA-DNN for practical CTR prediction services. Actually, the memory component can be augmented to other models as well (e.g., the Wide\&Deep model).},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Ouyang, Wentao and Zhang, Xiuwu and Ren, Shukui and Li, Li and Liu, Zhaojie and Du, Yanlong},
	year = {2019},
	note = {ISBN: 9781450367837},
	keywords = {Deep learning, Click-through rate prediction, Online advertising},
	pages = {1--4},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZCBBXW5K\\Click-Through Rate Prediction with the User Memory Network.pdf:application/pdf},
}

@article{Pontuso1995,
	title = {To {Touch} {Your} {Heart}},
	volume = {18},
	issn = {0160-6379},
	doi = {10.1097/00003727-199507000-00013},
	abstract = {Chatbot has become an important solution to rapidly increasing customer care demands on social media in recent years. However, current work on chatbot for customer care ignores a key to impact user experience - tones. In this work, we create a novel tone-aware chatbot that generates toned responses to user requests on social media. We first conduct a formative research, in which the effects of tones are studied. Significant and various influences of different tones on user experience are uncovered in the study. With the knowledge of effects of tones, we design a deep learning based chatbot that takes tone information into account. We train our system on over 1.5 million real customer care conversations collected from Twitter. The evaluation reveals that our tone-aware chatbot generates as appropriate responses to user requests as human agents. More importantly, our chatbot is perceived to be even more empathetic than human agents.},
	number = {2},
	journal = {Family \& Community Health},
	author = {Pontuso, Anne M.},
	year = {1995},
	note = {ISBN: 9781450356206},
	pages = {75},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZBSED3HE\\Touch Your Heart  A Tone-aware Chatbot for Customer Care on Social Media.pdf:application/pdf},
}

@article{Wang2020a,
	title = {Churn prediction using ensemble learning},
	doi = {10.1145/3380688.3380710},
	abstract = {With a wealth of information on hand from the Internet, customers now can easily identify and switch to alternatives. In addition to this, a consensus has been reached that the cost of securing new customers is substantially higher than the cost of retaining the current customers. Therefore, customer retention has become an essential part of operating strategy for any organisation. Churn prediction is a practice of data analysis on the historical data, which is aiming to predict if a customer will be leaving the business or not in advance. A wide range of algorithms have been proposed for churn prediction in the past, however there is no agreement on choosing the best one. Therefore, this study presents a comparative study of the most widely used classification methods on the problem of customer churning in the telecommunication sector. The main goal of this study is to analyse and benchmark the performance of some widely used classification algorithms on a public dataset.},
	journal = {ACM International Conference Proceeding Series},
	author = {Wang, Xing and Nguyen, Khang and Nguyen, Binh P.},
	year = {2020},
	note = {ISBN: 9781450376310},
	keywords = {Data science, Ensemble, Prediction, Machine learning, Churn},
	pages = {56--60},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PQRLK5KB\\Churn Prediction using Ensemble Learning.pdf:application/pdf},
}

@article{Csaky2019,
	title = {Deep {Learning} {Based} {Chatbot} {Models}},
	url = {http://arxiv.org/abs/1908.08835},
	abstract = {A conversational agent (chatbot) is a piece of software that is able to communicate with humans using natural language. Modeling conversation is an important task in natural language processing and artificial intelligence. While chatbots can be used for various tasks, in general they have to understand users' utterances and provide responses that are relevant to the problem at hand. In my work, I conduct an in-depth survey of recent literature, examining over 70 publications related to chatbots published in the last 3 years. Then, I proceed to make the argument that the very nature of the general conversation domain demands approaches that are different from current state-of-of-the-art architectures. Based on several examples from the literature I show why current chatbot models fail to take into account enough priors when generating responses and how this affects the quality of the conversation. In the case of chatbots, these priors can be outside sources of information that the conversation is conditioned on like the persona or mood of the conversers. In addition to presenting the reasons behind this problem, I propose several ideas on how it could be remedied. The next section focuses on adapting the very recent Transformer model to the chatbot domain, which is currently state-of-the-art in neural machine translation. I first present experiments with the vanilla model, using conversations extracted from the Cornell Movie-Dialog Corpus. Secondly, I augment the model with some of my ideas regarding the issues of encoder-decoder architectures. More specifically, I feed additional features into the model like mood or persona together with the raw conversation data. Finally, I conduct a detailed analysis of how the vanilla model performs on conversational data by comparing it to previous chatbot models and how the additional features affect the quality of the generated responses.},
	author = {Csaky, Richard},
	year = {2019},
	note = {arXiv: 1908.08835},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G5V54IGD\\Deep Learning Based Chatbot Models.pdf:application/pdf},
}

@techreport{Maestre2018,
	title = {Reinforcement {Learning} for {Fair} {Dynamic} {Pricing}},
	abstract = {Unfair pricing policies have been shown to be one of the most negative perceptions customers can have concerning pricing, and may result in long-term losses for a company. Despite the fact that dynamic pricing models help companies maximize revenue, fairness and equality should be taken into account in order to avoid unfair price differences between groups of customers. This paper shows how to solve dynamic pricing by using Reinforcement Learning (RL) techniques so that prices are maximized while keeping a balance between revenue and fairness. We demonstrate that RL provides two main features to support fairness in dynamic pricing: on the one hand, RL is able to learn from recent experience, adapting the pricing policy to complex market environments; on the other hand, it provides a trade-off between short and long-term objectives, hence integrating fairness into the model's core. Considering these two features, we propose the application of RL for revenue optimization, with the additional integration of fairness as part of the learning procedure by using Jain's index as a metric. Results in a simulated environment show a significant improvement in fairness while at the same time maintaining optimisation of revenue.},
	urldate = {2020-08-18},
	author = {Maestre, Roberto and Duque, Juan and Rubio, Alberto and Arevalo, Juan},
	year = {2018},
	note = {arXiv: 1803.09967v1},
	keywords = {Dynamic Pricing, Fair-ness, Jain's index, Reinforcement Learning},
}

@article{Aff2015,
	title = {a {C} {Omparison} of {M} {Ethods} for {D} {Etermining} the},
	volume = {26},
	number = {13},
	author = {Aff, G G Regory H and Uben, R Y A N P R and Ider, J Oshua L and Wine, C Orey T and Ormie, P R U E C},
	year = {2015},
	note = {ISBN: 9781624170904},
	pages = {386--395},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CJXCXZLY\\Machine Learning Methods to Perform Pricing Optimization.pdf:application/pdf},
}

@article{Jambhale2019,
	title = {Search {Engine} {Optimization} with {Google}-{Web}},
	volume = {9},
	doi = {10.32628/cseit195632},
	abstract = {Search engine optimization is a technique to take a web document in top search results of a search engine. Web presence Companies is not only an easy way to reach among the target users but it may be profitable for Business is exactly find the target users as of the reason that most of the time user search out with the keywords of their use rather than searching the Company name, and if the Company Website page come in the top positions then the page may be profitable. This work describes the tweaks of taking the page on top position in Google by increasing the Page rank which may result in the improved visibility and profitable deal for a Business. Google is most user-friendly search engine to prove for the all users which give user-oriented results. In addition ,most of other search engines use Google search patterns so we have concentrated on it. So, if a page is Register on Google it Is Display on most of the search engines.},
	number = {1},
	journal = {International Journal of Scientific Research in Computer Science, Engineering and Information Technology},
	author = {Jambhale, Pratik C.},
	year = {2019},
	pages = {227--230},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5FLEHHU5\\SearchengineoptimizationwithGoogle.pdf:application/pdf},
}

@article{UniversityofBaltimore2019,
	title = {the {Economic} {Cost} of {Bad} {Actors} on the {Internet}},
	author = {{University of Baltimore} and {CHEQ}},
	year = {2019},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\23UC8ASM\\THE_ECONOMIC_COST_Fake_News_final.pdf:},
}

@article{Saganowski2019,
	title = {Analysis of group evolution prediction in complex networks},
	volume = {14},
	issn = {19326203},
	doi = {10.1371/journal.pone.0224194},
	abstract = {In the world, in which acceptance and the identification with social communities are highly desired, the ability to predict the evolution of groups over time appears to be a vital but very complex research problem. Therefore, we propose a new, adaptable, generic, and multistage method for Group Evolution Prediction (GEP) in complex networks, that facilitates reasoning about the future states of the recently discovered groups. The precise GEP modularity enabled us to carry out extensive and versatile empirical studies on many real-world complex / social networks to analyze the impact of numerous setups and parameters like time window type and size, group detection method, evolution chain length, prediction models, etc. Additionally, many new predictive features reflecting the group state at a given time have been identified and tested. Some other research problems like enriching learning evolution chains with external data have been analyzed as well.},
	number = {10},
	journal = {PLoS ONE},
	author = {Saganowski, Stanislaw and Brodka, Piotr and Koziarski, Michal and Kazienko, Przemyslaw},
	year = {2019},
	pmid = {31661495},
	note = {arXiv: 1711.01867},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T7N7TE68\\Analysis of group evolution prediction in complex networks.pdf:application/pdf},
}

@article{Katsov2020,
	title = {Deep reinforcement learning for supply chain and price optimization},
	url = {https://blog.griddynamics.com/deep-reinforcement-learning-for-supply-chain-and-price-optimization/},
	urldate = {2020-08-18},
	author = {Katsov, Ilya},
	year = {2020},
}

@misc{MachineVisionMarket,
	title = {Machine {Vision} {Market} {Size} {Worth} \$18.24 {Billion} {By} 2025 {\textbar} {CAGR}: 7.7\%},
	url = {https://www.grandviewresearch.com/press-release/global-machine-vision-market},
	urldate = {2020-08-18},
}

@misc{RevenueManagementMarket,
	title = {Revenue {Management} {Market} worth \$22.4 billion by 2024},
	url = {https://www.marketsandmarkets.com/PressReleases/revenue-management.asp},
	urldate = {2020-08-18},
}

@misc{GlobalECommerceAnalytics,
	title = {Global e-{Commerce} {Analytics} {Market} {Set} to {Reach} \$22.4 {Billion} by 2025 - {Breakdown} by {Offering}, {Application} and {Geography} - {ResearchAndMarkets}.com {\textbar} {Business} {Wire}},
	url = {https://www.businesswire.com/news/home/20200414006099/en/Global-e-Commerce-Analytics-Market-Set-Reach-22.4},
	urldate = {2020-08-18},
}

@misc{CustomerExperienceManagement,
	title = {Customer {Experience} {Management} {Market} {Worth} \$23.6 {Billion} {By} 2027},
	url = {https://www.grandviewresearch.com/press-release/global-customer-experience-management-cem-market},
	urldate = {2020-08-18},
}

@misc{EmotionDetectionRecognition,
	title = {Emotion {Detection} and {Recognition} {Market} by {Technology} \& {Software} {Tool} - 2024{\textbar} {MarketsandMarkets}},
	url = {https://www.marketsandmarkets.com/Market-Reports/emotion-detection-recognition-market-23376176.html},
	urldate = {2020-08-18},
}

@misc{RecommendationEngineMarket,
	title = {Recommendation {Engine} {Market} {Share}, {Size} and {Industry} {Growth} {Analysis} 2020 - 2025},
	url = {https://www.industryarc.com/Research/Recommendation-Engine-Market-Research-500995},
	urldate = {2020-08-18},
}

@misc{CustomerAnalyticsMarket,
	title = {Customer {Analytics} {Market} by {Solution} \& {Service} – 2025 {\textbar} {MarketsandMarkets}},
	url = {https://www.marketsandmarkets.com/Market-Reports/customer-analytics-market-250688798.html?gclid=EAIaIQobChMIxd68qe2j6wIVD77ACh1g0gk1EAAYASAAEgLY8fD_BwE},
	urldate = {2020-08-18},
}

@misc{FakeNewsStatistics,
	title = {Fake {News} {Statistics} - {How} {Big} is the {Problem}? {\textbar} {Business} {Tips} {\textbar} {JournoLink}},
	url = {https://journolink.com/resource/319-fake-news-statistics-2019-uk-worldwide-data},
	urldate = {2020-08-18},
	keywords = {fake news},
}

@article{NgoupeyouTondji2018,
	title = {Web {Recommender} {System} for {Job} {Seeking} and {Recruiting}},
	doi = {10.13140/RG.2.2.26177.61286},
	abstract = {Shortlisting candidates and screening resumes are long time-consuming tasks for the company, especially when 80 percent to 90 percent of the resumes received for a role are unqualified. We have designed and proposed an hybrid personalized recommender system called skillake for job seeking and online recruiting websites adapted to the cold start problem using a clustering predictive algorithms. Keywords: Text mining, Recommender systems, Clustering, Cold-start problem, Unsupervised learning.},
	number = {February},
	author = {Ngoupeyou Tondji, Lionel},
	year = {2018},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PIBYZFE6\\Web Recommender System for Job Seeking and Recruiting.pdf:application/pdf;Tondji - 2018 - Web Recommender System for Job Seeking and Recruit.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DVJVJ6GB\\Tondji - 2018 - Web Recommender System for Job Seeking and Recruit.pdf:application/pdf;Tondji - 2018 - Web Recommender System for Job Seeking and Recruit.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D5KFWEE9\\Tondji - 2018 - Web Recommender System for Job Seeking and Recruit.pdf:application/pdf},
}

@article{Roy2020,
	title = {A {Machine} {Learning} approach for automation of {Resume} {Recommendation} system},
	volume = {167},
	issn = {18770509},
	url = {https://doi.org/10.1016/j.procs.2020.03.284},
	doi = {10.1016/j.procs.2020.03.284},
	abstract = {Finding suitable candidates for an open role could be a daunting task, especially when there are many applicants. It can impede team progress for getting the right person on the right time. An automated way of "Resume Classification and Matching" could really ease the tedious process of fair screening and shortlisting, it would certainly expedite the candidate selection and decisionmaking process. This system could work with a large number of resumes for first classifying the right categories using different classifier, once classification has been done then as per the job description, top candidates could be ranked using Content-based Recommendation, using cosine similarity and by using k-NN to identify the CVs that are nearest to the provided job description.},
	number = {2019},
	journal = {Procedia Computer Science},
	author = {Roy, Pradeep Kumar and Chowdhary, Sarabjeet Singh and Bhatia, Rocky},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Recommender Systems, Job Seekers, Logistic Regression, Online Job Search, Random Forests, Resume Recommendation, Resume Similarity, Support Vector Machines},
	pages = {2318--2327},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3BQEL8RR\\A Machine Learning Approach for Automation of Resume Recommendation System.pdf:application/pdf},
}

@article{Shapiro2017,
	title = {Unsupervised {Deep} {Learning} {Recommender} {System} for {Personal} {Computer} {Users}},
	abstract = {This work presents an unsupervised learning approach for training a virtual assistant recommender system, building upon prior work on deep learning neural networks, image processing, mixed-initiative systems, and recommender systems. Intelligent agents can understand the world in intuitive ways with neural networks, and make action recommendations to computer users. The system discussed in this work interprets a computer screen image in order to learn new keywords from the user's screen and associate them to new contexts in a completely unsupervised way, then produce action recommendations to assist the user. It can assist in automating various tasks such as genetics research, computer programming, engaging with social media, and legal research. The action recommendations are personalized to the user, and are produced without integration of the assistant into each individual application executing on the computer. Recommendations can be accepted with a single mouse click by the computer user.},
	number = {c},
	author = {Shapiro, Daniel and Qassoud, Hamza and Lemay, Mathieu and Bolic, Miodrag},
	year = {2017},
	note = {ISBN: 9781612085760},
	keywords = {recommender systems, deep, unsupervised learning},
	pages = {22--31},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DL9YRPXB\\Unsupervised Deep Learning Recommender.pdf:application/pdf},
}

@article{Mishra2018,
	title = {A comparative study of customer churn prediction in telecom industry using ensemble based classifiers},
	doi = {10.1109/ICICI.2017.8365230},
	abstract = {Churn Prediction plays a vital role in various domains like life insurance, banking and telecom industry. With the current advancement in Machine Learning and Artificial Intelligence, Churn Prediction is more realistic and accurate. It is very much essential for early stage detection of customers who are at high risk of leaving the company or services. In this paper, Ensemble based Classifiers namely Bagging, Boosting and Random Forest were utilized for Churn Prediction in telecom industry. The Ensemble based Classifiers were compared with the well-known classifiers namely Decision Tree, Naïve Bayes Classifier and Support Vector Machine (SVM). The experimental results shows that Random Forest has less error rate, low specificity, high sensitivity and greater accuracy of 91.66\% as compared to other methods.},
	number = {November},
	journal = {Proceedings of the International Conference on Inventive Computing and Informatics, ICICI 2017},
	author = {Mishra, Abinash and Reddy, U. Srinivasulu},
	year = {2018},
	note = {ISBN: 9781538640319},
	keywords = {Machine Learning, Artificial Intelligence, Base Classifiers, Churn Prediction, Ensemble based Classifiers, Telecom Industry},
	pages = {721--725},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\W6HKKAFF\\Predicting Customer Churn in Telecom using Ensemble Based Classifiers.pdf:application/pdf},
}

@misc{PDFComparativeStudy,
	title = {(1) ({PDF}) {A} comparative study of customer churn prediction in telecom industry using ensemble based classifiers},
	url = {https://www.researchgate.net/publication/325419986_A_comparative_study_of_customer_churn_prediction_in_telecom_industry_using_ensemble_based_classifiers},
	urldate = {2020-08-17},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NJHMYVNY\\Predicting Customer Churn in Telecom using Ensemble Based Classifiers.pdf:application/pdf},
}

@article{Cuayahuitl2019,
	title = {Ensemble-{Based} {Deep} {Reinforcement} {Learning} for {Chatbots}},
	url = {https://doi.org/10.1016/j.neucom.2019.08.007},
	doi = {10.1016/j.neucom.2019.08.007},
	abstract = {Trainable chatbots that exhibit fluent and human-like conversations remain a big challenge in artificial intelligence. Deep Reinforcement Learning (DRL) is promising for addressing this challenge, but its successful application remains an open question. This article describes a novel ensemble-based approach applied to value-based DRL chat-bots, which use finite action sets as a form of meaning representation. In our approach, while dialogue actions are derived from sentence clustering, the training datasets in our ensemble are derived from dialogue clustering. The latter aim to induce specialised agents that learn to interact in a particular style. In order to facilitate neural chatbot training using our proposed approach, we assume dialogue data in raw text only-without any manually-labelled data. Experimental results using chitchat data reveal that (1) near human-like dialogue policies can be induced, (2) generalisation to unseen data is a difficult problem, and (3) training an ensemble of chatbot agents is essential for improved performance over using a single agent. In addition to evaluations using held-out data, our results are further supported by a human evaluation that rated dialogues in terms of fluency, engagingness and consistency-which revealed that our proposed dialogue rewards strongly correlate with human judgements. 1 1 Work carried out while the first author was visiting Samsung Research. 1 https://doi.},
	urldate = {2020-08-17},
	author = {Cuayáhuitl, Heriberto and Lee, Donghyeon and Ryu, Seonghan and Cho, Yongjin and Choi, Sungja and Indurthi, Satish and Yu, Seunghak and Choi, Hyungtak and Hwang, Inchul and Kim, Jihie},
	year = {2019},
	note = {arXiv: 1908.10422v1},
	keywords = {Neural Chatbots},
}

@techreport{Csaky2019a,
	title = {{BUDAPEST} {UNIVERSITY} {OF} {TECHNOLOGY} {AND} {ECONOMICS} {FACULTY} {OF} {ELECTRICAL} {ENGINEERING} {AND} {INFORMATICS} {DEPARTMENT} {OF} {AUTOMATION} {AND} {APPLIED} {INFORMATICS} {Deep} {Learning} {Based} {Chatbot} {Models} {SCIENTIFIC} {STUDENTS}' {ASSOCIATIONS} {REPORT}},
	abstract = {A conversational agent (chatbot) is a piece of software that is able to communicate with humans using natural language. Modeling conversation is an important task in natural language processing and artificial intelligence (AI). Indeed, ever since the birth of AI, creating a good chatbot remains one of the field's hardest challenges. While chatbots can be used for various tasks, in general they have to understand users' utterances and provide responses that are relevant to the problem at hand. In the past, methods for constructing chatbot architectures have relied on handwritten rules and templates or simple statistical methods. With the rise of deep learning these models were quickly replaced by end-to-end trainable neural networks around 2015. More specifically, the recurrent encoder-decoder model [Cho et al., 2014] dominates the task of conversational mod-eling. This architecture was adapted from the neural machine translation domain, where it performs extremely well. Since then a multitude of variations [Serban et al., 2016] and features were presented that augment the quality of the conversation that chatbots are capable of. In my work, I conduct an in-depth survey of recent literature, examining over 70 publications related to chatbots published in the last 3 years. Then I proceed to make the argument that the very nature of the general conversation domain demands approaches that are different from current state-of-the-art architectures. Based on several examples from the literature I show why current chatbot models fail to take into account enough priors when generating responses and how this affects the quality of the conversation. In the case of chatbots these priors can be outside sources of information that the conversation is conditioned on like the persona [Li et al., 2016a] or mood of the conversers. In addition to presenting the reasons behind this problem, I propose several ideas on how it could be remedied. The next section of my paper focuses on adapting the very recent Tranformer [Vaswani et al., 2017] model to the chatbot domain, which is currently the state-of-the-art in neural machine translation. I first present my experiments with the vanilla model, using conversations extracted from the Cornell Movie-Dialog Corpus [Danescu-Niculescu-Mizil and Lee, 2011]. Secondly, I augment the model with some of my ideas regarding the issues of encoder-decoder architectures. More specifically, I feed additional features into the model like mood or persona together with the raw conversation data. Finally, I conduct a detailed analysis of how the vanilla model performs on conversational data by comparing it to previous chatbot models and how the additional features, affect the quality of the generated responses.},
	urldate = {2020-08-17},
	author = {Csáky, Richárd Krisztián},
	year = {2019},
	note = {arXiv: 1908.08835v1},
	keywords = {Natural Language Processing, Deep Learning, Virtual Assistant, Chatbot},
}

@misc{NoMoreSKU,
	title = {No {More} {SKU} {Blues}: {Why} {It}’s {Time} to {Embrace} {Price} {Optimization}},
	url = {https://www.mytotalretail.com/article/no-more-sku-blues-why-its-time-to-embrace-price-optimization/},
	urldate = {2020-08-17},
	keywords = {Price Optimization},
}

@misc{ModelReportRevenue,
	title = {Model {N} {Report}: {Revenue} {Management} {Complexity} {Costs} {Life} {Sciences} {Companies} {Billions} {\textbar} {Business} {Wire}},
	url = {https://www.businesswire.com/news/home/20200414005204/en/Model-Report-Revenue-Management-Complexity-Costs-Life},
	urldate = {2020-08-17},
	keywords = {Price Optimization},
}

@article{Xia2018,
	title = {Improving the performance of stock trend prediction by applying {GA} to feature selection},
	doi = {10.1109/SC2.2018.00025},
	abstract = {Predicting stock trend by using machining learning is a hot research issue today. However, due to the non linearity and instability of the stock data, it is still very difficult to predict the stock trend with high accuracy. In order to improve the accuracy, most researchers focus on the models selection and features construction. A variety of feature construction methods have been proposed. However, not all features constructed in those paper are equally useful. Further more, many features of significant importance may not be selected in prediction. In order to improve the accuracy of stock trend prediction, this paper will focus on the features selection problem. Most feature selection methods employed in the stock trend prediction are based on filtration methods. Wrapper methods are rarely used. Compared with filtration methods, wrapper methods have better stability and accuracy. In this paper, we propose a feature selection algorithm by extending genetic algorithm (GA). Experiments are conducted on real-world stock price data set. The experiment results show that our GA-based feature selection algorithm is better in both stability and performance.},
	journal = {Proceedings - 8th IEEE International Symposium on Cloud and Services Computing, SC2 2018},
	author = {Xia, Tian and Sun, Qibo and Zhou, Ao and Wang, Shanguang and Xiong, Shilong and Gao, Siyi and Li, Jinglin and Yuan, Quan},
	year = {2018},
	note = {ISBN: 9781728102368},
	keywords = {SVM, feature selection, genetic algorithm, out, stock price prediction, stock trend prediction},
	pages = {122--126},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NGHG66JF\\Improving the Performance of Stock Trend Prediction by Applying GA to Feature Selection.pdf:application/pdf},
}

@article{Zhang2019i,
	title = {Integrating harmony search algorithm and deep belief network for stock price prediction model},
	doi = {10.1109/ICSAI48974.2019.9010253},
	abstract = {Stock price predicting is of great practical value in stock market trading and is the foundation of programmatic trading and high-frequency trading. Deep Belief Network (DBN) has been widely used in the field of stock market predicting. However, due to the lack of effective parallel training, empirical methods are still used in the application to determine the important network structure parameters in DBN, resulting in high calculation cost and low efficiency. This study puts forward a hybrid model of integrating metaheuristic Harmony Search (HS) algorithm and DBN. By setting up the self-learning mechanism of the network, the feedback evaluation of objective function is applied to let the network itself calculate the most appropriate structural parameters and improve the model prediction performance. Through the experimental verification on the 160 historical trading days of the Shanghai Composite Index, the mean absolute percentage error (MAPE) and return profit margin of the hybrid model presented in this paper are 1.56\% and 16.04\%, and the overall performance is better than the existing stock price prediction models.},
	number = {2017},
	journal = {2019 6th International Conference on Systems and Informatics, ICSAI 2019},
	author = {Zhang, Lei and Ding, Xiangqian and Hou, Ruichun and Tao, Ye},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728152561},
	keywords = {stock price prediction, Deep Belief Network, Harmony Search, Hybrid model, in, Network structure, Stock price predict},
	pages = {631--636},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8NASPN7Z\\Integrating Harmony Search Algorithm and Deep Belief Network for Stock Price Prediction Model.pdf:application/pdf},
}

@article{Qiu2019,
	title = {Candlestick {Analysis} in {Forecasting} {U}.{S}. {Stock} {Market}: {Are} {They} {Informative} and {Effective}},
	doi = {10.1109/ICBDA.2019.8713248},
	abstract = {Stock price prediction is one of the hottest topics of research in both academia and industry. Being able to predict the trend of price correctly allows investors to gain profit. There have been multiple strategies in stock price prediction, such as multiple machine learning methodologies and forecast from sentiment analysis of the public and news feedings. Among these strategies, one of the oldest but still widely used strategy is the candlestick analysis, which is a simple way that allows general investors to predict the market trend. However, there lacks an unbiased estimation of the effectiveness of such a method. In this paper, using an unbiased and rigorous way to test multiple U.S. stocks, we were able to show that most of the candlestick patterns were not informative, while a small fraction of them provides some correct information for market trend compared with random guesses. Our study serves as a stepping-stone for re-evaluating the candlestick analysis and urges more similar and thorough studies to be conducted to guide the general public in stock market investment better.},
	journal = {2019 4th IEEE International Conference on Big Data Analytics, ICBDA 2019},
	author = {Qiu, Haoxuan and Liu, Fanzhuoqun},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728112824},
	keywords = {out, stock price prediction, candlestick, stock price, techinical analysis, trading strategy},
	pages = {325--328},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SHKBQP6D\\Candlestick Analysis in Forecasting U.S. Stock Market Are They Informative and Effective.pdf:application/pdf},
}

@article{Lee2019,
	title = {Global stock market prediction based on stock chart images using deep q-network},
	volume = {7},
	issn = {21693536},
	doi = {10.1109/ACCESS.2019.2953542},
	abstract = {We applied Deep Q-Network with a Convolutional Neural Network function approximator, which takes stock chart images as input for making global stock market predictions. Our model not only yields profit in the stock market of the country whose data was used for training our model but also generally yields profit in global stock markets. We trained our model only on US stock market data and tested it on the stock market data of 31 different countries over 12 years. The portfolios constructed based on our model's output generally yield about 0.1 to 1.0 percent return per transaction prior to transaction costs in the stock markets of 31 countries. The results show that some patterns in stock chart images indicate the same stock price movements across global stock markets. Moreover, the results show that future stock prices can be predicted even if the model is trained and tested on data from different countries. The model can be trained on the data of relatively large and liquid markets (e.g., US) and tested on the data of small markets. The results demonstrate that artificial intelligence based stock price forecasting models can be used in relatively small markets (emerging countries) even though small markets do not have a sufficient amount of data for training.},
	journal = {IEEE Access},
	author = {Lee, Jinho and Kim, Raehyun and Koh, Yookyung and Kang, Jaewoo},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {Artificial intelligence, Neural networks, stock price prediction, in, Finance, Stock markets},
	pages = {167260--167277},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IVR4S4AG\\Global Stock Market Prediction Based on Stock Chart Images Using Deep Q-Network.pdf:application/pdf},
}

@article{Darwen2018,
	title = {Questioning the efficient markets hypothesis: {Big} data evidence of non-random stock prices},
	doi = {10.1109/ICBDA.2018.8367677},
	abstract = {The efficient markets hypothesis claims that stock prices fully reflect all available information, and that prediction of future changes in stock prices is impossible. For 50 companies listed on stock exchanges in the United States, this paper compares the real data with random data that follows a similar distribution as the real data, in order to ascertain how much usefully predictive information is in the real data. Surprisingly, it turns out that if one can tolerate a modest number of random false positives, around twelve percent of the time there is a modest amount of information.},
	journal = {2018 IEEE 3rd International Conference on Big Data Analysis, ICBDA 2018},
	author = {Darwen, Paul J.},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538647936},
	keywords = {stock price prediction},
	pages = {201--205},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KDQM6JYH\\Questioning the efficient markets hypothesis Big data evidence of non-random stock prices.pdf:application/pdf},
}

@article{AsrafRoslan2019,
	title = {Stock prediction using sentiment analysis in twitter for day trader},
	doi = {10.1109/ICSGRC.2018.8657614},
	abstract = {Capital market is a huge investment in today market because it has a great value in optimizing capital allocation, funding and also increasing the value of properties. Stock prediction is essential for day trader to determine what their next moves are for that day and tomorrow since they have to be one step further than the long term trader. With social media that rapidly increasing in popularity, one website stands out for the potential to use it for mining the database for the stock prediction which is twitter. This also combining with the sentiment analysis is used to predict the value of the stock. This are being done using with a sentiment analysis software that can give score to each individual tweets. The core of the prediction method is Artificial Neural Network that is present inside MATLAB to help us train the data and predict the price of the stock. The findings were that there is a very strong correlation between the sentiment and the value of the stock price and the artificial Neural Network has been successfully implemented with 99.95\% success rate. Therefore, we can conclude that this method can be used to predict the price of the stock.},
	number = {August},
	journal = {2018 9th IEEE Control and System Graduate Research Colloquium, ICSGRC 2018 - Proceeding},
	author = {Asraf Roslan, Muhammad Aiman and Fazalul Rahiman, Mohd Hezri},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781538663219},
	keywords = {out, stock price prediction},
	pages = {177--182},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7P6QPFUF\\Stock Prediction Using Sentiment Analysis in Twitter for Day Trader.pdf:application/pdf},
}

@article{Kuttichira2017,
	title = {Stock price prediction using dynamic mode decomposition},
	volume = {2017-Janua},
	doi = {10.1109/ICACCI.2017.8125816},
	abstract = {Stock price prediction is a challenging problem as the market is quite unpredictable. We propose a method for price prediction using Dynamic Mode Decomposition assuming stock market as a dynamic system. DMD is an equation free, datadriven, spatio-temporal algorithm which decomposes a system to modes that have predetermined temporal behaviour associated with them. These modes help us determine how the system evolves and the future state of the system can be predicted. We have used these modes for the predictive assessment of the stock market. We worked with the time series data of the companies listed in National Stock Exchange. The granularity of time was minute. We have sampled a few companies across sectors listed in National Stock Exchange and used the minute-wise stock price to predict their price in next few minutes. The obtained price prediction results were compared with actual stock prices. We used Mean Absolute Percentage Error to calculate the deviation of predicted price from actual price for each company. Price prediction for each company was made in three different ways. In the first, we sampled companies belonging to the same sector to predict the future price. In the latter, we considered sampled companies from all sectors for prediction. In the first and second method, the sampling as well as the prediction window size were fixed. In the third method the sampling of companies was done from all sectors considered. The sampling window was kept fixed, but predictions were made until it crossed a threshold error. Prediction was found to be more accurate when samples were taken from all the sectors, than from a single sector. When sampling window alone was fixed; the predictions could be made for longer period for certain instances of sampling.},
	journal = {2017 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2017},
	author = {Kuttichira, Deepthi Praveenlal and Gopalakrishnan, E. A. and Menon, Vijay Krishna and Soman, K. P.},
	year = {2017},
	note = {ISBN: 9781509063673},
	keywords = {stock price prediction, Dynamic mode decomposition, Mean absolute percentage error, Proper orthogonal decomposition},
	pages = {55--60},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EBT82NPY\\Stock price prediction using dynamic mode decomposition.pdf:application/pdf},
}

@article{Lan2019,
	title = {Machine learning model with technical analysis for stock price prediction: {Empirical} study of {Semiconductor} {Company} in {Taiwan}},
	doi = {10.1109/ISPACS48206.2019.8986293},
	abstract = {The stock market was affected by different variables, such as the overall economic situation, political events, Sino-US relations and corporate operations. Therefore, if you want to get returns in the stock market, predicting the time series of financial markets in advance is the most important thing for analysts and investors. However, predicting the direction of the stock market need to access information from existing markets and past historical data. Under such complicated work and costs, it is always the most difficult and important issue to achieve accurate forecasting and reduce forecasting costs. In this paper, the backpropagation neural network is used as a research tool to analyze the historical data of Taiwan Semiconductor Manufacturing Company (hereinafter referred to as TSMC) during the sample period from 2014 to 2018. In this study, the standardized technical analysis indicators and the related variables of TSMC are taken as input variables, and the closing price of the next day is taken as the output variable to predict the closing price for TSMC of the next day. The empirical results confirm that this method does improve the forecast of stock price of TSMC.},
	journal = {Proceedings - 2019 International Symposium on Intelligent Signal Processing and Communication Systems, ISPACS 2019},
	author = {Lan, Po Chao and Kung, Wei Ling and Ou, Yao Lun and Lin, Chun Yueh and Hu, Wen Cheng and Wang, Yi Hsien},
	year = {2019},
	note = {ISBN: 9781728130385},
	keywords = {neural network, out, stock price prediction, forecast, technical analysis, Time Series},
	pages = {2019--2020},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IID3HGSU\\Machine learning model with technical analysis for stock price prediction Empirical study of Semiconductor Company in Taiwan.pdf:application/pdf},
}

@article{Deng2018,
	title = {Research on investor sentiment and stock market prediction based on {Weibo} text},
	doi = {10.1109/SPAC46244.2018.8965607},
	abstract = {Microblog can obtain investors' views of stock market accurately and timely, and grasping the fluctuation of investor sentiment is beneficial to predict the future trend of stock market. Based on behavioral finance theory, this paper uses text mining and natural language processing technology to obtain investor sentiment, and then combines price earnings ratio and turnover rate to build a stock market prediction model. The results show that the investor sentiment based on Weibo text has a certain predictive ability to the Shanghai stock index. The model has the best performance in the ascending period, and the effect of the shock period is the worst.},
	journal = {2018 International Conference on Security, Pattern Analysis, and Cybernetics, SPAC 2018},
	author = {Deng, Yongheng and Xie, Qing and Wang, Yong},
	year = {2018},
	note = {ISBN: 9781728105512},
	keywords = {out, stock price prediction, investor sentiment, sentiment analysis, stock market prediction, Weibo},
	pages = {1--5},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PRD3RZIV\\Research on investor sentiment and stock market prediction based on Weibo text.pdf:application/pdf},
}

@article{Lee2018b,
	title = {Modeling stock prices with text contents in 10-{Q} reports},
	doi = {10.1109/SNPD.2018.8441051},
	abstract = {Stock market prediction was once considered to be infeasible. Recent studies on using text contents of information reporting platforms has opened up new ways of analyzing the stock market with machine learning. we propose using the Securities and Exchange Committee (SEC) mandated 10-Q form as a possible source of data for stock predictions. Using the 10-Q reports of SP 500 companies, we create our corpus by extracting bag-of-words (BOW) of any additions made to the 10-Q documents. Then, we create feed-forward multilayer neural network on stock price ratios of different target prediction periods and achieve positive prediction rates. We demonstrate that text contents of 10-Q form may have information value to stock price prediction models.},
	journal = {Proceedings - 2018 IEEE/ACIS 19th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2018},
	author = {Lee, Wonho and Suh, Bongwon},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538658895},
	keywords = {stock price prediction, Multi-layer neural network, Natural Language, Stock Markets},
	pages = {224--229},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PV3ZLEQM\\Modeling Stock Prices with Text Contents in 10-Q Reports.pdf:application/pdf},
}

@article{Sruthi2019,
	title = {Deep stock prediction using visual interpretation: {Deepclue}},
	volume = {7},
	issn = {22773878},
	abstract = {This proposed paper builds Deep Clue system that links text related models, final users using visual interpretation. We try to implement following modules in this paper. 1.Designing an architecture for a 'deep neural network' used for interpretation and we apply algorithms to give similar relevant factors. 2.By exploring different levels of predictive(relevant) factors and visualizing them that can be interacted by the end users at different factor-levels. Interpretation method differentiates the predicted and unpredicted values of stock price. 3.We examine visualization integrated systems using some real-world scenarios like tweeter data, financial news data and obtained stock price values by predictions. The effective working of Deep Clue helps for proper investment in stocks and to analyses tasks.},
	number = {5C},
	journal = {International Journal of Recent Technology and Engineering},
	author = {Sruthi, C. Durga and Reddy, B. Dilip Kumar},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {out, stock price prediction, Deep Clue, Neural Network, Stck market, Text Based Visualization},
	pages = {23--24},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XQ43LIL5\\DeepClue Visual Interpretation of Text-Based Deep Stock Prediction.pdf:application/pdf},
}

@article{Sullivan1997,
	title = {Stock {Price} {Volatility} {Prediction} with {Long} {Short}- {Term} {Memory} {Neural} {Networks}},
	author = {Sullivan, Jason C},
	year = {1997},
	note = {Publisher: IEEE},
	keywords = {stock price prediction, as such, deep reinforcement learning, existing work on the, form of arbitrage, in this study, market, picture of volatility than, the rest of the, them to exploit a, thereby allowing, trading, volatility, we aim to extend},
	pages = {2--5},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FRQI582R\\Stock Price Prediction With Long Short-Term Memory Recurrent Neural Network.pdf:application/pdf},
}

@article{Xiong2017,
	title = {Hybrid {ARIMA}-{BPNN} model for time series prediction of the {Chinese} stock market},
	doi = {10.1109/INFOMAN.2017.7950353},
	abstract = {Stock price prediction is a challenging task owing to the complexity patterns behind time series. Autoregressive integrated moving average (ARIMA) model and back propagation neural network (BPNN) model are popular linear and nonlinear models for time series forecasting respectively. The integration of two models can effectively capture the linear and nonlinear patterns hidden in a time series and improve forecast accuracy. In this paper, a new hybrid ARIMA-BPNN model containing technical indicators is proposed to forecast four individual stocks consisting of both main board market and growth enterprise market in software and information services sector. Experiment results show that the proposed method achieves the better one-step-ahead forecasting accuracies namely 78.79\%, 72.73\%, 59.09\% and 66.67\% respectively for each series than those of ARIMA, BPNN, and Khashei and Bijari's hybrid models.},
	journal = {2017 3rd International Conference on Information Management, ICIM 2017},
	author = {Xiong, Li and Lu, Yue},
	year = {2017},
	note = {Publisher: IEEE
ISBN: 9781509063048},
	keywords = {stock price prediction, Autoregressive integrated moving average, Back propagation neural network, Hybrid models, Stock price prediction},
	pages = {93--97},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6LD9HAZ9\\Hybrid ARIMA-BPNN model for time series prediction of the Chinese stock market.pdf:application/pdf},
}

@article{Bharne2018,
	title = {Survey on combined swarm intelligence and {ANN} for optimized daily stock market price},
	volume = {2018-Janua},
	doi = {10.1109/ICSOFTCOMP.2017.8280083},
	abstract = {Swarm intelligence (SI) is powerful, newly emerged domain belongs to the field of Artificial Intelligence. The SI is inspired from the behavior of biological entities such as honey bee, fireflies, bat, cuckoo, ant etc. The basic idea of SI is that, the collective behavior of agents with a very limited set of rules. In recent SI is applied in various kind of application including appropriate stock market price movement. This paper makes survey of the use of SI in a stock market application. The paper initially describes the details of a stock market, SI and its various types of algorithm and finally describes some recent SI algorithm based approaches for stock market prediction. From this survey, we found that to improve the efficiency of SI and make optimized results, SI is combined with other approaches like Artificial Neural Network (ANN), Machine Learning ML etc. We found that the combination of SI and ANN produce more accurate and optimized results for stock price prediction than the combination of SI and machine learning. Finally paper provides the comparative analysis of recent techniques on the basis of a type of SI used, the algorithm with which SI is combined, comparable algorithm, the dataset used for performance evaluation, its advantages and future trend for each technique. Future trend will be used for further research in the field of SI and stock market applications.},
	journal = {2017 International Conference on Soft Computing and its Engineering Applications: Harnessing Soft Computing Techniques for Smart and Better World, icSoftComp 2017},
	author = {Bharne, Pankaj K. and Prabhune, Sameer S.},
	year = {2018},
	note = {ISBN: 9781538620533},
	keywords = {neural network, stock price prediction, bio inspried computing, stock market, stock price movements, swarm intelligence},
	pages = {1--6},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\THXI7Y6I\\Survey on combined swarm intelligence and ANN for optimized daily stock market price.pdf:application/pdf},
}

@article{Kumar2018b,
	title = {A {Comparative} {Study} of {Supervised} {Machine} {Learning} {Algorithms} for {Stock} {Market} {Trend} {Prediction}},
	doi = {10.1109/ICICCT.2018.8473214},
	abstract = {Impact of many factors on the stock prices makes the stock prediction a difficult and highly complicated task. In this paper, machine learning techniques have been applied for the stock price prediction in order to overcome such difficulties. In the implemented work, five models have been developed and their performances are compared in predicting the stock market trends. These models are based on five supervised learning techniques i.e., Support Vector Machine (SVM), Random Forest, K-Nearest Neighbor (KNN), Naive Bayes, and Softmax. The experimental results show that Random Forest algorithm performs the best for large datasets and Naive Bayesian Classifier is the best for small datasets. The results also reveal that reduction in the number of technical indicators reduces the accuracies of each algorithm.},
	number = {Icicct},
	journal = {Proceedings of the International Conference on Inventive Communication and Computational Technologies, ICICCT 2018},
	author = {Kumar, Indu and Dogra, Kiran and Utreja, Chetna and Yadav, Premlata},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538619742},
	keywords = {machine learning, SVM, Random Forest, stock price prediction, classifier, KNN, Naïve Bayes, Softmax},
	pages = {1003--1007},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8HFTY6NS\\A Comparative Study of Supervised Machine Learning Algorithms for Stock Market Trend Prediction.pdf:application/pdf},
}

@article{Misra2018,
	title = {Stock {Market} {Prediction} using {Machine} {Learning} {Algorithms}: {A} {Classification} {Study}},
	doi = {10.1109/ICRIEECE44171.2018.9009178},
	abstract = {Predicting the stock market has been an area of interest not only for traders but also for the computer engineers. Predictions can be performed by mainly two means, one by using previous data available against the stock and the other by analysing the social media information. Predictions based on previous data lack accuracy due to changing patterns in the stock market al.so, some fields might have been missed due to their insignificance in some stocks or unavailability of data. For example, some models may require 'return rate' as a parameter for stock prediction, but the available data might not have it. On the other hand, a model predicting only on the basis of the return rate may find opening and closing price to be insignificant parameters. The data has to be cleansed before it can be used for predictions. This paper focuses on categorising various methods used for predictive analytics in different domains to date, their shortcomings. Further, the authors of this paper have suggested some improvements that could be incorporated to achieve better accuracy in these approaches.},
	journal = {2018 International Conference on Recent Innovations in Electrical, Electronics and Communication Engineering, ICRIEECE 2018},
	author = {Misra, Meghna and Yadav, Ajay Prakash and Kaur, Harkiran},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538659946},
	keywords = {Machine Learning, stock price prediction, Data Analysis, Linear Regression., Predictive Analysis, Stock Prediction},
	pages = {2475--2478},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JSN44HGC\\Stock Market Prediction using Machine Learning Algorithms A Classification Study.pdf:application/pdf},
}

@article{Nelson2017,
	title = {Stock market's price movement prediction with {LSTM} neural networks},
	volume = {2017-May},
	doi = {10.1109/IJCNN.2017.7966019},
	abstract = {Predictions on stock market prices are a great challenge due to the fact that it is an immensely complex, chaotic and dynamic environment. There are many studies from various areas aiming to take on that challenge and Machine Learning approaches have been the focus of many of them. There are many examples of Machine Learning algorithms been able to reach satisfactory results when doing that type of prediction. This article studies the usage of LSTM networks on that scenario, to predict future trends of stock prices based on the price history, alongside with technical analysis indicators. For that goal, a prediction model was built, and a series of experiments were executed and theirs results analyzed against a number of metrics to assess if this type of algorithm presents and improvements when compared to other Machine Learning methods and investment strategies. The results that were obtained are promising, getting up to an average of 55.9\% of accuracy when predicting if the price of a particular stock is going to go up or not in the near future.},
	number = {Dcc},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	author = {Nelson, David M.Q. and Pereira, Adriano C.M. and De Oliveira, Renato A.},
	year = {2017},
	note = {Publisher: IEEE
ISBN: 9781509061815},
	keywords = {stock price prediction},
	pages = {1419--1426},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WTWSZS8I\\Stock market-s price movement prediction with LSTM neural networks.pdf:application/pdf},
}

@article{Jin2017a,
	title = {Stock price forecasting using support vector regression: {Based} on network behavior data},
	volume = {2018-Janua},
	doi = {10.1109/BigData.2017.8258436},
	abstract = {Stock market research based on network behavior data has become one of the focuses in behavioral finance. In this paper, we firstly construct a proxy variable of investors' attention based on comments data collected from the Snowball Finance, which is a popular online financial community in China. Then we analyze the lead-lag relationship between investors' attention and the stock price of all A-shares listed in both the Shanghai Stock Exchange and Shenzhen Stock Exchange, using Thermal Optimal Path (TOP) method. And we find that investors' attention and stock price have a dynamic relationship, which differs from stock to stock. In terms of quantity, only a small number of stocks have a relationship where investors' attention changes ahead of the stock price. Those two facts may account for conflicting conclusions drew by different studies. Further on, this paper establishes two support vector regression models, comparing the predictive capability of investors' attention to the selected two kinds of stocks. The results show that adding investors' attention to models can only enhance the prediction precision of the 'leading stocks', while it has little effect to the 'lagging stocks'.},
	number = {1992},
	journal = {Proceedings - 2017 IEEE International Conference on Big Data, Big Data 2017},
	author = {Jin, Quan and Guo, Kun and Sun, Yi},
	year = {2017},
	note = {ISBN: 9781538627143},
	keywords = {stock price prediction, Investors' Attention, Stock Price Forecasting, Support Vector Regression, Thermal Optimal Path},
	pages = {4148--4153},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KHY2APAK\\Stock price forecasting using support vector regression Based on network behavior data.pdf:application/pdf},
}

@article{Pun2018,
	title = {Nepal {Stock} {Exchange} {Prediction} {Using} {Support} {Vector} {Regression} and {Neural} {Networks}},
	doi = {10.1109/ICAECC.2018.8479456},
	abstract = {Stock Exchange price prediction is the task of estimating future price of certain stock listed in stock exchange by extracting the trend with the help of confidence learned from historical training data. In this research work, the data set has been created by extracting raw data from Nepal Stock Exchange (NEPSE) website. Data preprocessing is performed in order compute an accurate result. The data belonging to promoter share and unwanted feature are eliminated from considered data. The resulting data are normalized for better performance, before applying the machine learning methods. Min-Max and Z-score normalization are used for this purpose. Overall stock data are further divided into ten different sector of investment for sectorwise analysis. Support Vector Regression (SVR) and Artificial Neural Network (ANN) are applied in order to predict stock price for a next day. In order to measure the performance of two learning models, mean square error (MSE), mean absolute error (MAE), root mean square error (RMSE) and Coefficient of Determination (R2) are used. The result shows that SVR with min max normalization is performing better than ANN in all sectors except on Development bank, Finance, and Mutual Fund.},
	journal = {Proceedings of 2018 2nd International Conference on Advances in Electronics, Computers and Communications, ICAECC 2018},
	author = {Pun, Top Bahadur and Shahi, Tej Bahadur},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538637852},
	keywords = {Machine Learning, stock price prediction, Support Vector Regression, Artificial Neural Network, Min-Max, Nepal Stock Exchange, Normalization, Stock Market Prediction, Z-Score},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CZ3K7XBN\\Nepal Stock Exchange Prediction Using Support Vector Regression and Neural Networks.pdf:application/pdf},
}

@article{Kim2020,
	title = {Predicting the {Direction} of {US} {Stock} {Prices} {Using} {Effective} {Transfer} {Entropy} and {Machine} {Learning} {Techniques}},
	volume = {8},
	issn = {21693536},
	doi = {10.1109/ACCESS.2020.3002174},
	abstract = {This study aims to predict the direction of US stock prices by integrating time-varying effective transfer entropy (ETE) and various machine learning algorithms. At first, we explore that the ETE based on 3 and 6 months moving windows can be regarded as the market explanatory variable by analyzing the association between the financial crises and Granger-causal relationships among the stocks. Then, we discover that the prediction performance on the stock price direction can be improved when the ETE driven variable is integrated as a new feature in the logistic regression, multilayer perceptron, random forest, XGBoost, and long short-term memory network. Meanwhile, we suggest utilizing the adjusted accuracy derived from the risk-adjusted return in finance as a prediction performance measure. Lastly, we confirm that the multilayer perceptron and long short-term memory network are more suitable for stock price prediction. This study is the first attempt to predict the stock price direction using ETE, which can be conveniently applied to the practical field.},
	journal = {IEEE Access},
	author = {Kim, Sondo and Ku, Seungmo and Chang, Woojin and Chang, Woojin and Chang, Woojin and Song, Jae Wook},
	year = {2020},
	keywords = {machine learning, information entropy, out, stock price prediction, Econophysics, effective transfer entropy, feature engineering, prediction algorithms, stock markets, time series analysis},
	pages = {111660--111682},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DIKEDNKN\\Predicting the Direction of US Stock Prices Using Effective Transfer Entropy and Machine Learning Techniques.pdf:application/pdf},
}

@article{Merello2019,
	title = {Ensemble {Application} of {Transfer} {Learning} and {Sample} {Weighting} for {Stock} {Market} {Prediction}},
	volume = {2019-July},
	doi = {10.1109/IJCNN.2019.8851938},
	abstract = {Forecasting stock market behavior is an interesting and challenging problem. Regression of prices and classification of daily returns have been widely studied with the main goal of supplying forecasts useful in real trading scenarios. Unfortunately, the outcomes are not directly related with the maximization of the financial gain. Firstly, the optimal strategy requires to invest on the most performing asset every period and trading accordingly is not trivial given the predictions. Secondly, price fluctuations of different magnitude are often treated as equals even if during market trading losses or gains of different intensities are derived. In this paper, the problem of stock market forecasting is formulated as regression of market returns. This approach is able to estimate the amount of price change and thus the most performing assets. Price fluctuations of different magnitude are treated differently through the application of different weights on samples and the scarcity of data is addressed using transfer learning. Results on a real simulation of trading show how, given a finite amount of capital, the predictions can be used to invest in high performing stocks and, hence, achieve higher profits with less trades.},
	number = {July},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	author = {Merello, Simone and Ratto, Andrea Picasso and Oneto, Luca and Cambria, Erik},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728119854},
	keywords = {out, stock price prediction, Financial forecasting, Stock market prediction},
	pages = {1--8},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P3J76KQC\\Ensemble Application of Transfer Learning and Sample Weighting for Stock Market Prediction.pdf:application/pdf},
}

@article{Yao2018,
	title = {High-frequency stock trend forecast using {LSTM} model},
	doi = {10.1109/ICCSE.2018.8468703},
	abstract = {The prediction of price trend in stock market is a challenging task due to the inherent complexity and dynamics in price movement. Many machine learning algorithms, such as Support Vector Machine, Artificial Neural Network, and Hidden Markov Model, have been applied to it and achieved positive results. Long Short-Term Memory (LSTM), as a variant of RNN, can obtain hidden dependencies in data and has shown a significant performance in processing time series data. In this paper, we apply LSTM networks to predict the price movement of a short-term and test it by an experiment on some stocks randomly selected from CSI 300 constituent stocks. The experiment shows that the precision, recall rate and critical error of LSTM are all better than that of the random prediction. It indicates that LSTM can be used in the trend prediction of stock price. We also notice that many improvements need to be done in future.},
	number = {Iccse},
	journal = {13th International Conference on Computer Science and Education, ICCSE 2018},
	author = {Yao, Siyu and Luo, Linkai and Peng, Hong},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538654958},
	keywords = {stock price prediction, Financial time series, Long Short-Term Memory, Trend prediction},
	pages = {293--296},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SX2S2GX8\\High-Frequency Stock Trend Forecast Using LSTM Model.pdf:application/pdf},
}

@article{Abrishami2019,
	title = {Enhancing profit by predicting stock prices using deep neural networks},
	volume = {2019-Novem},
	issn = {10823409},
	doi = {10.1109/ICTAI.2019.00223},
	abstract = {Financial time series forecasting is a challenging task, which has attracted the interest of several researchers and is immensely important for investors. In this paper, we present a deep learning system, which uses a variety of data for a subset of the stocks on the NASDAQ exchange to forecast the stock price. The prediction model is trained on the minutely data for a specific stock ticker and predicts the closing price of that stock ticker for multi-step-ahead. Our deep learning framework consists of a Variational Autoencoder for removing noise and uses time-series data engineering to combine the higher-level features with the original features. This new set of features is fed to a Stacked LSTM Autoencoder for multi-step-ahead prediction of the stock closing price. Besides, this prediction is used by a profit-maximization strategy to provide advice on the appropriate time for buying and selling a specific stock. Results show that the proposed framework outperforms the state-of-the-art time series forecasting approaches with respect to predictive accuracy and profitability.},
	journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
	author = {Abrishami, Soheila and Turek, Michael and Roy Choudhury, Ahana and Kumar, Piyush},
	year = {2019},
	note = {ISBN: 9781728137988},
	keywords = {LSTM, stock price prediction, in, Feature engineering, Financial time series prediction, Stock price, Variational autoencoder},
	pages = {1551--1556},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L45V5AFZ\\Enhancing Profit by Predicting Stock Prices using Deep Neural Networks.pdf:application/pdf},
}

@article{Liu2019,
	title = {Application of {Regularized} {GRU}-{LSTM} {Model} in {Stock} {Price} {Prediction}},
	doi = {10.1109/ICCC47050.2019.9064035},
	abstract = {The stock market is a highly complex nonlinear movement system, and its fluctuation law is affected by many factors, so the prediction of the stock price index is a very challenging task. There are many examples showing that Neural Network algorithms are well suited for such time series predictions and often achieve satisfactory results. In this paper, based on the existing models, we proposed a Regularized GRULSTM neural network model and applied it to the short-term forecast of closing price of the two stocks. The experimental results show that our proposed model is superior to the existing GRU and LSTM network models in stock time series prediction.},
	journal = {2019 IEEE 5th International Conference on Computer and Communications, ICCC 2019},
	author = {Liu, Yiwei and Wang, Zhiping and Zheng, Baoyou},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728147437},
	keywords = {LSTM, stock price prediction, in, GRU, Time series prediction},
	pages = {1886--1890},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZPLEXVW3\\Application of Regularized GRU-LSTM Model in Stock Price Prediction.pdf:application/pdf},
}

@article{Coelho2019,
	title = {Social media and forecasting stock price change},
	volume = {2},
	issn = {07303157},
	doi = {10.1109/COMPSAC.2019.10206},
	abstract = {The Stock Market is a big influence on both national and international economies. Stock prices are driven by a number of factors: industry performance, company news and performance, investor confidence, micro and macro economic factors like employment rates, wage rates, etc. Stock pricing trends can be gauged from the factors that drive it as well as from the stock's historical performance. As fluctuations in stock prices become more volatile and unpredictable, forecasting models help reduce some of the randomness involved in investing and financial decision making. Users on social media platforms like twitter, StockTwits, and eToro discuss issues related to the stock market. Can the analysis of posts on StockTwits add value to the existing features of stock price predicting models? An existing model that uses twits as features was extended to include sentiment analysis of the text referenced by the URL in the twits to see if the model accuracy did improve. Initial results indicate that the addition of sentiment analysis of the text referenced by the URL does not improve the performance of the model when all twits for a given day are taken into account since the model only identifies the direction of change and not the degree of change. The stock prediction model achieves 65\% accuracy compared to the base case accuracy of 44\% and augmenting the model with sentiment analysis did not change the accuracy. The study highlights some interesting observations regarding users on the StockTwits social media platform and proposes the need for a domain specific sentiment analyzer in future work.},
	journal = {Proceedings - International Computer Software and Applications Conference},
	author = {Coelho, Joseph and D’Almeida, Dawson and Coyne, Scott and Gilkerson, Nathan and Mills, Katelyn and Madiraju, Praveen},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728126074},
	keywords = {Big data, Machine learning, Social media, out, stock price prediction, Index Terms—stock market, Sentiment analysis},
	pages = {195--200},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BAE6FBPW\\Social Media and Forecasting Stock Price Change.pdf:application/pdf},
}

@article{Jiao2017,
	title = {Predicting stock movement direction with machine learning: {An} extensive study on {S}\&{P} 500 stocks},
	volume = {2018-Janua},
	doi = {10.1109/BigData.2017.8258518},
	abstract = {Stocks movement direction forecasting has received a lot of attention. Indeed, being able to make accurate forecasts has strong implications on trading strategies. Surprisingly enough little has been published, relatively to the importance of the topic. In this paper, we reviewed how well four classic classification algorithms: random forest, gradient boosted trees, artificial neural network and logistic regression perform in predicting 463 stocks of the S\&P 500. Several experiments were conduced to thoroughly study the predictability of these stocks. To validate each prediction algorithm, three schemes we compared: standard cross validation, sequential validation and single validation. As expected, we were not able to predict stocks future prices from their past. However, unexpectedly, we were able to show that taking into account recent information - such as recently closed European and Asian indexes - to predict S\&P 500 can lead to a vast increase in predictability. Moreover, we also found out that, among various sectors, financial sector stocks are comparatively more easy to predict than others.},
	journal = {Proceedings - 2017 IEEE International Conference on Big Data, Big Data 2017},
	author = {Jiao, Yang and Jakubowicz, Jérémie},
	year = {2017},
	note = {ISBN: 9781538627143},
	keywords = {classification, evaluation, stock price prediction, stock prediction},
	pages = {4705--4713},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3S6963WE\\Predicting stock movement direction with machine learning An extensive study on S-P 500 stocks.pdf:application/pdf},
}

@article{Pasupulety2019,
	title = {Predicting stock prices using ensemble learning and sentiment analysis},
	doi = {10.1109/AIKE.2019.00045},
	abstract = {The recent success of the application of Artificial Intelligence in the financial sector has resulted in more firms relying on stochastic models for predicting the behaviour of the market. Everyday, quantitative analysts strive to attain better accuracies from their machine learning models for forecasting returns from stocks. Support Vector Machine (SVM) and Random Forest based regression models are known for their effectiveness in accurately predicting closing prices. In this work, we propose a technique for analyzing and predicting stock prices of companies using the aforementioned algorithms as an ensemble. Datasets from India's National Stock Exchange (NSE) containing basic market price information are preprocessed to include well known leading technical indicators as features. Feature selection, which ranks features based on their degree of influence on the final closing price has been incorporated to reduce the size of the training dataset. Additionally, we evaluate the effectiveness of considering the public opinion of a company by employing sentiment analysis. Using a trained Word2Vec model, company specific hash-tagged posts from Twitter are classified as positive or negative. Our proposed ensemble model is then trained on a new dataset which combines the technical indicator data along with the aggregated number of positive/negative tweets of a company over time. Our experiments indicate that in some scenarios, the ensemble model performs better than the constituent models and is highly dependent of the nature and size of the training data. However, combining technical indicator data with aggregated positive/negative tweet counts has a negligible effect on the performance of the ensemble model.},
	journal = {Proceedings - IEEE 2nd International Conference on Artificial Intelligence and Knowledge Engineering, AIKE 2019},
	author = {Pasupulety, Ujjwal and Abdullah Anees, Aiman and Anmol, Subham and Mohan, Biju R.},
	year = {2019},
	note = {ISBN: 9781728114880},
	keywords = {Machine learning, out, stock price prediction, Stock market prediction, Sentiment analysis, Ensemble regressors},
	pages = {215--222},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9KWI2CSR\\Predicting Stock Prices using Ensemble Learning and Sentiment Analysis.pdf:application/pdf},
}

@article{Ntakaris2019,
	title = {Feature {Engineering} for {Mid}-{Price} {Prediction} with {Deep} {Learning}},
	volume = {7},
	issn = {21693536},
	doi = {10.1109/ACCESS.2019.2924353},
	abstract = {Mid-price movement prediction based on the limit order book data is a challenging task due to the complexity and dynamics of the limit order book. So far, there have been very limited attempts for extracting relevant features based on the limit order book data. In this paper, we address this problem by designing a new set of handcrafted features and performing an extensive experimental evaluation on both liquid and illiquid stocks. More specifically, we present an extensive set of econometric features that capture the statistical properties of the underlying securities for the task of mid-price prediction. The experimental evaluation consists of a head-to-head comparison with other handcrafted features from the literature and with features extracted from a long short-term memory autoencoder by means of a fully automated process. Moreover, we develop a new experimental protocol for online learning that treats the task above as a multi-objective optimization problem and predicts: 1) the direction of the next price movement and; 2) the number of order book events that occur until the change takes place. In order to predict the mid-price movement, features are fed into nine different deep learning models based on multi-layer perceptrons, convolutional neural networks, and long short-term memory neural networks. The performance of the proposed method is then evaluated on liquid and illiquid stocks (i.e., TotalView-ITCH US and Nordic stocks). For some stocks, results suggest that the correct choice of a feature set and a model can lead to the successful prediction of how long it takes to have a stock price movement.},
	journal = {IEEE Access},
	author = {Ntakaris, Adamantios and Mirone, Giorgio and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros},
	year = {2019},
	note = {arXiv: 1904.05384
Publisher: IEEE},
	keywords = {Deep learning, stock price prediction, in, econometrics, high-frequency trading, limit order book, mid-price, U.S. data},
	pages = {82390--82412},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\B7X8CEMZ\\Feature Engineering for Mid-Price Prediction With Deep Learning.pdf:application/pdf},
}

@article{Coyne2018,
	title = {Forecasting stock prices using social media analysis},
	volume = {2018-Janua},
	doi = {10.1109/DASC-PICom-DataCom-CyberSciTec.2017.169},
	abstract = {Stock market prices are becoming more and more volatile, largely due to improvements in technology and increased trading volume. Speculation affects business owners, investors, and policymakers alike. While these seemingly unpredictable trends continue, investors and consumers take to social media to share thoughts and opinions. We use information shared over StockTwits, a social media platform for investors, to better understand and predict individual stock prices. We designed and implemented three machine learning models to forecast stock prices using the dataset collected from StockTwits. We also evaluated our models with conclusions drawn from previous researchers in this field. Our first model found no correlation between general StockTwits postings and stock price. However, our second and third models considered a novel approach and successfully filtered through the twits to find important posts. These important twits could predict stock price movements with greater accuracy (average around 65\%) based on sentiment analysis and smart user identification. We consider a user "smart" based on number of likes, follower count and more importantly how often the user is right about a stock.},
	journal = {Proceedings - 2017 IEEE 15th International Conference on Dependable, Autonomic and Secure Computing, 2017 IEEE 15th International Conference on Pervasive Intelligence and Computing, 2017 IEEE 3rd International Conference on Big Data Intelligence and Compu},
	author = {Coyne, Scott and Madiraju, Praveen and Coelho, Joseph},
	year = {2018},
	note = {ISBN: 9781538619551},
	keywords = {Prediction, Big data, Machine learning, stock price prediction, Sentiment analysis, Social media analysis, Stock market},
	pages = {1031--1038},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WXAUSYZC\\Forecasting Stock Prices Using Social Media Analysis.pdf:application/pdf},
}

@article{Attanasio2019,
	title = {Combining news sentiment and technical analysis to predict stock trend reversal},
	volume = {2019-Novem},
	issn = {23759259},
	doi = {10.1109/ICDMW.2019.00079},
	abstract = {The use of machine learning techniques to predict the next-day stock direction is established. To make prediction models more robust, a common approach is to combine historical time series and news sentiment analysis. Most of the trading simulations performed in this field rely on trend following strategies, which are aimed at identifying and following an ongoing price trend that is likely to persist in the next days. Conversely, a more limited effort has been devoted to applying machine learning techniques to predict trend reversal, i.e., changes in price directions. This paper investigates the relevance of news information and time series descriptors derived from technical analysis to predict trend reversal in the next days. It compares the performance of various classification models trained on (i) technical indicators, which indicate short-term overbought or oversold conditions, (ii) news sentiment descriptors, which express the opinion of the financial community, (iii) the historical time series, to highlight recurrences in price trends, and (iv) a combination of the above. The results achieved on an 11-year dataset related to the stocks of the U.S. S\&P 500 index show that the strategies combining the historical values of news sentiment and stock price indicators averagely perform better than all the other tested combinations. Hence, news information is worth considering by trend reversal strategies.},
	number = {iv},
	journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
	author = {Attanasio, Giuseppe and Cagliero, Luca and Garza, Paolo and Baralis, Elena},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728146034},
	keywords = {classification, out, stock price prediction, news sentiment analysis, quantitative trading, trend reversal prediction},
	pages = {514--521},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HKXPS47Z\\Combining News Sentiment and Technical Analysis to Predict Stock Trend Reversal.pdf:application/pdf},
}

@article{Ananthakumar2018,
	title = {Application of {Logistic} {Regression} in {Assessing} {Stock} {Performances}},
	volume = {2018-Janua},
	doi = {10.1109/DASC-PICom-DataCom-CyberSciTec.2017.199},
	abstract = {Stock market prediction pertains to predicting the future value of stock of a company or other financial instrument traded on an exchange. Though the successful prediction of a stock's future price is a complex phenomenon, even a reasonable prediction would yield significant profit and this study attempts to address this unpredictability with the help of a data mining technique. In this study, the companies listed on BSE SENSEX are chosen as representative set of companies that are most actively traded. Logistic Regression is used on various important financial ratios of these companies and certain macro financial variables to analyze which ratios are important and how they are affecting the stock prices. The proposed model results in better classification accuracy when compared to a similar study in the literature.},
	journal = {Proceedings - 2017 IEEE 15th International Conference on Dependable, Autonomic and Secure Computing, 2017 IEEE 15th International Conference on Pervasive Intelligence and Computing, 2017 IEEE 3rd International Conference on Big Data Intelligence and Compu},
	author = {Ananthakumar, Usha and Sarkar, Ratul},
	year = {2018},
	note = {ISBN: 9781538619551},
	keywords = {classification accuracy, out, stock price prediction, financial ratios, multicollinearity, ROC curve},
	pages = {1242--1247},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\R3A5LCZ9\\Application of Logistic Regression in Assessing Stock Performances.pdf:application/pdf},
}

@article{Ntemi2019,
	title = {A dyadic particle filter for price prediction},
	volume = {2019-Septe},
	issn = {22195491},
	doi = {10.23919/EUSIPCO.2019.8903079},
	abstract = {The most difficult task in financial forecasting is the accurate price prediction based on previous values. Two cases are studied: stock price prediction and flight price prediction. A dyadic particle filter is proposed that is based on sequential importance resampling. This dyadic particle filter captures the dynamic evolution of a pair of latent vectors. In stock price prediction, one latent vector is defined for each stock. This latent vector is paired with a market segment latent vector introduced for each group of companies of the same category. Both latent vectors capture the hidden information of the stock market and reinforce the state estimation procedure. This hidden information influences strongly the performance of the particle filter, yielding more accurate prediction of stock prices than the state-of-the-art techniques. For flight price prediction, the pair of latent vectors corresponds to route and destination, respectively. Given the price range of each flight, promising results are disclosed.},
	journal = {European Signal Processing Conference},
	author = {Ntemi, Myrsini and Kotropoulos, Constantine},
	year = {2019},
	note = {Publisher: EURASIP
ISBN: 9789082797039},
	keywords = {out},
	pages = {0--4},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5HMRIH8E\\A Dyadic Particle Filter for Price Prediction.pdf:application/pdf},
}

@article{Lai2017,
	title = {A new method for stock price prediction based on {MRFs} and {SSVM}},
	volume = {2017-Novem},
	issn = {23759259},
	doi = {10.1109/ICDMW.2017.113},
	abstract = {Trading strategies basing on both financial analysis and machine learning techniques are becoming increasingly popular due to their ability to capture micro market price movements and leverage big data. An important class of works are focusing on exploiting the structural relationships between companies for accurate stock price prediction. In this paper we develop an algorithm for learning the parameters of unary and binary potentials in binary markov random fields (MRFs) under the max-margin framework. We first show how to train unary potentials using market price data and Gaussian Mixture Models (GMMs). Then, we developed a graph-cut based algorithm to solve the inference problem exactly. We demonstrate the learning of potentials' parameters using a max-margin learning framework. Experiment is conducted by comparing performances between our formulation and conventional SVM method. Results show that our method outperforms SVM by 27.9\% on train set and 40.5\% on test set.},
	journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
	author = {Lai, Lin and Li, Chang and Long, Wen},
	year = {2017},
	note = {ISBN: 9781538614808},
	keywords = {stock price prediction},
	pages = {818--823},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GIKQ9G8X\\A New Method for Stock Price Prediction Based on MRFs and SSVM.pdf:application/pdf},
}

@article{Wang2019b,
	title = {Stock volatility prediction by hybrid neural network},
	volume = {7},
	issn = {21693536},
	doi = {10.1109/ACCESS.2019.2949074},
	abstract = {Stock price volatility forecasting is a hot topic in time series prediction research, which plays an important role in reducing investment risk. However, the trend of stock price not only depends on its historical trend, but also on its related social factors. This paper proposes a hybrid time-series predictive neural network (HTPNN) that combines the effection of news. The features of news headlines are expressed as distributed word vectors which are dimensionally reduced to optimize the efficiency of the model by sparse automatic encoders. Then, according to the timeliness of stocks, the daily K-line data is combined with the news. HTPNN captures the potential law of stock price fluctuation by learning the fusion feature of news and time series, which not only retains the effective information of news and stock data, but also eliminates the redundant information of the text. Compared with the state-of-the-art methods, our method combines more abundant stock characteristics and has more advantages in running speed. Besides, the accuracy is averagely improved by nearly 5\%.},
	journal = {IEEE Access},
	author = {Wang, Yujie and Liu, Hui and Guo, Qiang and Xie, Shenxiang and Zhang, Xiaofeng},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {natural language processing, out, stock price prediction, hybrid neural network, news, Stock prediction},
	pages = {154524--154534},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RT7989L9\\Stock Volatility Prediction by Hybrid Neural Network.pdf:application/pdf},
}

@article{Wen2019,
	title = {Stock market trend prediction using high-order information of time series},
	volume = {7},
	issn = {21693536},
	doi = {10.1109/ACCESS.2019.2901842},
	abstract = {Given a financial time series such as S\&P 500, or any historical data in stock markets, how can we obtain useful information from recent transaction data to predict the ups and downs at the next moment? Recent work on this issue shows initial evidence that machine learning techniques are capable of identifying (non-linear) dependency in the stock market price sequences. However, due to the high volatility and non-stationary nature of the stock market, forecasting the trend of a financial time series remains a big challenge. In this paper, we introduced a new method to simplify noisy-filled financial temporal series via sequence reconstruction by leveraging motifs (frequent patterns), and then utilize a convolutional neural network to capture spatial structure of time series. The experimental results show the efficiency of our proposed method in feature learning and outperformance with 4\%-7\% accuracy improvement compared with the traditional signal process methods and frequency trading patterns modeling approach with deep learning in stock trend prediction.},
	journal = {IEEE Access},
	author = {Wen, Min and Li, Ping and Zhang, Lingfei and Chen, Yan},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {convolutional neural network, out, stock price prediction, Trend prediction, financial time series, motif extraction},
	pages = {28299--28308},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TGCKZSSE\\Stock Market Trend Prediction Using High-Order Information of Time Series.pdf:application/pdf},
}

@article{Hou2018,
	title = {Stock price prediction based on {Grey} {Relational} {Analysis} and support vector regression},
	doi = {10.1109/CCDC.2018.8407547},
	abstract = {Stock market data is extremely large and complicated. In stock prediction research, the selection of technical indicators has not a scientific theory as a guide. This paper proposes a novel method based on Grey Relational Analysis to select the technical indicators. Then make predictions by Support Vector Regression that optimized by improved fruit fly optimization algorithm. Firstly, the fruit fly optimization algorithm is improved by decreasing footstep and simulated annealing. Secondly, the improved fruit fly optimization algorithm is adopted to optimize the penalty factor c and the kernel function parameter g of the support vector regression. Finally, modeling and forecasting of the stock price with optimized support vector regression are conducted and some simulation experiments are carried out. The Support Vector Regression is adept at analyzing small size and multi-dimensional samples, so it is suitable for short-term stock prediction. By comparing with other three methods, the one this paper proposed could fast convergence and improve the accuracy of forecasting and is an efficient and feasible method.},
	number = {61370154},
	journal = {Proceedings of the 30th Chinese Control and Decision Conference, CCDC 2018},
	author = {Hou, Xianxian and Zhu, Shaohan and Xia, Li and Wu, Gang},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538612439},
	keywords = {prediction, out, stock price prediction, Support Vector Regression, Fruit fly Optimization Algorithm, Grey Relational Analysis, Simulated Annealing},
	pages = {2509--2513},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RZLJINJ8\\Stock price prediction based on Grey Relational Analysis and support vector regression.pdf:application/pdf},
}

@article{Yeze2019,
	title = {Stock {Price} {Prediction} {Based} on {Information} {Entropy} and {Artificial} {Neural} {Network}},
	doi = {10.1109/INFOMAN.2019.8714662},
	abstract = {Stock market is one of the most important components of the financial system. It directs money from investors to support the activity and development of the associated company. Therefore, understanding and modeling the stock price dynamics become critically important, in terms of financial system stability, investment strategy, and market risk control. To better model the temporal dynamics of stock price, we propose a combined machine learning framework with information theory and Artificial Neural Network (ANN). This method creatively uses information entropy to inform non-linear causality as well as stock relevance and uses it to facilitate the ANN time series modeling. Our analysis with Google, Amazon, Facebook, and Apple stock prices demonstrates the feasibility of this machine learning framework.},
	journal = {5th International Conference on Information Management, ICIM 2019},
	author = {Yeze, Zang and Yiying, Wang},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728134307},
	keywords = {LSTM, Neural network, stock price prediction, in, Stock prediction, Transfer entropy},
	pages = {248--251},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4TZ99RJ4\\Stock Price Prediction Based on Information Entropy and Artificial Neural Network.pdf:application/pdf},
}

@article{Tantisripreecha2018,
	title = {Stock {M} arket {M} ovement {Prediction} using {LDA}-{Online} {Learning} {Model}},
	author = {Tantisripreecha, Tanapon},
	year = {2018},
	note = {ISBN: 9781538658895},
	keywords = {stock price prediction},
	pages = {135--139},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8XDXG92A\\Stock Market Movement Prediction using LDA-Online Learning Model.pdf:application/pdf},
}

@article{Sable2017,
	title = {Stock price prediction using genetic algorithms and evolution strategies},
	volume = {2017-Janua},
	doi = {10.1109/ICECA.2017.8212724},
	abstract = {Stock market is a very challenging and an interesting field. In this paper, we are trying to predict the target prices of the stocks for the short term. We are predicting the target priceof script individually for eight different scripts. For each script, six attributes are used which help us to find, whether the prices are going up or down. The evolutionary techniques used for this experiment are the genetic algorithms and evolution strategies. By using these algorithms, we are trying to find the connection weight for each attribute, which helps us in predicting the target price of the stock. An input for each attribute is given to a sigmoid function after it is amplified based on its connection weight. The experimental results show that using this approach, predicting the stock price is promising. In each case, the algorithms were able to predict with an accuracy of at least 70.00.},
	journal = {Proceedings of the International Conference on Electronics, Communication and Aerospace Technology, ICECA 2017},
	author = {Sable, Sonal and Porwal, Ankita and Singh, Upendra},
	year = {2017},
	note = {ISBN: 9781509056866},
	keywords = {Machine learning, genetic algorithm, stock price prediction, stock market, Evolutionary Strategies},
	pages = {549--553},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T7MIIGFZ\\Stock price prediction using genetic algorithms and evolution strategies.pdf:application/pdf},
}

@article{Sharma2017,
	title = {Combining of random forest estimates using {LSboost} for stock market index prediction},
	volume = {2017-Janua},
	doi = {10.1109/I2CT.2017.8226316},
	abstract = {This research work emphases on the prediction of future stock market index values based on historical data. The experimental evaluation is based on historical data of 10 years of two indices, namely, CNX Nifty and S\&P Bombay Stock Exchange (BSE) Sensex from Indian stock markets. The predictions are made for 1-10, 15, 30, and 40 days in advance. This work proposes to combine the predictions/estimates of the ensemble of trees in a Random Forest using LSboost (i.e. LS-RF). The prediction performance of the proposed model is compared with that of well-known Support Vector Regression. Technical indicators are selected as inputs to each of the prediction models. The closing value of the stock price is the predicted variable. Results show that the proposed scheme outperforms Support Vector Regression and can be applied successfully for building predictive models for stock prices prediction.},
	number = {1},
	journal = {2017 2nd International Conference for Convergence in Technology, I2CT 2017},
	author = {Sharma, Nonita and Juneja, Akanksha},
	year = {2017},
	note = {ISBN: 9781509043071},
	keywords = {Regression, Random Forest, stock price prediction, Stock Market Prediction, Least Square Boost},
	pages = {1199--1202},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BWRVENIQ\\Combining of random forest estimates using LSboost for stock market index prediction.pdf:application/pdf},
}

@article{Maurya2019,
	title = {Recursive {Stock} {Price} {Prediction} with {Machine} {Learning} and {Web} {Scrapping} for {Specified} {Time} {Period}},
	volume = {2019-Decem},
	issn = {21517703},
	doi = {10.1109/WOCN45266.2019.8995080},
	abstract = {In the finance world inventory trading is one of the most necessary activities. Stock market prediction is an act of attempting to decide the future price of a stock other monetary instrument traded on a financial exchange. The technical and integral or the time sequence evaluation is used with the aid of most of the stockbrokers while making the inventory predictions. This paper explains the prediction of a stock using Machine Learning. The input parameters include-open, high, low, close rate, trading volume, Price to Earning Ratio, MA, MACD for more accuracy. The Machine Learning algorithm, Random Forest Regression has been implemented in Python programming language which is used to predict the stock market. The algorithm has been used on the historical stock data along with web-scraping technique that has been applied to catch current market data of the stock. The recursive training model take its predicted value as input to predict further long term future stock rates.},
	journal = {IFIP International Conference on Wireless and Optical Communications Networks, WOCN},
	author = {Maurya, Bikrant Bikram P. and Ray, Ayush and Upadhyay, Aman and Gour, Bhupesh and Khan, Asif Ullah},
	year = {2019},
	note = {ISBN: 9781728100128},
	keywords = {stock price prediction, in, Stock Market Prediction, Accuracy, Bollinger Bands, CSV, Data Set, Ma-chine Learning, MA(Moving Average), Nifty50, NSE(National Stock Exchange), PE(Price Earning) Ratio, Random Forest Regression and Classification, Scikit-Learn, Stock Market, Web Scrapping},
	pages = {39--41},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CF3EBF6C\\Recursive Stock Price Prediction With Machine Learning And Web Scrapping For Specified Time Period.pdf:application/pdf},
}

@article{Zhang2019h,
	title = {Prediction on the highest price of the stock based on {PSO}-{LSTM} neural network},
	doi = {10.1109/EITCE47263.2019.9094982},
	abstract = {With the development of artificial intelligence technology, the stock market has entered a new research stage. Machine learning algorithm is widely used in the prediction of financial time series. The highest price of a stock is an important factor to measure the price of a stock. For this reason, this paper uses three representative historical data of American stocks as the research object to analyze the short-term price trend. In addition to the original short and long term memory networks, in order to improve the accuracy of prediction, we put forward a prediction model combining particle swarm optimization algorithm and long-and short-term memory neural network. Through empirical research, the PSO optimization algorithm proposed for LSTM model can quickly find the optimal network weight of the neural network, reduce the loss function, achieve the effect of rapid fitting, and has a more accurately predicted results.},
	journal = {2019 IEEE 3rd International Conference on Electronic Information Technology and Computer Engineering, EITCE 2019},
	author = {Zhang, Yushan and Yang, Sitong},
	year = {2019},
	note = {ISBN: 9781728135847},
	keywords = {Prediction, out, stock price prediction, LSTM neural network, PSO algorithm, The highest price of the stock},
	pages = {1565--1569},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C2RCHPD3\\Prediction on the Highest Price of the Stock Based on PSO-LSTM Neural Network.pdf:application/pdf},
}

@article{Wibowo2018,
	title = {Nonlinear autoregressive exogenous model ({NARX}) in stock price index's prediction},
	volume = {2018-Janua},
	doi = {10.1109/ICITISEE.2017.8285507},
	abstract = {The stock market can provide huge profits in a relatively short time in financial sector. However, it also has a high risk for investors and traders if they are not careful to look the factors that affect the stock market. Therefore, they should give attention to the dynamic fluctuations and movements of the stock market to optimize profits from their investment. In this paper, we present a nonlinear autoregressive exogenous model (NARX) to predict the movements of stock market especially the movements of the closing price index. As case study, we consider to predict the movement of the closing price in Indonesia composite index (IHSG) and choose the best structures of NARX for IHSG's prediction which the number of input neurons, neurons in its single layer, feedback delay, input delay and output neuron are 6, 10, 1, 2 and 1, respectively.},
	journal = {Proceedings - 2017 2nd International Conferences on Information Technology, Information Systems and Electrical Engineering, ICITISEE 2017},
	author = {Wibowo, Antoni and Pujianto, Harry and Saputro, Dewi Retno Sari},
	year = {2018},
	note = {ISBN: 9781538606582},
	keywords = {time series, prediction, stock price prediction, stock market, NARX},
	pages = {26--29},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9CTGFYQ7\\Nonlinear autoregressive exogenous model -NARX- in stock price index-s prediction.pdf:application/pdf},
}

@article{Yang2019,
	title = {Explainable {Text}-{Driven} {Neural} {Network} for {Stock} {Prediction}},
	doi = {10.1109/CCIS.2018.8691233},
	abstract = {It has been shown that financial news leads to the fluctuation of stock prices. However, previous work on news-driven financial market prediction focused only on predicting stock price movement without providing an explanation. In this paper, we propose a dual-layer attention-based neural network to address this issue. In the initial stage, we introduce a knowledge-based method to adaptively extract relevant financial news. Then, we use an input attention to pay more attention to the more influential news and concatenate the day embeddings with the output of the news representation. Finally, we use an output attention mechanism to allocate different weights to different days in terms of their contribution to stock price movement. Thorough empirical studies based upon historical prices of several individual stocks demonstrate the superiority of our proposed method in stock price prediction compared to state-of-the-art methods.},
	journal = {Proceedings of 2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems, CCIS 2018},
	author = {Yang, Linyi and Zhang, Zheng and Xiong, Su and Wei, Lirui and Ng, James and Xu, Lina and Dong, Ruihai},
	year = {2019},
	note = {arXiv: 1902.04994
Publisher: IEEE
ISBN: 9781538660041},
	keywords = {out, stock price prediction, Stock Prediction, Attention Mechanism, Explainable Model},
	pages = {441--445},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LXKSKVL8\\Explainable Text-Driven Neural Network for Stock Prediction.pdf:application/pdf},
}

@article{Loke2017,
	title = {Impact of financial ratios and technical analysis on stock price prediction using random forests},
	volume = {2018-Janua},
	doi = {10.1109/ICONDA.2017.8270396},
	abstract = {A stock movement prediction method is presented using quarterly financial ratio data from Hong Kong companies from the period, 2011-2014. We found that the accuracy of price movement prediction using Random Forest method over multiple quarters to be fairly weak. However we were able to predict with high accuracy in the last quarter of 2014 and not in other years. We attribute this not to the superiority of the method but to the non-stationary nature of the price signals.},
	journal = {1st International Conference on Computer and Drone Applications: Ethical Integration of Computer and Drone Technology for Humanity Sustainability, IConDA 2017},
	author = {Loke, K. S.},
	year = {2017},
	note = {ISBN: 9781538607657},
	keywords = {Random Forest, stock price prediction, Stock Prediction, Stock market, Financial Ratios},
	pages = {38--42},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T2DRLNL3\\Impact of financial ratios and technical analysis on stock price prediction using random forests.pdf:application/pdf},
}

@article{Gite2018,
	title = {Surveying various genetic programming ({GP}) approaches to forecast real-time trends \& prices in the stock market},
	volume = {2018-Janua},
	doi = {10.1109/SAI.2017.8252093},
	abstract = {The share prices in the stock market are known for their extreme unpredictability and attempts to identify any familiar patterns in the prices poses a confounding problem for both fundamental \& technical analysts. This article attempts to use symbolic regression capabilities of GP and a market trend indicator (RSI) to predict the price and trend of the particular stock as accurately as possible. The use of a market indicator to independently forecast the trend without any role of GP serves as a verification mechanism to the price predicted by GP for the next day to further validate the authenticity of the price of the stock in the context of the real-time stock market. Extensive testing has been done on the various evolution parameters and functions of GP to customize the GP approach as much as possible to suit the current application and optimize the results. Though obtained results can never be fully relied on by real technical analysts of the stock market, it could definitely be used as a decision making support.},
	number = {July},
	journal = {Proceedings of Computing Conference 2017},
	author = {Gite, Balasaheb and Sayed, Khalid and Mutha, Navin and Marpadge, Saurabhkumar and Patil, Kshitij},
	year = {2018},
	note = {ISBN: 9781509054435},
	keywords = {stock price prediction, Stock price, Genetic Programming (GP), Relative Strength Index (RSI), Stock trend},
	pages = {131--134},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\45ZWHNTD\\Surveying various genetic programming -GP- approaches to forecast real-time trends - prices in the stock market.pdf:application/pdf},
}

@article{Gao2018,
	title = {Applying long short term momory neural networks for predicting stock closing price},
	volume = {2017-Novem},
	issn = {23270594},
	doi = {10.1109/ICSESS.2017.8342981},
	abstract = {The main goal of this paper is to assess the hypothesis that combining RNNs with informative input variables can provide a more effective method for predicting the next-day stock movement. Moreover, we propose using long short term memory (LSTM) aand stock basic trading data to realize the stock prediction model. For training the model, we utilize some optimization strategies, such as adaptive moment estimation (Adam) and glorot uniform initialization. We present a case study based on Standard \& Poor's (S\&P500) and NASDAQ. Quantities of comparison experiments were performed to evaluate this model. At last we analyze the performance of different models with a series of evaluation criteria. Stock market prediction has garnered significant interest among investment and researchers. However, accurate prediction of stock market is an extremely challenging task. Hopefully, based on the case study, we show that our forecasting system gives slightly higher prediction accuracy for the stock closing price of next day, which outperforms the comparison models.},
	journal = {Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
	author = {Gao, Tingwei and Chai, Yueting and Liu, Yi},
	year = {2018},
	note = {ISBN: 9781538645703},
	keywords = {Recurrent Neural Network, stock price prediction, Stock Prediction, Closing Price, Long Short Term Memory},
	pages = {575--578},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P6XS7562\\Applying long short term momory neural networks for predicting stock closing price.pdf:application/pdf},
}

@article{Xie2019,
	title = {Neural network analysis of stock price based on news corpus impact},
	doi = {10.1109/ICIS46139.2019.8940242},
	abstract = {Investors are more concerned about the stock prices they care about. The trend of stock price is related to many factors, such as the operation status of the enterprise, the heat of the exchange market and the reaction of all aspects of society to the operation of the enterprise, and the analysis and grasp of the enterprise information by individual investors. For the latter two items, the main impact comes from the individual's initiative to explore the trading information of the company's stock situation, which results in the purchase intention. This paper mainly considers the influence of news corpus. The impact of news corpus on the stock price of enterprises is a method that needs quantitative analysis. In this paper, we adopt the independent and identical distribution model of the positive (positive) and negative (negative) attributes of lexical frequency classification and corpus information based on word bag and the influence of social attributes on stock price, and use the artificial neural network to calculate stock price parameters. The experimental results show that the influence of news corpus on stock trend is affirmative, and the method of adding news corpus influence effectively improves the accuracy of stock price prediction.},
	number = {1},
	journal = {Proceedings - 18th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2019},
	author = {Xie, Weihua and Feng, Shuang},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728108018},
	keywords = {Neural network, out, stock price prediction, Stock price, News classification, News corpus Impact},
	pages = {329--332},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QX72P3NV\\Neural Network Analysis of Stock Price Based on News Corpus Impact.pdf:application/pdf},
}

@article{Sakphoowadon2018,
	title = {Probabilistic {Lexicon}-{Based} {Approach} for {Stock} {Market} {Prediction}: {A} {Case} {Study} of the {Stock} {Exchange} of {Thailand} ({SET})},
	doi = {10.1109/ISCIT.2018.8587961},
	abstract = {Stock market prediction has long been a major research topic that exploits various machine learning techniques and diverse sets of data. Most existing works utilize multiple stock historical statistics as well as up-To-date data of relevant factors which could have impacted the stock price value such as oil price, gold price, etc. Very few works explore the possibility of incorporating financial news when predicting the stock price direction. In this paper, a predictive approach using the probabilistic lexicon generated from Thai financial news and stock market closing prices is investigated. Relevant event terms will be extracted and assigned probabilistic values according to the proposed Probabilistic Lexicon Based Stock Market Prediction (PLSP) algorithm. The obtained results of this study show that the proposed model is superior to other models.},
	number = {Iscit},
	journal = {ISCIT 2018 - 18th International Symposium on Communication and Information Technology},
	author = {Sakphoowadon, Surinthip and Wisitpongphan, Nawaporn and Haruechaiyasak, Choochart},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538684580},
	keywords = {stock price prediction, Stock market prediction, Event term, Event term efficiency, Probabilistic lexicon, Thai financial news},
	pages = {371--376},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3UCH7TSQ\\Probabilistic Lexicon-Based Approach for Stock Market Prediction A Case Study of The Stock Exchange of Thailand -SET-.pdf:application/pdf},
}

@article{PrashantMahasagara2017,
	title = {Indonesia infrastructure and consumer stock portfolio prediction using artificial neural network backpropagation},
	volume = {3},
	doi = {10.1109/ICoICT.2017.8074710},
	abstract = {Artificial Neural Network (ANN) method is increasingly popular to build predictive model that generated small error prediction. To have a good model, ANN needs large dataset as an input. ANN backpropagation is a gradient decrease method to minimize the output error squared. Stock price movements are suitable with ANN requirement: it is a large data set because stock price is recorded up to every seconds, usually called high frequency data. The implementation of stock price prediction using ANN approach is quite new. The predictive model help investor in building stock portfolio and their decision making process. Buying some stocks in portfolio decrease diversified risk and increases the chance of higher return. In this paper, we show how to generate prediction model using artificial neural network backpropagation of stock price and forming portfolio with predicted price that bring prediction of the portfolio with the smallest error. The data set we use is historical stock price data from ten different company stocks of infrastructure and consumer sector Indonesia Stock Exchage. The results is for lower risk condition, ANN predictive model gives higher expected return than the return from real condition, while for higher risk, the return from the real condition is higher than the ANN predictive model.},
	number = {c},
	journal = {2017 5th International Conference on Information and Communication Technology, ICoIC7 2017},
	author = {Prashant Mahasagara, S. and Alamsyah, Andry and Rikumahu, Brady},
	year = {2017},
	note = {ISBN: 9781509049127},
	keywords = {Backpropagation, stock price prediction, Artificial Neural Network, Stock Portfolio},
	pages = {1--4},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KN7FHG8Y\\Indonesia infrastructure and consumer stock portfolio prediction using artificial neural network backpropagation.pdf:application/pdf},
}

@article{Tiwari2018,
	title = {Stock price prediction using data analytics},
	volume = {2018-Janua},
	doi = {10.1109/ICAC3.2017.8318783},
	abstract = {Accurate financial prediction is of great interest for investors. This paper proposes use of Data analytics to be used in assist with investors for making right financial prediction so that right decision on investment can be taken by Investors. Two platforms are used for operation: Python and R. various techniques like Arima, Holt winters, Neural networks (Feed forward and Multi-layer perceptron), linear regression and time series are implemented to forecast the opening index price performance in R. While in python Multi-layer perceptron and support vector regression are implemented for forecasting Nifty 50 stock price and also sentiment analysis of the stock was done using recent tweets on Twitter. Nifty 50 (ANSEI) stock indices is considered as a data input for methods which are implemented. 9 years of data is used. The accuracy was calculated using 2-3 years of forecast results of R and 2 months of forecast results of Python after comparing with the actual price of the stocks. Mean squared error and other error parameters for every prediction system were calculated and it is found that feed forward network only produces 1.81598342\% error when opening price of stock is forecasted using it.},
	journal = {International Conference on Advances in Computing, Communication and Control 2017, ICAC3 2017},
	author = {Tiwari, Shashank and Bharadwaj, Akshay and Gupta, Sudha},
	year = {2018},
	note = {ISBN: 9781538638521},
	keywords = {Data analytics, Linear Regression, stock price prediction, Support Vector Regression, Moving Average, Web Scrapping, Artificial Neural Networks, ARIMA, Auto Regression, Big Data Analytics, Holt-Winters, Multi-Layer Perceptron, Predictive Analytics, Radial Basis Function(RBF), Stock Index Prediction, Time Series Model, Twitter sentiment analysis},
	pages = {1--5},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EEYVIX32\\Stock price prediction using data analytics.pdf:application/pdf},
}

@article{Yun2019,
	title = {Stock {Prices} {Prediction} using the {Title} of {Newspaper} {Articles} with {Korean} {Natural} {Language} {Processing}},
	doi = {10.1109/ICAIIC.2019.8668996},
	abstract = {Non-quantitative data have a significant impact on the financial market as well as quantitative data. In this paper, we propose CNN model of stock price prediction using Korean natural language processing. In the case of Korean natural language processing research was not actively performed compared to English language. We converted Korean sentences into nouns and vectorized them using skip-grams to extract the characteristics of the words. Then, the vectorized word sentence was used as input data of the CNN model to predict the stock price after 5 days of trading day. Most models have more than 50\% prediction accuracy for stock price up and down. The highest accuracy of the model was about 53\%. Since the result is not considerable but meaningful, it shows the possibility of developing the stock price prediction model through Korean natural language processing in the future.},
	journal = {1st International Conference on Artificial Intelligence in Information and Communication, ICAIIC 2019},
	author = {Yun, Hyungbin and Sim, Ghudae and Seok, Junhee},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781538678220},
	keywords = {out, stock price prediction, artificial neural network, convolution neural network, Korean natural language processing, skip-gram},
	pages = {19--21},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3MS6PZJ9\\Stock Prices Prediction using the Title of Newspaper Articles with Korean Natural Language Processing.pdf:application/pdf},
}

@article{Oguz2019,
	title = {On the use of technical analysis indicators for stock market price movement direction prediction},
	doi = {10.1109/SIU.2019.8806422},
	abstract = {Technical indicators are algorithms that take financial time series data as input and predict price movement directions based on mathematical calculations. Technical analysts can predict the stock price trends by interpreting the results of different technical indicators. In this study, we investigate the prediction of price movement directions based on the use of technical indicators in learning algorithms. We explore the technical indicators that provide the most successful prediction when they are used together in learning algorithms. In this paper, we investigate the technical indicators that lead to the most successful price movement direction prediction. To do this, we explore all possible combinations of indicators with various machine learning algorithms. Here, a decision support system is proposed to predict price movement direction on financial time series data by using technical indicators in machine learning algorithms.},
	journal = {27th Signal Processing and Communications Applications Conference, SIU 2019},
	author = {Oguz, Ramazan Faruk and Uygun, Yasin and Aktas, Mehmet S. and Aykurt, Ishak},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728119045},
	keywords = {Machine Learning, out, stock price prediction, Data Stream Mining, Financial Time Series Data, Technical Indicator},
	pages = {0--3},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\49TGRQ4P\\On the Use of Technical Analysis Indicators for Stock Market Price Movement Direction Prediction.pdf:application/pdf},
}

@article{Wang2020,
	title = {Forecasting method of stock market volatility in time series data based on mixed model of {ARIMA} and {XGBoost}},
	volume = {17},
	issn = {16735447},
	doi = {10.23919/JCC.2020.03.017},
	abstract = {Stock price forecasting is an important issue and interesting topic in financial markets. Because reasonable and accurate forecasts have the potential to generate high economic benefits, many researchers have been involved in the study of stock price forecasts. In this paper, the DWT-ARIMA-GSXGB hybrid model is proposed. Firstly, the discrete wavelet transform is used to split the data set into approximation and error parts. Then the ARIMA (0, 1, 1), ARIMA (1, 1, 0), ARIMA (2, 1, 1) and ARIMA (3, 1, 0) models respectively process approximate partial data and the improved xgboost model (GSXGB) handles error partial data. Finally, the prediction results are combined using wavelet reconstruction. According to the experimental comparison of 10 stock data sets, it is found that the errors of DWT-ARIMA-GSXGB model are less than the four prediction models of ARIMA, XGBoost, GSXGB and DWT-ARI-MA-XGBoost. The simulation results show that the DWT-ARIMA-GSXGB stock price prediction model has good approximation ability and generalization ability, and can fit the stock index opening price well. And the proposed model is considered to greatly improve the predictive performance of a single ARIMA model or a single XGBoost model in predicting stock prices.},
	number = {3},
	journal = {China Communications},
	author = {Wang, Yan and Guo, Yuankai},
	year = {2020},
	note = {Publisher: China Institute of Communications},
	keywords = {stock price prediction, in, ARIMA, discrete wavelet transform, grid search, hybrid model, stock price forecast, XGBoost},
	pages = {205--221},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BBKS8RTZ\\Forecasting method of stock market volatility in time series data based on mixed model of ARIMA and XGBoost.pdf:application/pdf},
}

@article{Lai2019,
	title = {Prediction {Stock} {Price} {Based} on {Different} {Index} {Factors} {Using} {LSTM}},
	volume = {2019-July},
	issn = {21601348},
	doi = {10.1109/ICMLC48188.2019.8949162},
	abstract = {Predicting stock price has been a challenging project for many researchers, investors, and analysts. Most of them are interested in knowing the stock price trend in the future. To get a precise and winning model is the wish of them. Recently, Neural Network has been a prevalent means for stock prediction. However, there are many ways and different predicting models such as Convolutional Neural Networks (CNN), Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). In this paper, we propose a novel idea that average previous five days stock market information (open, high, low, volume, close) as a new value then use this value to predict, and use the predicted value as the average of the stock price information for the next five days. Moreover, we utilize Technical Analysis Indicators to consider whether to buy stocks or continue to hold stocks or sell stocks. We use Foxconn company data collected from Taiwan Stock Exchange for testing with the Neural Network Long Short-Term Memory (LSTM).},
	journal = {Proceedings - International Conference on Machine Learning and Cybernetics},
	author = {Lai, Chun Yuan and Chen, Rung Ching and Caraka, Rezzy Eko},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728128160},
	keywords = {LSTM, out, stock price prediction, Stock price prediction, Technical analysis indicator},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RV93GHR6\\Prediction Stock Price Based on Different Index Factors Using LSTM.pdf:application/pdf},
}

@misc{StockPredictionUsing,
	title = {Stock prediction using recurrent neural networks {\textbar} by {Joshua} {Wyatt} {Smith} {\textbar} {Towards} {Data} {Science}},
	url = {https://towardsdatascience.com/stock-prediction-using-recurrent-neural-networks-c03637437578},
	urldate = {2020-08-24},
	keywords = {stock price prediction},
}

@misc{ImpactCryptocurrenciesRates,
	title = {Impact of cryptocurrencies rates on {PC} market {\textbar} {Kaggle}},
	url = {https://www.kaggle.com/raczeq/impact-of-cryptocurrencies-rates-on-pc-market},
	urldate = {2020-08-24},
	keywords = {stock price prediction},
}

@misc{PriceOptimizationEcommerce,
	title = {Price optimization for e-commerce: a case study {\textbar} {Tryolabs} {Blog}},
	url = {https://tryolabs.com/blog/2020/06/01/price-optimization-for-e-commerce-a-case-study/},
	urldate = {2020-08-24},
	keywords = {stock price prediction},
}

@misc{RecSysChallenge2015,
	title = {{RecSys} {Challenge} 2015 {\textbar} {Kaggle}},
	url = {https://www.kaggle.com/chadgostopp/recsys-challenge-2015},
	urldate = {2020-08-24},
}

@misc{RetailrocketRecommenderSystem,
	title = {Retailrocket recommender system dataset {\textbar} {Kaggle}},
	url = {https://www.kaggle.com/retailrocket/ecommerce-dataset},
	urldate = {2020-08-24},
}

@book{Baron2006,
	title = {Probability and {Statistics} for {Computer} {Scientists}},
	isbn = {978-3-319-64409-7},
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	author = {Baron, Michael},
	year = {2006},
	doi = {10.1201/9781420011425},
	note = {Publication Title: Probability and Statistics for Computer Scientists},
	keywords = {www.dbooks.org},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BWQUI7SG\\Probability and Statistics for Computer Science.pdf:application/pdf},
}

@article{Sheil2018,
	title = {Predicting purchasing intent: {Automatic} feature learning using recurrent neural networks},
	volume = {2319},
	issn = {16130073},
	abstract = {We present a neural network for predicting purchasing intent in an Ecommerce setting. Our main contribution is to address the significant investment in feature engineering that is usually associated with state-of-the-art methods such as Gradient Boosted Machines. We use trainable vector spaces to model varied, semi-structured input data comprising categoricals, quantities and unique instances. Multi-layer recurrent neural networks capture both session-local and dataset-global event dependencies and relationships for user sessions of any length. An exploration of model design decisions including parameter sharing and skip connections further increase model accuracy. Results on benchmark datasets deliver classification accuracy within 98\% of state-of-the-art on one and exceed state-of-the-art on the second without the need for any domain / dataset-specific feature engineering on both short and long event sequences.},
	journal = {CEUR Workshop Proceedings},
	author = {Sheil, Humphrey and Rana, Omer and Reilly, Ronan},
	year = {2018},
	note = {arXiv: 1807.08207},
	keywords = {Deep learning, Recurrent neural networks, customer\_intention, Ecommerce, Embedding, Long Short Term Memory (LSTM), Vector space models},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MR3H8EUR\\Predicting purchasing intent - Automatic Feature Learning using Recurrent Neural Networks.pdf:application/pdf},
}

@article{Wu2012,
	title = {A {Two}-{Stage} {Ensemble} of {Diverse} {Models} for {Advertisement} {Ranking} in {KDD} {Cup} 2012},
	abstract = {This paper describes the solution of National Taiwan Uni- versity for track 2 of KDD Cup 2012. Track 2 of KDD Cup 2012 aims to predict the click-through rate of ads on Ten- cent proprietary search engine. We exploit classification, regression, ranking, and factorization models to utilize a va- riety of different signatures captured from the dataset. We then blend our individual models to boost the performance through two stages, one on an internal validation set and one on the external test set. Our solution achieves 0.8069 AUC on the public test set and 0.8089 AUC on the private test set.},
	journal = {KDD KDD Cup Workshop},
	author = {Wu, Kuan-wei and Ferng, Chun-sung and Ho, Chia-hua and Liang, An-chun and Huang, Chun-heng and Shen, Wei-yuan and Jiang, Jyun-yu and Yang, Ming-hao and Lin, Ting-wei and Lee, Ching-pei and Kung, Perng-hwa and Wang, Chin-en and Ku, Ting-wei and Ho, Chun-yen and Tai, Yi-shu and Chen, I-kuei and Huang, Wei-lun and Chou, Che-ping and Lin, Tse-ju and Yang, Han-jay and Wang, Yen-kai and Li, Cheng-te and Lin, Shou-de and Lin, Hsuan-tien},
	year = {2012},
	keywords = {addition, adid, click times those, clicked, descriptionid, had, had been im, id, impression times, instances, pressed with ad, setting, titleid, under a specific, user, userid, which means},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U5G6ZTYB\\A Two Stage Ensemble of Diverse Models for Advertisement Ranking in KDD Cup 2012.pdf:application/pdf},
}

@article{Mcmahan2013,
	title = {Ad click prediction: {A} view from the trenches},
	volume = {Part F1288},
	doi = {10.1145/2487575.2488200},
	abstract = {Predicting ad click\{\vphantom{\}}through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Mcmahan, H. Brendan and Holt, Gary and Sculley, D. and Young, Michael and Ebner, Dietmar and Grady, Julian and Nie, Lan and Phillips, Todd and Davydov, Eugene and Golovin, Daniel and Chikkerur, Sharat and Liu, Dan and Wattenberg, Martin and Hrafnkelsson, Arnar Mar and Boulos, Tom and Kubica, Jeremy},
	year = {2013},
	note = {ISBN: 9781450321747},
	keywords = {Data mining, Online advertising, Large-scale learning},
	pages = {1222--1230},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FRS9EPGI\\Ad Click Prediction  a View from the Trenches.pdf:application/pdf},
}

@article{Gao2013,
	title = {Selection and peer-review under responsibility of {Elhadi} {M} {The} 4 th {International} {Conference} on {Ambient} {Systems}, {Networks} and {Technologies} ({ANT} 2013) {Ad}--through {Rate} peer-review under responsibility of [name organizer]},
	volume = {19},
	url = {www.sciencedirect.com},
	doi = {10.1016/j.procs.2013.06.025},
	abstract = {Click here and insert your abstract text. Search engine advertising has become one of the most important revenue models of electronic commerce. It strongly affects the probability that users click on the ads at the side of the search results page if the system shows the right ones. To maximize the outcome of search engine revenue and improve perception on those ads, it is important to understand the factors which affect the click through rate (CTR) on those ads. Tencent founded in 1998, is one of China's largest and most used Internet service portals. It provides a number of online services such as value-added Internet, mobile and telecom services and online advertising. As of September 30, 2011, Tencent had 711.7 million active Instant Messenger users. It forms the largest Internet Community in China. In this research, we use a very large dataset of Tencent click logs (soso.com) with millions records. First we describe how soso.com searching engine advertising works, our system architecture is designed with the click log dataset, and observations inside it aims at those ads with enough historical click logs. Then we show how to use ad-centric features to discover models that can find factors affecting CTR prediction performance. The proposed framework could help both optimizing the search engine system for soso.com and improving the ads designs for the advertisers.},
	urldate = {2020-08-24},
	journal = {Procedia Computer Science},
	author = {Gao, Zhe and Gao, Qigang},
	year = {2013},
	keywords = {ctr, CTR Prediction, Large Dataset, Regression Modeling;, Sponsored Search, System Design, Term Factors, User Behaviours},
	pages = {155--162},
}

@article{Neller2018,
	title = {{AI} education matters},
	volume = {4},
	issn = {2372-3483},
	doi = {10.1145/3236644.3236646},
	number = {2},
	journal = {AI Matters},
	author = {Neller, Todd W.},
	year = {2018},
	keywords = {ctr},
	pages = {5--7},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MR99U9NQ\\AI Education Matters  Lessons from a Kaggle Click-Through Rate Prediction Competition.pdf:application/pdf},
}

@article{Zhou2018a,
	title = {A {Novel} {Ensemble} {Strategy} {Combining} {Gradient} {Boosted} {Decision} {Trees} and {Factorization} {Machine} {Based} {Neural} {Network} for {Clicks} {Prediction}},
	doi = {10.1109/BDAI.2018.8546685},
	abstract = {Click-through rate (CTR) prediction is generally formulated as a supervised classification problem. One challenge in CTR prediction, especially the features with high-sparsity, is to exploit the potential generalization ability under the given samples. In this paper, we first present a novel Factorization Machine (FM) based Neural Network (FNN), which helps capture the nonlinear interactions between sparse inputs. And then, the gradient boosted decision trees (GBDT) model is combined with FNN via cascading and boosting (i.e., GBDT2FNN, GBDT+FNN) respectively to improve the CTR predictive accuracy. To illustrate the performance, we employ them on the open dataset, JData. The experiment results show that the proposed ensembles significantly increase AUC and RIG compared with the baseline GBDT2LR.},
	number = {June},
	journal = {International Conference on Big Data and Artificial Intelligence, BDAI 2018},
	author = {Zhou, Feng and Yin, Hua and Zhan, Lizhang and Li, Huafei and Fan, Yeliang and Jiang, Liu},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538661369},
	keywords = {ctr, CTR prediction, FM, ensemble learning, FNN, GBDT},
	pages = {29--33},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CEMJG7JC\\ANovelEnsembleStrategyCombiningGradientBoostedDecisionTreesandFactorizationMachineBasedNeuralNetworkforClicksPrediction.pdf:application/pdf},
}

@article{Cakmak2019,
	title = {Accurate prediction of advertisement clicks based on impression and click-through rate using extreme gradient boosting},
	doi = {10.5220/0007394306210629},
	abstract = {Online travel agencies (OTAs) aim to use digital media advertisements in the most efficient way to increase their market share. One of the most commonly used digital media environments by OTAs are the metasearch bidding engines. In metasearch bidding engines, many OTAs offer daily bids per click for each hotel to get reservations. Therefore, management of bidding strategies is crucial to minimize the cost and maximize the revenue for OTAs. In this paper, we aim to predict both the impression count and Click-Through-Rate (CTR) metrics of hotel advertisements for an OTA and then use these values to obtain the number of clicks the OTA will take for each hotel. The initial version of the dataset was obtained from the dashboard of an OTA which contains features for each hotel's last day performance values in the search engine. We enriched the initial dataset by creating features using window-sliding approach and integrating some domain-specific features that are considered to be important in hotel click prediction. The final set of features are used to predict next day's CTR and impression count values. We have used state-of-the-art prediction algorithms including decision tree-based ensemble methods, boosting algorithms and support vector regression. An important contribution of this study is the use of Extreme Gradient Boosting (XGBoost) algorithm for hotel click prediction, which overwhelmed state-of-the-art algorithms on various tasks. The results showed that XGBoost gives the highest R-Squared values in the prediction of all metrics used in our study. We have also applied a mutual information filter feature ranking method called minimum redundancy-maximum relevance (mRMR) to evaluate the importance of the features used for prediction. The bid value offered by OTA at time t − 1 is found to be the most informative feature both for impression count and CTR prediction. We have also observed that a subset of features selected by mRMR achieves comparable performance with using all of the features in the machine learning model.},
	journal = {ICPRAM 2019 - Proceedings of the 8th International Conference on Pattern Recognition Applications and Methods},
	author = {Çakmak, Tülin and Tekin, Ahmet T. and Şenel, Çagla and Çoban, Tugba and Uran, Zeynep Eda and Okan Sakar, C.},
	year = {2019},
	note = {ISBN: 9789897583513},
	keywords = {ctr, Click-through Rate, Ensemble Learning, Filter Feature Selection, Hotel Impression, Metasearch Bidding Engines},
	pages = {621--629},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3QFK9PPI\\Accurate Prediction of Click Through Rates based on Click-Through Rate using Extreme Gradient Boosting.pdf:application/pdf},
}

@article{Wang2019,
	title = {Research on {CTR} {Prediction} {Based} on {Deep} {Learning}},
	volume = {7},
	issn = {21693536},
	doi = {10.1109/ACCESS.2018.2885399},
	abstract = {Click-through rate (CTR) prediction is critical in Internet advertising and affects web publisher's profits and advertiser's payment. In the CTR prediction, the interaction between features is a key factor affecting the prediction rate. The traditional method of obtaining features using feature extraction did not consider the sparseness of advertising data and the highly nonlinear association between features. To reduce the sparseness of data and to mine the hidden features in advertising data, a method that learns the sparse features is proposed. Our method exploits dimension reduction based on decomposition and combines the power of field-aware factorization machines and deep learning to portray the nonlinear associated relationship of data to solve the sparse feature learning problem. The experiment shows that our method improves the effect of CTR prediction and produces economic benefits in Internet advertising.},
	journal = {IEEE Access},
	author = {Wang, Qianqian and Liu, Fang'Ai and Xing, Shuning and Zhao, Xiaohui and Li, Tianlai},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {deep learning, ctr, Click through rate, factorization machines, sponsored search, tensor decomposition},
	pages = {12779--12789},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BZVJPAPF\\Research on CTR Prediction Based on Deep Learning.pdf:application/pdf},
}

@article{Chenb,
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	url = {http://dx.doi.org/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quan-tile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	urldate = {2020-08-23},
	author = {Chen, Tianqi and Guestrin, Carlos},
	note = {ISBN: 9781450342322},
	keywords = {Learning, ctr, Machine, Large-scale},
	file = {XGBoost A Scalable Tree Boosting System.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8HQKR2CI\\XGBoost A Scalable Tree Boosting System.pdf:application/pdf},
}

@article{Gunduz2017,
	title = {Derin {Sinir} {A}˘ gları ile {Borsa} {Yönü} {Tahmini} {Stock} {Market} {Direction} {Prediction} {Using} {Deep} {Neural} {Networks}},
	abstract = {In this study, the daily movement directions of three frequently traded stocks (GARAN, THYAO and ISCTR) in Borsa Istanbul were predicted using deep neural networks. Technical indicators obtained from individual stock prices and dollar- gold prices were used as features in the prediction. Class labels indicating the movement direction were found using daily close prices of the stocks and they were aligned with the feature vectors. In order to perform the prediction process, the type of deep neural network, Convolutional Neural Network, was trained and the performance of the classification was evaluated by the accuracy and F-measure metrics. In the experiments performed, using both price and dollar-gold features, the movement directions in GARAN, THYAO and ISCTR stocks were predicted with the accuracy rates of 0.61, 0.578 and 0.574 respectively. Compared to using the price based features only, the use of dollar-gold features improved the classification performance. Keywords—stock},
	author = {Gunduz, Hakan and Cataltepe, Zehra and YASLAN Bilgisayar Mühendisli, Yusuf and Bölümü, Gi and Teknik Üniversitesi, Istanbul},
	year = {2017},
	note = {Publisher: IEEE
ISBN: 9781509064946},
	keywords = {stock price prediction, stock market prediction, deep neural networks},
	pages = {23--26},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RKNB89G2\\Stock market direction prediction using deep neural networks.pdf:application/pdf},
}

@article{Liu2018b,
	title = {Stock transaction prediction modeling and analysis based on {LSTM}},
	doi = {10.1109/ICIEA.2018.8398183},
	abstract = {Stock price volatility is a highly complex nonlinear dynamic system. The stock's trading volume affects the stock's self correlation, self correlation and inertial effect, and the adjustment of the stock is not to advance with a homogeneous time process, which has its own independent time to promote the process. LSTM (Term Memory Long-Short) is a kind of time recurrent neural network, which is suitable for processing and predicting the important events of interval and long delay in time series. Based on temporal characteristics of stock and LSTM neural network algorithm, this paper uses the LSTM recurrent neural networks to filter, extract feature value and analyze the stock data, and set up the the prediction model of the corresponding stock transaction.},
	journal = {Proceedings of the 13th IEEE Conference on Industrial Electronics and Applications, ICIEA 2018},
	author = {Liu, Siyuan and Liao, Guangzhong and Ding, Yifan},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538637579},
	keywords = {machine learning, LSTM, neural network, out, stock price prediction, stock transaction prediction},
	pages = {2787--2790},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7ZVBWKNV\\Stock transaction prediction modeling and analysis based on LSTM.pdf:application/pdf},
}

@article{Jeevan2018,
	title = {Share {Price} {Prediction} using {Machine} {Learning} {Technique}},
	doi = {10.1109/CIMCA.2018.8739647},
	abstract = {Stock Market has started to attract more people from academics and business point of view which has increased. So this paper is mostly based on the approach of predicting the share price using Long Short Term Memory (LSTM) and Recurrent Neural Networks (RNN) to predict the stock price on NSE data using various factors such as current market price, price-earning ratio, base value and some miscellaneous events. We use a numerical data and recommended data for a company selected from collaborative and content based recommendation system. So this paper is all about selecting the company based on the recommendation system using collaborative and content based on selecting a company for the machine learning model based on the LSTM and RNN method. The performance of the model is displayed by comparing the company data and the predicted data using a RNN graph.},
	number = {1},
	journal = {2018 IEEE 3rd International Conference on Circuits, Control, Communication and Computing, I4C 2018},
	author = {Jeevan, B. and Naresh, E. and Vijaya Kumar, B. P. and Kambli, Prashanth},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538684870},
	keywords = {Recurrent Neural Networks, Backpropagation, stock price prediction, Long Short Term Memory, CNN-sliding window, Stock Market and Stock Price},
	pages = {2018--2021},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6IXBSL4F\\Share Price Prediction using Machine Learning Technique.pdf:application/pdf},
}

@article{Wang2019c,
	title = {Stock market prediction analysis by incorporating social and news opinion and sentiment},
	volume = {2018-Novem},
	issn = {23759259},
	doi = {10.1109/ICDMW.2018.00195},
	abstract = {The price of the stocks is an important indicator for a company and many factors can affect their values. Different events may affect public sentiments and emotions differently, which may have an effect on the trend of stock market prices. Because of dependency on various factors, the stock prices are not static, but are instead dynamic, highly noisy and nonlinear time series data. Due to its great learning capability for solving the nonlinear time series prediction problems, machine learning has been applied to this research area. Learning-based methods for stock price prediction are very popular and a lot of enhanced strategies have been used to improve the performance of the learning based predictors. However, performing successful stock market prediction is still a challenge. News articles and social media data are also very useful and important in financial prediction, but currently no good method exists that can take these social media into consideration to provide better analysis of the financial market. This paper aims to successfully predict stock price through analyzing the relationship between the stock price and the news sentiments. A novel enhanced learning-based method for stock price prediction is proposed that considers the effect of news sentiments. Compared with existing learning-based methods, the effectiveness of this new enhanced learning-based method is demonstrated by using the real stock price data set with an improvement of performance in terms of reducing the Mean Square Error (MSE). The research work and findings of this paper not only demonstrate the merits of the proposed method, but also points out the correct direction for future work in this area.},
	journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
	author = {Wang, Zhaoxia and Ho, Seng Beng and Lin, Zhiping},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781538692882},
	keywords = {Machine learning, stock price prediction, in, sentiment analysis, stock market prediction, enhanced learning-based method, time series data prediction},
	pages = {1375--1380},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5798WSY4\\Stock Market Prediction Analysis by Incorporating Social and News Opinion and Sentiment.pdf:application/pdf},
}

@article{Jhang2019,
	title = {Share {Price} {Trend} {Prediction} {Using} {Attention} with {LSTM} {Structure}},
	doi = {10.1109/SNPD.2019.8935806},
	abstract = {Stock market has a considerable impact in the whole financial market. Among researches on prediction, stock price movements prediction is a quite hot topic. In this paper, stock price movements were predicted by utilizing various stock information by technical means of deep learning. The architecture based on LSTM using Attention proposed in this paper was proven through experiment to be able to effectively improve prediction accuracy.},
	journal = {Proceedings - 20th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2019},
	author = {Jhang, Wun Syun and Gao, Shao En and Wang, Chuin Mu and Hsieh, Ming Chu},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728116518},
	keywords = {convolutional neural network, deep learning, out, stock price prediction, stock prediction, recurrent neural network},
	pages = {208--211},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E2RTJ9WP\\Share Price Trend Prediction Using Attention with LSTM Structure.pdf:application/pdf},
}

@article{Sun2017,
	title = {Predicting {Stock} {Price} {Returns} {Using} {Microblog} {Sentiment} for {Chinese} {Stock} {Market}},
	doi = {10.1109/BIGCOM.2017.59},
	abstract = {Recently there have been many efforts to study the predictability of stock market trend using post sentiments on social media sites. These studies have mainly focused on US stock market. In this paper we investigate the relationship between stock price movement and social media sentiment in China, which has a large stock capitalization and a unique social media landscape. We collect data from several different types of social media sites (microblogs, chat rooms, web forums), and find that data from these sites exhibit distinct characteristics in activity level, post length, and correlation with stock market behavior. Users in stock market related chat rooms tend to post more but much shorter blogs. The activity level is much more highly correlated with market trading hours and stock trading volumes. We then investigated several machine learning models to classify post sentiment in chat rooms, and achieved a performance similar to the state-of-the-art sentiment analysis result for short posts. We find that there is strong correlation and Granger causality between chat room post sentiment and stock price movement, indicating that post sentiments can be used to improve the prediction of stock price return over using the historic stock trading information alone. We further propose a prediction model that uses chat room sentiment to forecast the market direction, and develop a trading strategy that utilizes the prediction as trading indicators. Backtest using our strategy achieves promising portfolio returns. A total return of 19.54\% is obtained at the end of the seven-month period when taking into account of slippage and commissions, compared to a loss of -25.26\% by a passive buy-and-hold baseline strategy.},
	journal = {Proceedings - 2017 3rd International Conference on Big Data Computing and Communications, BigCom 2017},
	author = {Sun, Tong and Wang, Jia and Zhang, Pengfei and Cao, Yu and Liu, Benyuan and Wang, Degang},
	year = {2017},
	note = {ISBN: 9781538633496},
	keywords = {stock price prediction, Stock Market Prediction, Sentiment Analysis, Microblog},
	pages = {87--96},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4N3C27VU\\Predicting Stock Price Returns Using Microblog Sentiment for Chinese Stock Market.pdf:application/pdf},
}

@article{Wu2017,
	title = {A study of patent analysis for stock price prediction},
	doi = {10.1109/ICISCE.2017.34},
	abstract = {Stock price prediction is considered as one of the most challenging and important tasks. It is so complex and uncertainly, so that it is not enough to only use financial indicators for stock price prediction. As we known, patents is not only can protect the companies technologies' development and promote the advance of the core technologies, but can be used as one of the evaluation indicators to estimate the company's innovation activities. Therefore, patents have an influence on turbulence and fluctuations of companies' stock price. The aim in this study is to adopt patent and financial indicators for stock-price prediction. Thus, we collect patent and financial data as the attributes to predict the stock price by regression analysis. The experimental results show the proposed approach can accurately predict companies' stock price.},
	journal = {Proceedings - 2017 4th International Conference on Information Science and Control Engineering, ICISCE 2017},
	author = {Wu, Si Qi and Tsao, Cheng Chin and Chang, Pei Chann and Fan, Chin Yuan and Chen, Meng Hui and Zhang, Xiao},
	year = {2017},
	note = {ISBN: 9781538630136},
	keywords = {Prediction, Machine learning, Support vector regression, stock price prediction, Patent analysis, Stock market price},
	pages = {115--119},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4XCSIAYJ\\A Study of Patent Analysis for Stock Price Prediction.pdf:application/pdf},
}

@article{Ravichandra2017,
	title = {Cumulative istributionfunction: {Stock} price forecasting},
	doi = {10.1109/ICIMIA.2017.7975643},
	abstract = {In this paper, an attempt has been made to predict the movement of the stock price for the next day using Cumulative Distribution Function (CDF). For the purpose of the research, three companies from the Bearings Industry, namely - ABC Bearings Ltd, SNL Bearings Ltd and Austin Engineering Company Ltd, and two companies from the chemical industry, namely-Nocil ltd and Manali Petrochemicals Ltd were chosen. Historical prices of these companies were analyzed and by using Cumulative Distribution Function (CDF) the movement of the stock price for the next day is predicted.},
	number = {Icimia},
	journal = {IEEE International Conference on Innovative Mechanisms for Industry Applications, ICIMIA 2017 - Proceedings},
	author = {Ravichandra, Thangjam and Thingom, Chintureena},
	year = {2017},
	note = {ISBN: 9781509059607},
	keywords = {Prediction, stock price prediction, Bearings Industry, Chemical Industry, Cumulative Distribution Function, Stock Price},
	pages = {394--398},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C8XZCM3D\\Cumulative istributionfunction Stock price forecasting.pdf:application/pdf},
}

@article{Sanboon2019,
	title = {A deep learning model for predicting buy and sell recommendations in stock exchange of {Thailand} using long short-term memory},
	doi = {10.1109/CCOMS.2019.8821779},
	abstract = {Nowadays, the stock price prediction has been one of the most challenging problem to the AI research community. Most prediction techniques concentrate on forecasting the future prices of stocks based on conventional Machine learning techniques. However, these techniques cannot capture long term dependencies in stock price data. Therefore, they cannot consider the relation between the current predicted data and the previous data in stock data. This research adopts deep learning techniques for predicting buy and sell recommendations in Stock Exchange of Thailand using Long Short-Term Memory. The proposed model can capture long term dependencies in stock price data in order to enhance the prediction accuracy. The accuracy of the proposed model is evaluated on five Stock Exchange of Thailand (SET) stocks, between 5 January 2015 and 29 December 2017, and compared the results with support vector Machine, multilayer perceptron, decision tree, random forest, logistic regression and k-nearest neighbors. The experimental results signify that the proposed model can outperform all comparative models.},
	journal = {2019 IEEE 4th International Conference on Computer and Communication Systems, ICCCS 2019},
	author = {Sanboon, Thaloengpattarakoon and Keatruangkamala, Kamol and Jaiyen, Saichon},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728113227},
	keywords = {Logistic regression, Support vector machine, out, stock price prediction, Stock prediction, Decision tree, K-nearest neighbors, Long short-term memory, Multilayer perceptron},
	pages = {757--760},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IL6FGVXU\\A Deep Learning Model for Predicting Buy and Sell Recommendations in Stock Exchange of Thailand using Long Short-Term Memory.pdf:application/pdf},
}

@article{Pooja2018,
	title = {Sentiment {Based} {Stock} {Market} {Prediction}},
	doi = {10.1109/CTEMS.2018.8769159},
	abstract = {The Stock Market prophets focus on developing a efficient approach to predict the future stock prices. Apart from achieving best results in the successful stock market prediction it is also necessary to minimize the inaccurate forecast of stock prices. The project aims to design and implement a predictive system for guiding stock market investiture. As stock market is a very volatile environment, it can be influenced by positive or negative stock price releases. The research shows that there is an effect of prices on stock market and that the stocks can be predicted with the help of those prices. Automated data in general and company's distinct data of Apple, Microsoft, Google and Amazon are used to test the effect of the efficient prices on the stock market. A different procedure is experimented with sentiment analysis and the results display that it is able to predict the stock market with the help of stock prices for different companies, where the linear regression model is possible to predict more accurately than the sentiment analysis method.},
	journal = {Proceedings of the International Conference on Computational Techniques, Electronics and Mechanical Systems, CTEMS 2018},
	author = {Pooja, B. L. and Kanakaraddi, Suvarna and Raikar, Meenaxi M.},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538677094},
	keywords = {linear regression, predictive, stock price prediction, sentiment analysis, investiture, Prophets},
	pages = {12--17},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\B6MKPBVZ\\Sentiment Based Stock Market Prediction.pdf:application/pdf},
}

@article{Alamsyah2019,
	title = {Supporting {Investment} {Decision} {Using} {Socio}-{Economic} {Issues} {Exploration} and {Stock} {Price} {Prediction}},
	doi = {10.1109/SAIN.2018.8673343},
	abstract = {Today, technology becomes the effective medium for spreading the information. They bring news and issues to reach a vast audience, including business-related news. The socio-economic implications are huge, one of them impacted the changes in stock prices and creates the value uncertainty. Therefore, the requirement of market information and stock prediction is crucial for investment decision making. the of this study is first to explore the relations between socio-economic issue and stock price, second to predict stock price future value. We choose IDX LQ 452017 as the case study. We use sentiment analysis to fulfill first objective and ANN model using their highest and lowest price, open and close price, and also transaction volume for second objective. We use Support Vector Machine (SVM) and Naive Bayes Classifier (NBC) as training classifier according to time and polarity. The results show 75\% positive correlation between sentiment and the dynamics of stock prices. SVM classifier gives a better result than Naive Bayes Classifier, proved by 62\% precision, 52.33\% recall, 78.01\% accuracy, 56.75\% f-measure and 0.1 KAPPA. Our ANN based prediction model able to predict future stock price with 99.93\% accuracy level. The conclusion of this research is sentiment analysis and stock price prediction have the potential for supporting investor decision using socio-economic issues and stock price prediction.},
	journal = {Proceeding - 2018 International Symposium on Advanced Intelligent Informatics: Revolutionize Intelligent Informatics Spectrum for Humanity, SAIN 2018},
	author = {Alamsyah, Andry and Arasyi, Muhammad Tahta and Rikumahu, Brady},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781538652800},
	keywords = {prediction, out, stock price prediction, sentiment analysis, artificial neural network, investment, socio-economic, stock},
	pages = {20--25},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TZX8KAKP\\Supporting Investment Decision Using Socio-Economic Issues Exploration and Stock Price Prediction.pdf:application/pdf},
}

@article{Alamsyah2018,
	title = {Artificial {Neural} {Network} for predicting {Indonesia} stock exchange composite using macroeconomic variables},
	doi = {10.1109/ICoICT.2018.8528774},
	abstract = {Stock is a high risk and high return investment. The risk-comparison scale for both losses and profits are not much different. The lure of profits temptations can be given by playing shares, sometimes make people less cautious and eventually fail to invest in stocks. To make right and profitable investment decisions, investors need to face uncertainty and fluctuating stock price movements. These phenomena cause investors to predict stock price movements for minimizing risks. The purpose of this study is to predict the Indonesian composite stock price index by using macroeconomic variables as a reflection of economic condition and as a good signal to forecast stock prices. This research is using Inflation, Interest Rates, and Exchange Rates as the macroeconomic variables. This study uses secondary data from Bank Indonesia and Indonesian Statistics Center from December 2005 to November 2017. The prediction uses Artificial Neural Network (ANN) Backpropagation method. The results gained the accuracy of 96, 38\% and mean-squared error of 0.0046 with the best time delay of 2 months before the predicted month. Based on the accuracy level and the error, macroeconomic variables (exchange rate, interest rate, inflation rate, and money supply M2) are the proper indicator to predict IDX Composite movement.},
	journal = {2018 6th International Conference on Information and Communication Technology, ICoICT 2018},
	author = {Alamsyah, Andry and Zahir, Asri Nurfathi},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538645710},
	keywords = {Prediction, Backpropagation, stock price prediction, Artificial Neural Network, Composite stock price index, Macroeconomic variable},
	pages = {44--48},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8R6MFN2S\\Artificial Neural Network for Predicting Indonesia Stock Exchange Composite Using Macroeconomic Variables.pdf:application/pdf},
}

@article{Peng2019,
	title = {Stock {Price} {Prediction} based on {Recurrent} {Neural} {Network} with {Long} {Short}-{Term} {Memory} {Units}},
	doi = {10.1109/ICESI.2019.8863005},
	abstract = {Stock price prediction has been playing a very comprehensive impact on the financial industry. However, it has been considered as one of the most challenging tasks due to the financial time series data has some unpredictable and volatile characteristics. Conventional statistical models can only give a reasonable prediction for the next following one step. In this paper, we propose two novel prediction methods embedded with a deep learning framework using long short-term memory units, providing predictions for both short and long-term horizons. Comparison tests has been carried out between the two proposed methods, and the experiment results have shown that our methods have a significant performance in capturing both the trend and the exact value of the stock data.},
	journal = {2019 International Conference on Engineering, Science, and Industrial Applications, ICESI 2019},
	author = {Peng, Cheng and Yin, Zhihong and Wei, Xinxin and Zhu, Anqi},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728121741},
	keywords = {stock price prediction, in},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H3SABHYT\\Stock Price Prediction based on Recurrent Neural Network with Long Short-Term Memory Units.pdf:application/pdf},
}

@article{Urolagin2017,
	title = {Text {Mining} of {Tweet} for {Sentiment} {Classification} and {Association} with {Stock} {Prices}},
	doi = {10.1109/COMAPP.2017.8079788},
	abstract = {In present days, the social media and networking act as one of the key platforms for sharing information and opinions. Many people share ideas, express their view points and opinions on various topic of their interest. Social media text has rich information about the companies, their products and various services offered by them. In this research we focus exploring the association of sentiments of social media text and stock prices of a company. The tweets of several company has been extracted and performed sentiment classification using Naïve Bayes classifier and SVM classifier. To perform the classification, N-gram based feature vectors are constructed using important words of tweets. Further, the pattern of association between number of tweets which are positive or negative and stock prices has been explored. Motivated by such an association, the features related to tweets such as number of positive, negative, neutral tweets and total number of tweets are used to predict the stock market status using Support Vector Machine classifier.},
	journal = {2017 International Conference on Computer and Applications, ICCA 2017},
	author = {Urolagin, Siddhaling},
	year = {2017},
	note = {ISBN: 9781538627525},
	keywords = {SVM, stock price prediction, Naïve Bayes, sentiment classification, Stock status prediction},
	pages = {384--388},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N4XB3LDE\\Text Mining of Tweet for Sentiment Classification and Association with Stock Prices.pdf:application/pdf},
}

@article{Singh2018,
	title = {Regression},
	author = {Singh, Shekar and Sharma, Seema},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538617199},
	keywords = {data mining, stock price prediction, stock trend prediction, cluster technique, efficient market, partial least, plsr, squares regression},
	pages = {587--591},
}

@article{Kamble2017,
	title = {Short and long term stock trend prediction using decision tree},
	volume = {2018-Janua},
	doi = {10.1109/ICCONS.2017.8250694},
	abstract = {This paper presents the results of method designed to predict price trends in the stock market. First objective of this research is to optimize the stock price trend prediction for short term using some oscillators and indicators: Moving Average Convergence Divergence (MACD), the Relative Strength Index (RSI), the Stochastic Oscillator (KDJ) and Bollinger Band (BB). It is observed that using appropriate pre-processing technique and Machine learning model, it is possible to improve accuracy rate of short-term trend prediction. Applying Preprocessing and then using combination of data can yield a better Accuracy rate in Short term Trades, while predicting for Long-term Trend of Stock this Technical indicators are not sufficient. Along with some of this Technical data and Fundamental Data of the company, it is possible to predict Long term stock movement. For Long term Prediction its Debt to Equity, Net profit of pervious 3 year, Promoters holding, Dividend yield and PE ratio is used along with Technical Factors. It is observed that using Fundamental and Technical Data, Long term Stock Prediction is Possible.},
	journal = {Proceedings of the 2017 International Conference on Intelligent Computing and Control Systems, ICICCS 2017},
	author = {Kamble, Rupesh A.},
	year = {2017},
	note = {ISBN: 9781538627457},
	keywords = {Random Forest, stock price prediction, Bagging, Fundamental data, J48 Decision tree, KDJ, MACD, RSI},
	pages = {1371--1375},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YCSI6TLH\\Short and long term stock trend prediction using decision tree.pdf:application/pdf},
}

@article{Qian2019,
	title = {Stock prediction based on {LSTM} under different stability},
	doi = {10.1109/ICCCBDA.2019.8725709},
	abstract = {The boom of Big Data has made the development of prediction algorithms more intelligent, so the studies have gradually shifted from the traditional linear prediction algorithm (a typical representative of time-series prediction algorithm) to the popular deep learning prediction algorithm. The nonlinear deep learning algorithm can better reflect the changeable internal laws and external relations of data, especially for complex stock price data. Long Short Term Memory network (LSTM) is a special algorithm for processing time-series problem. In this work, we conducted a stationary analysis of the stock's time-series data and then used the LSTM neural network algorithm to predict stock data under different stationary conditions, and performed statistical analysis on multiple experimental data. In addition, an ARIMA algorithm was introduced to compare with the LSTM. A large number of experimental results show that the LSTM neural network prediction algorithm has higher prediction accuracy and is not sensitive to the stability response.},
	journal = {2019 IEEE 4th International Conference on Cloud Computing and Big Data Analytics, ICCCBDA 2019},
	author = {Qian, Fei and Chen, Xianfu},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728114095},
	keywords = {LSTM, prediction, deep learning, stock price prediction, in, ARIMA, stationarity},
	pages = {483--486},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4LQ5GMGI\\Stock Prediction Based on LSTM under Different Stability.pdf:application/pdf},
}

@article{Yuan2020,
	title = {Integrated {Long}-{Term} {Stock} {Selection} {Models} {Based} on {Feature} {Selection} and {Machine} {Learning} {Algorithms} for {China} {Stock} {Market}},
	volume = {8},
	issn = {21693536},
	doi = {10.1109/ACCESS.2020.2969293},
	abstract = {The classical linear multi-factor stock selection model is widely used for long-Term stock price trend prediction. However, the stock market is chaotic, complex, and dynamic, for which reasons the linear model assumption may be unreasonable, and it is more meaningful to construct a better-integrated stock selection model based on different feature selection and nonlinear stock price trend prediction methods. In this paper, the features are selected by various feature selection algorithms, and the parameters of the machine learning-based stock price trend prediction models are set through time-sliding window cross-validation based on 8-year data of Chinese A-share market. Through the analysis of different integrated models, the model performs best when the random forest algorithm is used for both feature selection and stock price trend prediction. Based on the random forest algorithm, a long-short portfolio is constructed to validate the effectiveness of the best model.},
	journal = {IEEE Access},
	author = {Yuan, Xianghui and Yuan, Jin and Jiang, Tianzhao and Ain, Qurat Ul},
	year = {2020},
	keywords = {machine learning, feature selection, out, stock price prediction, long-Term investment, Stock, trend prediction},
	pages = {22672--22685},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ENDFJNHC\\Integrated Long-Term Stock Selection Models Based on Feature Selection and Machine Learning Algorithms for China Stock Market.pdf:application/pdf},
}

@article{Cheng2019,
	title = {Applied attention-based {LSTM} neural networks in stock prediction},
	doi = {10.1109/BigData.2018.8622541},
	abstract = {Prediction of stocks is complicated by the dynamic, complex, and chaotic environment of the stock market. Many studies predict stock price movements using deep learning models. Although the attention mechanism has gained popularity recently in neural machine translation, little focus has been devoted to attention-based deep learning models for stock prediction. This paper proposes an attention-based long short-term memory model to predict stock price movement and make trading strategies.},
	journal = {Proceedings - 2018 IEEE International Conference on Big Data, Big Data 2018},
	author = {Cheng, Li Chen and Huang, Yu Hsiang and Wu, Mu En},
	year = {2019},
	note = {ISBN: 9781538650356},
	keywords = {deep learning, out, stock price prediction, stock prediction, attention mechanism},
	pages = {4716--4718},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JFZASWWR\\Applied attention-based LSTM neural networks in stock prediction.pdf:application/pdf},
}

@article{Tsai2017,
	title = {The application of evolutionary approach for stock trend awareness},
	volume = {2018-Janua},
	doi = {10.1109/ICAwST.2017.8256468},
	abstract = {It has been an important task for business and individuals to make financial investments on stock market in order to wisely extend possible income sources. Such investments require precise timely decision and highly awareness of market changes at all time. Many well-known pricing models have already been proposed by different learnt researchers to explain the rationality between stock price and the covalent factors. These models were meant to assist information receivers to adjust their holding of stocks with reasonable pricing strategy and make wise financial decision timely. Since any newly entered information in the market shall be digested and cause stock price movement. By assuming that the stock market possesses sufficient efficiency to adjust stock price to the equilibrium status, a prediction made prior to such movement would be regarded possible. This paper has constructed a GPLAB financial customized prototype system and demonstrated certain accuracy in the forecast of stock price movements in TWSE (Taiwan Stock Exchange). The empirical study reveals that the system possesses a fair prediction ability of stock price movement in a random chosen period and a bear market period. Under certain restrictions that this model may serve as an early stock price changes awareness system. Such awareness may provide investors opportunity to adjust stock holding strategy timely. This study also believes the accuracy of forecast could have been further improved with the assistance of other tools such as deep learning and neuron network. The potential of genetic algorism application in the field of financing decisions could have also been further accomplished in the future.},
	number = {iCAST},
	journal = {Proceedings - 2017 IEEE 8th International Conference on Awareness Science and Technology, iCAST 2017},
	author = {Tsai, Yi Chi and Hong, Cheng Yih},
	year = {2017},
	note = {ISBN: 9781538629659},
	keywords = {Machine Learning, stock price prediction, Stock Price Prediction, Evolutionary Approach, Genetic Algorism},
	pages = {306--311},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\552VWJNV\\The application of evolutionary approach for stock trend awareness.pdf:application/pdf},
}

@article{Chen2019,
	title = {A {Dual}-{Attention}-{Based} {Stock} {Price} {Trend} {Prediction} {Model} with {Dual} {Features}},
	volume = {7},
	issn = {21693536},
	doi = {10.1109/ACCESS.2019.2946223},
	abstract = {Modeling and predicting stock prices is an important and challenging task in the field of financial market. Due to the high volatility of stock prices, traditional data mining methods cannot identify the most relevant and critical market data for predicting stock price trend. This paper proposes a stock price trend predictive model (TPM) based on an encoder-decoder framework that predicting the stock price movement and its duration adaptively. This model consists of two phases, first, a dual feature extraction method based on different time spans is proposed to get more information from the market data. While traditional methods only extract features from information at some specific time points, this proposed model applies the PLR method and CNN to extract the long-term temporal features and the short-term spatial features from market data. Then, in the second phase of the proposed TPM, a dual attention mechanism based encoder-decoder framework is used to select and merge relevant dual features and predict the stock price trend. To evaluate our proposed TPM, we collected high-frequency market data for stock indexes CSI300, SSE 50 and CSI 500, and conducted experiments based on these three data sets. The experimental results show that the proposed TPM outperforms the existing state-of-art methods, including SVR, LSTM, CNN, LSTM\_CNN and TPM\_NC, in terms of prediction accuracy.},
	journal = {IEEE Access},
	author = {Chen, Yingxuan and Lin, Weiwei and Wang, James Z.},
	year = {2019},
	keywords = {★, neural networks, stock price prediction, in, Stock price, dual attention mechanism, dual feature extraction, encoder-decoder, predictive models},
	pages = {148047--148058},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P57BSJJK\\A Dual-Attention-Based Stock Price Trend Prediction Model With Dual Features.pdf:application/pdf},
}

@article{Bhattacharjee2019,
	title = {Stock {Price} {Prediction}: {A} {Comparative} {Study} between {Traditional} {Statistical} {Approach} and {Machine} {Learning} {Approach}},
	doi = {10.1109/EICT48899.2019.9068850},
	abstract = {Stock market is one of the most important sectors of a country's economy. Prediction of stock prices is not easy since it is not stationary in nature. The objective of this paper is to find the best possible method to predict the closing prices of stocks through a comparative study between different traditional statistical approaches and machine learning techniques. Predictions using statistical methods like Simple Moving Average, Weighted Moving Average, Exponential Smoothing, Naive approach, and machine learning methods like Linear Regression, Lasso, Ridge, K-Nearest Neighbors, Support Vector Machine, Random Forest, Single Layer Perceptron, Multi-layer Perceptron, Long Short Term Memory are performed. Moreover, a comparative study between statistical approaches and machine learning approaches has been done in terms of prediction performances and accuracy. After studying all the methods individually, the machine learning approach, especially the neural network models are found to be the most accurate for stock price prediction.},
	number = {December},
	journal = {2019 4th International Conference on Electrical Information and Communication Technology, EICT 2019},
	author = {Bhattacharjee, Indronil and Bhattacharja, Pryonti},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728160405},
	keywords = {Neural Networks, Machine Learning, out, stock price prediction, Stock price prediction, Long Short Term memory, Multi-layer Perceptron, Statistical methods},
	pages = {20--22},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PNQYMZDA\\Stock Price Prediction A Comparative Study between Traditional Statistical Approach and Machine Learning Approach.pdf:application/pdf},
}

@article{Yu2017,
	title = {Stock price forecasting based on {BP} neural network model of network public opinion},
	doi = {10.1109/ICIVC.2017.7984716},
	abstract = {with the development of Internet, many companies announced the stock information by various medium, and stock buyers comment that information as well as make rational investment strategies to maximize their profit. At present, many retail investors, who lack of channels to obtain real information, scarce professional knowledge of investment theory and easy to be affected by public opinion of Internet, make an irrational investment. This paper aims to combine the text analysis technique with comments on Vanke A (sz000002) from website to calculate the percentage of people with a positive view. This analysis method attempts to predict the impacts of investor sentiment on the stock market in the short term. Based on the network public opinion formation mode and technical indicators in stock market, this paper puts forward a BP neural network model. Then we improve the prediction accuracy of stock price by predicting the closing price.},
	journal = {2017 2nd International Conference on Image, Vision and Computing, ICIVC 2017},
	author = {Yu, Yawen and Shanshan, Wang and Lijun, Zhang},
	year = {2017},
	note = {Publisher: IEEE
ISBN: 9781509062379},
	keywords = {stock price prediction, BP neural network, Network public opinion, Stock price forecast, Text analysis},
	pages = {1058--1062},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C49IFCFU\\Stock price forecasting based on BP neural network model of network public opinion.pdf:application/pdf},
}

@article{Gao2019,
	title = {Share price trend prediction using {CRNN} with {LSTM} structure},
	doi = {10.1109/IS3C.2018.00012},
	abstract = {The stock market plays an important role in the entire financial market, and the prediction of stock price volatility is one of the most attractive research issues. In this paper, we will use the historical information of stocks to predict future stock prices and use deep learning to achieve them. This paper uses deep learning to predict the future trend of stock prices. Since the trend of stocks is usually related to the previous stock price, this paper proposes a Convolutional Recurrent Neural Network (CRNN)-based architecture, ConvLSTM, in which long and short-term memory is used in RNN (Long short). -term memory, LSTM) architecture. LSTM improves the long-term dependence of traditional RNN and effectively improves the accuracy and stability of prediction. This paper collects a total of ten stock historical data to test and achieve an average error rate of 3.449 RMSE.},
	journal = {Proceedings - 2018 International Symposium on Computer, Consumer and Control, IS3C 2018},
	author = {Gao, Shao En and Lin, Bo Sheng and Wang, Chuin Mu},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781538670361},
	keywords = {Deep learning, Recurrent neural network, stock price prediction, in, Stock prediction, Long short-term memory, Convolutional neural network},
	pages = {10--13},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SKJQ4GZX\\Share Price Trend Prediction Using CRNN with LSTM Structure.pdf:application/pdf},
}

@article{Sezer2017,
	title = {An artificial neural network-based stock trading system using technical analysis and big data framework},
	doi = {10.1145/3077286.3077294},
	abstract = {In this paper, a neural network-based stock price prediction and trading system using technical analysis indicators is presented. The model developed first converts the financial time series data into a series of buy-sell-hold trigger signals using the most commonly preferred technical analysis indicators. Then, a Multilayer Perceptron (MLP) artificial neural network (ANN) model is trained in the learning stage on the daily stock prices between 1997 and 2007 for all of the Dow30 stocks. Apache Spark big data framework is used in the training stage. The trained model is then tested with data from 2007 to 2017. The results indicate that by choosing the most appropriate technical indicators, the neural net- work model can achieve comparable results against the Buy and Hold strategy in most of the cases. Furthermore, fine tuning the technical indicators and/or optimization strategy can enhance the overall trading performance.},
	number = {2},
	journal = {Proceedings of the SouthEast Conference, ACMSE 2017},
	author = {Sezer, Omer Berat and Ozbayoglu, Ahmet Murat and Dogdu, Erdogan},
	year = {2017},
	note = {ISBN: 9781450350242},
	keywords = {stock price prediction, Technical analysis, Stock market, Algorithmic trading, Artificial neural network, Multi layer perceptron},
	pages = {223--226},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PUJBQNHB\\An Artificial Neural Network-based Stock Trading System Using Technical Analysis and Big Data Framework.pdf:application/pdf},
}

@article{Deng2019,
	title = {Knowledge-driven stock trend prediction and explanation via temporal convolutional network},
	doi = {10.1145/3308560.3317701},
	abstract = {Deep neural networks have achieved promising results in stock trend prediction. However, most of these models have two common drawbacks, including (i) current methods are not sensitive enough to abrupt changes of stock trend, and (ii) forecasting results are not interpretable for humans. To address these two problems, we propose a novel Knowledge-Driven Temporal Convolutional Network (KDTCN) for stock trend prediction and explanation. Firstly, we extract structured events from financial news, and utilize external knowledge from knowledge graph to obtain event embeddings. Then, we combine event embeddings and price values together to forecast stock trend. We evaluate the prediction accuracy to show how knowledge-driven events work on abrupt changes. We also visualize the effect of events and linkage among events based on knowledge graph, to explain why knowledge-driven events are common sources of abrupt changes. Experiments demonstrate that KDTCN can (i) react to abrupt changes much faster and outperform state-of-the-art methods on stock datasets, as well as (ii) facilitate the explanation of prediction particularly with abrupt changes.},
	journal = {The Web Conference 2019 - Companion of the World Wide Web Conference, WWW 2019},
	author = {Deng, Shumin and Chen, Jiaoyan and Zhang, Ningyu and Pan, Jeff Z. and Zhang, Wen and Chen, Huajun},
	year = {2019},
	note = {ISBN: 9781450366755},
	keywords = {Explanation, ★, out, stock price prediction, Event extraction, Knowledge-driven, Predictive analytics, Stock trend prediction, Structured, Unstructured},
	pages = {678--685},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WQ93IZ8Y\\Knowledge-Driven Stock Trend Prediction and Explanation via Temporal Convolutional Network.pdf:application/pdf},
}

@article{Lin2018,
	title = {Short-term prediction of stock market price based on {GA} optimization {LSTM} neurons},
	doi = {10.1145/3234804.3234818},
	abstract = {Long-Short Term Memory Network stands out from the financial sector due to its long-term memory predictability, however, the speed of subsequent operations is extremely slow, and the timeliness of the inability to meet market changes has been criticized. In this paper. Aiming at the shortcomings of the slow running of three gates in each neuron of LSTM in back propagation, we propose to use GA to optimize the internal weights of LSTM neurons to optimize the defect. In this experiment, GA optimization could not change the accuracy of the model, but it achieved better results than the original LSTM in terms of speed, and satisfied the demand of the future business field for rapid response to market changes in terms of timeliness.},
	number = {22},
	journal = {ACM International Conference Proceeding Series},
	author = {Lin, Moule and Chen, Changxi},
	year = {2018},
	note = {ISBN: 9781450364737},
	keywords = {LSTM, stock price prediction, Stock prediction, GA, Timeliness},
	pages = {66--70},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Q5TFGU8P\\Short-term prediction of stock market price based on GA optimization LSTM neurons.pdf:application/pdf},
}

@article{Huynh2017,
	title = {A new model for stock price movements prediction using deep neural network},
	volume = {2017-Decem},
	doi = {10.1145/3155133.3155202},
	abstract = {In this paper, we introduce a new prediction model depend on Bidirectional Gated Recurrent Unit (BGRU). Our predictive model relies on both online financial news and historical stock prices data to predict the stock movements in the future. Experimental results show that our model accuracy achieves nearly 60\% in S\&P 500 index prediction whereas the individual stock prediction is over 65\%.},
	journal = {ACM International Conference Proceeding Series},
	author = {Huynh, Huy D. and Dang, L. Minh and Duong, Duc},
	year = {2017},
	note = {ISBN: 9781450353281},
	keywords = {LSTM, stock price prediction, Stock market prediction, GRU, BGRU},
	pages = {57--62},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IUSLYA8T\\A New Model for Stock Price Movements Prediction Using Deep Neural Network.pdf:application/pdf},
}

@article{Tang2018,
	title = {Stock market prediction based on historic prices and news titles},
	doi = {10.1145/3231884.3231887},
	abstract = {Predicting the trend of stock market prices is a very challengingtask, in that stock markets are complicated and can be influencedby a variety of factors. Despite the great difficulty, predicting thetrend of stock market prices accurately is very meaningful and canbring a large amount of profit. In the past several decades, a lot ofstudies have been done on this problem. But most of the methodstake only the historic prices data as the input, which is not enoughfor such a complicated problem. In this paper, a hybrid methodtaking both historic prices and the news as input is proposed. Thehybrid model combines the best of two kinds of networks-RNN-LSTM for time series data and CNN for abstract highdimensional data. These two different kinds of networks arecombined together to make a prediction. A set of experimentshave been carried out to show the performance of the proposedmethod. The result obtained is promising, and the propose methodachieves a great degree of accuracy in prediction and outperformsthe baselines a lot.},
	journal = {ACM International Conference Proceeding Series},
	author = {Tang, Jinqi and Chen, Xiong},
	year = {2018},
	note = {ISBN: 9781450364324},
	keywords = {★, stock price prediction, Stock market prediction, Combined data, Convolution neural network, Hybrid algorithm, Long-short term memory},
	pages = {29--34},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D9D94S3W\\Stock Market Prediction Based on Historic Prices and News Titles.pdf:application/pdf},
}

@article{Torralba2019,
	title = {Development of a deep learning-{LSTM} trend prediction model of stock prices},
	doi = {10.1145/3335550.3335585},
	abstract = {Stocks prices follow the Brownian motion process that moves in a random and erratic state making it impossible to determine the future price based on the historical performance of the stock. However, the availability of the historical data of stock prices can't be ignored as it might hold hidden patterns that manifest the behavior of investors as well as the market sentiments and political conditions. Thus, investors have adapted mathematical models of regression and neural network analysis to process the stock's historical data along with the traditional fundamental analysis in forecasting the trend of stock prices. Neural network models have changed the landscape of formulating informed decisions on how investors can minimize their risks on their investments. However, deep learning models are dependent on the different hyper-parameters such as epochs, number of nodes in hidden layers, and number of hidden layers in order to produce the best trend prediction model as applied in stock investments. Unfortunately, the available literature and empirical studies are limited specifically on the number of hidden layers of deep learning models. This study attempts to explore the appropriate number of hidden layers to optimize the accuracy of a trend prediction model in stock trading.},
	journal = {ACM International Conference Proceeding Series},
	author = {Torralba, Edwin M.},
	year = {2019},
	note = {ISBN: 9781450362641},
	keywords = {Deep Learning, Recurrent Neural Networks, Machine Learning, stock price prediction, Long-Short-Term-Memory, our, Stock Trading},
	pages = {126--133},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E5QWVKBA\\Development of a Deep Learning-LSTM Trend Prediction Model of Stock Prices.pdf:application/pdf},
}

@article{Patil2020,
	title = {Stock market prediction using ensemble of graph theory, machine learning and deep learning models},
	doi = {10.1145/3378936.3378972},
	abstract = {Efficient Market Hypothesis (EMH) is the cornerstone of the modern financial theory and it states that it is impossible to predict the price of any stock using any trend, fundamental or technical analysis. Stock trading is one of the most important activities in the world of finance. Stock price prediction has been an age-old problem and many researchers from academia and business have tried to solve it using many techniques ranging from basic statistics to machine learning using relevant information such as news sentiment and historical prices. Even though some studies claim to get prediction accuracy higher than a random guess, they consider nothing but a proper selection of stocks and time interval in the experiments. In this paper, a novel approach is proposed using graph theory. This approach leverages Spatio-temporal relationship information between different stocks by modeling the stock market as a complex network. This graph-based approach is used along with two techniques to create two hybrid models. Two different types of graphs are constructed, one from the correlation of the historical stock prices and the other is a causation-based graph constructed from the financial news mention of that stock over a period. The first hybrid model leverages deep learning convolutional neural networks and the second model leverages a traditional machine learning approach. These models are compared along with other statistical models and the advantages and disadvantages of graph-based models are discussed. Our experiments conclude that both graph-based approaches perform better than the traditional approaches since they leverage structural information while building the prediction model.},
	journal = {ACM International Conference Proceeding Series},
	author = {Patil, Pratik and Wu, Ching Seh M. and Potika, Katerina and Orang, Marjan},
	year = {2020},
	note = {ISBN: 9781450376907},
	keywords = {★, Deep learning, Machine learning, stock price prediction, in, Stock market, Big data analytics, Financial networks, Graph theory, Spatiotemporal, Time series forecasting},
	pages = {85--92},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MNG2NAGP\\Stock Market Prediction Using Ensemble of Graph Theory, Machine Learning and Deep Learning Models.pdf:application/pdf},
}

@article{Touzani2018,
	title = {Stock price forecasting: {New} model for uptrend detecting and downtrend anticipating based on long short-term memory.},
	doi = {10.1145/3264560.3264566},
	abstract = {Stock price forecasting has tremendous importance, it is one of the most challenging tasks due to the high volatility of stock market data. Share market investment is often risky, hence the need for an accurate forecasting model to minimize this risk. This article aims to present a new model for forecasting stock prices for a given horizon. The model consists of four phases: the first phase is filtering out of all the stocks those for which we cannot reject the normal distribution hypothesis of the return. Note that the returns in our case reflect the evolution of the price compared to the moving average of n previous days. Then a second filtration to keep only the stocks whose returns has a high average, low standard deviation and the error type II is low enough for the normality test. In the third phase, we will build classes by comparing returns to given limits (8 given limits). The last phase is for training and testing the LSTM classifier. The main objective of the model is to anticipate class 0 and 1, which correspond to a medium and strong downtrend, and to predict class 7 and 8 which characterize the price increase. Experience shows that our proposed model gives strong results and its accuracy is quite high.},
	number = {Lim},
	journal = {ACM International Conference Proceeding Series},
	author = {Touzani, Yassine and Douzi, Khadija and Khoukhi, Fadoul},
	year = {2018},
	note = {ISBN: 9781450364744},
	keywords = {LSTM, Deep Learning, Machine Learning, stock price prediction, Stock market prediction, Multiclass Classification, Normal Distribution},
	pages = {61--65},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4JEXFLES\\Stock Price Forecasting - New Model for Uptrend Detecting and Downtrend Anticipating Based on Long Short-Term Memory.pdf:application/pdf},
}

@article{Zhang2018a,
	title = {Predicting {Chinese} stock market price trend using machine learning approach},
	doi = {10.1145/3207677.3277966},
	abstract = {The stock1 market is dynamic, noisy and hard to predict. In this paper, we explored four machine learning models using technical indicators as input features to predict the price trend 30 days later. The experimental dataset is Shanghai Stock Exchange(SSE) 50 index stocks. The result demonstrates that ANN performs better than the other three models and is promising to find some profitable patterns.},
	journal = {ACM International Conference Proceeding Series},
	author = {Zhang, Chongyang and Ji, Zhi and Zhang, Jixiang and Wang, Yanqing and Zhao, Xingzhi and Yang, Yiping},
	year = {2018},
	note = {ISBN: 9781450365123},
	keywords = {Machine learning, Neural network, SVM, out, stock price prediction, Stock prediction},
	pages = {6--9},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D726MVW3\\Predicting Chinese Stock Market Price Trend Using Machine Learning Approach.pdf:application/pdf},
}

@article{Liu2018a,
	title = {Hierarchical complementary attention network for predicting stock price movements with news},
	doi = {10.1145/3269206.3269286},
	abstract = {It has been shown that stock price movements are influenced by news. To predict stock movements with news, many existing works rely only on the news title since the news content may contain irrelevancies which seriously degrade the prediction accuracy. However, we observe that there is still useful information in the content which is not reflected in the title, and simply ignoring the content will result in poor performance. In this paper, taking advantage of neural representation learning, we propose a hierarchical complementary attention network (HCAN) to capture valuable complementary information in news title and content for stock movement prediction. In HCAN, we adopt a two-level attention mechanism to quantify the importances of the words and sentences in a given news. Moreover, we design a novel measurement for calculating the attention weights to avoid capturing redundant information in the news title and content. Experimental results on news datasets show that our proposed model outperforms the state-of-the-art techniques.},
	journal = {International Conference on Information and Knowledge Management, Proceedings},
	author = {Liu, Qikai and Cheng, Xiang and Su, Sen and Zhu, Shuguang},
	year = {2018},
	note = {ISBN: 9781450360142},
	keywords = {Neural network, out, stock price prediction, Stock prediction, Attention mechanism, News representation},
	pages = {1603--1606},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZAIW36VL\\Hierarchical Complementary Attention Network for Predicting Stock Price Movements with News.pdf:application/pdf},
}

@article{Joosery2019,
	title = {Comparative analysis of time-series forecasting algorithms for stock price prediction},
	doi = {10.1145/3373477.3373699},
	abstract = {This paper predicts the average stock price for five datasets by utilizing the historical stock price data ranging from April 2009 to February 2019. Autoregressive Integrated Moving Average (ARIMA) model is used to generate the baseline, while Long Short-Term Memory (LSTM) networks is used to build the forecasting model for predicting the stock price. The efficiency of the two models is compared in terms of Mean Squared Error. The results show that the LSTM model predicts better than the ARIMA model with respect to time series forecasting. Additionally, Attention LSTM networks is employed to further study the improvement in accuracy of the stock price forecasting model.},
	journal = {ACM International Conference Proceeding Series},
	author = {Joosery, Baleshwarsingh and Deepa, G.},
	year = {2019},
	note = {ISBN: 9781450372916},
	keywords = {Deep Learning, Machine Learning, stock price prediction, in, Stock price, Time series forecasting},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UX5YBT9M\\Comparative Analysis of Time-Series Forecasting Algorithms for Stock Price Prediction.pdf:application/pdf},
}

@article{Rana2019,
	title = {Effects of activation functions and optimizers on stock price prediction using {LSTM} recurrent networks},
	doi = {10.1145/3374587.3374622},
	abstract = {In stock exchange market, investors need to decide which shares to buy based on their future market value. Because of the variable market, it is obligatory to have a reliable prediction of the values of the stocks. Now-a-days the machine learning system can forecast well than the contemporary stock prediction methods. Machine learning system provides a scientific demonstration based on sample data to forecast. In this work, Linear Regression (LR), Support Vector Regression (SVR) and, Long Short-Term Memory (LSTM) algorithms are used to predict stock market prediction. Among the several features, the most important feature has been selected by using the feature selection algorithm, which is closing price. The effects of different activation functions and optimizers are experimented on stock price prediction using LSTM networks.},
	journal = {ACM International Conference Proceeding Series},
	author = {Rana, Masud and Uddin, Md Mohsin and Hoque, Md Mohaimnul},
	year = {2019},
	note = {ISBN: 9781450376273},
	keywords = {Feature selection, LSTM, Support vector regression, stock price prediction, in, Activation function, Closing price, Linear regression, Optimizer},
	pages = {354--358},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C3LKIGJG\\Effects of Activation Functions and Optimizers on Stock Price Prediction using LSTM Recurrent Networks.pdf:application/pdf},
}

@article{Zhang2017d,
	title = {Stock price prediction via discovering multi-frequency trading patterns},
	volume = {Part F1296},
	doi = {10.1145/3097983.3098117},
	abstract = {Stock prices are formed based on short and/or long-term commercial and trading activities that reflect different frequencies of trading patterns. However, these patterns are often elusive as they are affected by many uncertain political-economic factors in the real world, such as corporate performances, government policies, and even breaking news circulated across markets. Moreover, time series of stock prices are non-stationary and non-linear, making the prediction of future price trends much challenging. To address them, we propose a novel State Frequency Memory (SFM) recurrent network to capture the multi-frequency trading patterns from past market data to make long and short term predictions over time. Inspired by Discrete Fourier Transform (DFT), the SFM decomposes the hidden states of memory cells into multiple frequency components, each of which models a particular frequency of latent trading pattern underlying the fluctuation of stock price. Then the future stock prices are predicted as a nonlinear mapping of the combination of these components in an Inverse Fourier Transform (IFT) fashion. Modeling multi-frequency trading patterns can enable more accurate predictions for various time ranges: while a shortterm prediction usually depends on high frequency trading patterns, a long-term prediction should focus more on the low frequency trading patterns targeting at long-term return. Unfortunately, no existing model explicitly distinguishes between various frequencies of trading patterns to make dynamic predictions in literature. The experiments on the real market data also demonstrate more competitive performance by the SFM as compared with the state-of-the-art methods.},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Zhang, Liheng and Aggarwal, Charu and Qi, Guo Jun},
	year = {2017},
	note = {ISBN: 9781450348874},
	keywords = {stock price prediction, Stock price prediction, Multi-frequency trading patterns, State frequency memory},
	pages = {2141--2149},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ICY844L8\\Stock Price Prediction via Discovering Multi-Frequency Trading Patterns.pdf:application/pdf},
}

@article{UhdoOlihVlwxdwlrq,
	title = {Uhdo olih vlwxdwlrq 7kh xqfhuwdlqw{\textbackslash} ri wkh vwrfn pdunhw},
	volume = {25},
	keywords = {stock price prediction},
	pages = {0--5},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FA86ZXAH\\Intraday Stock Prices Forecasting Using an Autoregressive Model.pdf:application/pdf},
}

@article{Bouktif2020,
	title = {Augmented {Textual} {Features}-{Based} {Stock} {Market} {Prediction}},
	volume = {8},
	issn = {21693536},
	doi = {10.1109/ACCESS.2020.2976725},
	abstract = {Due to its dynamics, non-linearity and complexity nature, stock market is inherently difficult to predict. One of the attractive objectives is to predict stock market movement direction by using public sentiments analysis. However, there is an active debate about the usefulness of this approach and the strength of causality between stock market trends and sentiments. The opinions of researchers range from rejecting the relationship to confirming a clear causality between sentiments and trading in stock markets. Nevertheless, many advanced computational methods have adopted sentiment-based features, yet did not attain maturity and performance. In this paper, we are contributing constructively in this debate by empirically investigating the predictability of stock market movement direction using an enhanced method of sentiments analysis. Precisely, we experiment on stock prices history, sentiments polarity, subjectivity, N-grams, customized text-based features in addition to features lags that are used for a finer-grained analysis. Five research questions have been investigated towards answering issues associated with stock market movement prediction using sentiment analysis. We have collected and studied the stocks of ten influential companies belonging to different stock domains in NASDAQ. Our analysis approach is complemented by a sophisticated causality analysis, an algorithmic feature selection and a variety of machine learning techniques including regularized models stacking. A comparison of our approach with other sentiment-based stock market prediction approaches including Deep learning, establishes that our proposed model is performing adequately and predicting stock movements with a higher accuracy of 60\%.},
	journal = {IEEE Access},
	author = {Bouktif, Salah and Fiaz, Ali and Awad, Mamoun},
	year = {2020},
	note = {Publisher: IEEE},
	keywords = {Machine learning, out, stock price prediction, sentiment analysis, model stacking, stock movement direction prediction, textual features extraction, tweets mining},
	pages = {40269--40282},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PUIGD9QS\\Augmented Textual Features-Based Stock Market Prediction.pdf:application/pdf},
}

@article{Faustryjak2018,
	title = {Forward forecast of stock prices using {LSTM} neural networks with statistical analysis of published messages},
	doi = {10.1109/IIPHDW.2018.8388375},
	abstract = {The article presents a new approach that combines two separate fields of stock exchange analysis. The aim of proposed solution is to support investors in their decisions and recommend to buy the assets which provide the greatest profits. To achieve this goal, decisive algorithms have been developed using artificial neural networks and technical analysis, which were used along with statistics that refer to the occurrence of single words in the fundamental analysis. Based on this, a model was prepared that in response gives a recommendation for future increases. The system consists of two algorithms. The first of them uses the LSTM (Long Short-Term Memory) artificial neural network. As inputs, information about the current closing price as well as technical analysis indicators along with the value of the current volume were used. The output has been specified as the closing price on the following day. In order to improve the response from the ANN (Artificial Neural Network), statistics of the occurrence of words in publications from last week were used. Subsequent signals gained much more importance if the volume of all transactions was much larger than the moving average of the last 15 periods and if the words that appeared in the last publication caused earlier increases. Additional information for the system are also data that come from Google Trends. This allows to verify the trend of interest and whether the published messages are important.},
	journal = {2018 International Interdisciplinary PhD Workshop, IIPhDW 2018},
	author = {Faustryjak, Damian and Jackowska-Strumiłło, Lidia and Majchrowicz, Michał},
	year = {2018},
	note = {ISBN: 9781538661437},
	keywords = {out, stock price prediction, Google Trends, artificial neural network, closing price prediction, long short-term memory, stock market analysis},
	pages = {288--292},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P9GMPPQG\\Forward forecast of stock prices using LSTM neural networks with statistical analysis of published messages.pdf:application/pdf},
}

@article{Ouahilal2016,
	title = {Optimizing stock market price prediction using a hybrid approach based on {HP} filter and support vector regression},
	volume = {0},
	issn = {23271884},
	doi = {10.1109/CIST.2016.7805059},
	abstract = {Predicting stock prices is an important task of financial time series forecasting, which is of great interest to stock investors, stock traders and applied researchers. Many machine learning techniques have been used in recent times to predict the stock price, including regression algorithms which can be useful tools to provide good accuracy of financial time series forecasting. In this paper, we propose a novel hybrid approach which combines Support Vector Regression and Hodrick-Prescott filter in order to optimize the prediction of stock price. To assess the performance of this proposed approach, we have conducted several experiments using Maroc Telecom (IAM) financial time series. It is daily data collected during the period between 2004 and 2016. The experimental results confirm that the proposed model is more powerful in term of predicting stock prices.},
	journal = {Colloquium in Information Science and Technology, CIST},
	author = {Ouahilal, Meryem and Mohajir, Mohammed El and Chahhou, Mohamed and El Mohajir, Badr Eddine},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781509007516},
	keywords = {Support vector regression, stock price prediction, Stock price prediction, Time series forecasting, Decision support, Hodrick-Prescott filter},
	pages = {290--294},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5IU3YIXI\\Optimizing stock market price prediction using a hybrid approach based on HP filter and support vector regression.pdf:application/pdf},
}

@article{Wang2019d,
	title = {{EAN}: {Event} attention network for stock price trend prediction based on sentimental embedding},
	doi = {10.1145/3292522.3326014},
	abstract = {It is only natural that events related to a listed company may cause its stock price to move (either up or down), and the trend of the price movement will be very much determined by the public opinions towards such events. With the help of the Internet and advanced natural language processing techniques, it becomes possible to predict the stock trend by analyzing great amount of online textual resources like news from websites and posts on social media. In this paper, we propose an event attention network (EAN) to exploit sentimental event-embedding for stock price trend prediction. Specially, this model combines the merits from both eventdriven prediction and sentiment-driven prediction models, in addition to exploiting sentimental event-embedding. Furthermore, we employ attention mechanism to figure out which event contributes the most to the result or, in another word, which event is the main cause of the price fluctuation. In our model, a convolution neural network (CNN) layer is used to extract salient features from transformed event representations, and the latter are originated from a bi-directional long short-Term memory (BiLSTM) layer. We conduct extensive experiments on a manually collected real-world dataset. Experimental results show that our model performs significantly better in terms of short-Term stock trend prediction.},
	journal = {WebSci 2019 - Proceedings of the 11th ACM Conference on Web Science},
	author = {Wang, Yaowei and Li, Qing and Huang, Zhexue and Li, Junjie},
	year = {2019},
	note = {ISBN: 9781450362023},
	keywords = {out, stock price prediction, Stock trend prediction, Attention-based deep learning, Financial text mining, Sentimental event embedding},
	pages = {311--320},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PAA5FQWM\\EAN -  Event Attention Network for Stock Price Trend Prediction based on Sentimental Embedding.pdf:application/pdf},
}

@article{Saad1998,
	title = {Comparative study of stock trend prediction using time delay, recurrent and probabilistic neural networks},
	volume = {9},
	issn = {10459227},
	doi = {10.1109/72.728395},
	abstract = {Three networks are compared for low false alarm stock trend predictions. Short-term trends, particularly attractive for neural network analysis, can be used profitably in scenarios such as option trading, but only with significant risk. Therefore, we focus on limiting false alarms, which improves the risk/reward ratio by preventing losses. To predict stock trends, we exploit time delay, recurrent, and probabilistic neural networks (TDNN, RNN, and PNN, respectively), utilizing conjugate gradient and multistream extended Kalman filter training for TDNN and RNN. We also discuss different predictability analysis techniques and perform an analysis of predictability based on a history of daily closing price. Our results indicate that all the networks are feasible, the primary preference being one of convenience. © 1998 IEEE.},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Saad, Emad W. and Prokhorov, Danil V. and Wunsch, Donald C.},
	year = {1998},
	keywords = {Recurrent neural network, stock price prediction, Financial forecasting, Time series analysis, Conjugate gradient, Extended Kalman filter, Financial engineering, Predictability analysis, Probablistic neural network, Stock market forecasting, Time delay neural network},
	pages = {1456--1470},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DFV2Q3AN\\Comparative study of stock trend prediction using time delay, recurrent and probabilistic neural networks.pdf:application/pdf},
}

@article{Boonmatham2020,
	title = {Stock {Price} {Analysis} with {Natural} {Language} {Processing} and {Machine} {Learning}},
	doi = {10.1145/3406601.3406652},
	abstract = {Finding stock price classification based on Thai news corporate is a challenging task. In this research, we try to build machine learning models that capture the relationship of news and stock prices of several companies. In this work, eight companies were selected randomly from Industry Group Index and Sectoral Index. Corporate news articles from the eight selected companies were collected along with their stock prices. Two of traditional machine learning models and two deep learning models were used in this study for comparison purpose. The models were based on Support Vector Machine (SVM), Multilayer Perceptron (MLP), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). Using news articles as inputs, the models were trained to classify stock prices into two classes: Up and Down of the stock closing price. For classification performance, Accuracy, Precision, Recall and F1 were used. The results showed that GRU had highest average accuracy, precision, recall and F1 higher than other model values with 0.79, 0.79, 0.79, 0.79, respectively.},
	journal = {ACM International Conference Proceeding Series},
	author = {Boonmatham, Sukanchalika and Meesad, Phayung},
	year = {2020},
	note = {ISBN: 9781450377591},
	keywords = {Natural language processing, Deep Learning, out, stock price prediction, Text Mining, Classification Model},
	pages = {2--7},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I9ZZ9KJE\\Stock Price Analysis with Natural Language Processing and Machine Learning.pdf:application/pdf},
}

@article{Chen2019a,
	title = {Investment behaviors can tell what inside: {Exploring} stock intrinsic properties for stock trend prediction},
	doi = {10.1145/3292500.3330663},
	abstract = {Stock trend prediction, aiming at predicting future price trend of stocks, plays a key role in seeking maximized profit from the stock investment. Recent years have witnessed increasing efforts in applying machine learning techniques, especially deep learning, to pursue more promising stock prediction. While deep learning has given rise to significant improvement, human investors still retain the leading position due to their understanding on stock intrinsic properties, which can imply invaluable principles for stock prediction. In this paper, we propose to extract and explore stock intrinsic properties to enhance stock trend prediction. Fortunately, we discover that the repositories of investment behaviors within mutual fund portfolio data form up a gold mine to extract latent representations of stock properties, since such collective investment behaviors can reflect the professional fund managers' common beliefs on stock intrinsic properties. Powered by extracted stock properties, we further propose to model the dynamic market state and trend using stock representations so as to generate the dynamic correlation between the stock and the market, and then we aggregate such correlation with dynamic stock indicators to achieve more accurate stock prediction. Extensive experiments on real-world stock market data demonstrate the effectiveness of stock properties extracted from collective investment behaviors in the task of stock prediction.},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Chen, Chi and Zhao, Li and Bian, Jiang and Xing, Chunxiao and Liu, Tie Yan},
	year = {2019},
	note = {ISBN: 9781450362016},
	keywords = {out, stock price prediction, Stock Prediction, Matrix Factorization, Mutual Fund Portfolio Data},
	pages = {2376--2384},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\M3NTWRRX\\Investment Behaviors Can Tell What Inside  Exploring Stock Intrinsic Properties for Stock Trend Prediction.pdf:application/pdf},
}

@book{Tukey1977a,
	title = {Eploratory {Data} {Analysis}},
	isbn = {0-201-07616-0},
	abstract = {IQR},
	author = {Tukey, John W.},
	year = {1977},
	keywords = {stock price prediction},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VIVARIUI\\Exploratory Data Analysis by John W. Tukey (z-lib.org).pdf:application/pdf},
}

@article{Stocks,
	title = {Banka {Hisse} {Senetlerinin} ø{QGLUJHQPLú} {7HNQLN} {Göstergelerle} {Fiyat} {Tahmini}},
	author = {Stocks, Bank and Technical, Reduced},
	keywords = {regression, prediction, stock price prediction, stock price, desicion tree, multiple, technical and basic indicators},
	pages = {206--210},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FXBCLYRY\\Prediction of Bank Stocks Price with Reduced Technical Indicators.pdf:application/pdf},
}

@article{Yu2019,
	title = {{CEAM}: {A} {Novel} {Approach} {Using} {Cycle} {Embeddings} with {Attention} {Mechanism} for {Stock} {Price} {Prediction}},
	doi = {10.1109/BIGCOMP.2019.8679218},
	abstract = {This paper presents a novel deep learning approach for the stock price prediction using a cycle embeddings with attention mechanism (CEAM) applying on Dual-Stage Attention-Based RNN (DA-RNN) model. The cycle characteristic is an important factor in time series prediction problem since it affects the trend of stock price. Thus, an effective cycle information can improve the prediction performance of stock price. In past years, many researches use the cycle feature with other features together as equally important, which might dilute the weight of cycle information since the cycle information should be paid more attention when making prediction on periodic data. As the result, we use CEAM making prediction with cycle information hidden in periodic data. The deep learning-based method has been developed in many fields and is a powerful prediction system. In addition, many researches use the embeddings feature and the attention mechanism to improve the prediction performance. In this paper, we propose a novel approach to capture the cycle information and use it to predict stock prices in U.S. stock market. The cycle information can be formed as a distributed vector as embeddings, called cycle embeddings. The CEAM approach use cycle embeddings to pay attention on periodically historical time series data by learning the cycle semantic relations between cycle characteristics and historical stock prices to optimize the prediction model. Therefore, the CEAM approach can improve the prediction performance for stock price. The experiments in this paper show that our proposed CEAM approach outperforms the another model which combines cycle feature with other features together as equally important.},
	journal = {2019 IEEE International Conference on Big Data and Smart Computing, BigComp 2019 - Proceedings},
	author = {Yu, Mu Hui and Wu, Jheng Long},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781538677896},
	keywords = {time series, out, stock price prediction, attention mechanism, cycle embeddings, deep recurrent neural network},
	pages = {0--3},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PLH3HDKG\\CEAM A Novel Approach Using Cycle Embeddings with Attention Mechanism for Stock Price Prediction.pdf:application/pdf},
}

@article{Achkar2019,
	title = {Comparison of {BPA}-{MLP} and {LSTM}-{RNN} for stocks prediction},
	doi = {10.1109/ISCBI.2018.00019},
	abstract = {Neural networks is considered one of the most developed concept in artificial intelligence, due to its ability to solve complex computational tasks, and its efficiency to find solutions. There is a wide range of applications that adopt this technique, one of which is in the financial investment issues. This paper presents an approach to predict stock market ratios using artificial neural networks. It considers two different techniques-BPA-MLP and LSTM-RNN-their potential, and their limitations. Tests were conducted on different data sets, such as FacebookTM stocks, GoogleTM stocks, and BitcoinTM stocks. We achieve a best case accuracy of 97\% for MLP algorithm, and 99.5\% for LSTM algorithm. While the results appear to be promising, a web interface is presented in order to accept a certain amount of money, and accordingly checks the best stock to invest in.},
	journal = {Proceedings - 6th International Symposium on Computational and Business Intelligence, ISCBI 2018},
	author = {Achkar, Roger and Elias-Sleiman, Fady and Ezzidine, Hasan and Haidar, Nourhane},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781538694503},
	keywords = {Recurrent Neural Networks, stock price prediction, in, Long Short-Term Memory, Multi-layer Perceptron, Back propagation Algorithm},
	pages = {48--51},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\22N4IITS\\Comparison of BPA-MLP and LSTM-RNN for Stocks Prediction.pdf:application/pdf},
}

@article{Chou2018,
	title = {Forward {Forecast} of {Stock} {Price} {Using} {Sliding}-{Window} {Metaheuristic}-{Optimized} {Machine}-{Learning} {Regression}},
	volume = {14},
	issn = {15513203},
	doi = {10.1109/TII.2018.2794389},
	abstract = {Time series forecasting has been widely used to determine the future prices of stock, and the analysis and modeling of finance time series importantly guide investors' decisions and trades. In addition, in a dynamic environment such as the stock market, the nonlinearity of the time series is pronounced, immediately affecting the efficacy of stock price forecasts. Thus, this paper proposes an intelligent time series prediction system that uses sliding-window metaheuristic optimization for the purpose of predicting the stock prices of Taiwan construction companies one step ahead. It may be of great interest to home brokers who do not possess sufficient knowledge to invest in such companies. The system has a graphical user interface and functions as a stand-alone application. The developed hybrid system exhibited outstanding prediction performance and it improves overall profit for investment performance. The proposed model is a promising predictive technique for highly nonlinear time series, whose patterns are difficult to capture by traditional models.},
	number = {7},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Chou, Jui Sheng and Nguyen, Thi Kha},
	year = {2018},
	note = {Publisher: IEEE},
	keywords = {time series, data mining, machine learning (ML), stock price prediction, in, stock price forecasting, Construction company, prediction system, sliding window, swarm intelligence and metaheuristic optimization},
	pages = {3132--3142},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XRL8TPNV\\Forward Forecast of Stock Price Using Sliding-Window Metaheuristic-Optimized Machine-Learning Regression.pdf:application/pdf},
}

@article{Sinaga2019,
	title = {Stock {Trend} {Prediction} using {SV}-{kNNC} and {SOM}},
	doi = {10.1109/ICIC47613.2019.8985731},
	abstract = {In the capital market, a stockbroker often finds it difficult to decide whether we should buy or sell the stock. The lengthy time for stock price analysis and the enormous amount of stock data are the two main reasons which cause difficulty. The purpose of this research is the proposed hybrid prediction model for stock trend prediction. The prediction will let the user know whether the stock price will go up or down. It can be used to assist the stockbroker by reducing the analysis time and simplifying the analysis process. The dataset is consists of five blue-chip stocks in the Indonesian Stock Exchange. The model proposed is a modified SV-kNNC (Support Vector - k Nearest Neighbor Clustering) by replacing the K-Means clustering technique with SOM (Self Organizing Map). Our proposed model is short-term stock trend prediction and its performance will be measured by f-measure. The experiments demonstrate that SV-kNNC + SOM is fairly effective model to predict stock trend to go up or down.},
	journal = {Proceedings of 2019 4th International Conference on Informatics and Computing, ICIC 2019},
	author = {Sinaga, Frans Mikael and Jonas, Maradona and {Felix} and Halim, Arwin},
	year = {2019},
	note = {ISBN: 9781728122076},
	keywords = {stock price prediction, in, Stock prediction, SOM, SV-kNNC},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GQ287766\\Stock Trend Prediction using SV-kNNC and SOM.pdf:application/pdf},
}

@article{Li2019f,
	title = {Application of deep learning in stock market valuation index forecasting},
	volume = {2019-Octob},
	issn = {23270594},
	doi = {10.1109/ICSESS47205.2019.9040833},
	abstract = {Deep learning is the core technology of artificial intelligence, which has higher accuracy than traditional algorithms. The characteristics of high-risk and high-yield in stock market make investors hope to make predictions on it through scientific methods, so as to reduce investment risks. Long short-term memory (LSTM) model in deep learning can effectively describe the long memory of data and is suitable for predicting financial time series. Therefore, this paper uses LSTM model in deep learning to learn and forecast the stock market valuation indicator, price-earnings ratio (P/E ratio). Then the prediction bias is measured by forecast trend accuracy (FTA), average forecast deviation rate (AFDR), and root mean square error (RMSE). Empirical results show that LSTM model has a good predictive effect on P/E ratio sequence, indicating that there is practical research value for applying deep learning network algorithm to the field of stock market forecasting. At the same time, this paper also provides a reference for stock market investors.},
	journal = {Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
	author = {Li, Ge and Xiao, Ming and Guo, Ying},
	year = {2019},
	note = {ISBN: 9781728109459},
	keywords = {Deep learning, out, stock price prediction, Financial time series, Long short-term memory, Price earnings ratio, Stock market forecast},
	pages = {551--554},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RL3TT9G2\\Application of Deep Learning in Stock Market Valuation Index Forecasting.pdf:application/pdf},
}

@article{NizarYogaPratama2019,
	title = {Multilayer {Perceptron} and {Long} {Short}-{Term} {Memory} for {Predicting} {Indonesian} {Composite} {Stock} {Price} {Index} {Using} {Macroeconomic} {Factors}},
	doi = {10.1109/ICIC47613.2019.8985785},
	abstract = {Capital markets are complex and dynamic, so they have risks and uncertainties. The main activity in the capital market is investment. There are several instruments for investing, namely gold, land, savings, deposits, bonds and stocks. Investors need a variety of information to help determine the right momentum to invest, one of which is the stock price in the future. By making predictions, investors can minimize the risk of loss in investing. The purpose of this study is to compare the Multilayer Perceptron and Long Short-Term Memory algorithm in predicting Indonesian Composite Stock Price Index in the future based on macroeconomic factors. This study uses Dow Jones Industrial Average, Shanghai Stock Exchange, world gold prices, world oil prices, USD to IDR exchange rate, inflation, BI Rate, and money supply as macroeconomic factors to predict the price of the Indonesian Composite Stock Price Index. The time series used for the research data is monthly, from January 1991 to December 2018. Based on the results of the evaluations that have been conducted, this study found that modeling using the Multilayer Perceptron algorithm has a better performance than Long Short-Term Memory. Evaluation using Root Mean Squared Error and R-squared each produces a value 86.86\% and 3.19\% for Multilayer Perceptron; 74.24\% and 3.94\% for Long Short- Term Memory.},
	journal = {Proceedings of 2019 4th International Conference on Informatics and Computing, ICIC 2019},
	author = {Nizar Yoga Pratama, Muhammad and {Ernastuti}},
	year = {2019},
	note = {ISBN: 9781728122076},
	keywords = {stock price prediction, in, Long Short-Term Memory, Stock Price Prediction, Multilayer Perceptron, Indonesian Composite Stock Price Index, Macroeconomic Factor},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CG35YSHG\\Multilayer Perceptron and Long Short-Term Memory for Predicting Indonesian Composite Stock Price Index Using Macroeconomic Factors.pdf:application/pdf},
}

@article{Xuan2020,
	title = {Prediction of {Short}-term {Stock} {Prices} {Based} on {EMD}-{LSTM}-{CSI} {Neural} {Network} {Method}},
	doi = {10.1109/ICBDA49040.2020.9101194},
	abstract = {In order to improve the accuracy and efficiency of short-term stock price trend prediction, a new prediction model based on Empirical Mode Decomposition, Long Short-Term Memory neural network and Cubic Spline Interpolation (EMD-LSTM-CSI) was proposed. Firstly, the stock price data series are decomposed into several Intrinsic Mode Functions (IMF) and a Residual component (RES) with different time scales. Then, in order to avoid over fitting, components are divided into two categories according to their gradient order of magnitude, and the larger one is recorded as the first category. The LSTM model is built for each component in the first category, and the CSI model is built for each component in the other category. Finally, the final prediction results are obtained by summing up the prediction series of each component. Compared with LSTM model, EMD-LSTM model and SVM model, the EMD-LSTM-CSI model can achieve better accuracy, which provides a more credible reference for investors.},
	journal = {2020 5th IEEE International Conference on Big Data Analytics, ICBDA 2020},
	author = {Xuan, Yuze and Yu, Yue and Wu, Kaisu},
	year = {2020},
	note = {ISBN: 9781728141114},
	keywords = {stock price prediction, in, Stock price prediction, Cubic Spline Interpolation, Empirical Mode Decomposition, Long Short-Term Memory neural network},
	pages = {135--139},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HHXLK92A\\Prediction of Short-term Stock Prices Based on EMD-LSTM-CSI Neural Network Method.pdf:application/pdf},
}

@article{Wang2019a,
	title = {High and {Low} {Prices} {Prediction} of {Soybean} {Futures} with {LSTM} {Neural} {Network}},
	volume = {2018-Novem},
	issn = {23270594},
	doi = {10.1109/ICSESS.2018.8663896},
	abstract = {The prediction of futures prices is a great challenge. On the other hand, it can bring investors great profits. Most researches just show the predictions of closing prices but we can also predict high and low prices. The high and low prices have lower noises than closing prices, making it easier to predict them and to use them for making profitable strategies. In this paper, we build a model to predict high and low prices of soybean futures with the LSTM neural network using the dataset from the Dalian Commodity Exchange. Then we use mean absolute error (MAE) and trend accuracy to evaluate the performance of this model. For comparison, we predict the closing price using the LSTM neural network and build another prediction model based on the BP neural network. Results show that we get higher accuracy predicting the trends of high and low prices. Also, the prediction model based on the LSTM neural network performs better and it gets more than 80\% of the accuracy in trend estimation when the predicting high prices or low prices have high volatilities.},
	number = {1},
	journal = {Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
	author = {Wang, Chenhao and Gao, Qiang},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781538665640},
	keywords = {out, stock price prediction, LSTM neural network, high prices, low prices, price prediction, soybean futures},
	pages = {140--143},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QV8XI582\\High and Low Prices Prediction of Soybean Futures with LSTM Neural Network.pdf:application/pdf},
}

@article{Windasari2018,
	title = {Indonesia stock exchange securities buy/sell signal detection using bollinger bands and williams percent range},
	doi = {10.1109/ISRITI.2018.8864452},
	abstract = {For stock investor, it is important to be able to predict future stock prices to find out the possible gains or losses obtained. In the world of stock investment, stock price prediction is applied with two approaches, namely analysis and fundamental analysis. This detection application system was developed using a technical analysis approach.Technical analysis is a method used to observe price fluctuations in a certain time span by utilizing historical data using indicators. Bollinger Bands and Williams Percent Range are indicator used in this study to provide information on the movement of a stock by observing a particular pattern of patterns used as a basis for buying or selling. The data used in this study is six companies whose shares are listed as LQ45 in Indonesia Stock Exchanges. The results show that the combination of indicators over a period of one year can provide the greatest benefit, namely by 80.20\% in Aneka Tambang shares and the average performance of a total of 6 shares is 10.72\% using strategy 1, an Exit Bollinger with confirmation by Williams Percent Range.},
	journal = {2018 International Seminar on Research of Information Technology and Intelligent Systems, ISRITI 2018},
	author = {Windasari, Ike Pertiwi and Prasetijo, Agung Budi and Pangabean, Reza Pahlevi},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538674222},
	keywords = {stock price prediction, Bollinger Bands, Indonesia Stock Exchange, LQ45, William Percent Range},
	pages = {633--636},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NFF2GFQQ\\Indonesia Stock Exchange Securities Buy- Sell Signal Detection using Bollinger Bands and Williams Percent Range.pdf:application/pdf},
}

@article{Otsuka2018,
	title = {Effectiveness of {Momentum} {Indicators} to {Improve} {Accuracy} of {Stock} {Price} {Prediction} for {Large}-{Captial} {Stocks}},
	doi = {10.1109/IIAI-AAI.2018.00152},
	abstract = {This paper considers the effectiveness of momentum indicators through the prediction of stock price for the large-capital stocks. In recent years, FinTech, which is the integration of finance and technology, has been recently expanding. One of the hot topics in FinTech is whether or not the momentum indications give an effective impact for the financial market. This paper focuses the large-capital stocks in Japanese financial market, particularly two specific stocks. Using real market data, the effectiveness of several momentum factors is discussed multidirectionally.},
	journal = {Proceedings - 2018 7th International Congress on Advanced Applied Informatics, IIAI-AAI 2018},
	author = {Otsuka, Yuto and Hasuike, Takashi},
	year = {2018},
	note = {ISBN: 9781538674475},
	keywords = {out, stock price prediction, Stock price prediction, Factor model, Large-capital stock, Momentum effect},
	pages = {735--740},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TRVRVCZ4\\Effectiveness of Momentum Indicators to Improve Accuracy of Stock Price Prediction for Large-Captial Stocks.pdf:application/pdf},
}

@article{Vwrfn2019,
	title = {{QHXUDO} {QHWZRUN}},
	number = {Itnec},
	author = {Vwrfn, Ruhfdvwlqj and Lq, Sulfhv and Zd, W Z R and Rq, V Edvhg and Frp, T T and Tt, F R P and Tt, F R P and Lqirupdwlrq, Surfhvvlqj and Wkh, R I and Wlph, Fxuuhqw and Wkh, R I and Wkh, R Q and Vlgh, Ohiw and Ljxuh, R I and Wkh, V Uhsuhvhqwv and Ri, Phpru},
	year = {2019},
	note = {ISBN: 9781538662434},
	keywords = {stock price prediction, in},
	pages = {1083--1086},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VNXIJCBM\\Forecasting stock prices in two ways based on LSTM neural network.pdf:application/pdf},
}

@article{Peng2019a,
	title = {Analysis of investor sentiment and stock market volatility trend based on big data strategy},
	doi = {10.1109/ICRIS.2019.00077},
	abstract = {This paper mainly studies the specific mechanism of investor sentiment affecting stock market volatility. With the help of Pollet and Wilson's theory of volatility decomposition, it performs a comparative analysis based on big data strategy and sources. This paper collects the data of web news emotion index, web search volume, social network emotion index, social network heat index, and establishes corresponding analysis index. After correlation analysis and Granger causality tests, it extracts the indicators which have significant correlation with the financial market and brings them into forecasting analysis. The model constructs market volatility index and analyzes the correlation between investor sentiment and stock price changes. In empirical study, the deviation between stock price and value is introduced as an explanatory variable, and the logarithmic return of stock is used to measure the volatility of stock price. It is found that the stock market volatility index compounded by the stock market sentiment index has a strong predictive ability for the stock market volatility turning point in the larger turbulent situation, especially for the one to two day decline turning point ahead of schedule, and it has a strong practical role for the stock market volatility prediction, as well as for financial market risk aversion.},
	journal = {Proceedings - 2019 International Conference on Robots and Intelligent System, ICRIS 2019},
	author = {Peng, Du},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728126326},
	keywords = {Big data, out, stock price prediction, Stock market, Investor sentiment, iVIX},
	pages = {269--272},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N4SZ2C7P\\Analysis of Investor Sentiment and Stock Market Volatility Trend Based on Big Data Strategy.pdf:application/pdf},
}

@article{Prastyo2017,
	title = {Stock {Price} {Forecasting} {Using} {Artificial} {Neural} {Network}},
	volume = {0},
	abstract = {Investment in stocks is one of the best alternatives for investing the assets, because with a stake of exposure to risk of inflation is smaller when compared to the savings. But the problem is the difficulty prospective shareholders in stock options because they do not know the stock price predictions for the future. This resulted in the growing level of loss if there are errors in determining the decisions taken in regard to these shares. To solve these problems, required future stock price prediction by using the method of forecasting. Forecasting is done by using Artificial Neural Network (ANN) and for the training, Backpropagation algorithm is used. In this study, stock price prediction using neural network with backpropagation algorithm. ANN is used due to have the ability to perform activities based on past data, where the data of the past will be studied so as to have the ability to give a decision on the data that has never been studied. By using Backpropagation algorithms, network architectures are trained to get the best architecture. After the training, the best architecture that is obtained is 8: 9: 1. Then, the test carried out on test data using the best network architecture and found that the mean squared error (MSE) is equal to 0.1830},
	number = {c},
	journal = {Fifth International Conference on Information and Communication Technology (ICoICT)},
	author = {Prastyo, Adetya and Junaedi, Danang and Sulistiyo, Mahmud Dwi},
	year = {2017},
	note = {ISBN: 9781509049110},
	keywords = {Forecasting, Backpropagation, stock price prediction, Artificial Neural Network, Stock},
	pages = {191--196},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\26GCPKGE\\Stock price forecasting using artificial neural network -Case Study PT. Telkom Indonesia-.pdf:application/pdf},
}

@article{Vijh2020,
	title = {Stock {Closing} {Price} {Prediction} using {Machine} {Learning} {Techniques}},
	volume = {167},
	issn = {18770509},
	doi = {10.1016/j.procs.2020.03.326},
	abstract = {Accurate prediction of stock market returns is a very challenging task due to volatile and non-linear nature of the financial stock markets. With the introduction of artificial intelligence and increased computational capabilities, programmed methods of prediction have proved to be more efficient in predicting stock prices. In this work, Artificial Neural Network and Random Forest techniques have been utilized for predicting the next day closing price for five companies belonging to different sectors of operation. The financial data: Open, High, Low and Close prices of stock are used for creating new variables which are used as inputs to the model. The models are evaluated using standard strategic indicators: RMSE and MAPE. The low values of these two indicators show that the models are efficient in predicting stock closing price.},
	number = {Iciss},
	journal = {Procedia Computer Science},
	author = {Vijh, Mehar and Chandola, Deeksha and Tikkiwal, Vinay Anand and Kumar, Arun},
	year = {2020},
	note = {Publisher: IEEE
ISBN: 9781538677995},
	keywords = {out, stock price prediction, Artificial Neural Network, Stock market prediction, Random Forest Regression},
	pages = {599--606},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\II9JGELN\\Stock Price Prediction Using Machine Learning Techniques.pdf:application/pdf},
}

@article{Landis2019,
	title = {On {Optimization} of {Stock} {Market} {Prediction} {Methods}},
	doi = {10.1109/BigData47090.2019.9005612},
	abstract = {In the modern-day stock markets, accuracy and timeliness of price predictive methods can be key in making a profit and edging out the competition. Stockbrokers utilize predictive systems based on these methods that rely on a plethora of data to assist in decision making. To work towards developing our own system that will eventually utilize big data, we performed research with recurrent neural networks (RNNs) and support vector machines (SVMs) to derive a backing method for the predictive system. In this paper we discuss the experimentation we have performed, and the proposed approach derived from the results of the experimentation.},
	journal = {Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019},
	author = {Landis, Warren and Cha, Sangwhan and Shaalan, Majid},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728108582},
	keywords = {Neural Networks, out, stock price prediction, Stock Market Prediction, Big Data, System Assisted Trading},
	pages = {6116--6118},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6CYRUHMV\\On Optimization of Stock Market Prediction Methods.pdf:application/pdf},
}

@article{Gupta2018,
	title = {Stock {Prediction} {Using} {Functional} {Link} {Artificial} {Neural} {Network} ({FLANN})},
	doi = {10.1109/CINE.2017.25},
	abstract = {Stock exchange that is, buying and selling of stock is considered to be an important factor in the economy sector. The Stockbrokers typically use time series or technical analysis in predicting the stock price. These techniques are based on trends and not the actual stock value. Therefore a method of prediction which takes into account the historical values of stock is desired. Neural Networks once again have become famous for prediction of stock. This is due to their ability to deal with non-linear data. The use of Artificial Neural Networks to for predicting the stock prices is proposed in this paper. The input features to the model sometimes can be non-related to the output. Hence, Functional Link Artificial Neural Networks is used here to increase the number of related features in the form of inputs. The data is taken from NSE and is converted into a suitable form for FLANN and then prediction is carried out using Multi-layer feed forward Perceptron model.},
	journal = {Proceedings - 2017 International Conference on Computational Intelligence and Networks, CINE 2017},
	author = {Gupta, Abhinandan and Chaudhary, Dev Kumar and Choudhury, Tanupriya},
	year = {2018},
	note = {ISBN: 9781538625293},
	keywords = {stock price prediction, Stock Market, ANN, FLANN, MLP},
	pages = {10--16},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HETY5UHR\\Stock Prediction Using Functional Link Artificial Neural Network -FLANN-.pdf:application/pdf},
}

@article{Liu2013,
	title = {Ensuring connectivity via data plane mechanisms},
	abstract = {We typically think of network architectures as having two basic components: a data plane responsible for forwarding packets at line-speed, and a control plane that instantiates the forwarding state the data plane needs. With this separation of concerns, ensuring connectivity is the responsibility of the control plane. However, the control plane typically operates at timescales several orders of magnitude slower than the data plane, which means that failure recovery will always be slow compared to data plane forwarding rates. In this paper we propose moving the responsibility for connectivity to the data plane. Our design, called Data-Driven Connectivity (DDC) ensures routing connectivity via data plane mechanisms. We believe this new separation of concerns - basic connectivity on the data plane, optimal paths on the control plane - will allow networks to provide a much higher degree of availability, while still providing flexible routing control.},
	journal = {Proceedings of the 10th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2013},
	author = {Liu, Junda and Panda, Aurojit and Singla, Ankit and Godfrey, Brighten and Schapira, Michael and Shenker, Scott},
	year = {2013},
	note = {ISBN: 9781931971003},
	keywords = {cloud},
	pages = {113--126},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S2AL6R5I\\Ensuring Connectivity via Data Plane Mechanisms.pdf:application/pdf},
}

@article{Guo2020,
	title = {{DeText}: {A} {Deep} {Text} {Ranking} {Framework} with {BERT}},
	doi = {10.1145/3340531.3412699},
	abstract = {Ranking is the most important component in a search system. Most search systems deal with large amounts of natural language data, hence an effective ranking system requires a deep understanding of text semantics. Recently, deep learning based natural language processing (deep NLP) models have generated promising results on ranking systems. BERT is one of the most successful models that learn contextual embedding, which has been applied to capture complex query-document relations for search ranking. However, this is generally done by exhaustively interacting each query word with each document word, which is inefficient for online serving in search product systems. In this paper, we investigate how to build an efficient BERT-based ranking model for industry use cases. The solution is further extended to a general ranking framework, DeText, that is open sourced and can be applied to various ranking productions. Offline and online experiments of DeText on three real-world search systems present significant improvement over state-of-the-art approaches.},
	journal = {International Conference on Information and Knowledge Management, Proceedings},
	author = {Guo, Weiwei and Liu, Xiaowei and Wang, Sida and Gao, Huiji and Sankar, Ananth and Yang, Zimeng and Guo, Qi and Zhang, Liang and Long, Bo and Chen, Bee Chung and Agarwal, Deepak},
	year = {2020},
	note = {arXiv: 2008.02460
ISBN: 9781450368599},
	keywords = {natural language processing, deep language models, ranking},
	pages = {2509--2516},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CMIWBEPQ\\DeText\; A Deep Text Ranking Framework with BERT.pdf:application/pdf},
}

@article{zotero-3653,
	title = {F {I} {V} {E} {C} {O} {M} {M} {O} {N} {D} a {T} a {C} {H} a {L} {L} {E} {N} {G} {E} {S}},
	url = {https://www.forbes.com/sites/louiscolumbus/2017/12/24/53-of-companies-are-adopting-big-data-analytics/#2619693f39a1},
}

@article{Svore2009,
	title = {A machine learning approach for improved {BM25} retrieval},
	doi = {10.1145/1645953.1646237},
	abstract = {Despite the widespread use of BM25, there have been few studies examining its effectiveness on a document description over single and multiple field combinations. We determine the effectiveness of BM25 on various document fields. We find that BM25 models relevance on popularity fields such as anchor text and query click information no better than a linear function of the field attributes. We also find query click information to be the single most important field for retrieval. In response, we develop a machine learning approach to BM25-style retrieval that learns, using LambdaRank, from the input attributes of BM25. Our model significantly improves retrieval effectiveness over BM25 and BM25F. Our data-driven approach is fast, effective, avoids the problem of parameter tuning, and can directly optimize for several common information retrieval measures. We demonstrate the advantages of our model on a very large real-world Web data collection. Copyright 2009 ACM.},
	journal = {International Conference on Information and Knowledge Management, Proceedings},
	author = {Svore, Krysta M. and Burges, Christopher J.C.},
	year = {2009},
	note = {ISBN: 9781605585123},
	keywords = {BM25, Learning to rank, Retrieval models, Web search},
	pages = {1811--1814},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Q7G8HDVS\\A Machine Learning Approach for Improved BM25 Retrieval.pdf:application/pdf},
}

@article{Cao2006,
	title = {Adapting ranking {SVM} to document retrieval},
	volume = {2006},
	doi = {10.1145/1148170.1148205},
	abstract = {The paper is concerned with applying learning to rank to document retrieval. Ranking SVM is a typical method of learning to rank. We point out that there are two factors one must consider when applying Ranking SVM, in general a "learning to rank" method, to document retrieval. First, correctly ranking documents on the top of the result list is crucial for an Information Retrieval system. One must conduct training in a way that such ranked results are accurate. Second, the number of relevant documents can vary from query to query. One must avoid training a model biased toward queries with a large number of relevant documents. Previously, when existing methods that include Ranking SVM were applied to document retrieval, none of the two factors was taken into consideration. We show it is possible to make modifications in conventional Ranking SVM, so it can be better used for document retrieval. Specifically, we modify the "Hinge Loss" function in Ranking SVM to deal with the problems described above. We employ two methods to conduct optimization on the loss function: gradient descent and quadratic programming. Experimental results show that our method, referred to as Ranking SVM for IR, can outperform the conventional Ranking SVM and other existing methods for document retrieval on two datasets. Copyright 2006 ACM.},
	number = {49},
	journal = {Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
	author = {Cao, Yunbo and Xu, Jun and Liu, Tie Yan and Li, Hang and Huang, Yalou and Hon, Hsiao Wuen},
	year = {2006},
	note = {ISBN: 1595933697},
	keywords = {Information retrieval, Loss function, Ranking SVM},
	pages = {186--193},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7PIV2SUE\\Adapting ranking SVM to document retrieval.pdf:application/pdf},
}

@article{Bruch2020,
	title = {A stochastic treatment of learning to rank scoring functions},
	doi = {10.1145/3336191.3371844},
	abstract = {Learning to Rank, a central problem in information retrieval, is a class of machine learning algorithms that formulate ranking as an optimization task. The objective is to learn a function that produces an ordering of a set of documents in such a way that the utility of the entire ordered list is maximized. Learning-to-rank methods do so by learning a function that computes a score for each document in the set. A ranked list is then compiled by sorting documents according to their scores. While such a deterministic mapping of scores to permutations makes sense during inference where stability of ranked lists is required, we argue that its greedy nature during training leads to less robust models. This is particularly problematic when the loss function under optimization—in agreement with ranking metrics—largely penalizes incorrect rankings and does not take into account the distribution of raw scores. In this work, we present a stochastic framework where, instead of a deterministic derivation of permutations from raw scores, permutations are sampled from a distribution defined by raw scores. Our proposed sampling method is differentiable and works well with gradient descent optimizers. We analytically study our proposed method and demonstrate when and why it leads to model robustness. We also show empirically, through experiments on publicly available learning-to-rank datasets, that the application of our proposed method to a class of ranking loss functions leads to significant model quality improvements.},
	journal = {WSDM 2020 - Proceedings of the 13th International Conference on Web Search and Data Mining},
	author = {Bruch, Sebastian and Han, Shuguang and Bendersky, Michael and Najork, Marc},
	year = {2020},
	note = {ISBN: 9781450368223},
	keywords = {Learning to rank, Ranking metric optimization, Stochastic scoring functions for learning to rank},
	pages = {61--69},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WXTS4V74\\A Stochastic Treatment of Learning to Rank Scoring Functions.pdf:application/pdf},
}

@article{practices10BestPractices2019,
	title = {10 {Best} {Practices} in {Data} {Preparation}},
	author = {Practices, Best and Preparation, Data},
	year = {2019},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4GRLEWKZ\\What is Data Preparation_ 10 Best Practices in Data Preparation _ Import.io.pdf:application/pdf},
}

@book{Malley2016,
	title = {Data pre-processing},
	isbn = {978-3-319-43742-2},
	author = {Malley, Brian and Ramazzotti, Daniele and Wu, Joy Tzung yu},
	year = {2016},
	doi = {10.1007/978-3-319-43742-2_12},
	note = {Publication Title: Secondary Analysis of Electronic Health Records},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\M5D9UMUY\\2016_Book_SecondaryAnalysisOfElectronicH.pdf:application/pdf},
}

@article{Zhu2009,
	title = {A general magnitude-preserving boosting algorithm for search ranking},
	doi = {10.1145/1645953.1646057},
	abstract = {Traditional boosting algorithms for the ranking problems usually employ the pairwise approach and convert the document rating preference into a binary-value label, like RankBoost. However, such a pairwise approach ignores the information about the magnitude of preference in the learning process. In this paper, we present the directed distance function (DDF) as a substitute for binary labels in pairwise approach to preserve the magnitude of preference and propose a new boosting algorithm called MPBoost, which applies GentleBoost optimization and directly incorporates DDF into the exponential loss function. We give the boundedness property of MPBoost through theoretic analysis. Experimental results demonstrate that MPBoost not only leads to better NDCG accuracy as compared to state-of-the-art ranking solutions in both public and commercial datasets, but also has good properties of avoiding the overfitting problem in the task of learning ranking functions. Copyright 2009 ACM.},
	journal = {International Conference on Information and Knowledge Management, Proceedings},
	author = {Zhu, Chenguang and Chen, Weizhu and Zhu, Zeyuan Allen and Wang, Gang and Wang, Dong and Chen, Zheng},
	year = {2009},
	note = {ISBN: 9781605585123},
	keywords = {Directed distance function (ddf), Magnitude-preserving, Pairwise-preference},
	pages = {817--825},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7IA3PZNM\\A General Magnitude-Preserving Boosting Algorithm for Search Ranking.pdf:application/pdf},
}

@article{Mei,
	title = {Plsa-{Note}.{Pdf}},
	number = {2},
	author = {Mei, Qiaozhu and Zhai, Chengxiang},
	pages = {2--5},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HUNEGIE8\\plsa-note.pdf:application/pdf},
}

@book{Lafore,
	title = {Object-{Oriented} {Programming} in {C} ++ , {Fourth} {Edition}},
	isbn = {0-672-32308-7},
	abstract = {This book teaches you how to write programs in a the C++ programming language. However, it does more than that. In the past few years, several major innovations in software develop- ment have appeared on the scene. This book teaches C++ in the context of these new develop- ments. Lets see what they are.},
	author = {Lafore, Robert},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XYNTK2RQ\\ObjectOrientedProgramminginC4thEdition.pdf:application/pdf},
}

@article{HowSetCI,
	title = {How {To} {Set} {Up} {CI} / {CD} {Workflows} {In} {GitLab}},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TK2FSKDZ\\How+to+set+up+CI_CD+workflows+in+GitLab+and+GitHub.pdf:application/pdf},
}

@article{amazonHowAutomaticallyBackup,
	title = {How to automatically backup {MYSQL} to {Amazon} {S3} ? {Why} {Amazon} {S3} ?},
	author = {Amazon, Why},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4D775QXH\\How to automatically backup MYSQL to Amazon S3_.pdf:application/pdf},
}

@article{Deveaud2019,
	title = {Learning to {Adaptively} {Rank} {Document} {Retrieval} {System} {Configurations} {To} cite this version : {HAL} {Id} : hal-02092955},
	author = {Deveaud, Romain and Mothe, Josiane and Ullah, Zia and Nie, Jian-yun and Deveaud, Romain and Mothe, Josiane and Ullah, Zia and Learning, Jian-yun Nie},
	year = {2019},
	keywords = {"Information systems -  nformation retrieval - Lea},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\46GJEB33\\Learning to Adaptively Rank Document Retrieval System Configurations.pdf:application/pdf},
}

@article{Deveaud2016,
	title = {Learning to rank system configurations},
	volume = {24-28-Octo},
	doi = {10.1145/2983323.2983894},
	abstract = {Information Retrieval (IR) systems heavily rely on a large number of parameters, such as the retrieval model or various query expansion parameters, whose values greatly influence the overall retrieval effectiveness. However, setting all these parameters individually can often be a tedious task, since they can all affect one another, while also vary for different queries. We propose to tackle this problem by dealing with entire system configurations (i.e. a set of parameters representing an IR system) instead of single parameters, and to apply state-of-the-art Learning to Rank techniques to select the most appropriate configuration for a given query. The experiments we conducted on two TREC AdHoc collections show that this approach is feasible and significantly outperforms the traditional way to configure a system, as well as the top performing systems of the TREC tracks. We also show an analysis on the impact of different features on the model's learning capability.},
	journal = {International Conference on Information and Knowledge Management, Proceedings},
	author = {Deveaud, Romain and Mothe, Josiane and Nie, Jian Yun},
	year = {2016},
	note = {ISBN: 9781450340731},
	keywords = {Information Retrieval, Learning to rank, Retrieval system parameters},
	pages = {2001--2004},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JHJF2Y3K\\Learning to Rank System Configurations.pdf:application/pdf},
}

@article{Pang2019,
	title = {{SetRank}: {Learning} a permutation-invariant ranking model for information retrieval},
	abstract = {In learning-to-rank for information retrieval, a ranking model is automatically learned from the data and then utilized to rank the sets of retrieved documents. Therefore, an ideal ranking model would be a mapping from a document set to a permutation on the set, and should satisfy two critical requirements: (1) it should have the ability to model cross-document interactions so as to capture local context information in a query; (2) it should be permutation-invariant, which means that any permutation of the inputted documents would not change the output ranking. Previous studies on learning-to-rank either design uni-variate scoring functions that score each document separately, and thus failed to model the cross-document interactions; or construct multivariate scoring functions that score documents sequentially, which inevitably sacrifice the permutation invariance requirement. In this paper, we propose a neural learning-to-rank model called SetRank which directly learns a permutation-invariant ranking model defined on document sets of any size. SetRank employs a stack of (induced) multi-head self attention blocks as its key component for learning the embeddings for all of the retrieved documents jointly. The self-attention mechanism not only helps SetRank to capture the local context information from cross-document interactions, but also to learn permutation-equivariant representations for the inputted documents, which therefore achieving a permutation-invariant ranking model. Experimental results on three large scale benchmarks showed that the SetRank significantly outperformed the baselines include the traditional learning-to-rank models and state-of-the-art Neural IR models.},
	journal = {arXiv},
	author = {Pang, Liang and Xu, Jun and Ai, Qingyao and Lan, Yanyan and Cheng, Xueqi and Wen, Jirong},
	year = {2019},
	note = {ISBN: 9781450380164},
	keywords = {learning to rank, permutation-invariant ranking model},
	pages = {499--508},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7IWVVFDK\\SetRank - Learning a Permutation-Invariant Ranking Model for Information Retrieval.pdf:application/pdf},
}

@article{Sanderson2010,
	title = {Test collection based evaluation of information retrieval systems},
	volume = {4},
	issn = {15540669},
	doi = {10.1561/1500000009},
	abstract = {Use of test collections and evaluation measures to assess the effectiveness of information retrieval systems has its origins in work dating back to the early 1950s. Across the nearly 60 years since that work started, use of test collections is a de facto standard of evaluation. This monograph surveys the research conducted and explains the methods and measures devised for evaluation of retrieval systems, including a detailed look at the use of statistical significance testing in retrieval experimentation. This monograph reviews more recent examinations of the validity of the test collection approach and evaluation measures as well as outlining trends in current research exploiting query logs and live labs. At its core, the modern-day test collection is little different from the structures that the pioneering researchers in the 1950s and 1960s conceived of. This tutorial and review shows that despite its age, this long-standing evaluation method is still a highly valued tool for retrieval research. © 2010 M. Sanderson.},
	number = {4},
	journal = {Foundations and Trends in Information Retrieval},
	author = {Sanderson, Mark},
	year = {2010},
	pages = {247--375},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\A2ZVJMLP\\Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval 4.pdf:application/pdf},
}

@article{Amati2002,
	title = {Probabilistic {Models} of {Information} {Retrieval} {Based} on {Measuring} the {Divergence} from {Randomness}},
	volume = {20},
	issn = {15582868},
	doi = {10.1145/582415.582416},
	abstract = {We introduce and create a framework for deriving probabilistic models of Information Retrieval. The models are nonparametric models of IR obtained in the language model approach. We derive term-weighting models by measuring the divergence of the actual term distribution from that obtained under a random process. Among the random processes we study the binomial distribution and Bose-Einstein statistics.We define two types of term frequency normalization for tuning term weights in the document-query matching process. The first normalization assumes that documents have the same length and measures the information gain with the observed term once it has been accepted as a good descriptor of the observed document. The second normalization is related to the document length and to other statistics. These two normalization methods are applied to the basic models in succession to obtain weighting formulae. Results show that our framework produces different nonparametric models forming baseline alternatives to the standard tf-idf model. © 2002, ACM. All rights reserved.},
	number = {4},
	journal = {ACM Transactions on Information Systems},
	author = {Amati, Gianni and Van Rijsbergen, Cornelis Joost},
	year = {2002},
	keywords = {Algorithms, information retrieval, BM25, Aftereffect model, binomial law, Bose-Einstein statistics, document length normalization, eliteness, Experimentation, idf, Laplace, Poisson, probabilistic models, randomness, succession law, term frequency normalization, term weighting},
	pages = {357--389},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BQ5IBQGN\\Probabilistic models of information retrieval based on measuring the divergence from randomness.pdf:application/pdf},
}

@article{AltexSoft2017,
	title = {Preparing {Your} {Dataset} for {Machine} {Learning} : 8 {Basic} {Techniques} {That} {Make} {Your} {Data} {Better} {Dataset} preparation is sometimes a {DIY} project},
	author = {{AltexSoft}},
	year = {2017},
	pages = {1--8},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PW6A247C\\Preparing Your Dataset for Machine Learning_ 8 Steps _ AltexSoft.pdf:application/pdf},
}

@article{Bickel2006,
	title = {Regularization in statistics},
	volume = {15},
	issn = {11330686},
	doi = {10.1007/BF02607055},
	abstract = {This paper is a selective review of the regularization methods scattered in statistics literature. We introduce a general conceptual approach to regularization and fit most existing methods into it. We have tried to focus on the importance of regularization when dealing with today's high-dimensional objects: data and models. A wide range of examples are discussed, including nonparametric regression, boosting, covariance matrix estimation, principal component estimation, subsampling.},
	number = {2},
	journal = {Test},
	author = {Bickel, Peter J. and Li, Bo},
	year = {2006},
	keywords = {Model selection, Boosting, Regularization, Bootstrap, Linear regression, Covariance matrix, Nonparametric regression, Principal component, Subsampling},
	pages = {271--344},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TFID2P7U\\Regularization in Statistics.pdf:application/pdf},
}

@article{Dai2020,
	title = {Context-{Aware} {Document} {Term} {Weighting} for {Ad}-{Hoc} {Search}},
	volume = {2},
	doi = {10.1145/3366423.3380258},
	abstract = {Bag-of-words document representations play a fundamental role in modern search engines, but their power is limited by the shallow frequency-based term weighting scheme. This paper proposes HDCT, a context-aware document term weighting framework for document indexing and retrieval. It first estimates the semantic importance of a term in the context of each passage. These fine-grained term weights are then aggregated into a document-level bag-of-words representation, which can be stored into a standard inverted index for efficient retrieval. This paper also proposes two approaches that enable training HDCT without relevance labels. Experiments show that an index using HDCT weights significantly improved the retrieval accuracy compared to typical term-frequency and state-of-the-art embedding-based indexes.},
	journal = {The Web Conference 2020 - Proceedings of the World Wide Web Conference, WWW 2020},
	author = {Dai, Zhuyun and Callan, Jamie},
	year = {2020},
	note = {ISBN: 9781450370233},
	keywords = {Document Representation, Neural IR, Term Weighting},
	pages = {1897--1907},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XT3NGD34\\Context-Aware Document Term Weighting for Ad-Hoc Search.pdf:application/pdf},
}

@article{Yilmaz2020,
	title = {Applying {BERT} to document retrieval with birch},
	doi = {10.18653/v1/d19-3004},
	abstract = {We present Birch, a system that applies BERT to document retrieval via integration with the open-source Anserini information retrieval toolkit to demonstrate end-to-end search over large document collections. Birch implements simple ranking models that achieve state-of-the-art effectiveness on standard TREC newswire and social media test collections. This demonstration focuses on technical challenges in the integration of NLP and IR capabilities, along with the design rationale behind our approach to tightly-coupled integration between Python (to support neural networks) and the Java Virtual Machine (to support document retrieval using the open-source Lucene search library). We demonstrate integration of Birch with an existing search interface as well as interactive notebooks that highlight its capabilities in an easy-to-understand manner.},
	journal = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Proceedings of System Demonstrations},
	author = {Yilmaz, Zeynep Akkalyoncu and Wang, Shengjin and Yang, Wei and Zhang, Haotian and Lin, Jimmy},
	year = {2020},
	note = {ISBN: 9781950737925},
	pages = {19--24},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7LDJ8TGE\\Applying BERT to Document Retrieval with Birch.pdf:application/pdf},
}

@article{alizadehDataCenterTCP2010,
	title = {Data {Center} {TCP} ({DCTCP})},
	volume = {40},
	issn = {01464833},
	doi = {10.1145/1851275.1851192},
	abstract = {Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP's demands on the limited buffer space available in data center switches. For example, bandwidth hungry "background" flows build up queues at the switches, and thus impact the performance of latency sensitive "foreground" traffic. To address these problems, we propose DCTCP, a TCP-like protocol for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to the end hosts. We evaluate DCTCP at 1 and 10Gbps speeds using commodity, shallow buffered switches. We find DCTCP delivers the same or better throughput than TCP, while using 90\% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems. Copyright 2010 ACM.},
	number = {4},
	journal = {Computer Communication Review},
	author = {Alizadeh, Mohammad and Greenberg, Albert and Maltz, David A. and Padhye, Jitendra and Patel, Parveen and Prabhakar, Balaji and Sengupta, Sudipta and Sridharan, Murari},
	year = {2010},
	note = {tex.ids: alizadehDataCenterTCP2010
ISBN: 9781450302012},
	keywords = {Data center network, ECN, TCP},
	pages = {63--74},
	file = {Data Center TCP (DCTCP) - Literature Note.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Data Center TCP (DCTCP) - Literature Note.md:text/plain;Data Center TCP (DCTCP)-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Data Center TCP (DCTCP)-metadata.md:text/plain;Data Center TCP (DCTCP)-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Data Center TCP (DCTCP)-metadata.md:text/plain;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JEPNGZYG\\Data Center TCP (DCTCP).pdf:application/pdf},
}

@article{Barapatre2017,
	title = {Data preparation on large datasets for data science},
	volume = {10},
	issn = {24553891},
	doi = {10.22159/ajpcr.2017.v10s1.20526},
	abstract = {According to interviews and experts, data scientists spend 50-80\% of the valuable time in the mundane task of collecting and preparing structured or unstructured data, before it can be explored for useful analysis. It is very valuable for a data scientist to restructure and refine the data into more meaningful datasets, which can be used further for analytics. Hence, the idea is to build a tool which will contain all the required data preparation techniques to make data well-structured by providing greater flexibility and easy to use UI. Tool will contain different data preparation techniques which will include the process of data cleaning, data structuring, transforming data, data compression, and data profiling and implementation of related machine learning algorithms.},
	journal = {Asian Journal of Pharmaceutical and Clinical Research},
	author = {Barapatre, Darshan and Vijayalakshmi, A.},
	year = {2017},
	keywords = {Machine learning, Data mining, Apache oozie, Apache pig, Data preparation, Map reduce, SPARK},
	pages = {485--488},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\64E6ACB5\\Data Preparation on Large Datasets for Data Science.pdf:application/pdf},
}

@article{AgileIterationGitLab,
	title = {An {Agile} iteration with {GitLab} {What} ' s inside ?},
	pages = {1--11},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FS4D9HYF\\Agile Iterations with Gitlab.pdf:application/pdf},
}

@article{plansBuildingDataScience2016,
	title = {Building a {Data} {Science} {Portfolio} : {Storytelling} with {Data}},
	author = {Plans, View},
	year = {2016},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9966E9YQ\\Building a Data Science Portfolio_ Storytelling with Data – Dataquest.pdf:application/pdf},
}

@article{BenefitsSingleApplication,
	title = {The benefits of single application {CI} / {CD}},
	pages = {1--9},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LSTVMQKZ\\Gitlab CI the-benefits-of-sing.pdf:application/pdf},
}

@article{zhaiNoteExpectationMaximizationEM,
	title = {A {Note} on the {Expectation}-{Maximization} ( {EM} ) {Algorithm}},
	author = {Zhai, Chengxiang},
	pages = {1--6},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YCW7BUM6\\Expectation Maximization.pdf:application/pdf},
}

@article{Garcia2016,
	title = {Big data preprocessing: methods and prospects},
	volume = {1},
	issn = {2058-6345},
	doi = {10.1186/s41044-016-0014-0},
	abstract = {The massive growth in the scale of data has been observed in recent years being a key factor of the Big Data scenario. Big Data can be defined as high volume, velocity and variety of data that require a new high-performance processing. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The presence of data preprocessing methods for data mining in big data is reviewed in this paper. The definition, characteristics, and categorization of data preprocessing approaches in big data are introduced. The connection between big data and data preprocessing throughout all families of methods and big data technologies are also examined, including a review of the state-of-the-art. In addition, research challenges are discussed, with focus on developments on different big data framework, such as Hadoop, Spark and Flink and the encouragement in devoting substantial research efforts in some families of data preprocessing methods and applications on new big data learning paradigms.},
	number = {1},
	journal = {Big Data Analytics},
	author = {García, Salvador and Ramírez-Gallego, Sergio and Luengo, Julián and Benítez, José Manuel and Herrera, Francisco},
	year = {2016},
	keywords = {big data, data mining, feature selection, data preprocessing, data transformation, hadoop, imperfect data, instance reduction, spark},
	pages = {1--22},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CH8ZZWLH\\Big Data Preprocessing Methods and Prospects.pdf:application/pdf},
}

@article{Geigle2016,
	title = {The {EM} {Algorithm} : {An} {Optimization} {View}},
	abstract = {Suppose you have a probability model for some data you've obtained, but you want to model its generative process using values that are unobserved. For example, you might assume that you have words being generated from a two-component mixture model λp(w i {\textbar} C) + (1 − λ)p(w i {\textbar} θ) but you only observe the individual words w i and do not know from which component they were generated. Thus, you would have a log-likelihood function log p(D {\textbar} θ) = d∈D {\textbar}d{\textbar} i=1 log \{λp(w i {\textbar} C) + (1 − λ)p(w i {\textbar} θ)\}. Finding the maximum likelihood estimate for θ analytically is difficult because of the summation inside of the logarithm, so we are forced to use some sort of numerical algorithm. The EM algorithm [1] is one such optimization algorithm for solving for maximum-likelihood estimates. This note will briefly introduce one particular way of deriving and thinking about the EM algorithm by looking at it from an coordinate-ascent optimization (maximization-maximization) perspective. We first will give a general derivation of the EM algorithm, and then will investigate a few specific examples where we might leverage the EM algorithm for parameter learning. 1 Maximizing the Marginal Likelihood Suppose you have a probability model p(X , Z {\textbar} Θ) where X are observed data and Z are latent variables, and Θ are the model's parameters. (For the above case, the latent variables Z would be the indicator variables z i that tell us from which component word w i was drawn.) We wish to solve the problem Θ * = arg max Θ p(X {\textbar} Θ) = arg max Θ log p(X {\textbar} Θ) = arg max Θ log Z p(X , Z {\textbar} Θ). This is problematic due to the summation inside the logarithm (which results from having to marginalize out the hidden 1 variables Z), which makes finding an analytical solution for Θ * either too difficult or flat-out impossible.},
	author = {Geigle, Chase},
	year = {2016},
	pages = {1--9},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZIS8KFMN\\em-algorithm.pdf:application/pdf},
}

@article{Khattab2020,
	title = {Finding the {Best} of {Both} {Worlds}: {Faster} and {More} {Robust} {Top}-k {Document} {Retrieval}},
	doi = {10.1145/3397271.3401076},
	abstract = {Many top-k document retrieval strategies have been proposed based on the WAND and MaxScore heuristics and yet, from recent work, it is surprisingly difficult to identify the "fastest" strategy. This becomes even more challenging when considering various retrieval criteria, like different ranking models and values of k. In this paper, we conduct the first extensive comparison between ten effective strategies, many of which were never compared before to our knowledge, examining their efficiency under five representative ranking models. Based on a careful analysis of the comparison, we propose LazyBM, a remarkably simple retrieval strategy that bridges the gap between the best performing WAND-based and MaxScore-based approaches. Empirically, LazyBM considerably outperforms all of the considered strategies across ranking models, values of k, and index configurations under both mean and tail query latency.},
	journal = {SIGIR 2020 - Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	author = {Khattab, Omar and Hammoud, Mohammad and Elsayed, Tamer},
	year = {2020},
	note = {ISBN: 9781450380164},
	keywords = {efficiency, dynamic pruning, query evaluation, web search},
	pages = {1031--1040},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ABXNM88Y\\Finding the Best of Both Worlds\; Faster and More Robust Top-k Document Retrieval.pdf:application/pdf},
}

@article{Name2014,
	title = {Data {Preparation} in the {Big} {Data} {Era}},
	abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
	number = {1},
	journal = {Igarss 2014},
	author = {Name, Last and Name, First and Training, Online and Training, Practical and Darin, C and Training, Rank Online and Kimberly, M and Deepa, G and Board, Ethics and Principal, Enter and Primary, Investigator and Systems, Food and Study, Emu Behaviour and Co-investigator, New},
	year = {2014},
	note = {ISBN: 9780874216561},
	keywords = {research, high resolution images, risks management, sustainable reconstruction},
	pages = {1--5},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\K66596DH\\Data_Preparation_in_the_Big_Data_Era_Tamr.pdf:application/pdf},
}

@article{al-faresScalableCommodityData2008,
	title = {A scalable, commodity data center network architecture},
	volume = {38},
	issn = {01464833},
	doi = {10.1145/1402946.1402967},
	abstract = {Today's data centers may contain tens of thousands of computers with significant aggregate bandwidth requirements. The network architecture typically consists of a tree of routing and switching elements with progressively more specialized and expensive equipment moving up the network hierarchy. Unfortunately, even when deploying the highest-end IP switches/routers, resulting topologies may only support 50\% of the aggregate bandwidth available at the edge of the network, while still incurring tremendous cost. Non-uniform bandwidth among data center nodes complicates application design and limits overall system performance. In this paper, we show how to leverage largely commodity Ethernet switches to support the full aggregate bandwidth of clusters consisting of tens of thousands of elements. Similar to how clusters of commodity computers have largely replaced more specialized SMPs and MPPs, we argue that appropriately architected and interconnected commodity switches may deliver more performance at less cost than available from today's higher-end solutions. Our approach requires no modifications to the end host network interface, operating system, or applications; critically, it is fully backward compatible with Ethernet, IP, and TCP. Copyright 2008 ACM.},
	number = {4},
	journal = {Computer Communication Review},
	author = {Al-Fares, Mohammad and Loukissas, Alexander and Vahdat, Amin},
	year = {2008},
	note = {tex.ids: al-faresScalableCommodityData2008
ISBN: 9781605581750},
	keywords = {cloud, Data center topology, Equal-cost routing},
	pages = {63--74},
	file = {A scalable, commodity data center network architecture - Literature Note.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\A scalable, commodity data center network architecture - Literature Note.md:text/plain;A scalable, commodity data center network architecture-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\A scalable, commodity data center network architecture-metadata.md:text/plain;A scalable, commodity data center network architecture.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\A scalable, commodity data center network architecture.pdf:application/pdf;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KBN8YHPZ\\A Scalable, Commodity Data Center Network Architecture.pdf:application/pdf},
}

@article{Pfaff2015,
	title = {The design and implementation of open {vSwitch}},
	issn = {1044-6397},
	abstract = {We describe the design and implementation of Open vSwitch, a multi-layer, open source virtual switch for all major hypervisor platforms. Open vSwitch was designed de novo for networking in virtual environments, resulting in major design departures from traditional software switching architectures. We detail the advanced flow classification and caching techniques that Open vSwitch uses to optimize its operations and conserve hypervisor resources. We evaluate Open vSwitch performance, drawing from our deployment experiences over the past seven years of using and improving Open vSwitch.},
	journal = {Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2015},
	author = {Pfaff, Ben and Pettit, Justin and Koponen, Teemu and Jackson, Ethan J. and Zhou, Andy and Rajahalme, Jarno and Gross, Jesse and Wang, Alex and Stringer, Jonathan and Shelar, Pravin and Amidon, Keith and Casado, Martín},
	year = {2015},
	note = {ISBN: 9781931971218},
	keywords = {cloud},
	pages = {117--130},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MQKJMHNR\\The Design and Implementation of Open vSwitch.pdf:application/pdf},
}

@article{McKeown2008,
	title = {{OpenFlow}: enabling innovation in campus networks},
	volume = {38},
	issn = {0146-4833},
	doi = {10.1145/1355734.1355746},
	abstract = {This whitepaper proposes OpenFlow: a way for researchers to run experimental protocols in the networks they use ev-ery day. OpenFlow is based on an Ethernet switch, with an internal flow-table, and a standardized interface to add and remove flow entries. Our goal is to encourage network-ing vendors to add OpenFlow to their switch products for deployment in college campus backbones and wiring closets. We believe that OpenFlow is a pragmatic compromise: on one hand, it allows researchers to run experiments on hetero-geneous switches in a uniform way at line-rate and with high port-density; while on the other hand, vendors do not need to expose the internal workings of their switches. In addition to allowing researchers to evaluate their ideas in real-world traffic settings, OpenFlow could serve as a useful campus component in proposed large-scale testbeds like GENI. Two buildings at Stanford University will soon run OpenFlow networks, using commercial Ethernet switches and routers. We will work to encourage deployment at other schools; and We encourage you to consider deploying OpenFlow in your university network too.},
	number = {2},
	journal = {ACM SIGCOMM Computer Communication Review},
	author = {McKeown, Nick and Anderson, Tom and Balakrishnan, Hari and Parulkar, Guru and Peterson, Larry and Rexford, Jennifer and Shenker, Scott and Turner, Jonathan},
	year = {2008},
	keywords = {cloud, compromise, computer network, computer science, distributed computing, elephant flow, fibre channel over ethernet, network as a service, openflow, sflow, software defined data center, software defined networking},
	pages = {69--74},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YJMHX4RR\\OpenFlow  Enabling Innovation in Campus Networks.pdf:application/pdf},
}

@article{Deveaud2018,
	title = {Learning to {Adaptively} {Rank} {Docu}-ment {Retrieval} {System} {Configurations}},
	volume = {37},
	url = {https://doi.org/10.1145/3231937},
	doi = {10.1145/3231937},
	abstract = {Modern Information Retrieval (IR) systems have become more and more complex, involving a large number of parameters. For example, a system may choose from a set of possible retrieval models (BM25, language model, etc.), or various query expansion parameters, whose values greatly influence the overall retrieval effectiveness. Traditionally, these parameters are set at a system level based on training queries, and the same parameters are then used for different queries. We observe that it may not be easy to set all these parameters separately, since they can be dependent. In addition, a global setting for all queries may not best fit all individual queries with different characteristics. The parameters should be set according to these characteristics. In this article, we propose a novel approach to tackle this problem by dealing with the entire system configurations (i.e., a set of parameters representing an IR system behaviour) instead of selecting a single parameter at a time. The selection of the best configuration is cast as a problem of ranking different possible configurations given a query. We apply learning-to-rank approaches for this task. We exploit both the query features and the system configuration features in the learning-to-rank method so that the selection of configuration is query dependent. The experiments we conducted on four TREC ad hoc collections show that this approach can significantly outperform the traditional method to tune system configuration globally (i.e., grid search) and leads to higher effectiveness than the top performing systems of the TREC tracks. We also perform an ablation analysis on the impact of different features on the model learning capability and show that query expansion features are among the most important for adaptive systems. 1 INTRODUCTION Modern Information Retrieval (IR) systems involve more and more complex operations, which require setting a large number of parameters. For example, at the very basic preprocessing level, we have to choose among different options of word stemming. Then a retrieval model should be chosen. This latter often involves a set of parameters as well-language models require smoothing parameters and BM25 has another set of parameters. Finally, the pseudo-relevance feedback step requires yet another set of parameters: the number of expansion terms to be added to the query, their weighting scheme, the number of feedback documents to consider, and so on [11, 13]. Over the years, and through evaluation forums such as TREC, 1 CLEF, 2 and NTCIR, 3 the IR community has produced an abundant field of knowledge, however scattered in the literature, on setting the appropriate values of these parameters to optimise the performance of the retrieval systems. For example, we know that the number of pseudo-relevance feedback documents used in IR experiments typically varies between 10 and 50, and the number of expansion terms is in the range of 10 to 20 [16, 36]. BM25 or a language model is often chosen, and they are believed to be effective on most test collections. When a specific retrieval method involves some parameters (e.g., the parameters related to query expansion), one typically tunes them on a set of training queries to maximise the global effectiveness. The typical method for parameter tuning is through grid search [73]: A set of possible values is defined for each parameter, and grid search determines the best value for each parameter to maximise the effectiveness of the retrieval system on a set of training queries. To be more robust, one also test different settings on several test collections. For example, Reference [80] analysed the influence of the smoothing function in Language Modelling (LM) on several test collections, and some specific range of the smoothing parameter is recommended. Alternatively , it is possible to optimize the studied parameter value using a collection and observe its effects on other collections [35], which is a form of transfer learning. These methodologies for parameter tuning assume that the same selected parameters would fit all the queries. In practice, even if the selected system configuration is the best for a set of queries, it has been often observed that it behaves differently on different queries: It may excel on some queries while failing miserably on some others [1, 37, 60]. This fact indicates a critical problem in the usual way to set system parameters: It is done once and for all queries. It is desirable that we choose the appropriate parameters for each query at hand, thus avoiding the problem of the one-size-fits-all solution. There is an abundant literature on the effects of individual system parameters on retrieval results. Indeed, for any new method proposed, it is required that an analysis is made in depth to evaluate the effect of parameter setting [83], e.g., how the method behaves along with the changes of its inherent parameters. However, there have been few studies trying to determine the parameters automatically for a given query. There are also only a few descriptive analyses of cross parameter effects [9, 23, 29], which examine the results and the effects of various parameter settings. In Reference [23], the authors analysed the influence of indexing and retrieval parameters on retrieval effectiveness; while the authors of Reference [9] analysed an even larger set of parameters. The authors of Reference [29] analysed the correlation between effectiveness measures and system parameters. However, none of these studies attempted to determine automatically the best parameters at the query level for new queries.},
	number = {3},
	urldate = {2020-10-25},
	journal = {ACM Transactions on Information Systems},
	author = {Deveaud, Romain and Mothe, Josiane and Ullah, Zia and Nie, Jian-Yun},
	year = {2018},
	keywords = {Learning to rank, text information systems, Additional Key Words and Phrases: Information syst, CCS Concepts: • Information systems → Retrieval ef, Information retrieval query processing},
}

@book{Smith2011,
	title = {Information {Retrieval}: {Implementing} and {Evaluating} {Search} {Engines}},
	volume = {29},
	isbn = {978-0-262-02651-2},
	author = {Smith, Alastair},
	year = {2011},
	doi = {10.1108/02640471111188088},
	note = {Publication Title: The Electronic Library
Issue: 6
ISSN: 02640473},
	keywords = {text information systems},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CU74DZ9X\\Information Retrieval Implementing and Evaluating Search Engines-2010-mit.pdf:application/pdf},
}

@article{Shiri2004,
	title = {Introduction to {Modern} {Information} {Retrieval} (2nd edition)},
	volume = {53},
	issn = {00242535},
	doi = {10.1108/00242530410565256},
	number = {9},
	journal = {Library Review},
	author = {Shiri, Ali},
	year = {2004},
	keywords = {Information retrieval, text information systems, Storage, Worldwide web},
	pages = {462--463},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\84KYV2HV\\Introduction to Information Retrieval.pdf:application/pdf},
}

@article{Lv2011,
	title = {Lower-bounding term frequency normalization},
	doi = {10.1145/2063576.2063584},
	abstract = {In this paper, we reveal a common deficiency of the current retrieval models: the component of term frequency (TF) normalization by document length is not lower-bounded properly; as a result, very long documents tend to be overly penalized. In order to analytically diagnose this problem, we propose two desirable formal constraints to capture the heuristic of lower-bounding TF, and use constraint analysis to examine several representative retrieval functions. Analysis results show that all these retrieval functions can only satisfy the constraints for a certain range of parameter values and/or for a particular set of query terms. Empirical results further show that the retrieval performance tends to be poor when the parameter is out of the range or the query term is not in the particular set. To solve this common problem, we propose a general and efficient method to introduce a sufficiently large lower bound for TF normalization which can be shown analytically to fix or alleviate the problem. Our experimental results demonstrate that the proposed method, incurring almost no additional computational cost, can be applied to state-of-the-art retrieval functions, such as Okapi BM25, language models, and the divergence from randomness approach, to significantly improve the average precision, especially for verbose queries. © 2011 ACM.},
	journal = {International Conference on Information and Knowledge Management, Proceedings},
	author = {Lv, Yuanhua and Zhai, Chengxiang},
	year = {2011},
	note = {ISBN: 9781450307178},
	keywords = {data analysis, text information systems, BM25+, DIR+, document length, formal constraints, lower bound, Pl2+, term frequency},
	pages = {7--16},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TRDA4RBX\\Lower-Bounding Term Frequency Normalization.pdf:application/pdf},
}

@book{Witten1995,
	title = {Managing {Gigabytes}: {Compressing} and {Indexing} {Documents} and {Images}},
	volume = {41},
	isbn = {1-55860-570-3},
	abstract = {In this fully updated second edition of the highly acclaimed Managing Gigabytes, authors Witten, Moffat, and Bell continue to provide unparalleled coverage of state-of-the-art techniques for compressing and indexing data. Whatever your field, if you work with large quantities of information, this book is essential reading-an authoritative theoretical resource and a practical guide to meeting the toughest storage and access challenges. It covers the latest developments in compression and indexing and their application on the Web and in digital libraries. It also details dozens of powerful techniques supported by mg, the authors' own system for compressing, storing, and retrieving text, images, and textual images. mg's source code is freely available on the Web. Up-to-date coverage of new text compression algorithms such as block sorting, approximate arithmetic coding, and fat Huffman coding New sections on content-based index compression and distributed querying, with 2 new data structures for fast indexing New coverage of image coding, including descriptions of de facto standards in use on the Web (GIF and PNG), information on CALIC, the new proposed JPEG Lossless standard, and JBIG2 New information on the Internet and WWW, digital libraries, web search engines, and agent-based retrieval Accompanied by a public domain system called MG which is a fully worked-out operational example of the advanced techniques developed and explained in the book New appendix on an existing digital library system that uses the MG software},
	author = {Witten, I.H. and Moffat, A. and Bell, T.C.},
	year = {1995},
	doi = {10.1109/tit.1995.476344},
	note = {Publication Title: IEEE Transactions on Information Theory
Issue: 6
ISSN: 0018-9448},
	keywords = {text information systems},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IVT52SKY\\Managing Gigabytes.pdf:application/pdf},
}

@article{Robertson1994,
	title = {Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval},
	doi = {10.1007/978-1-4471-2099-5_24},
	abstract = {The 2-Poisson model for term frequencies is used to suggest ways of incorporating certain variables in probabilistic models for information retrieval. The variables concerned are within-document term frequency, document length, and within-query term frequency. Simple weighting functions are developed, and tested on the TREC test collection. Considerable performance improvements (over simple inverse collection frequency weighting) are demonstrated.},
	number = {1},
	journal = {Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 1994},
	author = {Robertson, S. E. and Walker, S.},
	year = {1994},
	note = {ISBN: 038719889X},
	keywords = {text information systems},
	pages = {232--241},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PPG3KDHX\\Some Simple Effective Approximations of the 2-Poisson Model for Probabilistic Weighted Retrieval.pdf:application/pdf},
}

@article{Singhal1996,
	title = {Pivoted document length normalization},
	issn = {01635840},
	doi = {10.1145/243199.243206},
	abstract = {Automatic information retrieval systems have to deal with documents of varying lengths in a text collection. Document length normalization is used to fairly retrieve documents of all lengths. In this study, we observe that a normalization scheme that retrieves documents of all lengths with similar chances as their likelihood of relevance will outperform another scheme which retrieves documents with chances very different from their likelihood of relevance. We show that the retrieval probabilities for a particular normalization method deviate systematically from the relevance probabilities across different collections. We present pivoted normalization, a technique that can be used to modify any normalization function thereby reducing the gap between the relevance and the retrieval probabilities. Training pivoted normalization on one collection, we can successfully use it on other (new) text collections, yielding a robust, collection independent normalization technique. We use the idea of pivoting with the well known cosine normalization function. We point out some shortcomings of the cosine function and present two new normalization functions - pivoted unique normalization and pivoted byte size normalization.},
	journal = {SIGIR Forum (ACM Special Interest Group on Information Retrieval)},
	author = {Singhal, Amit and Buckley, Chris and Mitra, Mandar},
	year = {1996},
	keywords = {text information systems},
	pages = {21--29},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3PQUPV47\\Pivoted Document Length Normalization.pdf:application/pdf},
}

@article{accuracyCalculateEconomicImpact2020,
	title = {Calculate {The} {Economic} {Impact} {Of} {Your} {Click}-{Through} {Prediction}},
	author = {Accuracy, Beyond},
	year = {2020},
	pages = {1--14},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EFBLTKC7\\Calculate The Economic Impact Of Your Click-Through Prediction _ by Myriam Barnés _ Towards Data Science.pdf:application/pdf},
}

@article{DCMSC2019,
	title = {House of {Commons} {Digital}, {Culture}, {Media} and {Sport} {Committee}: {Disinformation} and ‘fake news’: {Final} {Report}: {Eighth} {Report} of {Session} 2017–19},
	number = {February},
	author = {{DCMSC}},
	year = {2019},
	pages = {1--109},
}

@article{Zhang2017e,
	title = {Advertisement {Click}-{Through} {Rate} {Prediction} {Based} on the {Weighted}-{ELM} and {Adaboost} {Algorithm}},
	volume = {2017},
	issn = {10589244},
	doi = {10.1155/2017/2938369},
	abstract = {Accurate click-through rate (CTR) prediction can not only improve the advertisement company's reputation and revenue, but also help the advertisers to optimize the advertising performance. There are two main unsolved problems of the CTR prediction: low prediction accuracy due to the imbalanced distribution of the advertising data and the lack of the real-time advertisement bidding implementation. In this paper, we will develop a novel online CTR prediction approach by incorporating the real-time bidding (RTB) advertising by the following strategies: user profile system is constructed from the historical data of the RTB advertising to describe the user features, the historical CTR features, the ID features, and the other numerical features. A novel CTR prediction approach is presented to address the imbalanced learning sample distribution by integrating the Weighted-ELM (WELM) and the Adaboost algorithm. Compared to the commonly used algorithms, the proposed approach can improve the CTR significantly.},
	journal = {Scientific Programming},
	author = {Zhang, Sen and Fu, Qiang and Xiao, Wendong},
	year = {2017},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6IB7FS8F\\Advertisement Click-Through Rate Prediction Based on the Weighted-ELM and Adaboost Algorithm.pdf:application/pdf},
}

@article{Results2020,
	title = {Average display advertising clickthrough rates ( {CTRs} ) – 2020 compilation {US} , {Europe} and {Worldwide} ad clickthrough rates statistics comparing display ads to paid social and {Outpace} your competition in a challenging {SME} market {Display} {Ad} {CTR} benchmarks -},
	author = {Results, G E T},
	year = {2020},
	pages = {1--15},
}

@article{opportunityHowWriteAnalytics2020,
	title = {How to write up {Analytics} / {Data} {Science} {Projects}},
	volume = {2020},
	number = {28},
	author = {Opportunity, Business},
	year = {2020},
	pages = {1--7},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2GAPFFT4\\How to write up Analytics_Data Science Projects _ LinkedIn.pdf:application/pdf},
}

@article{KNAFLIC2014,
	title = {Exploratory {Vs} {Explanatory} {Analysis}},
	url = {http://www.storytellingwithdata.com/blog/2014/04/exploratory-vs-explanatory-analysis},
	journal = {Storytelling With Data},
	author = {KNAFLIC, COLE NUSSBAUMER},
	year = {2014},
	pages = {2--7},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QSNV2IKA\\exploratory vs explanatory analysis — storytelling with data.pdf:application/pdf},
}

@article{Wahner2017,
	title = {Data {Preprocessing} vs . {Data} {Wrangling} in {Machine} {Learning} {Projects}},
	url = {https://www.infoq.com/articles/ml-data-processing},
	author = {Wähner, Kai and Penchikala, Srini},
	year = {2017},
	pages = {1--15},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SU9PV4HU\\Data Preprocessing vs. Data Wrangling in Machine Learning Projects.pdf:application/pdf},
}

@article{Dataquest,
	title = {An {In}-{Depth} {Style} {Guide} for {Data} {Science} {Projects} – {Dataquest}},
	url = {https://www.dataquest.io/blog/data-science-project-style-guide/},
	author = {Dataquest, At},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\K3D484VS\\An In-Depth Style Guide for Data Science Projects – Dataquest.pdf:application/pdf},
}

@article{Blogs,
	title = {{THE} {ULTIMATE} {Create} {Blogs} that {Deliver} the {Results} {You}},
	author = {Blogs, Create and Need, Results You and Your, Hit and Picture, Big},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FBLKUVAD\\Ultimate_Blogging_Checklist.pdf:application/pdf},
}

@article{Hart2016,
	title = {How to tell a story with photographs},
	volume = {2016},
	issn = {2047-8917},
	doi = {10.12968/prtu.2016.52.20},
	number = {52},
	journal = {Primary Teacher Update},
	author = {Hart, Karen},
	year = {2016},
	pages = {20--22},
	file = {How to Tell a Story with Data (HBR).PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\26NEIYMB\\How to Tell a Story with Data (HBR).PDF:application/pdf},
}

@article{DiFranza2019,
	title = {How {To} {Tell} {Stories} {With} {Data}: {Tips} {For} {Presenting} {Data} {Effectively}},
	url = {https://www.northeastern.edu/graduate/blog/blog-how-to-tell-stories-with-data/},
	author = {DiFranza, Ashley},
	year = {2019},
	note = {ISBN: 1750588900},
	pages = {1--7},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KTCLAM5Z\\How To Tell Stories With Data_ Tips For Presenting Data Effectively.pdf:application/pdf},
}

@article{dataFantasticExamplesData2020,
	title = {8 fantastic examples of data storytelling {Minard} ’ s {Map} of {Napoleon} ’ s {Russian} {Campaign} in 1812 {New} {York} ’ s {Noisiest} {Neighborhoods}},
	author = {Data, Big Data},
	year = {2020},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IL899G4K\\8 fantastic examples of data storytelling _ Import.io.pdf:application/pdf},
}

@misc{KDDSIG,
	title = {{SIGKDD} : {KDD} {Cup} 2012 ({Track} 2) : {Predict} the click-through rate of ads given the query and user information},
	url = {https://www.kdd.org/kdd-cup/view/kdd-cup-2012-track-2},
	urldate = {2020-08-31},
	author = {{KDDSIG}},
	keywords = {ctr},
}

@misc{Labs,
	title = {Click {Through} {Rate} prediction at scale using {Open} {Technologies} - {Criteo} {AI} {Lab}},
	url = {https://ailab.criteo.com/ctr-at-scale-using-open-technologies/},
	urldate = {2020-08-31},
	author = {Labs, Criteo},
	keywords = {ctr},
}

@book{Yang2018,
	title = {Stock {Price} {Movement} {Prediction}},
	volume = {11016},
	isbn = {978-3-319-97288-6},
	url = {http://link.springer.com/10.1007/978-3-319-97289-3},
	abstract = {Data shifting in machine learning problems violates the common assumption that the training and testing samples should be drawn from the same distribution. Most of the algorithms which provide the solution for data shifting problems first try to evaluate the distributions and then reweight samples based on their distributions. Due to the difficulty of evaluating a precise distribution, conventional methods cannot achieve good classification performance. In this paper, we introduce two types of data-shift problems and propose a model-based co-clustering transfer learning based solution which consistently deals with both scenarios of data shift. Experimental results demonstrate that our proposed method achieves better generalization and running efficiency compared to tra-ditional methods under data or covariate shift setting.},
	publisher = {Springer International Publishing},
	author = {Yang, Wenli and B, Saurabh Garg and Raza, Ali and Herbert, David and Kang, Byeong},
	year = {2018},
	doi = {10.1007/978-3-319-97289-3},
	note = {Publication Title: 15th Pacific Rim Knowledge Acquisition Workshop
Issue: August},
	keywords = {stock price prediction, blockchain, Blockchain, crypto-currency, Crypto-currency, scalability, Scalability},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BIG4KDJT\\Stock Price Movement Prediction from Financial News with Deep Learning and Knowledge Graph Embedding.pdf:application/pdf},
}

@article{Asaithambi2017,
	title = {Why, {How} and {When} to {Scale} your {Features} - {GreyAtom} - {Medium}},
	abstract = {Feature scaling can vary your results a lot while using certain algorithms and have a minimal or no effect in others. To understand this, let’s look why features need to be scaled, varieties of scaling methods and when we should scale our features.},
	journal = {Medium},
	author = {Asaithambi, Sudharsan},
	year = {2017},
	pages = {1--4},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\A867M25L\\Why, How and When to Scale your Features _ by Sudharsan Asaithambi _ GreyAtom _ Medium.pdf:application/pdf},
}

@article{Tan2019,
	title = {A {Tensor}-based {eLSTM} {Model} to {Predict} {Stock} {Price} {Using} {Financial} {News}},
	volume = {6},
	doi = {10.24251/hicss.2019.201},
	abstract = {Stock market prediction has attracted much attention from both academia and business. Both traditional finance and behavioral finance believe that market information affects stock movements. Typically, market information consists of fundamentals and news information. To study how information shapes stock markets, common strategies are to concatenate various information into one compound vector. However, such concatenating ignores the interlinks between fundamentals and news information. In addition, the fundamental data are continuous values sampled at fixed time intervals, while news information occurred randomly. Such heterogeneity leads to miss valuable information partially or twist the feature spaces. In this article, we propose a tensor-based event-LSTM (eLSTM) to solve these two challenges. In particular, we model the market information with tensors instead of concatenated vectors and balance the heterogeneity of different data types with event-driven mechanism in LSTM. Experiments performed on an entire year data of China Securities markets demonstrate the supreme of the proposed approach over the state-of-the-art algorithms including AZfinText, eMAQT, and TeSIA.},
	journal = {Proceedings of the 52nd Hawaii International Conference on System Sciences},
	author = {Tan, Jinghua and Wang, Jun and Rinprasertmeechai, Denisa and Xing, Rong and Li, Qing},
	year = {2019},
	note = {ISBN: 9780998133126},
	keywords = {★, stock price prediction},
	pages = {1658--1667},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6MSK6WZA\\A Tensor-based eLSTM Model to Predict Stock Price Using Financial News.pdf:application/pdf},
}

@article{Matsunaga2019,
	title = {Exploring {Graph} {Neural} {Networks} for {Stock} {Market} {Predictions} with {Rolling} {Window} {Analysis}},
	url = {http://arxiv.org/abs/1909.10660},
	abstract = {Recently, there has been a surge of interest in the use of machine learning to help aid in the accurate predictions of financial markets. Despite the exciting advances in this cross-section of finance and AI, many of the current approaches are limited to using technical analysis to capture historical trends of each stock price and thus limited to certain experimental setups to obtain good prediction results. On the other hand, professional investors additionally use their rich knowledge of inter-market and inter-company relations to map the connectivity of companies and events, and use this map to make better market predictions. For instance, they would predict the movement of a certain company's stock price based not only on its former stock price trends but also on the performance of its suppliers or customers, the overall industry, macroeconomic factors and trade policies. This paper investigates the effectiveness of work at the intersection of market predictions and graph neural networks, which hold the potential to mimic the ways in which investors make decisions by incorporating company knowledge graphs directly into the predictive model. The main goal of this work is to test the validity of this approach across different markets and longer time horizons for backtesting using rolling window analysis. In this work, we concentrate on the prediction of individual stock prices in the Japanese Nikkei 225 market over a period of roughly 20 years. For the knowledge graph, we use the Nikkei Value Search data, which is a rich dataset showing mainly supplier relations among Japanese and foreign companies. Our preliminary results show a 29.5\% increase and a 2.2-fold increase in the return ratio and Sharpe ratio, respectively, when compared to the market benchmark, as well as a 6.32\% increase and 1.3-fold increase, respectively, compared to the baseline LSTM model.},
	author = {Matsunaga, Daiki and Suzumura, Toyotaro and Takahashi, Toshihiro},
	year = {2019},
	note = {arXiv: 1909.10660},
	keywords = {★, stock price prediction},
	pages = {1--10},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CRYQ9EU2\\Exploring Graph Neural Networks for Stock Market Predictions with Rolling Window Analysis.pdf:application/pdf},
}

@article{Signal2017,
	title = {Lecture 2 : {Statistical} {Decision} {Theory} {Basic} {Elements} of {Statistical} {Decision} {Theory} {Optimality} {Criterion} of {Decision} {Rules}},
	number = {1},
	author = {Signal, Statistical and Lecture, Processing and Scribe, Jiantao Jiao and Hilger, Andrew and Wald, Abraham},
	year = {2017},
	pages = {342--357},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E4N8XJUM\\Statistical Decision Theory Lecture Notes (Stanford).pdf:application/pdf},
}

@article{Abe2020,
	title = {Cross-sectional {Stock} {Price} {Prediction} using {Deep} {Learning} for {Actual} {Investment} {Management}},
	doi = {10.1145/3399871.3399889},
	abstract = {Stock price prediction has been an important research theme both academically and practically. Various methods to predict stock prices have been studied until now. The feature that explains the stock price by a cross-section analysis is called a "factor" in the field of finance. Many empirical studies in finance have identified which stocks having features in the cross-section relatively increase and which decrease in terms of price. Recently, stock price prediction methods using machine learning, especially deep learning, have been proposed since the relationship between these factors and stock prices is complex and non-linear. However, there are no practical examples for actual investment management. In this paper, therefore, we present a cross-sectional daily stock price prediction framework using deep learning for actual investment management. For example, we build a portfolio with information available at the time of market closing and invest at the time of market opening the next day. We perform empirical analysis in the Japanese stock market and confirm the profitability of our framework.},
	journal = {ACM International Conference Proceeding Series},
	author = {Abe, Masaya and Nakagawa, Kei},
	year = {2020},
	note = {arXiv: 2002.06975
ISBN: 9781450377102},
	keywords = {Deep Learning, stock price prediction, Cross-Section, Multi-factor Model, Stock Return Prediction},
	pages = {9--15},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\99BQ85K7\\Cross-sectional Stock Price Prediction using Deep Learning for Actual Investment Management.pdf:application/pdf},
}

@article{Jiang2020,
	title = {Predicting {Stock} {Market} {N}-{Days} {Ahead} {Using} {SVM} {Optimized} by {Selective} {Thresholds}},
	doi = {10.1145/3383972.3384010},
	abstract = {For a long time, many have tried to predict the stock market and identify its trends and patterns. Using Support V ector Machines (SVM), we predict various N days ahead of the NASDAQ Index from Jan 2000 to Dec 2018 using various selective predicting patterns. We only use the given stock opening, closing, high, low, and volume data to derive its corresponding technical indicators to predict N days ahead. This study utilizes selective thresholds to predict stock markets, which selects certain stock days to train and test the model based on whether a certain technical indicator meets the criteria of values. W e ended with 71.36\% accuracy with Momentum 20 Selection (greater than 0.2 and less than -0.2 after normalization) when predicting 25 days ahead. This accuracy is significantly higher than previous studies that used only technical indicators mainly due to the selective predicting methods. Contrary to the direction of turning towards more niche data sets, we show that traditional technical indicators are sufficient in achieving high accuracy predictions in the stock market.},
	journal = {ACM International Conference Proceeding Series},
	author = {Jiang, Jason and Liu, Jianguo},
	year = {2020},
	note = {ISBN: 9781450376426},
	keywords = {SVM, stock price prediction, Data Classification, Movement, NASDAQ, Selective Predicting},
	pages = {11--16},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BZ2JRK8I\\Predicting Stock Market N-Days Ahead Using SVM Optimized Selective Thresholds.pdf:application/pdf},
}

@article{SIDDHARTHAN2002,
	title = {Christopher {D}. {Manning} and {Hinrich} {Schutze}. {Foundations} of {Statistical} {Natural} {Language} {Processing} . {MIT} {Press}, 2000. {ISBN} 0-262-13360-1. 620 pp. \$64.95/£44.95 (cloth).},
	volume = {8},
	issn = {1351-3249},
	doi = {10.1017/s1351324902212851},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1351324902212851/resource/name/firstPage-S1351324902212851a.jpg},
	number = {1},
	journal = {Natural Language Engineering},
	author = {SIDDHARTHAN, ADVAITH},
	year = {2002},
	pages = {91--92},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Q8GSLQP5\\Foundations of Statistical Natural Language Processing (Manning, Schutze).pdf:application/pdf},
}

@article{Jiang2020a,
	title = {Applications of deep learning in stock market prediction: recent progress},
	url = {http://arxiv.org/abs/2003.01859},
	abstract = {Stock market prediction has been a classical yet challenging problem, with the attention from both economists and computer scientists. With the purpose of building an effective prediction model, both linear and machine learning tools have been explored for the past couple of decades. Lately, deep learning models have been introduced as new frontiers for this topic and the rapid development is too fast to catch up. Hence, our motivation for this survey is to give a latest review of recent works on deep learning models for stock market prediction. We not only category the different data sources, various neural network structures, and common used evaluation metrics, but also the implementation and reproducibility. Our goal is to help the interested researchers to synchronize with the latest progress and also help them to easily reproduce the previous studies as baselines. Base on the summary, we also highlight some future research directions in this topic.},
	author = {Jiang, Weiwei},
	year = {2020},
	note = {arXiv: 2003.01859},
	keywords = {machine learning, ★, convolutional neural network, deep learning, stock market prediction, feedforward neural network, recurrent neural},
	pages = {1--97},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WLEBYBA9\\Applications of deep learning in stock market price prediction.pdf:application/pdf},
}

@article{Zhai2008,
	title = {Statistical language models for information retrieval a critical review},
	volume = {2},
	issn = {15540669},
	doi = {10.1561/1500000008},
	abstract = {Statistical language models have recently been successfully applied to many information retrieval problems. A great deal of recent work has shown that statistical language models not only lead to superior empirical performance, but also facilitate parameter tuning and open up possibilities for modeling nontraditional retrieval problems. In general, statistical language models provide a principled way of modeling various kinds of retrieval problems. The purpose of this survey is to systematically and critically review the existing work in applying statistical language models to information retrieval, summarize their contributions, and point out outstanding challenges.},
	number = {3},
	journal = {Foundations and Trends in Information Retrieval},
	author = {Zhai, Cheng Xiang},
	year = {2008},
	pages = {137--213},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YKSMI4EH\\Statistical Language Models for Information Retrieval A Critical Review.pdf:application/pdf},
}

@article{Hanley1982,
	title = {The meaning and use of the area under a receiver operating characteristic ({ROC}) curve},
	volume = {143},
	issn = {00338419},
	doi = {10.1148/radiology.143.1.7063747},
	abstract = {A representation and interpretation of the area under a receiver operating characteristic (ROC) surve obtained by the 'rating' method, or by mathematical predictions based on patient characteristics, is presented. It is shown that in such a setting the area represents the probability that a randomly chosen diseased subject is (correctly) rated or ranked with greater suspicion than a randomly chosen non-diseased subject. Moreover, this probability of a correct ranking is the same quantity that is estimated by the already well-studied nonparametric Wilcoxon statistic. These two relationships are exploited to (a) provide rapid closed-form expressions for the approximate magnitude of the sampling variability, i.e., standard error that one uses to accompany the area under a smoothed ROC curve, (b) guide in determining the size of the sample required to provide a sufficiently reliable estimate of this area, and (c) determine how large sample sizes should be to ensure that one can statistically detect differences in the accuracy of diagnostic techniques.},
	number = {1},
	journal = {Radiology},
	author = {Hanley, J. A. and McNeil, B. J.},
	year = {1982},
	pmid = {7063747},
	pages = {29--36},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VR7E72Z7\\The Meaning and Use of the Area Under Receive Operating Characteristic Curve.pdf:application/pdf},
}

@book{Zhai2016,
	title = {Text {Data} {Management} and {Analysis}: {A} {Practical} {Introduction} to {Information} {Retrieval} and {Text} {Mining}},
	isbn = {978-1-970001-19-8},
	abstract = {First Edition. The growth of "big data" created unprecedented opportunities to leverage computational and statistical approaches to turn raw data into actionable knowledge that can support various application tasks. This is especially true for the optimization of decision making in virtually all application domains such as health and medicine, security and safety, learning and education, scientific discovery, and business intelligence. Just as a microscope enables us to see things in the "micro world" and a telescope allows us to see things far away, one can imagine a "big data scope" would enable us to extend our perception ability to "see" useful hidden information and knowledge buried in the data, which can help make predictions and improve the optimality of a chosen decision. This book covers general computational techniques for managing and analyzing large amounts of text data that can help users manage and make use of text data in all kinds of applications.},
	author = {Zhai, ChengXiang and Massung, Sean},
	year = {2016},
	doi = {10.1145/2915031},
	note = {Publication Title: Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\49RSZSAK\\Text Data Management and Analysis_ A Practical Introduction to Information Retrieval and Text Mining.pdf:application/pdf},
}

@article{Liu2019c,
	title = {Combining {Enterprise} {Knowledge} {Graph} and {News} {Sentiment} {Analysis} for {Stock} {Price} {Prediction}},
	volume = {6},
	doi = {10.24251/hicss.2019.153},
	abstract = {Many state of the art methods analyze sentiments in news to predict stock price. When predicting stock price movement, the correlation between stocks is a factor that can’t be ignored because correlated stocks could cause co-movement. Traditional methods of measuring the correlation between stocks are mostly based on the similarity between corresponding stock price data, while ignoring the business relationships between companies, such as shareholding, cooperation and supply-customer relationships. To solve this problem, this paper proposes a new method to calculate the correlation by using the enterprise knowledge graph embedding that systematically considers various types of relationships between listed stocks. Further, we employ Gated Recurrent Unit (GRU) model to combine the correlated stocks’ news sentiment, the focal stock’s news sentiment and the focal stock’s quantitative features to predict the focal stock’s price movement. Results show that our method has an improvement of 8.1\% compared with the traditional method.},
	journal = {Proceedings of the 52nd Hawaii International Conference on System Sciences},
	author = {Liu, Jue and Lu, Zhuocheng and DU, Wei},
	year = {2019},
	note = {ISBN: 9780998133126},
	keywords = {★},
	pages = {1247--1255},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\53RIGR38\\Combining enterprise knowledge graph and News Sentiment Analysis for Stock Price Volatility Prediction.pdf:application/pdf},
}

@article{chenPracticalStatisticalLearning,
	title = {Practical {Statistical} {Learning} ( {F18} )},
	author = {Chen, Yinyin Elaine},
}

@article{Jordan2010,
	title = {Lecture 3 - {Decision} {Theory}, {Priors}},
	abstract = {Recall that decision theory provides a quantification of what it means for a procedure to be 'good'. This quantification comes from the loss function, l(θ, δ(X)). Frequentists and Bayesians use the loss function differently. 1.1 Frequentist interpretation, the risk function In frequentist usage, the parameter θ is fixed and thus the data are averaged over. Letting R(θ, δ) denote the frequentist risk, we have R(θ, δ) = E θ l(θ, δ(X)). (1) This expectation is taken over the data X, with the parameter θ held fixed. Note that the data, X, is capitalized, emphasizing that it is a random variable. Example 1 (Squared-error loss). A commonly chosen loss function for parameter estimation is the squared error loss, defined by l(θ, δ(X)) = (θ − δ(X)) 2 . In this case, we have R(θ, δ) = E θ l(θ, δ(X)) = E θ (θ − δ(X)) 2 = E θ (θ − E θ δ(X) + E θ δ(X) − δ(X)) 2 = (θ − E θ δ(X)) 2 Bias 2 + E θ (δ(X) − E θ δ(X)) 2 Variance . (2) This result allows a frequentist to analyze the variance and bias of an estimator separately, and can be used to motivate frequentist ideas, e.g. minimum variance unbiased estimators. 1.2 Bayesian interpretation and posterior risk Bayesian do not find the previous idea compelling, because it doesn't adhere to the conditionality principle by averaging over all possible data sets. Hence, in a Bayesian framework, we define the posterior risk, ρ(x, π), based on the data x and a prior, π: ρ(x, π) = l(θ, δ(x))p(θ{\textbar}x)dθ. (3) Note that the prior enters the equation when calculating the posterior density. Using the Bayes risk, we can define a bit of jargon.},
	number = {x},
	journal = {Lecture Notes in Bayesian Modeling and Inference},
	author = {Jordan, Michael},
	year = {2010},
	pages = {1--5},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AC7EQRXC\\Bayesian Modeling and Inference.pdf:},
}

@article{brooksDataScienceResume,
	title = {Data {Science} {Resume} {Guide} {Welcome} !},
	author = {Brooks, Hannah and Gutierrez, Sebastian},
}

@article{Bar,
	title = {{VueScan} – {Getting} {Started} {Guide} {Table} of {Contents} {Welcome} to {VueScan}},
	journal = {System},
	author = {Bar, Button},
	pages = {1--14},
}

@article{gutierrezProjectPortfolioGuide,
	title = {Project {Portfolio} {Guide} {Table} of {Contents}},
	author = {Gutierrez, By Sebastian},
}

@article{Dominich2008,
	title = {Probabilistic {Retrieval}},
	doi = {10.1007/978-3-540-77659-8_10},
	journal = {The Modern Algebra of Information Retrieval},
	author = {Dominich, Sándor},
	year = {2008},
	pages = {215--236},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PNPR9V7C\\Information Retrieval Chapter 6 - Probabilistic Retrieval.pdf:application/pdf},
}

@article{Belkin1992,
	title = {Information filtering and {Information} retrieval: {Two} {Sides} of the {Same} {Coin}?},
	volume = {35},
	issn = {15577317},
	doi = {10.1145/138859.138861},
	number = {12},
	journal = {Communications of the ACM},
	author = {Belkin, Nicholas J. and Croft, W. Bruce},
	year = {1992},
	keywords = {information retrieval, information filtering},
	pages = {29--38},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SCF5TAEV\\Information Filtering and Information Retrieval - Two sides of same coin.pdf:application/pdf},
}

@article{Allen2017,
	title = {Maximum {Likelihood} {Estimation}},
	doi = {10.4135/9781483381411.n322},
	number = {Iid},
	journal = {The SAGE Encyclopedia of Communication Research Methods},
	author = {Allen, Mike},
	year = {2017},
	pages = {1--3},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ACPGYE6K\\Maximum Likelihood Estimation.pdf:application/pdf},
}

@article{Fang2011,
	title = {Diagnostic evaluation of information retrieval models},
	volume = {29},
	issn = {10468188},
	doi = {10.1145/1961209.1961210},
	abstract = {Developing effective retrieval models is a long-standing central challenge in information retrieval research. In order to develop more effective models, it is necessary to understand the deficiencies of the current retrieval models and the relative strengths of each of them. In this article, we propose a general methodology to analytically and experimentally diagnose the weaknesses of a retrieval function, which provides guidance on how to further improve its performance. Our methodology is motivated by the empirical observation that good retrieval performance is closely related to the use of various retrieval heuristics. We connect the weaknesses and strengths of a retrieval function with its implementations of these retrieval heuristics, and propose two strategies to check how well a retrieval function implements the desired retrieval heuristics. The first strategy is to formalize heuristics as constraints, and use constraint analysis to analytically check the implementation of retrieval heuristics. The second strategy is to define a set of relevance-preserving perturbations and perform diagnostic tests to empirically evaluate how well a retrieval function implements retrieval heuristics. Experiments show that both strategies are effective to identify the potential problems in implementations of the retrieval heuristics. The performance of retrieval functions can be improved after we fix these problems. © 2011 ACM.},
	number = {2},
	journal = {ACM Transactions on Information Systems},
	author = {Fang, Hui and Tao, Tao and Zhai, Chengxiang},
	year = {2011},
	keywords = {Constraints, Diagnostic evaluation, Formal models, Retrieval heuristics, TF-IDF weighting},
	pages = {1--46},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PKVLGBZR\\Diagnostic Evaluation of Information Retrieval.pdf:application/pdf},
}

@article{Jagwani2018,
	title = {Finance and {Analysing} {Seasonal} and {Nonseasonal}},
	number = {Iciccs},
	journal = {2018 Second International Conference on Intelligent Computing and Control Systems (ICICCS)},
	author = {Jagwani, Jai and Gupta, Manav and Sachdeva, Hardik and Singhal, Alka},
	year = {2018},
	note = {Publisher: IEEE
ISBN: 9781538628423},
	keywords = {out, stock price prediction, stock},
	pages = {462--467},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\A3BZM5SS\\Stock Price Forecasting Using Data from Yahoo Finance and Analysing Seasonal and Nonseasonal Trend.pdf:application/pdf},
}

@article{Izzah2018,
	title = {Mobile app for stock prediction using {Improved} {Multiple} {Linear} {Regression}},
	volume = {2018-Janua},
	doi = {10.1109/SIET.2017.8304126},
	abstract = {Stock Prediction is developed in both of two studies, economics, and data mining. Stock predictions got special attention due to its importance for creating a more effective and efficient planning. In this study, Improved Multiple Linear Regression (IMLR) was built into a mobile application based android platform for stock price prediction. IMLR is a hybrid Multiple Linear Regression with Moving Average technique. The app was built in several steps, which are requirement analysis, system design, implementation, and testing. Data were collected from the finance.yahoo.com page with category 'Jakarta Composite Index (A JKSE)' which were automatically taken by using Yahoo Finance API. In this app, users not only could see daily stock history but also stock price predictions in real time. The mobile app accuracy prediction give the better result than the common algorithm with the value are 15087.465 in MSE, 122.831 in RMSE, and 3.255 in MAPE.},
	journal = {Proceedings - 2017 International Conference on Sustainable Information Engineering and Technology, SIET 2017},
	author = {Izzah, Abidatul and Sari, Yuita Arum and Widyastuti, Ratna and Cinderatama, Toga Aldila},
	year = {2018},
	note = {ISBN: 9781538621820},
	keywords = {Prediction, Regression, stock price prediction, Moving Average, Stock, Android, Mobile App},
	pages = {150--154},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AAP733IP\\Mobile app for stock prediction using Improved Multiple Linear Regression.pdf:application/pdf},
}

@article{Alizadeh2014,
	title = {{CONGA}: {Distributed} {Congestion}-{Aware} {Load} {Balancing} for {Datacenters}},
	url = {http://dx.doi.org/10.1145/2619239.2626316},
	doi = {10.1145/2619239.2626316},
	abstract = {We present the design, implementation, and evaluation of CONGA, a network-based distributed congestion-aware load balancing mechanism for datacenters. CONGA exploits recent trends including the use of regular Clos topologies and overlays for network vir-tualization. It splits TCP flows into flowlets, estimates real-time congestion on fabric paths, and allocates flowlets to paths based on feedback from remote switches. This enables CONGA to efficiently balance load and seamlessly handle asymmetry, without requiring any TCP modifications. CONGA has been implemented in custom ASICs as part of a new datacenter fabric. In testbed experiments , CONGA has 5× better flow completion times than ECMP even with a single link failure and achieves 2-8× better through-put than MPTCP in Incast scenarios. Further, the Price of Anarchy for CONGA is provably small in Leaf-Spine topologies; hence CONGA is nearly as effective as a centralized scheduler while being able to react to congestion in microseconds. Our main thesis is that datacenter fabric load balancing is best done in the network, and requires global schemes such as CONGA to handle asymmetry.},
	urldate = {2020-12-27},
	author = {Alizadeh, Mohammad and Edsall, Tom and Dharmapurikar, Sarang and Vaidyanathan, Ramanan and Chu, Kevin and Fingerhut, Andy and The Lam, Vinh and Matus, Francis and Pan, Rong and Yadav, Navindra and Varghese, George},
	year = {2014},
	note = {ISBN: 9781450328364},
	keywords = {C21 [Computer-Communication Networks]: Network Arc, Distributed, Load balancing},
}

@article{Delgosha2020,
	title = {{EM} {Algorithm} on the {Topic} {Model} in {Matrix} {Form} {Review} of {EM} update formulas for the topic model {E}-step in {Matrix} {Form}},
	author = {Delgosha, Professors Payam and Morales, Marco and Forsyth, David and Saleh, Ehsan},
	year = {2020},
	pages = {1--4},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LAKQ8EZU\\EMTopicModel.pdf:application/pdf},
}

@article{Ng2000c,
	title = {9 - {The} {EM} algorithm},
	volume = {1},
	abstract = {The EM algorithm},
	number = {X},
	journal = {CS229 Lecture notes},
	author = {Ng, Andrew},
	year = {2000},
	pages = {1--8},
}

@article{Sridharan2017,
	title = {Gaussian mixture models and the {EM} {Algorithm} ({Lecture} {Notes} - {MIT} {CSAIL})},
	author = {Sridharan, R.},
	year = {2017},
	pages = {11},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6Q2MBZV7\\Gaussian mixture models and the EM algorithm.pdf:},
}

@article{Raykov2016a,
	title = {What to do when {K}-means clustering fails: {A} simple yet principled alternative algorithm},
	volume = {11},
	issn = {19326203},
	doi = {10.1371/journal.pone.0162259},
	abstract = {The K-means algorithm is one of the most popular clustering algorithms in current use as it is relatively fast yet simple to understand and deploy in practice. Nevertheless, its use entails certain restrictive assumptions about the data, the negative consequences of which are not always immediately apparent, as we demonstrate. While more flexible algorithms have been developed, their widespread use has been hindered by their computational and technical complexity. Motivated by these considerations, we present a flexible alternative to K-means that relaxes most of the assumptions, whilst remaining almost as fast and simple. This novel algorithm which we call MAP-DP (maximum a-posteriori Dirichlet process mixtures), is statistically rigorous as it is based on nonparametric Bayesian Dirichlet process mixture modeling. This approach allows us to overcome most of the limitations imposed by K-means. The number of clusters K is estimated from the data instead of being fixed a-priori as in K-means. In addition, while K-means is restricted to continuous data, the MAP-DP framework can be applied to many kinds of data, for example, binary, count or ordinal data. Also, it can efficiently separate outliers from the data. This additional flexibility does not incur a significant computational overhead compared to K-means with MAP-DP convergence typically achieved in the order of seconds for many practical problems. Finally, in contrast to K-means, since the algorithm is based on an underlying statistical model, the MAP-DP framework can deal with missing data and enables model testing such as cross validation in a principled way. We demonstrate the simplicity and effectiveness of this algorithm on the health informatics problem of clinical sub-typing in a cluster of diseases known as parkinsonism.},
	number = {9},
	journal = {PLoS ONE},
	author = {Raykov, Yordan P. and Boukouvalas, Alexis and Baig, Fahd and Little, Max A.},
	year = {2016},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8UTJ7XHH\\What to Do When K-Means Clustering Fails_ A Simple yet Principled Alternative Algorithm.pdf:application/pdf},
}

@article{Patel2020a,
	title = {Clustering {Cloud} {Workloads}: {K}-{Means} vs {Gaussian} {Mixture} {Model}},
	volume = {171},
	issn = {18770509},
	url = {https://doi.org/10.1016/j.procs.2020.04.017},
	doi = {10.1016/j.procs.2020.04.017},
	abstract = {The growing heterogeneity due to diverse Cloud workloads such as Big Data, IoT and Business Data analytics, requires precise characterization to design a successful capacity plan and maintain the competitiveness of Cloud service providers. K-Means is a simple and fast clustering method, but it may not truly capture heterogeneity inherent in Cloud workloads. Gaussian Mixture Models can discover complex patterns and group them into cohesive, homogeneous components that are close representatives of real patterns within the data set. This work compares K-Means and Gaussian Mixture Model to evaluate cluster representativeness of the two methods for heterogeneity in resource usage of Cloud workloads. Experiments conducted with Google cluster trace and business critical workloads by Bitbrains reveal that clusters obtained using K-Means give a very abstracted information. Gaussian Mixture Model provides better clustering with distinct usage boundaries. Although, Gaussian Mixture Model has higher computation time than K-Means, it can be used when more fine-grained workload characterization and analysis is required.},
	number = {2019},
	journal = {Procedia Computer Science},
	author = {Patel, Eva and Kushwaha, Dharmender Singh},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Clustering, Cloud Computing, Expectation-Maximization, Gaussian Mixture Model, K-Means Clustering, Workload characterization},
	pages = {158--167},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IR7NPILT\\Clusterng Cloud Workloads - K-Means vs Gaussian Mixture Model.pdf:application/pdf},
}

@article{Andreyev2014a,
	title = {Introducing data center fabric, the next-generation {Facebook} data center network},
	url = {https://code.facebook.com/posts/360346274145943/introducing-data-center-fabric-the-next-generation-facebook-data-center-network/},
	abstract = {The more than 1.35 billion people who use Facebook on an ongoing basis rely on a seamless, “always on” site performance. On the back end, we have many advanced sub-systems and infrastructures in place that make such a real-time experience possible, and our scalable, high-performance network is one of them.},
	number = {November 2014},
	journal = {Facebook},
	author = {Andreyev, Alexey},
	year = {2014},
	note = {ISBN: 3603462741459},
	pages = {1--16},
	file = {Andreyev2014a-zotero.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Andreyev2014a-zotero.md:text/plain;Introducing data center fabric, the next-generation Facebook data center network - Literature Note.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Introducing data center fabric, the next-generation Facebook data center network - Literature Note.md:text/plain;Introducing data center fabric, the next-generation Facebook data center network-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Introducing data center fabric, the next-generation Facebook data center network-metadata.md:text/plain;PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\49DDVP3N\\Introducing data center fabric, the next-generation Facebook data center network - Facebook Engineering.pdf:application/pdf},
}

@article{Saleh2010,
	title = {Guidelines for effort and cost allocation in medium to large software development projects},
	issn = {17924863},
	abstract = {The proper allocation of financial and human resources to the various software development activities is a very important and critical task. To provide a realistic allocation, the project manager of a software development project should account for the various activities needed to ensure the completion of the project with the required quality and on-time and within-budget. In this paper, we provide guideline for cost and effort allocation based on typical software development activities and using existing requirements-based estimation techniques.},
	number = {October},
	journal = {International Conference on Applied Computer Science - Proceedings},
	author = {Saleh, Kassem},
	year = {2010},
	note = {ISBN: 9789604742318},
	keywords = {Estimation, Resource allocation, Software project},
	pages = {33--34},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YGYHIUBC\\Guidelines_for_Effort_and_Cost_Allocation_in_Mediu.pdf:application/pdf},
}

@article{Mackey2014a,
	title = {Lecture 3 — {April} 7th {Recap} : {Gaussian} {Mixture} {Modeling} {EM} for {General} {Latent} {Variable} {Models}},
	volume = {1},
	author = {Mackey, Lecturer Lester},
	year = {2014},
	pages = {1--8},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SJMIGUX5\\Guassian Mixture Models (Stanford Stat306b).pdf:application/pdf},
}

@article{Mackey2014,
	title = {Lecture 2 — {April} 2 {Gaussian} {Mixture} {Modeling} {Clustering} with {GMMs}},
	volume = {1},
	author = {Mackey, Lecturer Lester},
	year = {2014},
	pages = {1--6},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N7D9IX82\\Gaussian Mixture Models, Expectation Maximiation, Clustering (Stanford Stat306b).pdf:application/pdf},
}

@article{Jaakkola2006,
	title = {Lecture 17 - {Mixture} models and clustering},
	volume = {17},
	url = {http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/},
	number = {1},
	author = {Jaakkola, Tommi},
	year = {2006},
	pages = {1--8},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EYVRLNN7\\Mixture Models and Clustering.pdf:application/pdf},
}

@article{boudreault-fournierConga2019,
	title = {Conga},
	doi = {10.4324/9780429456961-6},
	abstract = {Today's data centers offer tremendous aggregate bandwidth to clusters of tens of thousands of machines. However, because of limited port densities in even the highest-end switches, data center topologies typically consist of multi-rooted trees with many equal-cost paths between any given pair of hosts. Existing IP multipathing protocols usually rely on per-flow static hashing and can cause substantial bandwidth losses due to longterm collisions. In this paper, we present Hedera, a scalable, dynamic flow scheduling system that adaptively schedules a multi-stage switching fabric to efficiently utilize aggregate network resources. We describe our implementation using commodity switches and unmodified hosts, and show that for a simulated 8,192 host data center, Hedera delivers bisection bandwidth that is 96\% of optimal and up to 113\% better than static load-balancing methods.},
	journal = {Aerial Imagination in Cuba},
	author = {Boudreault-Fournier, Alexandrine and Fernández Lavado, José Manuel and Boudreault-Fournier, Alexandrine and Fernández Lavado, José Manuel},
	year = {2019},
	note = {ISBN: 9781450328364},
	keywords = {distributed, datacenter fabric, load balancing},
	pages = {84--100},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XKH2EDRA\\CONGA - Distributed Congestion-Aware Load Balancing for Data Centers.pdf:application/pdf},
}

@article{dunleavyOrganizingChapterPaper2003,
	title = {Organizing a {Chapter} or {Paper}: the {Micro}-{Structure}},
	doi = {10.1007/978-0-230-80208-7_4},
	abstract = {Authoring a PhD involves having creative ideas, working out how to organize them, writing up from plans, upgrading text, and finishing it speedily and to a good standard. It also involves being examined and getting work published. This book provides a huge range of ideas and suggestions to help PhD candidates cope with both the intellectual issues involved and the practical difficulties of organizing their work effectively.},
	journal = {Authoring a PhD},
	author = {Dunleavy, Patrick and Dunleavy, Patrick},
	year = {2003},
	pages = {76--102},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IY4ZRSRM\\Authoring a PhD.pdf:application/pdf},
}

@book{maniQuantumOptimizationMachine2020,
	title = {Quantum optimization for machine learning},
	isbn = {978-3-11-067070-7},
	abstract = {Machine learning is a branch of Artificial Intelligence that seeks to make machines learn from data. It is being applied for solving real world problems with huge amount of data. Though, Machine Learning is receiving wide acceptance, however, execution time is one of the major concerns in practical implementations of Machine Learning techniques. It largely comprises of a set of techniques that trains a model by reducing the error between the desired or actual outcome and an estimated or predicted outcome, which is often called as loss function. Thus, training in machine learning techniques often requires solving a difficult optimization problem, which is the most expensive step in the entire model-building process and its applications. One of the possible solutions in near future for reducing execution time of training process in Machine learning techniques is to implement them on quantum computers instead of classical computers. It is conjectured that quantum computers may be exponentially faster than classical computers for solving problems which involve matrix operations. Some of the machine learning techniques like support vector machines make extensive use of matrices, which can be made faster by implementing them on quantum computers. However, their efficient implementation is non-trivial and requires existence of quantum memories. Thus, another possible solution in near term is to use a hybrid of Classical Quantum approach, where a machine learning model is implemented in classical computer but the optimization of loss function during training is performed on quantum computer instead of classical computer. Several Quantum optimization algorithms have been proposed in recent years, which can be classified as gradient based and gradient free optimization techniques. Gradient based techniques require the nature of optimization problem being solved to be convex, continuous and differentiable otherwise if the problem is non-convex then they can find local optima only whereas gradient free optimization techniques work well even with non-continuous, non-linear and nonconvex optimization problems. This chapter discusses a global optimization technique based on Adiabatic Quantum Computation (AQC) to solve minimization of loss function without any restriction on its structure and the underlying model, which is being learned. Further, it is also shown that in the proposed framework, AQC based approach would be superior to circuit-based approach in solving global optimization problems.},
	author = {Mani, Ashish and Bhattacharyya, Siddhartha and Chatterjee, Amlan},
	year = {2020},
	doi = {10.1515/9783110670707-003},
	note = {Publication Title: Quantum Machine Learning},
	keywords = {Non-convex optimization, Artificial Intelligence, Quantum computing},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8ULTDT8W\\Optimization for Machine Learning [Sra, Nowozin & Wright 2011-09-30].pdf:application/pdf},
}

@book{forsythAppliedMachineLearning2019,
	address = {Cham},
	title = {Applied {Machine} {Learning}},
	isbn = {978-3-030-18113-0 978-3-030-18114-7},
	url = {http://link.springer.com/10.1007/978-3-030-18114-7},
	language = {en},
	urldate = {2020-12-28},
	publisher = {Springer International Publishing},
	author = {Forsyth, David},
	year = {2019},
	doi = {10.1007/978-3-030-18114-7_15},
	file = {Applied Machine Learning - Literature Note.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Applied Machine Learning - Literature Note.md:text/plain;Applied Machine Learning-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Applied Machine Learning-metadata.md:text/plain;Forsyth - 2019 - Applied Machine Learning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DC22WNER\\Forsyth - 2019 - Applied Machine Learning.pdf:application/pdf},
}

@incollection{forsythMeanFieldInference2019,
	address = {Cham},
	title = {Mean {Field} {Inference}},
	isbn = {978-3-030-18113-0 978-3-030-18114-7},
	url = {http://link.springer.com/10.1007/978-3-030-18114-7_15},
	language = {en},
	urldate = {2020-12-28},
	booktitle = {Applied {Machine} {Learning}},
	publisher = {Springer International Publishing},
	author = {Forsyth, David},
	collaborator = {Forsyth, David},
	year = {2019},
	doi = {10.1007/978-3-030-18114-7_15},
	pages = {351--364},
	file = {Mean Field Inference - Literature Note.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Mean Field Inference - Literature Note.md:text/plain;Mean Field Inference-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Mean Field Inference-metadata.md:text/plain},
}

@inproceedings{singhJupiterRisingDecade2015,
	address = {London United Kingdom},
	title = {Jupiter {Rising}: {A} {Decade} of {Clos} {Topologies} and {Centralized} {Control} in {Google}'s {Datacenter} {Network}},
	isbn = {978-1-4503-3542-3},
	shorttitle = {Jupiter {Rising}},
	url = {https://dl.acm.org/doi/10.1145/2785956.2787508},
	doi = {10.1145/2785956.2787508},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {Proceedings of the 2015 {ACM} {Conference} on {Special} {Interest} {Group} on {Data} {Communication}},
	publisher = {ACM},
	author = {Singh, Arjun and Ong, Joon and Agarwal, Amit and Anderson, Glen and Armistead, Ashby and Bannon, Roy and Boving, Seb and Desai, Gaurav and Felderman, Bob and Germano, Paulie and Kanagala, Anand and Provost, Jeff and Simmons, Jason and Tanda, Eiichi and Wanderer, Jim and Hölzle, Urs and Stuart, Stephen and Vahdat, Amin},
	month = aug,
	year = {2015},
	pages = {183--197},
	file = {Jupiter Rising A Decade of Clos Topologies and Centralized Control in Google's Datacenter Network - Literature Note.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Jupiter Rising A Decade of Clos Topologies and Centralized Control in Google's Datacenter Network - Literature Note.md:text/plain;Jupiter Rising A Decade of Clos Topologies and Centralized Control in Google's Datacenter Network-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Jupiter Rising A Decade of Clos Topologies and Centralized Control in Google's Datacenter Network-metadata.md:text/plain;Jupiter Rising A Decade of Clos Topologies and Centralized Control in Google's Datacenter Network-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Jupiter Rising A Decade of Clos Topologies and Centralized Control in Google's Datacenter Network-metadata.md:text/plain;Jupiter Rising A Decade of Clos Topologies and Centralized Control in Google's Datacenter Network-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Jupiter Rising A Decade of Clos Topologies and Centralized Control in Google's Datacenter Network-metadata.md:text/plain;Jupiter Rising.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Jupiter Rising.pdf:application/pdf},
}

@techreport{lapukhovUseBGPRouting2016,
	title = {Use of {BGP} for {Routing} in {Large}-{Scale} {Data} {Centers}},
	url = {https://www.rfc-editor.org/info/rfc7938},
	language = {en},
	number = {RFC7938},
	urldate = {2020-12-31},
	institution = {RFC Editor},
	author = {Lapukhov, P. and Premji, A. and Mitchell, J.},
	month = aug,
	year = {2016},
	doi = {10.17487/RFC7938},
	pages = {RFC7938},
	file = {Use of BGP for Routing in Large-Scale Data Centers - Literature Note.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Use of BGP for Routing in Large-Scale Data Centers - Literature Note.md:text/plain;Use of BGP for Routing in Large-Scale Data Centers-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Use of BGP for Routing in Large-Scale Data Centers-metadata.md:text/plain;Use of BGP for Routing in Large-Scale Data Centers.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Use of BGP for Routing in Large-Scale Data Centers.pdf:application/pdf},
}

@article{montufarRestrictedBoltzmannMachines2018,
	title = {Restricted {Boltzmann} {Machines}: {Introduction} and {Review}},
	shorttitle = {Restricted {Boltzmann} {Machines}},
	url = {http://arxiv.org/abs/1806.07066},
	abstract = {The restricted Boltzmann machine is a network of stochastic units with undirected interactions between pairs of visible and hidden units. This model was popularized as a building block of deep learning architectures and has continued to play an important role in applied and theoretical machine learning. Restricted Boltzmann machines carry a rich structure, with connections to geometry, applied algebra, probability, statistics, machine learning, and other areas. The analysis of these models is attractive in its own right and also as a platform to combine and generalize mathematical tools for graphical models with hidden variables. This article gives an introduction to the mathematical analysis of restricted Boltzmann machines, reviews recent results on the geometry of the sets of probability distributions representable by these models, and suggests a few directions for further investigation.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:1806.07066 [cs, math, stat]},
	author = {Montufar, Guido},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.07066},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning},
	file = {Montufar - 2018 - Restricted Boltzmann Machines Introduction and Re.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\76LWAHVT\\Montufar - 2018 - Restricted Boltzmann Machines Introduction and Re.pdf:application/pdf;Restricted Boltzmann Machines Introduction and Review-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Restricted Boltzmann Machines Introduction and Review-metadata.md:text/plain},
}

@book{kittlerProgressPatternRecognition2015,
	address = {Cham},
	edition = {1st ed. 2015},
	series = {Image {Processing}, {Computer} {Vision}, {Pattern} {Recognition}, and {Graphics}},
	title = {Progress in {Pattern} {Recognition}, {Image} {Analysis}, {Computer} {Vision}, and {Applications}: 20th {Iberoamerican} {Congress}, {CIARP} 2015, {Montevideo}, {Uruguay}, {November} 9-12, 2015, {Proceedings}},
	isbn = {978-3-319-25751-8},
	shorttitle = {Progress in {Pattern} {Recognition}, {Image} {Analysis}, {Computer} {Vision}, and {Applications}},
	abstract = {This book constitutes the refereed proceedings of the 20th Iberoamerican Congress on Pattern Recognition, CIARP 2015, held in Montevideo, Uruguay, in November 2015. The 95 papers presented were carefully reviewed and selected from 185 submissions. The papers are organized in topical sections on applications on pattern recognition; biometrics; computer vision; gesture recognition; image classification and retrieval; image coding, processing and analysis; segmentation, analysis of shape and texture; signals analysis and processing; theory of pattern recognition; video analysis, segmentation and tracking},
	number = {9423},
	publisher = {Springer International Publishing : Imprint: Springer},
	editor = {Kittler, Josef and Pardo, Alvaro},
	year = {2015},
	doi = {10.1007/978-3-319-25751-8},
	keywords = {Algorithms, Artificial intelligence, Artificial Intelligence, Algorithm Analysis and Problem Complexity, Application software, Biometrics, Biometrics (Biology), Image Processing and Computer Vision, Information Systems Applications (incl. Internet), Optical data processing, Pattern recognition, Pattern Recognition},
	file = {Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications 20th Iberoamerican Congress, CIARP 2015, Montevideo, Uruguay, November 9-12, 2015, Proceedings-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications 20th Iberoamerican Congress, CIARP 2015, Montevideo, Uruguay, November 9-12, 2015, Proceedings-metadata.md:text/plain;Progress in Pattern Recognition, Image Analysis, Computer Vision, and.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Progress in Pattern Recognition, Image Analysis, Computer Vision, and.pdf:application/pdf},
}

@book{aggarwalLinearAlgebraOptimization2020,
	title = {Linear algebra and optimization for machine learning a textbook},
	isbn = {978-3-030-40343-0},
	language = {English},
	author = {Aggarwal, Charu C},
	year = {2020},
	note = {OCLC: 1157208529},
}

@article{abramsonPatternRecognitionMachine1963,
	title = {Pattern recognition and machine learning},
	volume = {9},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1057854/},
	doi = {10.1109/TIT.1963.1057854},
	language = {en},
	number = {4},
	urldate = {2020-12-31},
	journal = {IEEE Transactions on Information Theory},
	author = {Abramson, N. and Braverman, D. and Sebestyen, G.},
	month = oct,
	year = {1963},
	note = {tex.ids: Bishop},
	pages = {257--261},
	file = {Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ED7782PZ\\Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf:application/pdf},
}

@article{bleiVariationalInferenceReview2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	number = {518},
	urldate = {2021-01-01},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	pages = {859--877},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4UGYP4HZ\\1601.html:text/html;Variational Inference A Review for Statisticians-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Variational Inference A Review for Statisticians-metadata.md:text/plain;Variational Inference.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Variational Inference.pdf:application/pdf},
}

@incollection{forsythSimpleNeuralNetworks2019,
	address = {Cham},
	title = {Simple {Neural} {Networks}},
	isbn = {978-3-030-18113-0 978-3-030-18114-7},
	url = {http://link.springer.com/10.1007/978-3-030-18114-7_16},
	language = {en},
	urldate = {2021-01-04},
	booktitle = {Applied {Machine} {Learning}},
	publisher = {Springer International Publishing},
	author = {Forsyth, David},
	collaborator = {Forsyth, David},
	year = {2019},
	doi = {10.1007/978-3-030-18114-7_16},
	pages = {367--398},
	file = {Simple Neural Networks-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Simple Neural Networks-metadata.md:text/plain},
}

@article{lecunGradientbasedLearningApplied1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/726791/},
	doi = {10.1109/5.726791},
	number = {11},
	urldate = {2021-01-04},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	pages = {2278--2324},
	file = {Gradient-based learning applied to document recognition-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Gradient-based learning applied to document recognition-metadata.md:text/plain;Gradient-based learning applied to document recognition.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Gradient-based learning applied to document recognition.pdf:application/pdf},
}

@article{ioffeBatchNormalizationAccelerating2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2021-01-04},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NGUSWGD4\\1502.html:text/html;Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift-metadata.md:text/plain;Batch Normalization.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Batch Normalization.pdf:application/pdf},
}

@article{russakovskyImageNetLargeScale2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	url = {http://arxiv.org/abs/1409.0575},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
	urldate = {2021-01-04},
	journal = {arXiv:1409.0575 [cs]},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = jan,
	year = {2015},
	note = {arXiv: 1409.0575},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.8, I.5.2},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\R69EDYRN\\1409.html:text/html;ImageNet Large Scale Visual Recognition Challenge-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\ImageNet Large Scale Visual Recognition Challenge-metadata.md:text/plain;ImageNet Large Scale Visual Recognition Challenge.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf},
}

@article{simonyanVeryDeepConvolutional2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2021-01-04},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3YPUCEL3\\1409.html:text/html;Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JC8ZB4H3\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;Very Deep Convolutional Networks for Large-Scale Image Recognition-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Very Deep Convolutional Networks for Large-Scale Image Recognition-metadata.md:text/plain;Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:application/pdf},
}

@article{girshickRichFeatureHierarchies2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	urldate = {2021-01-04},
	journal = {arXiv:1311.2524 [cs]},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv: 1311.2524},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\USHRWXZA\\1311.html:text/html;Rich feature hierarchies for accurate object detection and semantic segmentation-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Rich feature hierarchies for accurate object detection and semantic segmentation-metadata.md:text/plain;Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:application/pdf},
}

@article{girshickFastRCNN2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2021-01-04},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CN3ULSMK\\1504.html:text/html;Fast R-CNN-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Fast R-CNN-metadata.md:text/plain;Fast R-CNN.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Fast R-CNN.pdf:application/pdf},
}

@article{renFasterRCNNRealTime2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2021-01-04},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KSP85CR9\\1506.html:text/html;Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks-metadata.md:text/plain;Faster R-CNN.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Faster R-CNN.pdf:application/pdf},
}

@article{redmonYouOnlyLook2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://arxiv.org/abs/1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	urldate = {2021-01-04},
	journal = {arXiv:1506.02640 [cs]},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = may,
	year = {2016},
	note = {arXiv: 1506.02640},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BNHU3CH2\\1506.html:text/html;You Only Look Once Unified, Real-Time Object Detection-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\You Only Look Once Unified, Real-Time Object Detection-metadata.md:text/plain;You Only Look Once.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\You Only Look Once.pdf:application/pdf},
}

@inproceedings{sobrinhoConvergencePathVector2001,
	address = {Dallas, TX, USA},
	title = {On the convergence of path vector routing protocols},
	isbn = {978-0-7803-6711-1},
	url = {http://ieeexplore.ieee.org/document/923649/},
	doi = {10.1109/HPSR.2001.923649},
	urldate = {2021-01-07},
	booktitle = {2001 {IEEE} {Workshop} on {High} {Performance} {Switching} and {Routing} ({IEEE} {Cat}. {No}.{01TH8552})},
	publisher = {IEEE},
	author = {Sobrinho, J.L.},
	year = {2001},
	pages = {292--296},
}

@inproceedings{chandrashokarLimitingPathExploration2005,
	address = {Miami, FL, USA},
	title = {Limiting path exploration in {GBP}},
	volume = {4},
	isbn = {978-0-7803-8968-7},
	url = {http://ieeexplore.ieee.org/document/1498520/},
	doi = {10.1109/INFCOM.2005.1498520},
	urldate = {2021-01-15},
	booktitle = {Proceedings {IEEE} 24th {Annual} {Joint} {Conference} of the {IEEE} {Computer} and {Communications} {Societies}.},
	publisher = {IEEE},
	author = {Chandrashokar, J. and {Zhenhai Duan} and {Zhi-Li Zhang} and Krasky, J.},
	year = {2005},
	pages = {2337--2348},
	file = {Limiting path exploration in GBP - Literature Note.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Limiting path exploration in GBP - Literature Note.md:text/plain;Limiting path exploration in GBP-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Limiting path exploration in GBP-metadata.md:text/plain},
}

@article{choiUsingRecurrentNeural2017,
	title = {Using recurrent neural network models for early detection of heart failure onset},
	volume = {24},
	issn = {1067-5027, 1527-974X},
	url = {https://academic.oup.com/jamia/article/24/2/361/2631499},
	doi = {10.1093/jamia/ocw112},
	abstract = {Objective: We explored whether use of deep learning to model temporal relations among events in electronic health records (EHRs) would improve model performance in predicting initial diagnosis of heart failure (HF) compared to conventional methods that ignore temporality.
            Materials and Methods: Data were from a health system’s EHR on 3884 incident HF cases and 28 903 controls, identified as primary care patients, between May 16, 2000, and May 23, 2013. Recurrent neural network (RNN) models using gated recurrent units (GRUs) were adapted to detect relations among time-stamped events (eg, disease diagnosis, medication orders, procedure orders, etc.) with a 12- to 18-month observation window of cases and controls. Model performance metrics were compared to regularized logistic regression, neural network, support vector machine, and K-nearest neighbor classifier approaches.
            Results: Using a 12-month observation window, the area under the curve (AUC) for the RNN model was 0.777, compared to AUCs for logistic regression (0.747), multilayer perceptron (MLP) with 1 hidden layer (0.765), support vector machine (SVM) (0.743), and K-nearest neighbor (KNN) (0.730). When using an 18-month observation window, the AUC for the RNN model increased to 0.883 and was significantly higher than the 0.834 AUC for the best of the baseline methods (MLP).
            Conclusion: Deep learning models adapted to leverage temporal relations appear to improve performance of models for detection of incident heart failure with a short observation window of 12–18 months.},
	language = {en},
	number = {2},
	urldate = {2021-01-27},
	journal = {Journal of the American Medical Informatics Association},
	author = {Choi, Edward and Schuetz, Andy and Stewart, Walter F and Sun, Jimeng},
	month = mar,
	year = {2017},
	pages = {361--370},
	file = {Using recurrent neural network models for early detection of heart failure onset - Literature Note.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\MCS\\Deep Learning for Healthcare\\Using recurrent neural network models for early detection of heart failure onset - Literature Note.md:text/plain;Using recurrent neural network models for early detection of heart failure onset-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Using recurrent neural network models for early detection of heart failure onset-metadata.md:text/plain},
}

@article{huangCASTERPredictingDrug2019,
	title = {{CASTER}: {Predicting} {Drug} {Interactions} with {Chemical} {Substructure} {Representation}},
	shorttitle = {{CASTER}},
	url = {http://arxiv.org/abs/1911.06446},
	abstract = {Adverse drug-drug interactions (DDIs) remain a leading cause of morbidity and mortality. Identifying potential DDIs during the drug design process is critical for patients and society. Although several computational models have been proposed for DDI prediction, there are still limitations: (1) specialized design of drug representation for DDI predictions is lacking; (2) predictions are based on limited labelled data and do not generalize well to unseen drugs or DDIs; and (3) models are characterized by a large number of parameters, thus are hard to interpret. In this work, we develop a ChemicAl SubstrucTurE Representation (CASTER) framework that predicts DDIs given chemical structures of drugs.CASTER aims to mitigate these limitations via (1) a sequential pattern mining module rooted in the DDI mechanism to efficiently characterize functional sub-structures of drugs; (2) an auto-encoding module that leverages both labelled and unlabelled chemical structure data to improve predictive accuracy and generalizability; and (3) a dictionary learning module that explains the prediction via a small set of coefficients which measure the relevance of each input sub-structures to the DDI outcome. We evaluated CASTER on two real-world DDI datasets and showed that it performed better than state-of-the-art baselines and provided interpretable predictions.},
	urldate = {2021-01-29},
	journal = {arXiv:1911.06446 [cs, q-bio, stat]},
	author = {Huang, Kexin and Xiao, Cao and Hoang, Trong Nghia and Glass, Lucas M. and Sun, Jimeng},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.06446},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Quantitative Methods},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\467TSAF9\\1911.html:text/html;CASTER Predicting Drug Interactions with Chemical Substructure Representation-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\CASTER Predicting Drug Interactions with Chemical Substructure Representation-metadata.md:text/plain;CASTER.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\CASTER.pdf:application/pdf},
}

@article{shangGAMENetGraphAugmented2019,
	title = {{GAMENet}: {Graph} {Augmented} {MEmory} {Networks} for {Recommending} {Medication} {Combination}},
	shorttitle = {{GAMENet}},
	url = {http://arxiv.org/abs/1809.01852},
	abstract = {Recent progress in deep learning is revolutionizing the healthcare domain including providing solutions to medication recommendations, especially recommending medication combination for patients with complex health conditions. Existing approaches either do not customize based on patient health history, or ignore existing knowledge on drug-drug interactions (DDI) that might lead to adverse outcomes. To fill this gap, we propose the Graph Augmented Memory Networks (GAMENet), which integrates the drug-drug interactions knowledge graph by a memory module implemented as a graph convolutional networks, and models longitudinal patient records as the query. It is trained end-to-end to provide safe and personalized recommendation of medication combination. We demonstrate the effectiveness and safety of GAMENet by comparing with several state-of-the-art methods on real EHR data. GAMENet outperformed all baselines in all effectiveness measures, and also achieved 3.60\% DDI rate reduction from existing EHR data.},
	urldate = {2021-01-29},
	journal = {arXiv:1809.01852 [cs, stat]},
	author = {Shang, Junyuan and Xiao, Cao and Ma, Tengfei and Li, Hongyan and Sun, Jimeng},
	month = mar,
	year = {2019},
	note = {arXiv: 1809.01852},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7PCP7IMJ\\1809.html:text/html;GAMENet Graph Augmented MEmory Networks for Recommending Medication Combination-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\GAMENet Graph Augmented MEmory Networks for Recommending Medication Combination-metadata.md:text/plain;GAMENet.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\GAMENet.pdf:application/pdf},
}

@inproceedings{zhangLEAPLearningPrescribe2017,
	address = {Halifax NS Canada},
	title = {{LEAP}: {Learning} to {Prescribe} {Effective} and {Safe} {Treatment} {Combinations} for {Multimorbidity}},
	isbn = {978-1-4503-4887-4},
	shorttitle = {{LEAP}},
	url = {https://dl.acm.org/doi/10.1145/3097983.3098109},
	doi = {10.1145/3097983.3098109},
	language = {en},
	urldate = {2021-01-29},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Zhang, Yutao and Chen, Robert and Tang, Jie and Stewart, Walter F. and Sun, Jimeng},
	month = aug,
	year = {2017},
	pages = {1315--1324},
	file = {LEAP Learning to Prescribe Effective and Safe Treatment Combinations for Multimorbidity-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\LEAP Learning to Prescribe Effective and Safe Treatment Combinations for Multimorbidity-metadata.md:text/plain},
}

@article{fuCOREAutomaticMolecule2020,
	title = {{CORE}: {Automatic} {Molecule} {Optimization} {Using} {Copy} \& {Refine} {Strategy}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{CORE}},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/5404},
	doi = {10.1609/aaai.v34i01.5404},
	abstract = {Molecule optimization is about generating molecule Y with more desirable properties based on an input molecule X. The state-of-the-art approaches partition the molecules into a large set of substructures S and grow the new molecule structure by iteratively predicting which substructure from S to add. However, since the set of available substructures S is large, such an iterative prediction task is often inaccurate especially for substructures that are infrequent in the training data. To address this challenge, we propose a new generating strategy called “Copy\&Refine” (CORE), where at each step the generator first decides whether to copy an existing substructure from input X or to generate a new substructure, then the most promising substructure will be added to the new molecule. Combining together with scaffolding tree generation and adversarial training, CORE can significantly improve several latest molecule optimization methods in various measures including drug likeness (QED), dopamine receptor (DRD2) and penalized LogP. We tested CORE and baselines using the ZINC database and CORE obtained up to 11\% and 21\% relatively improvement over the baselines on success rate on the complete test set and the subset with infrequent substructures, respectively.},
	number = {01},
	urldate = {2021-01-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Fu, Tianfan and Xiao, Cao and Sun, Jimeng},
	month = apr,
	year = {2020},
	pages = {638--645},
	file = {CORE Automatic Molecule Optimization Using Copy & Refine Strategy-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\CORE Automatic Molecule Optimization Using Copy & Refine Strategy-metadata.md:text/plain;CORE.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\CORE.pdf:application/pdf},
}

@article{biswalDoctor2VecDynamicDoctor2020,
	title = {{Doctor2Vec}: {Dynamic} {Doctor} {Representation} {Learning} for {Clinical} {Trial} {Recruitment}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{Doctor2Vec}},
	url = {https://www.aiide.org/ojs/index.php/AAAI/article/view/5394},
	doi = {10.1609/aaai.v34i01.5394},
	abstract = {Massive electronic health records (EHRs) enable the success of learning accurate patient representations to support various predictive health applications. In contrast, doctor representation was not well studied despite that doctors play pivotal roles in healthcare. How to construct the right doctor representations? How to use doctor representation to solve important health analytic problems? In this work, we study the problem on clinical trial recruitment, which is about identifying the right doctors to help conduct the trials based on the trial description and patient EHR data of those doctors. We propose Doctor2Vec which simultaneously learns 1) doctor representations from EHR data and 2) trial representations from the description and categorical information about the trials. In particular, Doctor2Vec utilizes a dynamic memory network where the doctor's experience with patients are stored in the memory bank and the network will dynamically assign weights based on the trial representation via an attention mechanism. Validated on large real-world trials and EHR data including 2,609 trials, 25K doctors and 430K patients, Doctor2Vec demonstrated improved performance over the best baseline by up to 8.7\% in PR-AUC. We also demonstrated that the Doctor2Vec embedding can be transferred to benefit data insufficiency settings including trial recruitment in less populated/newly explored country with 13.7\% improvement or for rare diseases with 8.1\% improvement in PR-AUC.},
	number = {01},
	urldate = {2021-01-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Biswal, Siddharth and Xiao, Cao and Glass, Lucas M. and Milkovits, Elizabeth and Sun, Jimeng},
	month = apr,
	year = {2020},
	pages = {557--564},
	file = {Doctor2Vec Dynamic Doctor Representation Learning for Clinical Trial Recruitment-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Doctor2Vec Dynamic Doctor Representation Learning for Clinical Trial Recruitment-metadata.md:text/plain;Doctor2Vec.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Doctor2Vec.pdf:application/pdf},
}

@misc{HowHasSpending,
	title = {How has {U}.{S}. spending on healthcare changed over time?},
	url = {https://www.healthsystemtracker.org/chart-collection/u-s-spending-healthcare-changed-time/},
	abstract = {This chart collection explores recently released National Health Expenditure (NHE) data from the Centers for Medicare and Medicaid Services. These data offer insight into changes in health spending over time as well as the driving forces behind spending growth.},
	language = {en-US},
	urldate = {2021-01-29},
	journal = {Peterson-KFF Health System Tracker},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6ER5FLTF\\u-s-spending-healthcare-changed-time.html:text/html},
}

@misc{HealthCareGlobal,
	title = {U.{S}. {Health} {Care} from a {Global} {Perspective}, 2019 {\textbar} {Commonwealth} {Fund}},
	url = {https://www.commonwealthfund.org/publications/issue-briefs/2020/jan/us-health-care-global-perspective-2019},
	abstract = {Americans are living shorter, unhealthier lives. Yet, the United States outspends other wealthy nations when it comes to health care, according to a new Commonwealth Fund report. This analysis compares the U.S. to 10 other high-income nations on spending, outcomes, risk factors, and quality.},
	language = {en},
	urldate = {2021-01-29},
	doi = {https://doi.org/10.26099/7avy-fc29},
	doi = {https://doi.org/10.26099/7avy-fc29},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IZTHD9RG\\us-health-care-global-perspective-2019.html:text/html},
}

@book{hanDataMiningConcepts2012,
	address = {Burlington, MA},
	edition = {3rd ed},
	title = {Data mining: concepts and techniques},
	isbn = {978-0-12-381479-1},
	shorttitle = {Data mining},
	publisher = {Elsevier},
	author = {Han, Jiawei and Kamber, Micheline},
	year = {2012},
	keywords = {Data mining},
	file = {Data mining concepts and techniques-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Data mining concepts and techniques-metadata.md:text/plain},
}

@inproceedings{agrawalMiningAssociationRules1993,
	address = {Washington, D.C., United States},
	title = {Mining association rules between sets of items in large databases},
	isbn = {978-0-89791-592-2},
	url = {http://portal.acm.org/citation.cfm?doid=170035.170072},
	doi = {10.1145/170035.170072},
	language = {en},
	urldate = {2021-01-31},
	booktitle = {Proceedings of the 1993 {ACM} {SIGMOD} international conference on {Management} of data  - {SIGMOD} '93},
	publisher = {ACM Press},
	author = {Agrawal, Rakesh and Imieliński, Tomasz and Swami, Arun},
	year = {1993},
	pages = {207--216},
	file = {Mining association rules between sets of items in large databases-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Mining association rules between sets of items in large databases-metadata.md:text/plain},
}

@inproceedings{bayardoEfficientlyMiningLong1998,
	address = {Seattle, Washington, United States},
	title = {Efficiently mining long patterns from databases},
	isbn = {978-0-89791-995-1},
	url = {http://portal.acm.org/citation.cfm?doid=276304.276313},
	doi = {10.1145/276304.276313},
	language = {en},
	urldate = {2021-01-31},
	booktitle = {Proceedings of the 1998 {ACM} {SIGMOD} international conference on {Management} of data  - {SIGMOD} '98},
	publisher = {ACM Press},
	author = {Bayardo, Roberto J.},
	year = {1998},
	pages = {85--93},
	file = {Efficiently mining long patterns from databases-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Efficiently mining long patterns from databases-metadata.md:text/plain;Efficiently mining long patterns from databases.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Efficiently mining long patterns from databases.pdf:application/pdf},
}

@incollection{pasquierDiscoveringFrequentClosed1999,
	address = {Berlin, Heidelberg},
	title = {Discovering {Frequent} {Closed} {Itemsets} for {Association} {Rules}},
	volume = {1540},
	isbn = {978-3-540-65452-0 978-3-540-49257-3},
	url = {http://link.springer.com/10.1007/3-540-49257-7_25},
	language = {en},
	urldate = {2021-01-31},
	booktitle = {Database {Theory} — {ICDT}’99},
	publisher = {Springer Berlin Heidelberg},
	author = {Pasquier, Nicolas and Bastide, Yves and Taouil, Rafik and Lakhal, Lotfi},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Beeri, Catriel and Buneman, Peter},
	year = {1999},
	doi = {10.1007/3-540-49257-7_25},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {398--416},
	file = {Discovering Frequent Closed Itemsets for Association Rules-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Discovering Frequent Closed Itemsets for Association Rules-metadata.md:text/plain;Discovering Frequent Closed Itemsets for Association Rules.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Discovering Frequent Closed Itemsets for Association Rules.pdf:application/pdf},
}

@article{hanFrequentPatternMining2007,
	title = {Frequent pattern mining: current status and future directions},
	volume = {15},
	issn = {1384-5810, 1573-756X},
	shorttitle = {Frequent pattern mining},
	url = {http://link.springer.com/10.1007/s10618-006-0059-1},
	doi = {10.1007/s10618-006-0059-1},
	language = {en},
	number = {1},
	urldate = {2021-01-31},
	journal = {Data Mining and Knowledge Discovery},
	author = {Han, Jiawei and Cheng, Hong and Xin, Dong and Yan, Xifeng},
	month = jul,
	year = {2007},
	pages = {55--86},
	file = {Frequent pattern mining current status and future directions-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Frequent pattern mining current status and future directions-metadata.md:text/plain;Frequent pattern mining.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Frequent pattern mining.pdf:application/pdf},
}

@inproceedings{Agrawal:1994:FAM:645920.672836,
	address = {San Francisco, CA, USA},
	series = {{VLDB} '94},
	title = {Fast algorithms for mining association rules in large databases},
	isbn = {1-55860-153-8},
	url = {http://dl.acm.org/citation.cfm?id=645920.672836},
	booktitle = {Proceedings of the 20th international conference on very large data bases},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Agrawal, Rakesh and Srikant, Ramakrishnan},
	year = {1994},
	note = {Number of pages: 13
tex.acmid: 672836
tex.added-at: 2016-11-26T13:15:06.000+0100
tex.biburl: https://www.bibsonomy.org/bibtex/2255ab37d32875266fd54103db2c20787/machinelearning
tex.interhash: 960c924ccbe1ff429a30f7433ec53122
tex.intrahash: 255ab37d32875266fd54103db2c20787
tex.timestamp: 2016-11-26T13:17:02.000+0100},
	keywords = {ml},
	pages = {487--499},
	file = {Fast algorithms for mining association rules in large databases-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Fast algorithms for mining association rules in large databases-metadata.md:text/plain;Fast algorithms for mining association rules in large databases.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Fast algorithms for mining association rules in large databases.pdf:application/pdf},
}

@inproceedings{jianEfficientAssociationRule2008,
	address = {Adelaide, Australia},
	title = {An {Efficient} {Association} {Rule} {Mining} {Algorithm} {In} {Distributed} {Databases}},
	isbn = {978-0-7695-3090-1},
	url = {http://ieeexplore.ieee.org/document/4470359/},
	doi = {10.1109/WKDD.2008.33},
	urldate = {2021-02-02},
	booktitle = {First {International} {Workshop} on {Knowledge} {Discovery} and {Data} {Mining} ({WKDD} 2008)},
	publisher = {IEEE},
	author = {Jian, Wu and Ming, Li Xing},
	month = jan,
	year = {2008},
	pages = {108--113},
	file = {An Efficient Association Rule Mining Algorithm In Distributed Databases.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\An Efficient Association Rule Mining Algorithm In Distributed Databases.md:text/plain;An Efficient Association Rule Mining Algorithm In Distributed Databases.PDF:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\An Efficient Association Rule Mining Algorithm In Distributed Databases.PDF:application/pdf},
}

@article{parkEffectiveHashbasedAlgorithm1995,
	title = {An effective hash-based algorithm for mining association rules},
	volume = {24},
	issn = {0163-5808},
	url = {https://dl.acm.org/doi/10.1145/568271.223813},
	doi = {10.1145/568271.223813},
	language = {en},
	number = {2},
	urldate = {2021-02-02},
	journal = {ACM SIGMOD Record},
	author = {Park, Jong Soo and Chen, Ming-Syan and Yu, Philip S.},
	month = may,
	year = {1995},
	pages = {175--186},
}

@inproceedings{sarawagiIntegratingAssociationRule1998,
	address = {Seattle, Washington, United States},
	title = {Integrating association rule mining with relational database systems: alternatives and implications},
	isbn = {978-0-89791-995-1},
	shorttitle = {Integrating association rule mining with relational database systems},
	url = {http://portal.acm.org/citation.cfm?doid=276304.276335},
	doi = {10.1145/276304.276335},
	language = {en},
	urldate = {2021-02-02},
	booktitle = {Proceedings of the 1998 {ACM} {SIGMOD} international conference on {Management} of data  - {SIGMOD} '98},
	publisher = {ACM Press},
	author = {Sarawagi, Sunita and Thomas, Shiby and Agrawal, Rakesh},
	year = {1998},
	pages = {343--354},
	file = {Integrating association rule mining with relational database systems alternatives and implications.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Integrating association rule mining with relational database systems alternatives and implications.md:text/plain;Integrating association rule mining with relational database systems.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Integrating association rule mining with relational database systems.pdf:application/pdf},
}

@book{abbadiMarkingMillennium2000,
	address = {Orlando, Fla},
	title = {Marking the millennium},
	isbn = {978-1-55860-715-6},
	language = {eng},
	publisher = {Morgan Kaufman},
	editor = {Abbadi, Amr el- and International Conference on Very Large Databases},
	year = {2000},
	note = {Meeting Name: International Conference on Very Large Data Bases
OCLC: 248567596},
}

@article{Netz2000,
	title = {Integration of data mining and relational databases},
	abstract = {In this paper. we review the past work and discuss the future of integration of data mining and relational database systems. We also discuss support for integration in Microsoft SQL Server 2000.},
	journal = {Proceedings of the 26th International Conference on Very Large Data Bases, VLDB'00},
	author = {Netz, Amir and Chaudhuri, Surajit and Bernhardt, Jeff and Fayyad, Usama},
	year = {2000},
	note = {ISBN: 1558607153},
	pages = {719--722},
	file = {Integration of data mining and relational databases.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Integration of data mining and relational databases.md:text/plain},
}

@article{nashPicosecondRamanStudies1988,
	title = {Picosecond {Raman} studies of the {Fröhlich} interaction in semiconductor alloys},
	volume = {60},
	issn = {1079-7114},
	doi = {10.1103/PhysRevLett.60.863},
	language = {eng},
	number = {9},
	journal = {Physical Review Letters},
	author = {Nash, null and Skolnick, null},
	month = feb,
	year = {1988},
	pmid = {10038675},
	pages = {863},
}

@article{zakiParallelAlgorithmsDiscovery1997,
	title = {Parallel {Algorithms} for {Discovery} of {Association} {Rules}},
	volume = {1},
	issn = {13845810},
	url = {http://link.springer.com/10.1023/A:1009773317876},
	doi = {10.1023/A:1009773317876},
	number = {4},
	urldate = {2021-02-02},
	journal = {Data Mining and Knowledge Discovery},
	author = {Zaki, Mohammed J. and Parthasarathy, Srinivasan and Ogihara, Mitsunori and Li, Wei},
	year = {1997},
	pages = {343--373},
	file = {Parallel Algorithms for Discovery of Association Rules.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Parallel Algorithms for Discovery of Association Rules.md:text/plain;Parallel Algorithms for Discovery of Association Rules.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Parallel Algorithms for Discovery of Association Rules.pdf:application/pdf},
}

@article{hanMiningFrequentPatterns2000,
	title = {Mining frequent patterns without candidate generation},
	volume = {29},
	issn = {0163-5808},
	url = {https://dl.acm.org/doi/10.1145/335191.335372},
	doi = {10.1145/335191.335372},
	language = {en},
	number = {2},
	urldate = {2021-02-02},
	journal = {ACM SIGMOD Record},
	author = {Han, Jiawei and Pei, Jian and Yin, Yiwen},
	month = jun,
	year = {2000},
	pages = {1--12},
	file = {Mining frequent patterns without candidate generation.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Mining frequent patterns without candidate generation.md:text/plain;Mining frequent patterns without candidate generation.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Mining frequent patterns without candidate generation.pdf:application/pdf},
}

@inproceedings{zakiCHARMEfficientAlgorithm2002,
	title = {{CHARM}: {An} {Efficient} {Algorithm} for {Closed} {Itemset} {Mining}},
	isbn = {978-0-89871-517-0 978-1-61197-272-6},
	shorttitle = {{CHARM}},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972726.27},
	doi = {10.1137/1.9781611972726.27},
	language = {en},
	urldate = {2021-02-02},
	booktitle = {Proceedings of the 2002 {SIAM} {International} {Conference} on {Data} {Mining}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Zaki, Mohammed J. and Hsiao, Ching-Jui},
	month = apr,
	year = {2002},
	pages = {457--473},
}

@inproceedings{wangCLOSETSearchingBest2003,
	address = {Washington, D.C.},
	title = {{CLOSET}+: searching for the best strategies for mining frequent closed itemsets},
	isbn = {978-1-58113-737-8},
	shorttitle = {{CLOSET}+},
	url = {http://portal.acm.org/citation.cfm?doid=956750.956779},
	doi = {10.1145/956750.956779},
	language = {en},
	urldate = {2021-02-02},
	booktitle = {Proceedings of the ninth {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining  - {KDD} '03},
	publisher = {ACM Press},
	author = {Wang, Jianyong and Han, Jiawei and Pei, Jian},
	year = {2003},
	pages = {236},
	file = {CLOSET+ searching for the best strategies for mining frequent closed itemsets.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\CLOSET+ searching for the best strategies for mining frequent closed itemsets.md:text/plain;CLOSET+.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\CLOSET+.pdf:application/pdf},
}

@incollection{aggarwalFrequentPatternMining2014,
	address = {Cham},
	title = {Frequent {Pattern} {Mining} {Algorithms}: {A} {Survey}},
	isbn = {978-3-319-07820-5 978-3-319-07821-2},
	shorttitle = {Frequent {Pattern} {Mining} {Algorithms}},
	url = {http://link.springer.com/10.1007/978-3-319-07821-2_2},
	language = {en},
	urldate = {2021-02-02},
	booktitle = {Frequent {Pattern} {Mining}},
	publisher = {Springer International Publishing},
	author = {Aggarwal, Charu C. and Bhuiyan, Mansurul A. and Hasan, Mohammad Al},
	editor = {Aggarwal, Charu C. and Han, Jiawei},
	year = {2014},
	doi = {10.1007/978-3-319-07821-2_2},
	pages = {19--64},
}

@incollection{liuDownwardClosureProperty2009,
	address = {Boston, MA},
	title = {Downward {Closure} {Property}},
	isbn = {978-0-387-35544-3 978-0-387-39940-9},
	url = {http://link.springer.com/10.1007/978-0-387-39940-9_2515},
	language = {en},
	urldate = {2021-02-04},
	booktitle = {Encyclopedia of {Database} {Systems}},
	publisher = {Springer US},
	editor = {Liu, Ling and Özsu, M. Tamer},
	year = {2009},
	doi = {10.1007/978-0-387-39940-9_2515},
	pages = {947--947},
}

@article{rajkomarScalableAccurateDeep2018,
	title = {Scalable and accurate deep learning with electronic health records},
	volume = {1},
	issn = {2398-6352},
	url = {http://www.nature.com/articles/s41746-018-0029-1},
	doi = {10.1038/s41746-018-0029-1},
	language = {en},
	number = {1},
	urldate = {2021-02-08},
	journal = {npj Digital Medicine},
	author = {Rajkomar, Alvin and Oren, Eyal and Chen, Kai and Dai, Andrew M. and Hajaj, Nissan and Hardt, Michaela and Liu, Peter J. and Liu, Xiaobing and Marcus, Jake and Sun, Mimi and Sundberg, Patrik and Yee, Hector and Zhang, Kun and Zhang, Yi and Flores, Gerardo and Duggan, Gavin E. and Irvine, Jamie and Le, Quoc and Litsch, Kurt and Mossin, Alexander and Tansuwan, Justin and Wang, De and Wexler, James and Wilson, Jimbo and Ludwig, Dana and Volchenboum, Samuel L. and Chou, Katherine and Pearson, Michael and Madabushi, Srinivasan and Shah, Nigam H. and Butte, Atul J. and Howell, Michael D. and Cui, Claire and Corrado, Greg S. and Dean, Jeffrey},
	month = dec,
	year = {2018},
	pages = {18},
	file = {Full Text:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C956VV2X\\Rajkomar et al. - 2018 - Scalable and accurate deep learning with electroni.pdf:application/pdf;Full Text:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HDGAWDPC\\Rajkomar et al. - 2018 - Scalable and accurate deep learning with electroni.pdf:application/pdf},
}

@article{gatesImmensePowerUnleash,
	title = {With immense power to unleash improvements in cost, quality and access, {AI} is exploding in popularity. {Growth} in the {AI} health market is expected to reach \$6.6 billion by 2021—that’s a compound annual growth rate of 40 percent (see {Figure} 1).},
	language = {en},
	author = {Gates, Bill},
	pages = {8},
	file = {Gates - With immense power to unleash improvements in cost.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YQA2ZZYZ\\Gates - With immense power to unleash improvements in cost.pdf:application/pdf},
}

@misc{ArtificialIntelligenceHealthcare,
	title = {Artificial {Intelligence} in {Healthcare} {\textbar} {Accenture}},
	url = {https://www.accenture.com/us-en/insight-artificial-intelligence-healthcare},
	abstract = {The artificial intelligence (AI) market is poised to explode in US healthcare, and will reach \$6.6 billion by 2021.},
	language = {en},
	urldate = {2021-02-08},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SYTWH8TZ\\insight-artificial-intelligence-healthcare .html:text/html},
}

@misc{healthitanalyticsFutureAIOpportunities2019,
	title = {Future {AI} {Opportunities} for {Improving} {Care} {Delivery}, {Cost}, and {Efficacy}},
	url = {https://healthitanalytics.com/news/future-ai-opportunities-for-improving-care-delivery-cost-and-efficacy},
	abstract = {Artificial intelligence in healthcare has the ability to significantly improve care delivery and cost, particularly through care standardization, cost prediction, and drug development.},
	language = {en-US},
	urldate = {2021-02-08},
	journal = {HealthITAnalytics},
	author = {HealthITAnalytics},
	month = jul,
	year = {2019},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WUK764XN\\future-ai-opportunities-for-improving-care-delivery-cost-and-efficacy.html:text/html},
}

@misc{HomeClinicalTrialsGov,
	title = {Home - {ClinicalTrials}.gov},
	url = {https://www.clinicaltrials.gov/},
	language = {en},
	urldate = {2021-02-08},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GG2FTL62\\www.clinicaltrials.gov.html:text/html},
}

@misc{MedicalGuidelinesPocket,
	title = {Medical {Guidelines} {Pocket} {Guides}, {Clinical} {Guides} \& {Apps}},
	url = {https://www.guidelinecentral.com/},
	abstract = {Your official resource for medical quick reference guidelines, mobile apps and a free library of over 24 million clinical practice tools and calculators.},
	language = {en-US},
	urldate = {2021-02-08},
	journal = {Guideline Central},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KII6L5T3\\www.guidelinecentral.com.html:text/html},
}

@misc{PubMeda,
	title = {{PubMed}},
	url = {https://pubmed.ncbi.nlm.nih.gov/},
	abstract = {PubMed® comprises more than 30 million citations for biomedical literature from MEDLINE, life science journals, and online books. Citations may include links to full-text content from PubMed Central and publisher web sites.},
	language = {en},
	urldate = {2021-02-08},
	journal = {PubMed},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QJ372JUT\\pubmed.ncbi.nlm.nih.gov.html:text/html},
}

@misc{5yearForecastExponential2019,
	title = {5-year forecast: exponential growth for {AI} healthcare market},
	shorttitle = {5-year forecast},
	url = {https://www.healthcareitnews.com/ai-powered-healthcare/5-year-forecast-exponential-growth-ai-healthcare-market},
	abstract = {The AI in healthcare market is projected to expand from its current \$2.1 billion to \$36.1 billion in 2025, representing a staggering compound annual growth rate (CAGR) of 50.2 percent. That’s according to new research from ReportLinker, which notes that the rapid increase in value will be driven largely by North American investment, with the United States at the forefront of innovation and spending. Hospitals and physician providers will be the major investors in machine learning and artificial intelligence solutions and services, the report predicts.},
	language = {en},
	urldate = {2021-02-08},
	journal = {AI Powered Healthcare {\textbar} Healthcare IT News},
	month = jan,
	year = {2019},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5PS2QLIH\\5-year-forecast-exponential-growth-ai-healthcare-market.html:text/html},
}

@article{guang-binhuangExtremeLearningMachine2012,
	title = {Extreme {Learning} {Machine} for {Regression} and {Multiclass} {Classification}},
	volume = {42},
	issn = {1083-4419, 1941-0492},
	url = {http://ieeexplore.ieee.org/document/6035797/},
	doi = {10.1109/TSMCB.2011.2168604},
	number = {2},
	urldate = {2021-02-12},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
	author = {{Guang-Bin Huang} and {Hongming Zhou} and {Xiaojian Ding} and {Rui Zhang}},
	month = apr,
	year = {2012},
	keywords = {psl},
	pages = {513--529},
}

@article{friedmanStochasticGradientBoosting2002,
	title = {Stochastic gradient boosting},
	volume = {38},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947301000652},
	doi = {10.1016/S0167-9473(01)00065-2},
	language = {en},
	number = {4},
	urldate = {2021-02-12},
	journal = {Computational Statistics \& Data Analysis},
	author = {Friedman, Jerome H.},
	month = feb,
	year = {2002},
	keywords = {psl},
	pages = {367--378},
}

@article{cortesSupportvectorNetworks1995,
	title = {Support-vector networks},
	volume = {20},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	language = {en},
	number = {3},
	urldate = {2021-02-12},
	journal = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	month = sep,
	year = {1995},
	keywords = {psl},
	pages = {273--297},
	file = {Support-vector networks.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Support-vector networks.pdf:application/pdf},
}

@article{tinkamhoRandomSubspaceMethod1998,
	title = {The random subspace method for constructing decision forests},
	volume = {20},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/709601/},
	doi = {10.1109/34.709601},
	number = {8},
	urldate = {2021-02-12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {{Tin Kam Ho}},
	month = aug,
	year = {1998},
	keywords = {psl},
	pages = {832--844},
	file = {The random subspace method for constructing decision forests.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\The random subspace method for constructing decision forests.pdf:application/pdf},
}

@article{wuTop10Algorithms2008,
	title = {Top 10 algorithms in data mining},
	volume = {14},
	issn = {0219-1377, 0219-3116},
	url = {http://link.springer.com/10.1007/s10115-007-0114-2},
	doi = {10.1007/s10115-007-0114-2},
	language = {en},
	number = {1},
	urldate = {2021-02-12},
	journal = {Knowledge and Information Systems},
	author = {Wu, Xindong and Kumar, Vipin and Ross Quinlan, J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
	month = jan,
	year = {2008},
	keywords = {psl},
	pages = {1--37},
	file = {Top 10 algorithms in data mining.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Top 10 algorithms in data mining.pdf:application/pdf},
}

@article{fixDiscriminatoryAnalysisNonparametric1989,
	title = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}: {Consistency} {Properties}},
	volume = {57},
	issn = {03067734},
	shorttitle = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}},
	url = {https://www.jstor.org/stable/1403797?origin=crossref},
	doi = {10.2307/1403797},
	number = {3},
	urldate = {2021-02-12},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Fix, Evelyn and Hodges, J. L.},
	month = dec,
	year = {1989},
	keywords = {psl},
	pages = {238},
}

@article{LIIEssaySolving1763,
	title = {{LII}. {An} essay towards solving a problem in the doctrine of chances. {By} the late {Rev}. {Mr}. {Bayes}, {F}. {R}. {S}. communicated by {Mr}. {Price}, in a letter to {John} {Canton}, {A}. {M}. {F}. {R}. {S}},
	volume = {53},
	issn = {0261-0523, 2053-9223},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstl.1763.0053},
	doi = {10.1098/rstl.1763.0053},
	abstract = {Dear Sir, I Now send you an essay which I have found among the papers of our deceased friend Mr. Bayes, and which, in my opinion, has great merit, and well deserves to be preserved.},
	language = {la},
	urldate = {2021-02-12},
	journal = {Philosophical Transactions of the Royal Society of London},
	month = dec,
	year = {1763},
	keywords = {psl},
	pages = {370--418},
	file = {LII.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\LII.pdf:application/pdf},
}

@article{berksonApplicationLogisticFunction1944,
	title = {Application to the {Logistic} {Function} to {Bio}-{Assay}},
	volume = {39},
	issn = {01621459},
	url = {https://www.jstor.org/stable/2280041?origin=crossref},
	doi = {10.2307/2280041},
	number = {227},
	urldate = {2021-02-12},
	journal = {Journal of the American Statistical Association},
	author = {Berkson, Joseph},
	month = sep,
	year = {1944},
	keywords = {psl},
	pages = {357},
}

@article{vapnikOverviewStatisticalLearning1999,
	title = {An overview of statistical learning theory},
	volume = {10},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/788640/},
	doi = {10.1109/72.788640},
	number = {5},
	urldate = {2021-02-12},
	journal = {IEEE Transactions on Neural Networks},
	author = {Vapnik, V.N.},
	month = sep,
	year = {1999},
	keywords = {psl},
	pages = {988--999},
	file = {An overview of statistical learning theory.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\An overview of statistical learning theory.pdf:application/pdf},
}

@article{vonluxburgStatisticalLearningTheory2008,
	title = {Statistical {Learning} {Theory}: {Models}, {Concepts}, and {Results}},
	shorttitle = {Statistical {Learning} {Theory}},
	url = {http://arxiv.org/abs/0810.4752},
	abstract = {Statistical learning theory provides the theoretical basis for many of today's machine learning algorithms. In this article we attempt to give a gentle, non-technical overview over the key ideas and insights of statistical learning theory. We target at a broad audience, not necessarily machine learning researchers. This paper can serve as a starting point for people who want to get an overview on the field before diving into technical details.},
	urldate = {2021-02-12},
	journal = {arXiv:0810.4752 [math, stat]},
	author = {von Luxburg, Ulrike and Schoelkopf, Bernhard},
	month = oct,
	year = {2008},
	note = {arXiv: 0810.4752},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, psl},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3EUI3Y5P\\0810.html:text/html;Statistical Learning Theory.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Statistical Learning Theory.pdf:application/pdf},
}

@article{wangOriginDeepLearning2017,
	title = {On the {Origin} of {Deep} {Learning}},
	url = {http://arxiv.org/abs/1702.07800},
	abstract = {This paper is a review of the evolutionary history of deep learning models. It covers from the genesis of neural networks when associationism modeling of the brain is studied, to the models that dominate the last decade of research in deep learning like convolutional neural networks, deep belief networks, and recurrent neural networks. In addition to a review of these models, this paper primarily focuses on the precedents of the models above, examining how the initial ideas are assembled to construct the early models and how these preliminary models are developed into their current forms. Many of these evolutionary paths last more than half a century and have a diversity of directions. For example, CNN is built on prior knowledge of biological vision system; DBN is evolved from a trade-off of modeling power and computation complexity of graphical models and many nowadays models are neural counterparts of ancient linear models. This paper reviews these evolutionary paths and offers a concise thought flow of how these models are developed, and aims to provide a thorough background for deep learning. More importantly, along with the path, this paper summarizes the gist behind these milestones and proposes many directions to guide the future research of deep learning.},
	urldate = {2021-02-13},
	journal = {arXiv:1702.07800 [cs, stat]},
	author = {Wang, Haohan and Raj, Bhiksha},
	month = mar,
	year = {2017},
	note = {arXiv: 1702.07800},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DN5YR64D\\1702.html:text/html;On the Origin of Deep Learning.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\On the Origin of Deep Learning.pdf:application/pdf},
}

@article{bengioLearningDeepArchitectures2009,
	title = {Learning {Deep} {Architectures} for {AI}},
	volume = {2},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-006},
	doi = {10.1561/2200000006},
	language = {en},
	number = {1},
	urldate = {2021-02-13},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Bengio, Y.},
	year = {2009},
	keywords = {psl},
	pages = {1--127},
	file = {Learning Deep Architectures for AI.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Learning Deep Architectures for AI.pdf:application/pdf},
}

@article{doganComparativeAnalysisClassification2013,
	title = {A comparative analysis of classification algorithms in data mining for accuracy, speed and robustness},
	volume = {14},
	issn = {1385-951X, 1573-7667},
	url = {http://link.springer.com/10.1007/s10799-012-0135-8},
	doi = {10.1007/s10799-012-0135-8},
	language = {en},
	number = {2},
	urldate = {2021-02-13},
	journal = {Information Technology and Management},
	author = {Dogan, Neslihan and Tanrikulu, Zuhal},
	month = jun,
	year = {2013},
	keywords = {psl},
	pages = {105--124},
}

@article{wolpertNoFreeLunch1997,
	title = {No free lunch theorems for optimization},
	volume = {1},
	issn = {1089778X},
	url = {http://ieeexplore.ieee.org/document/585893/},
	doi = {10.1109/4235.585893},
	number = {1},
	urldate = {2021-02-13},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Wolpert, D.H. and Macready, W.G.},
	month = apr,
	year = {1997},
	keywords = {psl},
	pages = {67--82},
	file = {No free lunch theorems for optimization.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\No free lunch theorems for optimization.pdf:application/pdf},
}

@article{khondokerComparisonMachineLearning2016,
	title = {A comparison of machine learning methods for classification using simulation with multiple real data examples from mental health studies},
	volume = {25},
	issn = {0962-2802, 1477-0334},
	url = {http://journals.sagepub.com/doi/10.1177/0962280213502437},
	doi = {10.1177/0962280213502437},
	abstract = {Background
              Recent literature on the comparison of machine learning methods has raised questions about the neutrality, unbiasedness and utility of many comparative studies. Reporting of results on favourable datasets and sampling error in the estimated performance measures based on single samples are thought to be the major sources of bias in such comparisons. Better performance in one or a few instances does not necessarily imply so on an average or on a population level and simulation studies may be a better alternative for objectively comparing the performances of machine learning algorithms.
            
            
              Methods
              We compare the classification performance of a number of important and widely used machine learning algorithms, namely the Random Forests (RF), Support Vector Machines (SVM), Linear Discriminant Analysis (LDA) and k-Nearest Neighbour (kNN). Using massively parallel processing on high-performance supercomputers, we compare the generalisation errors at various combinations of levels of several factors: number of features, training sample size, biological variation, experimental variation, effect size, replication and correlation between features.
            
            
              Results
              For smaller number of correlated features, number of features not exceeding approximately half the sample size, LDA was found to be the method of choice in terms of average generalisation errors as well as stability (precision) of error estimates. SVM (with RBF kernel) outperforms LDA as well as RF and kNN by a clear margin as the feature set gets larger provided the sample size is not too small (at least 20). The performance of kNN also improves as the number of features grows and outplays that of LDA and RF unless the data variability is too high and/or effect sizes are too small. RF was found to outperform only kNN in some instances where the data are more variable and have smaller effect sizes, in which cases it also provide more stable error estimates than kNN and LDA. Applications to a number of real datasets supported the findings from the simulation study.},
	language = {en},
	number = {5},
	urldate = {2021-02-13},
	journal = {Statistical Methods in Medical Research},
	author = {Khondoker, Mizanur and Dobson, Richard and Skirrow, Caroline and Simmons, Andrew and Stahl, Daniel},
	month = oct,
	year = {2016},
	keywords = {psl},
	pages = {1804--1823},
	file = {A comparison of machine learning methods for classification using simulation.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\A comparison of machine learning methods for classification using simulation.pdf:application/pdf},
}

@article{chalkDoesReflectiveWriting,
	title = {Does reflective writing in the {PDP} improve science and engineering students’ learning},
	language = {en},
	author = {Chalk, Peter and Hardbattle, Dafna},
	pages = {9},
	file = {Chalk and Hardbattle - Does reflective writing in the PDP improve science.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EM66DUM2\\Chalk and Hardbattle - Does reflective writing in the PDP improve science.pdf:application/pdf},
}

@article{georgeLearningReflectiveJournal,
	title = {Learning and the {Reflective} {Journal} in {Computer} {Science}},
	abstract = {This paper describes the use of the reflective journal in a computer programming course at the University of South Australia. We describe rationale for the journal relating it to the contribution it can make to generic skills of lifelong learning, problem-solving, communication and awareness of personal learning strategies. We also relate it to the Personal Software Process (PSP) used by industry to encourage software engineers to improve productivity by ‘review’, with collation of software productivity metrics and awareness of personal and team level practice.},
	language = {en},
	author = {George, Susan E},
	pages = {10},
	file = {George - Learning and the Reflective Journal in Computer Sc.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T4G4XC4K\\George - Learning and the Reflective Journal in Computer Sc.pdf:application/pdf},
}

@article{wallinReflectiveDiaryMethod2018,
	title = {The reflective diary as a method for the formative assessment of self-regulated learning},
	volume = {43},
	issn = {0304-3797, 1469-5898},
	url = {https://www.tandfonline.com/doi/full/10.1080/03043797.2017.1290585},
	doi = {10.1080/03043797.2017.1290585},
	abstract = {An increasingly desired outcome of engineering education is the ability to engage in self-regulated learning (SRL). One promising method for the formative assessment of SRL is the reflective diary. There is, however, a paucity of research on the use of reflective diaries in engineering education. To mitigate this gap, we report on a case study where reflective diaries were implemented in a master’s course on tissue engineering. The objective of this paper is to explore the potential of reflective diaries for the formative assessment of three central aspects of SRL: conceptions of knowledge, conceptions of learning, and strategies for monitoring and regulating learning. Based on a theoretical thematic analysis of the diary entries, we show that reflective diaries can be used to assess these three aspects of SRL. We discuss ways of providing feedback to students, with a focus on dialogic feedback.},
	language = {en},
	number = {4},
	urldate = {2021-02-15},
	journal = {European Journal of Engineering Education},
	author = {Wallin, Patric and Adawi, Tom},
	month = jul,
	year = {2018},
	pages = {507--521},
	file = {Wallin and Adawi - 2018 - The reflective diary as a method for the formative.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SDC2Y9NP\\Wallin and Adawi - 2018 - The reflective diary as a method for the formative.pdf:application/pdf},
}

@article{hubbsLearningOutMethod2010,
	title = {Learning {From} the {Inside} {Out}: {A} {Method} for {Analyzing} {Reflective} {Journals} in the {College} {Classroom}},
	volume = {33},
	issn = {1053-8259},
	shorttitle = {Learning {From} the {Inside} {Out}},
	url = {http://aee.metapress.com/openurl.asp?genre=article&id=doi:10.5193/JEE33.1.56},
	doi = {10.5193/JEE33.1.56},
	abstract = {The literature on reflective journals reveals that unless instructors use reflection in an educationally meaningful way, students often view journaling as busywork. The instrument we have designed and propose here for analyzing reflective journal entries provides students with useful methods for reviewing and critiquing connections between classroom learning and practical experience. Because this matrix graphically portrays how concrete or abstract and how cognitive or affective a given journal entry is judged, it holds promise for developing reflective skills and self-understanding. We present a definition of reflective journals, a rationale for the instructional use of reflective journals in professional education, a method for analyzing students’ journal entries, and a means for developing reflective skills. Although this work is rooted in human services education, the instrument described here—a matrix for analyzing reflective journal entries—can be used in disciplines in which the use of reflective skills is a valuable component.},
	language = {en},
	number = {1},
	urldate = {2021-02-15},
	journal = {Journal of Experiential Education},
	author = {Hubbs, Delaura and Brand, Charles F.},
	month = jan,
	year = {2010},
	pages = {56--71},
	file = {Hubbs and Brand - 2010 - Learning From the Inside Out A Method for Analyzi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\33CMYJFI\\Hubbs and Brand - 2010 - Learning From the Inside Out A Method for Analyzi.pdf:application/pdf},
}

@article{williamsReflectiveJournalWriting,
	title = {Reflective {Journal} {Writing} as an {Alternative} {Assessment}},
	abstract = {This article describes the use of reflective journal writing in an 8th grade, inner city general music classroom. Reflective journal writing entries replaced tests and quizzes. They provided the students with a guided opportunity to demonstrate their learning using their own words. Reflective journal writing created greater class participation (especially among lowerperforming students), stronger academic achievement, and a better opportunity for students to connect with general music.},
	language = {en},
	author = {Williams, Nicole},
	pages = {15},
	file = {Williams - Reflective Journal Writing as an Alternative Asses.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DRDSYKBK\\Williams - Reflective Journal Writing as an Alternative Asses.pdf:application/pdf},
}

@article{orangeEncouragingReflectivePractices,
	title = {Encouraging {Reflective} {Practices} in {Doctoral} {Students} through {Research} {Journals}},
	abstract = {This study developed after I read numerous research journals created by my doctoral students. At times, students included considerable amounts of detail, reflecting on their research processes and their roles as researchers. At other times, the journals appeared to be a mere afterthought, seemingly completed in an evening to satisfy the requirement and get a grade. And, as with many things in the introductory qualitative research course, students expressed a need for more structured guidelines for their journals. In response, I developed a set of guidelines and prompts students could use to guide their journal entries. With this study, I discovered that the introduction of guidelines and prompts increased student reflexivity, the level of detail in their journal entries, and the length of their journals increased.},
	language = {en},
	author = {Orange, Amy},
	pages = {17},
	file = {Orange - Encouraging Reflective Practices in Doctoral Stude.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XYDFGAT2\\Orange - Encouraging Reflective Practices in Doctoral Stude.pdf:application/pdf},
}

@article{guceInvestigatingCollegeStudents2017,
	title = {Investigating {College} {Students}’ {Views} on {Mathematics} {Learning} {Through} {Reflective} {Journal} {Writing}},
	volume = {6},
	issn = {2620-5440, 2252-8822},
	url = {http://ijere.iaescore.com/index.php/IJERE/article/view/6345},
	doi = {10.11591/ijere.v6i1.6345},
	abstract = {The study on reflective journal writing (RJW) and its benefits as assessed by the teachers has long been an inclination in mathematics education. However, little research has been done to explore the feelings of students towards RJW and how such has an effect on their mathematics learning. This study aimed to describe the feelings of the students about RJW. Being a qualitative type of research study, data were acquired through focus group discussion and were analyzed using axial coding. Results revealed that RJW (i) provided opportunities for the students to construct meaning and express personal views and ideas; (ii) built a connection between the teacher and the students; (iii) through the use of prompts, allowed the students to relate mathematics to real-life facts improving their understanding of the subject; (iv) helped the students build association of ideas using their prior knowledge and experiences; and (v) enabled the students to develop self-awareness. The findings suggest that when students are engaged in reflective writing experiences which explicitly promote meaning-making or self-questioning, teachers create an opportunity for them to see how the process of writing can enrich their mathematical learning.},
	language = {en},
	number = {1},
	urldate = {2021-02-15},
	journal = {International Journal of Evaluation and Research in Education (IJERE)},
	author = {Guce, Ivee Kalalo},
	month = mar,
	year = {2017},
	pages = {38},
	file = {Guce - 2017 - Investigating College Students’ Views on Mathemati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\X2TE52SF\\Guce - 2017 - Investigating College Students’ Views on Mathemati.pdf:application/pdf},
}

@misc{RethinkingDeadlineLate2019,
	title = {Rethinking {Deadline} and {Late} {Penalty} {Policies}...{Again} {\textbar} {Faculty} {Focus}},
	url = {https://www.facultyfocus.com/articles/effective-classroom-management/rethinking-deadline-and-late-penalty-policies-again/},
	abstract = {Teachers have to implement some sort of late work penalty policy and often modify it. Learn how a flexible deadline policy can decrease late work.},
	language = {en-US},
	urldate = {2021-02-17},
	journal = {Faculty Focus {\textbar} Higher Ed Teaching \& Learning},
	month = aug,
	year = {2019},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VWLV8PUU\\rethinking-deadline-and-late-penalty-policies-again.html:text/html},
}

@misc{messageItTimeDitch,
	title = {It’s {Time} to {Ditch} {Our} {Deadlines}},
	url = {https://community.chronicle.com/news/1531-it-s-time-to-ditch-our-deadlines},
	abstract = {Why you should stop penalizing your students for submitting work late.},
	language = {en},
	urldate = {2021-02-17},
	journal = {Chronicle Community for higher ed jobs, career tools and advice},
	author = {Message, Send and Follow},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EL3RHNYB\\1531-it-s-time-to-ditch-our-deadlines.html:text/html},
}

@misc{IncentivizeDonPenalize2019,
	title = {Incentivize! {Don}’t {Penalize}: {Revisiting} {Late} {Policies} for {Online} {Students}},
	shorttitle = {Incentivize! {Don}’t {Penalize}},
	url = {https://onlinenetworkofeducators.org/2019/03/25/incentivize-dont-penalize-revisiting-late-policies-for-online-students/},
	abstract = {Coming from a culture of storytellers, I’d like to share a story that inspired this post. I was at my local supermarket in the northeast side of Los Angeles when […]},
	language = {en-US},
	urldate = {2021-02-17},
	journal = {Online Network of Educators},
	month = mar,
	year = {2019},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RB9WPX9Q\\incentivize-dont-penalize-revisiting-late-policies-for-online-students.html:text/html},
}

@misc{ArgumentAcceptingLate2019,
	title = {An {Argument} for {Accepting} {Late} {Work} {\textbar} {Faculty} {Focus}},
	url = {https://www.facultyfocus.com/articles/effective-classroom-management/late-work-penalty/},
	abstract = {Finding an effective penalty for late work can be a tricky task. Learn how this college instructor gets some of the best work from students with late work.},
	language = {en-US},
	urldate = {2021-02-17},
	journal = {Faculty Focus {\textbar} Higher Ed Teaching \& Learning},
	month = aug,
	year = {2019},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S77C3HU7\\late-work-penalty.html:text/html},
}

@misc{ValueNotAccepting2007,
	title = {The {Value} of {Not} {Accepting} {Late} {Work}},
	url = {https://www.productiveflourishing.com/late-work/},
	abstract = {I had an amazing thing happen this semester: every student turned in every assignment on time. I don’t think that happened due to the caliber of students or the time I was […]},
	language = {en-US},
	urldate = {2021-02-17},
	journal = {Productive Flourishing},
	month = may,
	year = {2007},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\X6UV7VGF\\late-work.html:text/html},
}

@misc{WhatExactlyPoint,
	title = {(8) {What} exactly is the point of teachers taking off points for late assignments? - {Quora}},
	url = {https://www.quora.com/What-exactly-is-the-point-of-teachers-taking-off-points-for-late-assignments},
	urldate = {2021-02-17},
	file = {(8) What exactly is the point of teachers taking off points for late assignments? - Quora:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H8FBREN7\\What-exactly-is-the-point-of-teachers-taking-off-points-for-late-assignments.html:text/html},
}

@misc{ExaminingWhyYour,
	title = {Examining the {Why} {Behind} {Your} {Late} or {Missed} {Work} {Policies}},
	url = {https://nobaproject.com/blog/2019-05-08-examining-the-why-behind-your-late-or-missed-work-policies},
	abstract = {{\textless}p{\textgreater}By Christine Harrington{\textless}/p{\textgreater}{\textless}p{\textgreater}We all include policies on our syllabi, often simply copying and pasting them from one version to another.  But, how often do we revisit the {\textless}em{\textgreater}why{\textless}/em{\textgreater} behind our policies?  What do your ...},
	language = {en},
	urldate = {2021-02-17},
	journal = {Noba},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y7LQZEMP\\2019-05-08-examining-the-why-behind-your-late-or-missed-work-policies.html:text/html},
}

@misc{MethodsManagingLate,
	title = {Methods for {Managing} {Late} {Work}},
	url = {https://www.edutopia.org/article/methods-managing-late-work},
	abstract = {Examining the reasoning behind your assessments can help shape your approach to tardy work, says Jennifer Gonzalez.},
	language = {en},
	urldate = {2021-02-17},
	journal = {Edutopia},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L85NQANK\\methods-managing-late-work.html:text/html},
}

@article{estevaGuideDeepLearning2019,
	title = {A guide to deep learning in healthcare},
	volume = {25},
	issn = {1078-8956, 1546-170X},
	url = {http://www.nature.com/articles/s41591-018-0316-z},
	doi = {10.1038/s41591-018-0316-z},
	language = {en},
	number = {1},
	urldate = {2021-02-22},
	journal = {Nature Medicine},
	author = {Esteva, Andre and Robicquet, Alexandre and Ramsundar, Bharath and Kuleshov, Volodymyr and DePristo, Mark and Chou, Katherine and Cui, Claire and Corrado, Greg and Thrun, Sebastian and Dean, Jeff},
	month = jan,
	year = {2019},
	pages = {24--29},
}

@misc{VBSpecialIssue,
	title = {{VB} {Special} {Issue}: {AI} and the future of health care {\textbar} {VentureBeat}},
	url = {https://venturebeat.com/vb-special-issue-ai-and-the-future-of-health-care/},
	urldate = {2021-02-22},
	file = {VB Special Issue\: AI and the future of health care | VentureBeat:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5AEMIZVH\\vb-special-issue-ai-and-the-future-of-health-care.html:text/html},
}

@incollection{srikantMiningSequentialPatterns1996,
	address = {Berlin, Heidelberg},
	title = {Mining sequential patterns: {Generalizations} and performance improvements},
	volume = {1057},
	isbn = {978-3-540-61057-1 978-3-540-49943-5},
	shorttitle = {Mining sequential patterns},
	url = {http://link.springer.com/10.1007/BFb0014140},
	urldate = {2021-02-26},
	booktitle = {Advances in {Database} {Technology} — {EDBT} '96},
	publisher = {Springer Berlin Heidelberg},
	author = {Srikant, Ramakrishnan and Agrawal, Rakesh},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Apers, Peter and Bouzeghoub, Mokrane and Gardarin, Georges},
	year = {1996},
	doi = {10.1007/BFb0014140},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1--17},
	file = {Mining sequential patterns Generalizations and performance improvements.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Mining sequential patterns Generalizations and performance improvements.md:text/plain},
}

@article{zakiSPADEEfficientAlgorithm2001,
	title = {{SPADE}: {An} {Efficient} {Algorithm} for {Mining} {Frequent} {Sequences}},
	volume = {42},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1007652502315},
	doi = {10.1023/A:1007652502315},
	number = {1/2},
	urldate = {2021-02-26},
	journal = {Machine Learning},
	author = {Zaki, Mohammed J.},
	year = {2001},
	pages = {31--60},
	file = {SPADE - An Efficient Algorithm for Mining Frequent Sequences:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\SPADE - An Efficient Algorithm for Mining Frequent Sequences.pdf:application/pdf;SPADE An Efficient Algorithm for Mining Frequent Sequences.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\SPADE An Efficient Algorithm for Mining Frequent Sequences.md:text/plain},
}

@article{jianpeiMiningSequentialPatterns2004,
	title = {Mining sequential patterns by pattern-growth: the {PrefixSpan} approach},
	volume = {16},
	issn = {1041-4347},
	shorttitle = {Mining sequential patterns by pattern-growth},
	url = {http://ieeexplore.ieee.org/document/1339268/},
	doi = {10.1109/TKDE.2004.77},
	language = {en},
	number = {11},
	urldate = {2021-02-26},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {{Jian Pei} and {Jiawei Han} and Mortazavi-Asl, B. and {Jianyong Wang} and Pinto, H. and {Qiming Chen} and Dayal, U. and {Mei-Chun Hsu}},
	month = nov,
	year = {2004},
	pages = {1424--1440},
	file = {Mining sequential patterns by pattern-growth the PrefixSpan approach.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Mining sequential patterns by pattern-growth the PrefixSpan approach.md:text/plain;Mining sequential patterns by pattern-growth the PrefixSpan approach.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Mining sequential patterns by pattern-growth the PrefixSpan approach.md:text/plain;Mining sequential patterns by pattern-growth.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Mining sequential patterns by pattern-growth.pdf:application/pdf;Mining sequential patterns by pattern-growth.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Mining sequential patterns by pattern-growth2.pdf:application/pdf},
}

@inproceedings{yanCloSpanMiningClosed2003,
	title = {{CloSpan}: {Mining}: {Closed} {Sequential} {Patterns} in {Large} {Datasets}},
	isbn = {978-0-89871-545-3 978-1-61197-273-3},
	shorttitle = {{CloSpan}},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972733.15},
	doi = {10.1137/1.9781611972733.15},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {Proceedings of the 2003 {SIAM} {International} {Conference} on {Data} {Mining}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Yan, Xifeng and Han, Jiawei and Afshar, Ramin},
	month = may,
	year = {2003},
	pages = {166--177},
	file = {CloSpan Mining Closed Sequential Patterns in Large Datasets.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\CloSpan Mining Closed Sequential Patterns in Large Datasets.md:text/plain;CloSpan.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\CloSpan.pdf:application/pdf},
}

@article{mikolovDistributedRepresentationsWords2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://arxiv.org/abs/1310.4546},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	urldate = {2021-03-01},
	journal = {arXiv:1310.4546 [cs, stat]},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = oct,
	year = {2013},
	note = {arXiv: 1310.4546},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7LXSYAGE\\1310.html:text/html;Distributed Representations of Words and Phrases and their Compositionality.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Distributed Representations of Words and Phrases and their Compositionality.md:text/plain;Distributed Representations of Words and Phrases and their Compositionality.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Distributed Representations of Words and Phrases and their Compositionality.pdf:application/pdf},
}

@article{choiMedicalConceptRepresentation2017,
	title = {Medical {Concept} {Representation} {Learning} from {Electronic} {Health} {Records} and its {Application} on {Heart} {Failure} {Prediction}},
	url = {http://arxiv.org/abs/1602.03686},
	abstract = {Objective: To transform heterogeneous clinical data from electronic health records into clinically meaningful constructed features using data driven method that rely, in part, on temporal relations among data. Materials and Methods: The clinically meaningful representations of medical concepts and patients are the key for health analytic applications. Most of existing approaches directly construct features mapped to raw data (e.g., ICD or CPT codes), or utilize some ontology mapping such as SNOMED codes. However, none of the existing approaches leverage EHR data directly for learning such concept representation. We propose a new way to represent heterogeneous medical concepts (e.g., diagnoses, medications and procedures) based on co-occurrence patterns in longitudinal electronic health records. The intuition behind the method is to map medical concepts that are co-occuring closely in time to similar concept vectors so that their distance will be small. We also derive a simple method to construct patient vectors from the related medical concept vectors. Results: For qualitative evaluation, we study similar medical concepts across diagnosis, medication and procedure. In quantitative evaluation, our proposed representation significantly improves the predictive modeling performance for onset of heart failure (HF), where classification methods (e.g. logistic regression, neural network, support vector machine and K-nearest neighbors) achieve up to 23\% improvement in area under the ROC curve (AUC) using this proposed representation. Conclusion: We proposed an effective method for patient and medical concept representation learning. The resulting representation can map relevant concepts together and also improves predictive modeling performance.},
	urldate = {2021-03-01},
	journal = {arXiv:1602.03686 [cs]},
	author = {Choi, Edward and Schuetz, Andy and Stewart, Walter F. and Sun, Jimeng},
	month = jun,
	year = {2017},
	note = {arXiv: 1602.03686},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RG5RYBDV\\1602.html:text/html;Medical Concept Representation Learning from Electronic Health Records and its Application on Heart Failure Prediction.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Medical Concept Representation Learning from Electronic Health Records and its Application on Heart Failure Prediction.md:text/plain;Medical Concept Representation Learning from Electronic Health Records and its.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Medical Concept Representation Learning from Electronic Health Records and its.pdf:application/pdf},
}

@article{goldbergWord2vecExplainedDeriving,
	title = {word2vec {Explained}: {Deriving} {Mikolov} et al.’s {Negative}-{Sampling} {Word}-{Embedding} {Method}},
	language = {en},
	author = {Goldberg, Yoav and Levy, Omer},
	pages = {5},
	file = {Goldberg and Levy - word2vec Explained Deriving Mikolov et al.’s Nega.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MU74GARY\\Goldberg and Levy - word2vec Explained Deriving Mikolov et al.’s Nega.pdf:application/pdf;word2vec Explained Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\word2vec Explained Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method.md:text/plain},
}

@article{mikolovEfficientEstimationWord2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2021-03-01},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G2GI3S6Y\\1301.html:text/html;arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ADMFXCYZ\\1301.html:text/html;Efficient Estimation of Word Representations in Vector Space.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Efficient Estimation of Word Representations in Vector Space.md:text/plain;Efficient Estimation of Word Representations in Vector Space.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\Efficient Estimation of Word Representations in Vector Space.md:text/plain;Efficient Estimation of Word Representations in Vector Space.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Efficient Estimation of Word Representations in Vector Space.pdf:application/pdf;Mikolov et al_2013_Efficient Estimation of Word Representations in Vector Space.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LNC537R7\\Mikolov et al_2013_Efficient Estimation of Word Representations in Vector Space.pdf:application/pdf},
}

@article{maatenVisualizingDataUsing2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	number = {86},
	urldate = {2021-03-01},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\78GKUI4E\\vandermaaten08a.html:text/html;Visualizing Data using t-SNE.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Visualizing Data using t-SNE.pdf:application/pdf},
}

@article{ghojoghStochasticNeighborEmbedding2020,
	title = {Stochastic {Neighbor} {Embedding} with {Gaussian} and {Student}-t {Distributions}: {Tutorial} and {Survey}},
	shorttitle = {Stochastic {Neighbor} {Embedding} with {Gaussian} and {Student}-t {Distributions}},
	url = {http://arxiv.org/abs/2009.10301},
	abstract = {Stochastic Neighbor Embedding (SNE) is a manifold learning and dimensionality reduction method with a probabilistic approach. In SNE, every point is consider to be the neighbor of all other points with some probability and this probability is tried to be preserved in the embedding space. SNE considers Gaussian distribution for the probability in both the input and embedding spaces. However, t-SNE uses the Student-t and Gaussian distributions in these spaces, respectively. In this tutorial and survey paper, we explain SNE, symmetric SNE, t-SNE (or Cauchy-SNE), and t-SNE with general degrees of freedom. We also cover the out-of-sample extension and acceleration for these methods. Some simulations to visualize the embeddings are also provided.},
	urldate = {2021-03-01},
	journal = {arXiv:2009.10301 [cs, stat]},
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.10301},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NVXEJ6XJ\\2009.html:text/html;Stochastic Neighbor Embedding with Gaussian and Student-t Distributions.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Stochastic Neighbor Embedding with Gaussian and Student-t Distributions.pdf:application/pdf},
}

@article{hintonStochasticNeighborEmbedding,
	title = {Stochastic {Neighbor} {Embedding}},
	abstract = {We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to deﬁne a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional “images” of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word “bank”, to have versions close to the images of both “river” and “ﬁnance” without forcing the images of outdoor concepts to be located close to those of corporate concepts.},
	language = {en},
	author = {Hinton, Geoffrey E and Roweis, Sam T},
	pages = {8},
	file = {Hinton and Roweis - Stochastic Neighbor Embedding.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\J8SH99QV\\Hinton and Roweis - Stochastic Neighbor Embedding.pdf:application/pdf},
}

@inproceedings{choiMultilayerRepresentationLearning2016,
	address = {San Francisco California USA},
	title = {Multi-layer {Representation} {Learning} for {Medical} {Concepts}},
	isbn = {978-1-4503-4232-2},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939823},
	doi = {10.1145/2939672.2939823},
	language = {en},
	urldate = {2021-03-01},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Choi, Edward and Bahadori, Mohammad Taha and Searles, Elizabeth and Coffey, Catherine and Thompson, Michael and Bost, James and Tejedor-Sojo, Javier and Sun, Jimeng},
	month = aug,
	year = {2016},
	pages = {1495--1504},
	file = {Multi-layer Representation Learning for Medical Concepts.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Multi-layer Representation Learning for Medical Concepts.pdf:application/pdf},
}

@article{choiMiMEMultilevelMedical2018,
	title = {{MiME}: {Multilevel} {Medical} {Embedding} of {Electronic} {Health} {Records} for {Predictive} {Healthcare}},
	shorttitle = {{MiME}},
	url = {http://arxiv.org/abs/1810.09593},
	abstract = {Deep learning models exhibit state-of-the-art performance for many predictive healthcare tasks using electronic health records (EHR) data, but these models typically require training data volume that exceeds the capacity of most healthcare systems. External resources such as medical ontologies are used to bridge the data volume constraint, but this approach is often not directly applicable or useful because of inconsistencies with terminology. To solve the data insufficiency challenge, we leverage the inherent multilevel structure of EHR data and, in particular, the encoded relationships among medical codes. We propose Multilevel Medical Embedding (MiME) which learns the multilevel embedding of EHR data while jointly performing auxiliary prediction tasks that rely on this inherent EHR structure without the need for external labels. We conducted two prediction tasks, heart failure prediction and sequential disease prediction, where MiME outperformed baseline methods in diverse evaluation settings. In particular, MiME consistently outperformed all baselines when predicting heart failure on datasets of different volumes, especially demonstrating the greatest performance improvement (15\% relative gain in PR-AUC over the best baseline) on the smallest dataset, demonstrating its ability to effectively model the multilevel structure of EHR data.},
	urldate = {2021-03-01},
	journal = {arXiv:1810.09593 [cs, stat]},
	author = {Choi, Edward and Xiao, Cao and Stewart, Walter F. and Sun, Jimeng},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.09593},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NIVY8RJ6\\1810.html:text/html;MiME.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\MiME.pdf:application/pdf},
}

@article{wuRepresentationLearningEHR2019,
	title = {Representation {Learning} of {EHR} {Data} via {Graph}-{Based} {Medical} {Entity} {Embedding}},
	url = {http://arxiv.org/abs/1910.02574},
	abstract = {Automatic representation learning of key entities in electronic health record (EHR) data is a critical step for healthcare informatics that turns heterogeneous medical records into structured and actionable information. Here we propose ME2Vec, an algorithmic framework for learning low-dimensional vectors of the most common entities in EHR: medical services, doctors, and patients. ME2Vec leverages diverse graph embedding techniques to cater for the unique characteristic of each medical entity. Using real-world clinical data, we demonstrate the efficacy of ME2Vec over competitive baselines on disease diagnosis prediction.},
	urldate = {2021-03-01},
	journal = {arXiv:1910.02574 [cs, stat]},
	author = {Wu, Tong and Wang, Yunlong and Wang, Yue and Zhao, Emily and Yuan, Yilian and Yang, Zhi},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.02574},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MC9AXJPP\\1910.html:text/html;Representation Learning of EHR Data via Graph-Based Medical Entity Embedding.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Representation Learning of EHR Data via Graph-Based Medical Entity Embedding.pdf:application/pdf},
}

@misc{AthenahealthLaunchesNew,
	title = {Athenahealth launches new {EHR}-embedded telehealth tool {\textbar} {Healthcare} {IT} {News}},
	url = {https://www.healthcareitnews.com/news/athenahealth-launches-new-ehr-embedded-telehealth-tool},
	urldate = {2021-03-01},
	file = {Athenahealth launches new EHR-embedded telehealth tool | Healthcare IT News:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\X3SCBBVW\\athenahealth-launches-new-ehr-embedded-telehealth-tool.html:text/html},
}

@inproceedings{liAdaptiveProbabilisticWord2020,
	address = {Taipei Taiwan},
	title = {Adaptive {Probabilistic} {Word} {Embedding}},
	isbn = {978-1-4503-7023-3},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380147},
	doi = {10.1145/3366423.3380147},
	language = {en},
	urldate = {2021-03-01},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {ACM},
	author = {Li, Shuangyin and Zhang, Yu and Pan, Rong and Mo, Kaixiang},
	month = apr,
	year = {2020},
	pages = {651--661},
}

@inproceedings{nugaliyaddeEnhancingSemanticWord2019,
	address = {Perth, WN, Australia},
	title = {Enhancing {Semantic} {Word} {Representations} by {Embedding} {Deep} {Word} {Relationships}},
	isbn = {978-1-4503-6287-0},
	url = {http://dl.acm.org/citation.cfm?doid=3313991.3314019},
	doi = {10.1145/3313991.3314019},
	language = {en},
	urldate = {2021-03-01},
	booktitle = {Proceedings of the 2019 11th {International} {Conference} on {Computer} and {Automation} {Engineering} - {ICCAE} 2019},
	publisher = {ACM Press},
	author = {Nugaliyadde, Anupiya and Wong, Kok Wai and Sohel, Ferdous and Xie, Hong},
	year = {2019},
	pages = {82--87},
}

@article{zhangBioWordVecImprovingBiomedical2019,
	title = {{BioWordVec}, improving biomedical word embeddings with subword information and {MeSH}},
	volume = {6},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/s41597-019-0055-0},
	doi = {10.1038/s41597-019-0055-0},
	language = {en},
	number = {1},
	urldate = {2021-03-01},
	journal = {Scientific Data},
	author = {Zhang, Yijia and Chen, Qingyu and Yang, Zhihao and Lin, Hongfei and Lu, Zhiyong},
	month = dec,
	year = {2019},
	pages = {52},
	file = {BioWordVec, improving biomedical word embeddings with subword information and.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\BioWordVec, improving biomedical word embeddings with subword information and.pdf:application/pdf},
}

@article{hettigeMedGraphStructuralTemporal2020,
	title = {{MedGraph}: {Structural} and {Temporal} {Representation} {Learning} of {Electronic} {Medical} {Records}},
	shorttitle = {\${\textbackslash}mathtt\{\vphantom{\}}{MedGraph}},
	url = {http://arxiv.org/abs/1912.03703},
	abstract = {Electronic medical record (EMR) data contains historical sequences of visits of patients, and each visit contains rich information, such as patient demographics, hospital utilisation and medical codes, including diagnosis, procedure and medication codes. Most existing EMR embedding methods capture visit-code associations by constructing input visit representations as binary vectors with a static vocabulary of medical codes. With this limited representation, they fail in encapsulating rich attribute information of visits (demographics and utilisation information) and/or codes (e.g., medical code descriptions). Furthermore, current work considers visits of the same patient as discrete-time events and ignores time gaps between them. However, the time gaps between visits depict dynamics of the patient's medical history inducing varying influences on future visits. To address these limitations, we present \${\textbackslash}mathtt\{MedGraph\}\$, a supervised EMR embedding method that captures two types of information: (1) the visit-code associations in an attributed bipartite graph, and (2) the temporal sequencing of visits through a point process. \${\textbackslash}mathtt\{MedGraph\}\$ produces Gaussian embeddings for visits and codes to model the uncertainty. We evaluate the performance of \${\textbackslash}mathtt\{MedGraph\}\$ through an extensive experimental study and show that \${\textbackslash}mathtt\{MedGraph\}\$ outperforms state-of-the-art EMR embedding methods in several medical risk prediction tasks.},
	urldate = {2021-03-01},
	journal = {arXiv:1912.03703 [cs, stat]},
	author = {Hettige, Bhagya and Li, Yuan-Fang and Wang, Weiqing and Le, Suong and Buntine, Wray},
	month = jul,
	year = {2020},
	note = {arXiv: 1912.03703},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {\$-mathtt MedGraph.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\\$-mathtt MedGraph.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MVDDINBC\\1912.html:text/html},
}

@book{hastieElementsStatisticalLearning2009,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	url = {http://link.springer.com/10.1007/978-0-387-84858-7},
	urldate = {2021-03-02},
	publisher = {Springer New York},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	doi = {10.1007/978-0-387-84858-7},
	file = {The Elements of Statistical Learning.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\The Elements of Statistical Learning.md:text/plain;The Elements of Statistical Learning.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\The Elements of Statistical Learning.pdf:application/pdf},
}

@inproceedings{chenTwoStageApproach2009,
	address = {Las Vegas, NV, USA},
	title = {A two stage approach for {Contiguous} {Sequential} {Pattern} mining},
	isbn = {978-1-4244-4114-3},
	url = {http://ieeexplore.ieee.org/document/5211583/},
	doi = {10.1109/IRI.2009.5211583},
	urldate = {2021-03-04},
	booktitle = {2009 {IEEE} {International} {Conference} on {Information} {Reuse} \& {Integration}},
	publisher = {IEEE},
	author = {Chen, Jinlin and Shankar, Subash and Kelly, Angela and Gningue, Serigne and Rajaravivarma, Rathika},
	month = aug,
	year = {2009},
	pages = {382--387},
}

@article{brigatoCloseLookDeep2020,
	title = {A {Close} {Look} at {Deep} {Learning} with {Small} {Data}},
	url = {http://arxiv.org/abs/2003.12843},
	abstract = {In this work, we perform a wide variety of experiments with different deep learning architectures on datasets of limited size. According to our study, we show that model complexity is a critical factor when only a few samples per class are available. Differently from the literature, we show that in some configurations, the state of the art can be improved using low complexity models. For instance, in problems with scarce training samples and without data augmentation, low-complexity convolutional neural networks perform comparably well or better than state-of-the-art architectures. Moreover, we show that even standard data augmentation can boost recognition performance by large margins. This result suggests the development of more complex data generation/augmentation pipelines for cases when data is limited. Finally, we show that dropout, a widely used regularization technique, maintains its role as a good regularizer even when data is scarce. Our findings are empirically validated on the sub-sampled versions of popular CIFAR-10, Fashion-MNIST and, SVHN benchmarks.},
	urldate = {2021-03-08},
	journal = {arXiv:2003.12843 [cs, stat]},
	author = {Brigato, L. and Iocchi, L.},
	month = oct,
	year = {2020},
	note = {arXiv: 2003.12843},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {A Close Look at Deep Learning with Small Data.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\A Close Look at Deep Learning with Small Data.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TVQTM38R\\2003.html:text/html},
}

@article{desousaCOVID19ClassificationXray2021,
	title = {{COVID}-19 classification in {X}-ray chest images using a new convolutional neural network: {CNN}-{COVID}},
	issn = {2446-4732, 2446-4740},
	shorttitle = {{COVID}-19 classification in {X}-ray chest images using a new convolutional neural network},
	url = {http://link.springer.com/10.1007/s42600-020-00120-5},
	doi = {10.1007/s42600-020-00120-5},
	language = {en},
	urldate = {2021-03-08},
	journal = {Research on Biomedical Engineering},
	author = {de Sousa, Pedro Moisés and Carneiro, Pedro Cunha and Oliveira, Mariane Modesto and Pereira, Gabrielle Macedo and da Costa Junior, Carlos Alberto and de Moura, Luis Vinicius and Mattjie, Christian and da Silva, Ana Maria Marques and Patrocinio, Ana Claudia},
	month = jan,
	year = {2021},
	file = {COVID-19 classification in X-ray chest images using a new convolutional neural.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\COVID-19 classification in X-ray chest images using a new convolutional neural.pdf:application/pdf;de Sousa et al. - 2022 - COVID-19 classification in X-ray chest images usin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JPJ54R3K\\de Sousa et al. - 2022 - COVID-19 classification in X-ray chest images usin.pdf:application/pdf},
}

@article{greenbergVL2ScalableFlexible2009,
	title = {{VL2}: a scalable and flexible data center network},
	volume = {39},
	issn = {0146-4833},
	shorttitle = {{VL2}},
	url = {https://dl.acm.org/doi/10.1145/1594977.1592576},
	doi = {10.1145/1594977.1592576},
	abstract = {To be agile and cost effective, data centers should allow dynamic resource allocation across large server pools. In particular, the data center network should enable any server to be assigned to any service. To meet these goals, we present VL2, a practical network architecture that scales to support huge data centers with uniform high capacity between servers, performance isolation between services, and Ethernet layer-2 semantics. VL2 uses (1) flat addressing to allow service instances to be placed anywhere in the network, (2) Valiant Load Balancing to spread traffic uniformly across network paths, and (3) end-system based address resolution to scale to large server pools, without introducing complexity to the network control plane. VL2's design is driven by detailed measurements of traffic and fault data from a large operational cloud service provider. VL2's implementation leverages proven network technologies, already available at low cost in high-speed hardware implementations, to build a scalable and reliable network architecture. As a result, VL2 networks can be deployed today, and we have built a working prototype. We evaluate the merits of the VL2 design using measurement, analysis, and experiments. Our VL2 prototype shuffles 2.7 TB of data among 75 servers in 395 seconds - sustaining a rate that is 94\% of the maximum possible.},
	language = {en},
	number = {4},
	urldate = {2021-05-21},
	journal = {ACM SIGCOMM Computer Communication Review},
	author = {Greenberg, Albert and Hamilton, James R. and Jain, Navendu and Kandula, Srikanth and Kim, Changhoon and Lahiri, Parantap and Maltz, David A. and Patel, Parveen and Sengupta, Sudipta},
	month = aug,
	year = {2009},
	pages = {51--62},
	file = {VL2 a scalable and flexible data center network - Literature Note.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Computer Science\\VL2 a scalable and flexible data center network - Literature Note.md:text/plain;VL2 a scalable and flexible data center network.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\VL2 a scalable and flexible data center network.md:text/plain;VL2.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\VL2.pdf:application/pdf},
}

@book{internationalconferenceonmobilesystemsapplicationsandservicesProceedingsMobiSys20032003,
	address = {Berkeley, Calif},
	title = {Proceedings of {MobiSys} 2003, the {First} {International} {Conference} on {Mobile} {Systems}, {Applications}, and {Services}: {May} 5 - 8, 2003, {San} {Francisco}, {CA}, {USA}},
	isbn = {978-1-931971-09-6},
	shorttitle = {Proceedings of {MobiSys} 2003, the {First} {International} {Conference} on {Mobile} {Systems}, {Applications}, and {Services}},
	language = {eng},
	publisher = {USENIX Association},
	editor = {International Conference on Mobile Systems, Applications, {and} Services and Association for Computing Machinery and USENIX Association and Association for Computing Machinery},
	year = {2003},
	note = {Meeting Name: International Conference on Mobile Systems, Applications, and Services
OCLC: 249708838},
}

@inproceedings{179731,
	address = {Seattle, WA},
	title = {Network virtualization in multi-tenant datacenters},
	isbn = {978-1-931971-09-6},
	url = {https://www.usenix.org/conference/nsdi14/technical-sessions/presentation/koponen},
	booktitle = {11th {USENIX} symposium on networked systems design and implementation ({NSDI} 14)},
	publisher = {USENIX Association},
	author = {Koponen, Teemu and Amidon, Keith and Balland, Peter and Casado, Martin and Chanda, Anupam and Fulton, Bryan and Ganichev, Igor and Gross, Jesse and Ingram, Paul and Jackson, Ethan and Lambeth, Andrew and Lenglet, Romain and Li, Shih-Hao and Padmanabhan, Amar and Pettit, Justin and Pfaff, Ben and Ramanathan, Rajiv and Shenker, Scott and Shieh, Alan and Stribling, Jeremy and Thakkar, Pankaj and Wendlandt, Dan and Yip, Alexander and Zhang, Ronghua},
	month = apr,
	year = {2014},
	pages = {203--216},
	file = {Network virtualization in multi-tenant datacenters.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Network virtualization in multi-tenant datacenters.md:text/plain},
}

@article{maggsAlgorithmicNuggetsContent2015,
	title = {Algorithmic {Nuggets} in {Content} {Delivery}},
	volume = {45},
	issn = {0146-4833},
	url = {https://dl.acm.org/doi/10.1145/2805789.2805800},
	doi = {10.1145/2805789.2805800},
	abstract = {This paper “peeks under the covers” at the subsystems that provide the basic functionality of a leading content delivery network. Based on our experiences in building one of the largest distributed systems in the world, we illustrate how sophisticated algorithmic research has been adapted to balance the load between and within server clusters, manage the caches on servers, select paths through an overlay routing network, and elect leaders in various contexts. In each instance, we ﬁrst explain the theory underlying the algorithms, then introduce practical considerations not captured by the theoretical models, and ﬁnally describe what is implemented in practice. Through these examples, we highlight the role of algorithmic research in the design of complex networked systems. The paper also illustrates the close synergy that exists between research and industry where research ideas cross over into products and product requirements drive future research.},
	language = {en},
	number = {3},
	urldate = {2021-05-25},
	journal = {ACM SIGCOMM Computer Communication Review},
	author = {Maggs, Bruce M. and Sitaraman, Ramesh K.},
	month = jul,
	year = {2015},
	pages = {52--66},
	file = {Algorithmic Nuggets in Content Delivery.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Algorithmic Nuggets in Content Delivery.md:text/plain;Maggs and Sitaraman - 2015 - Algorithmic Nuggets in Content Delivery.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JGSG6AB5\\Maggs and Sitaraman - 2015 - Algorithmic Nuggets in Content Delivery.pdf:application/pdf},
}

@article{frankCollaborationOpportunitiesContent,
	title = {Collaboration {Opportunities} for {Content} {Delivery} and {Network} {Infrastructures}},
	abstract = {This chapter builds upon the student’s basic knowledge of how the Internet infrastructure operates, i.e., as a network of networks. After reading this chapter the student should have a fundamental understanding about how content distribution via the Internet works today, what the challenges are, and which opportunities lie ahead. Moreover, the chapter points out how all parties—including end users—can beneﬁt from the collaboration between ISPs and content providers. Indeed, simple, almost intuitive, means will enable such collaboration.},
	language = {en},
	author = {Frank, Benjamin and Poese, Ingmar and Smaragdakis, Georgios and Feldmann, Anja and Maggs, Bruce M and Uhlig, Steve and Aggarwal, Vinay and Schneider, Fabian},
	pages = {87},
	file = {Frank et al. - Collaboration Opportunities for Content Delivery a.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C2DEEQ4C\\Frank et al. - Collaboration Opportunities for Content Delivery a.pdf:application/pdf},
}

@article{clevelandGraphicalPerceptionTheory1984,
	title = {Graphical {Perception}: {Theory}, {Experimentation}, and {Application} to the {Development} of {Graphical} {Methods}},
	volume = {79},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Graphical {Perception}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1984.10478080},
	doi = {10.1080/01621459.1984.10478080},
	language = {en},
	number = {387},
	urldate = {2021-05-28},
	journal = {Journal of the American Statistical Association},
	author = {Cleveland, William S. and McGill, Robert},
	month = sep,
	year = {1984},
	pages = {531--554},
	file = {Graphical Perception Theory, Experimentation, and Application to the Development of Graphical Methods.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Graphical Perception Theory, Experimentation, and Application to the Development of Graphical Methods.md:text/plain;Graphical Perception.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Graphical Perception.pdf:application/pdf},
}

@article{kanwalAttentionbasedClinicalNote2021,
	title = {Attention-based {Clinical} {Note} {Summarization}},
	url = {http://arxiv.org/abs/2104.08942},
	abstract = {The trend of deploying digital systems in numerous industries has induced a hike in recording digital information. The health sector has observed a large adoption of digital devices and systems generating large volumes of personal medical health records. Electronic health records contain valuable information for retrospective and prospective analysis that is often not entirely exploited because of the dense information storage. The crude purpose of condensing health records is to select the information that holds most characteristics of the original documents based on reported disease. These summaries may boost diagnosis and extend a doctor’s interaction time with the patient during a high workload situation like the COVID-19 pandemic. In this paper, we propose a multi-head attention-based mechanism to perform extractive summarization of meaningful phrases in clinical notes. This method finds major sentences for a summary by correlating tokens, segments and positional embeddings. The model outputs attention scores that are statistically transformed to extract key phrases and can be used for a projection on the heat-mapping tool for visual and human use.},
	language = {en},
	urldate = {2021-05-30},
	journal = {arXiv:2104.08942 [cs]},
	author = {Kanwal, Neel and Rizzo, Giuseppe},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.08942},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Healthcare - Clinical Notes, NLP - Text Summarization},
	file = {Kanwal and Rizzo - 2021 - Attention-based Clinical Note Summarization.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JRNGMTFL\\Kanwal and Rizzo - 2021 - Attention-based Clinical Note Summarization.pdf:application/pdf},
}

@article{vanakenClinicalOutcomePrediction2021,
	title = {Clinical {Outcome} {Prediction} from {Admission} {Notes} using {Self}-{Supervised} {Knowledge} {Integration}},
	url = {http://arxiv.org/abs/2102.04110},
	abstract = {Outcome prediction from clinical text can prevent doctors from overlooking possible risks and help hospitals to plan capacities. We simulate patients at admission time, when decision support can be especially valuable, and contribute a novel admission to discharge task with four common outcome prediction targets: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay prediction. The ideal system should infer outcomes based on symptoms, pre-conditions and risk factors of a patient. We evaluate the effectiveness of language models to handle this scenario and propose clinical outcome pretraining to integrate knowledge about patient outcomes from multiple public sources. We further present a simple method to incorporate ICD code hierarchy into the models. We show that our approach improves performance on the outcome tasks against several baselines. A detailed analysis reveals further strengths of the model, including transferability, but also weaknesses such as handling of vital values and inconsistencies in the underlying data.},
	language = {en},
	urldate = {2021-05-30},
	journal = {arXiv:2102.04110 [cs]},
	author = {van Aken, Betty and Papaioannou, Jens-Michalis and Mayrdorfer, Manuel and Budde, Klemens and Gers, Felix A. and Löser, Alexander},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.04110},
	keywords = {Computer Science - Computation and Language, Project},
	file = {van Aken et al. - 2021 - Clinical Outcome Prediction from Admission Notes u.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZKBJI25E\\van Aken et al. - 2021 - Clinical Outcome Prediction from Admission Notes u.pdf:application/pdf},
}

@article{huangClinicalBERTModelingClinical2020,
	title = {{ClinicalBERT}: {Modeling} {Clinical} {Notes} and {Predicting} {Hospital} {Readmission}},
	shorttitle = {{ClinicalBERT}},
	url = {http://arxiv.org/abs/1904.05342},
	abstract = {Clinical notes contain information about patients beyond structured data such as lab values or medications. However, clinical notes have been underused relative to structured data, because notes are highdimensional and sparse. We aim to develop and evaluate a continuous representation of clinical notes. Given this representation, our goal is to predict 30-day hospital readmission at various timepoints of admission, including early stages and at discharge. We apply bidirectional encoder representations from transformers (bert) to clinical text. Publicly-released bert parameters are trained on standard corpora such as Wikipedia and BookCorpus, which differ from clinical text. We therefore pre-train bert using clinical notes and finetune the network for the task of predicting hospital readmission. This defines ClinicalBERT. ClinicalBERT uncovers high-quality relationships between medical concepts, as judged by physicians. ClinicalBERT outperforms various baselines on 30-day hospital readmission prediction using both discharge summaries and the first few days of notes in the intensive care unit on various clinically-motivated metrics. The attention weights of ClinicalBERT can also be used to interpret predictions. To facilitate research, we open-source model parameters, and scripts for training and evaluation. ClinicalBERT is a flexible framework to represent clinical notes. It improves on previous clinical text processing methods and with little engineering can be adapted to other clinical predictive tasks.},
	language = {en},
	urldate = {2021-05-30},
	journal = {arXiv:1904.05342 [cs]},
	author = {Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
	month = nov,
	year = {2020},
	note = {arXiv: 1904.05342},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {ClinicalBERT - Modeling Clinical Notes and Predicting Hospital Readmission.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VSTEB2YF\\ClinicalBERT - Modeling Clinical Notes and Predicting Hospital Readmission.pdf:application/pdf},
}

@article{leeBioBERTPretrainedBiomedical2019,
	title = {{BioBERT}: a pre-trained biomedical language representation model for biomedical text mining},
	issn = {1367-4803, 1460-2059},
	shorttitle = {{BioBERT}},
	url = {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btz682/5566506},
	doi = {10.1093/bioinformatics/btz682},
	abstract = {Motivation: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.},
	language = {en},
	urldate = {2021-05-30},
	journal = {Bioinformatics},
	author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
	editor = {Wren, Jonathan},
	month = sep,
	year = {2019},
	pages = {btz682},
	file = {Lee et al. - 2019 - BioBERT a pre-trained biomedical language represe.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YUYFHHUT\\Lee et al. - 2019 - BioBERT a pre-trained biomedical language represe.pdf:application/pdf},
}

@misc{TwelveMillionPatients,
	title = {Twelve million patients misdiagnosed yearly in {America}, says {VA} researcher},
	url = {https://www.research.va.gov/currents/summer2014/summer2014-8.cfm},
	urldate = {2021-06-01},
	file = {Twelve million patients misdiagnosed yearly in America, says VA researcher:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I2SFDYI4\\summer2014-8.html:text/html},
}

@book{BestCareLower2013,
	address = {Washington, D.C.},
	title = {Best {Care} at {Lower} {Cost}: {The} {Path} to {Continuously} {Learning} {Health} {Care} in {America}},
	isbn = {978-0-309-28281-9},
	shorttitle = {Best {Care} at {Lower} {Cost}},
	url = {http://www.nap.edu/catalog/13444},
	language = {en},
	urldate = {2021-06-01},
	publisher = {National Academies Press},
	month = may,
	year = {2013},
	doi = {10.17226/13444},
	note = {Pages: 13444},
	file = {2013 - Best Care at Lower Cost The Path to Continuously .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7V6R6GV3\\2013 - Best Care at Lower Cost The Path to Continuously .pdf:application/pdf},
}

@misc{pinnaclecareHumanCostFinancial2016,
	title = {The {Human} {Cost} and {Financial} {Impact} of {Misdiagnosis}},
	abstract = {Medical misdiagnosis, in the form of inaccurate, late, and delayed diagnoses, is an ongoing problem in the U.S. Not only do these diagnostic errors present an ongoing risk to the health and safety of patients, but they also cost the economy billions of dollars.
This paper summarizes the key research findings on the frequency, human cost, and financial impact of these diagnostic errors, while providing new data on the value of second opinions. An extensive Institute of Medicine (IOM) report underscores the importance of this with its conclusion that most Americans will receive an inaccurate or late diagnosis at least once in their lives, often with life-threatening consequences.},
	publisher = {PinnacleCare},
	author = {{PinnacleCare}},
	year = {2016},
	file = {Human-Cost-Financial-Impact-Whitepaper.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\A526Q4UX\\Human-Cost-Financial-Impact-Whitepaper.pdf:application/pdf},
}

@book{committeeondiagnosticerrorinhealthcareImprovingDiagnosisHealth2015,
	address = {Washington, D.C.},
	title = {Improving {Diagnosis} in {Health} {Care}},
	isbn = {978-0-309-37769-0},
	url = {http://www.nap.edu/catalog/21794},
	language = {en},
	urldate = {2021-06-01},
	publisher = {National Academies Press},
	author = {{Committee on Diagnostic Error in Health Care} and {Board on Health Care Services} and {Institute of Medicine} and {The National Academies of Sciences, Engineering, and Medicine}},
	editor = {Balogh, Erin P. and Miller, Bryan T. and Ball, John R.},
	month = dec,
	year = {2015},
	doi = {10.17226/21794},
	note = {Pages: 21794},
	file = {Committee on Diagnostic Error in Health Care et al. - 2015 - Improving Diagnosis in Health Care.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4DV6XQ4L\\Committee on Diagnostic Error in Health Care et al. - 2015 - Improving Diagnosis in Health Care.pdf:application/pdf},
}

@article{haywardCountingDeathsDue2002,
	title = {Counting {Deaths} {Due} to {Medical} {Errors}},
	volume = {288},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.288.19.2404-JLT1120-2-2},
	doi = {10.1001/jama.288.19.2404-JLT1120-2-2},
	abstract = {To the Editor: Quoting statistics derived from the Harvard Medical Practice Study (HMPS), Dr Leape and colleagues1 remark upon "The epidemiologic finding that . . . nearly 100 000 [hospital] deaths occur in the United States annually as a result of mistakes in medical care. . . . "1 I believe that authors need to stop perpetuating this number of "100 000 hospital deaths," a statistic for which there is no valid epidemiologic evidence.2-4 This dramatic statistic is largely the by-product of bias introduced by a combination of outlier opinion and the low reliability of physician-implicit review (the method used to produce almost all published estimates of deaths and injuries due to medical errors).2-4},
	number = {19},
	urldate = {2021-06-01},
	journal = {JAMA},
	author = {Hayward, Rodney A.},
	month = nov,
	year = {2002},
	pages = {2404--2404},
}

@article{johnsonMIMICIIIFreelyAccessible2016,
	title = {{MIMIC}-{III}, a freely accessible critical care database},
	volume = {3},
	issn = {2052-4463},
	url = {https://doi.org/10.1038/sdata.2016.35},
	doi = {10.1038/sdata.2016.35},
	abstract = {MIMIC-III (‘Medical Information Mart for Intensive Care’) is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
	number = {1},
	journal = {Scientific Data},
	author = {Johnson, Alistair E.W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G.},
	month = may,
	year = {2016},
	pages = {160035},
}

@misc{DBMIPortal,
	title = {{DBMI} {Portal}},
	url = {https://portal.dbmi.hms.harvard.edu/},
	urldate = {2021-06-02},
	file = {DBMI Portal:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SLX2WQDI\\portal.dbmi.hms.harvard.edu.html:text/html},
}

@misc{TranscribedMedicalTranscription,
	title = {Transcribed {Medical} {Transcription} {Sample} {Reports} and {Examples} - {MTSamples}},
	url = {https://www.mtsamples.com/},
	urldate = {2021-06-02},
	file = {Transcribed Medical Transcription Sample Reports and Examples - MTSamples:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PWLCEMPY\\www.mtsamples.com.html:text/html},
}

@misc{DeveloperResources,
	title = {Developer {Resources}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/tools/developers/},
	urldate = {2021-06-02},
	file = {Developer Resources:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\52PG9SE3\\developers.html:text/html},
}

@misc{HomePMCNCBI,
	title = {Home - {PMC} - {NCBI}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/},
	urldate = {2021-06-02},
	file = {Home - PMC - NCBI:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2HLPNP6I\\pmc.html:text/html},
}

@article{BenAbacha-BMC-2019,
	title = {A question-entailment approach to question answering},
	volume = {20},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3119-4},
	number = {1},
	journal = {BMC Bioinform.},
	author = {Ben Abacha, Asma and Demner-Fushman, Dina},
	year = {2019},
	pages = {511:1--511:23},
}

@article{arnold2019sector,
	title = {{SECTOR}: {A} neural model for coherent topic segmentation and classification},
	volume = {7},
	doi = {10.1162/tacl\_a\_00261},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Arnold, Sebastian and Schneider, Rudolf and Cudré-Mauroux, Philippe and Gers, Felix A. and Löser, Alexander},
	year = {2019},
	pages = {169--184},
}

@misc{ICD9CMDiagnosisProcedure,
	title = {{ICD}-9-{CM} {Diagnosis} and {Procedure} {Codes}: {Abbreviated} and {Full} {Code} {Titles} {\textbar} {CMS}},
	url = {https://www.cms.gov/Medicare/Coding/ICD9ProviderDiagnosticCodes/codes},
	urldate = {2021-06-02},
	file = {ICD-9-CM Diagnosis and Procedure Codes\: Abbreviated and Full Code Titles | CMS:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZYAUQB26\\codes.html:text/html},
}

@article{chuanCreatingEvaluatingChatbots2021,
	title = {Creating and {Evaluating} {Chatbots} as {Eligibility} {Assistants} for {Clinical} {Trials}: {An} {Active} {Deep} {Learning} {Approach} towards {User}-centered {Classification}},
	volume = {2},
	issn = {2691-1957, 2637-8051},
	shorttitle = {Creating and {Evaluating} {Chatbots} as {Eligibility} {Assistants} for {Clinical} {Trials}},
	url = {https://dl.acm.org/doi/10.1145/3403575},
	doi = {10.1145/3403575},
	abstract = {Clinical trials are important tools to improve knowledge about the effectiveness of new treatments for all diseases, including cancers. However, studies show that fewer than 5\% of cancer patients are enrolled in any type of research study or clinical trial. Although there is a wide variety of reasons for the low participation rate, we address this issue by designing a chatbot to help users determine their eligibility via interactive, two-way communication. The chatbot is supported by a user-centered classifier that uses an active deep learning approach to separate complex eligibility criteria into questions that can be easily answered by users and information that requires verification by their doctors. We collected all the available clinical trial eligibility criteria from the National Cancer Institute's website to evaluate the chatbot and the classifier. Experimental results show that the active deep learning classifier outperforms the baseline k-nearest neighbor method. In addition, an in-person experiment was conducted to evaluate the effectiveness of the chatbot. The results indicate that the participants who used the chatbot achieved better understanding about eligibility than those who used only the website. Furthermore, interfaces with chatbots were rated significantly better in terms of perceived usability, interactivity, and dialogue.},
	language = {en},
	number = {1},
	urldate = {2021-06-02},
	journal = {ACM Transactions on Computing for Healthcare},
	author = {Chuan, Ching-Hua and Morgan, Susan},
	month = jan,
	year = {2021},
	pages = {1--19},
	file = {Chuan and Morgan - 2021 - Creating and Evaluating Chatbots as Eligibility As.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T49EIJ2K\\Chuan and Morgan - 2021 - Creating and Evaluating Chatbots as Eligibility As.pdf:application/pdf},
}

@inproceedings{sinhaDesigningAIDigital2020,
	address = {Genoa Italy},
	title = {Designing with {AI} for {Digital} {Marketing}},
	isbn = {978-1-4503-7950-2},
	url = {https://dl.acm.org/doi/10.1145/3386392.3397600},
	doi = {10.1145/3386392.3397600},
	abstract = {We present an interactive user interface that allows digital marketing professionals to have real time access to insights from a back-end AI that predicts potential click-through rates of composed content based on similar past campaigns. We wanted to investigate the extent to which digital marketing professionals would find our system usable and useful and whether or not the advice our system generated would create content that had higher click through rates than content developed without the system’s advice. Our framework decomposes aspects of prior campaigns into features including image quality, memorability, and placement; and text readability, formality and sentiment. We show our algorithm has high predictive value on a historical test set (AUC .80); that digital marketing professionals give the system an overall high satisfaction rating and that, using the advice of the AI agent, we can generate content that creates up to 22\% click-through rate lift on a 700 A/B preference tasks given to master workers on AMT.},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {Adjunct {Publication} of the 28th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {ACM},
	author = {Sinha, Moumita and Healey, Jennifer and Sengupta, Tathagata},
	month = jul,
	year = {2020},
	pages = {65--70},
	file = {Sinha et al. - 2020 - Designing with AI for Digital Marketing.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7KFYJMUR\\Sinha et al. - 2020 - Designing with AI for Digital Marketing.pdf:application/pdf},
}

@article{chenDeepLearningMobile2020,
	title = {Deep {Learning} on {Mobile} and {Embedded} {Devices}: {State}-of-the-art, {Challenges}, and {Future} {Directions}},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Deep {Learning} on {Mobile} and {Embedded} {Devices}},
	url = {https://dl.acm.org/doi/10.1145/3398209},
	doi = {10.1145/3398209},
	abstract = {Recent years have witnessed an exponential increase in the use of mobile and embedded devices. With the great success of deep learning in many fields, there is an emerging trend to deploy deep learning on mobile and embedded devices to better meet the requirement of real-time applications and user privacy protection. However, the limited resources of mobile and embedded devices make it challenging to fulfill the intensive computation and storage demand of deep learning models. In this survey, we conduct a comprehensive review on the related issues for deep learning on mobile and embedded devices. We start with a brief introduction of deep learning and discuss major challenges of implementing deep learning models on mobile and embedded devices. We then conduct an in-depth survey on important compression and acceleration techniques that help adapt deep learning models to mobile and embedded devices, which we specifically classify as pruning, quantization, model distillation, network design strategies, and low-rank factorization. We elaborate on the hardware-based solutions, including mobile GPU, FPGA, and ASIC, and describe software frameworks for mobile deep learning models, especially the development of frameworks based on OpenCL and RenderScript. After that, we present the application of mobile deep learning in a variety of areas, such as navigation, health, speech recognition, and information security. Finally, we discuss some future directions for deep learning on mobile and embedded devices to inspire further research in this area.},
	language = {en},
	number = {4},
	urldate = {2021-06-03},
	journal = {ACM Computing Surveys},
	author = {Chen, Yanjiao and Zheng, Baolin and Zhang, Zihan and Wang, Qian and Shen, Chao and Zhang, Qian},
	month = sep,
	year = {2020},
	pages = {1--37},
	file = {Chen et al. - 2020 - Deep Learning on Mobile and Embedded Devices Stat.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\A4S6ITBR\\Chen et al. - 2020 - Deep Learning on Mobile and Embedded Devices Stat.pdf:application/pdf},
}

@inproceedings{hanPriceSuggestionOnline2020,
	address = {Seattle WA USA},
	title = {Price {Suggestion} for {Online} {Second}-hand {Items} with {Texts} and {Images}},
	isbn = {978-1-4503-7988-5},
	url = {https://dl.acm.org/doi/10.1145/3394171.3413759},
	doi = {10.1145/3394171.3413759},
	abstract = {This paper presents an intelligent price suggestion system for online second-hand listings based on their uploaded images and text descriptions. The goal of price prediction is to help sellers set effective and reasonable prices for their second-hand items with the images and text descriptions uploaded to the online platforms. Specifically, we design a multi-modal price suggestion system which takes as input the extracted visual and textual features along with some statistical item features collected from the second-hand item shopping platform to determine whether the image and text of an uploaded second-hand item are qualified for reasonable price suggestion with a binary classification model, and provide price suggestions for second-hand items with qualified images and text descriptions with a regression model. To satisfy different demands, two different constraints are added into the joint training of the classification model and the regression model. Moreover, a customized loss function is designed for optimizing the regression model to provide price suggestions for second-hand items, which can not only maximize the gain of the sellers but also facilitate the online transaction. We also derive a set of metrics to better evaluate the proposed price suggestion system. Extensive experiments on a large real-world dataset demonstrate the effectiveness of the proposed multi-modal price suggestion system.},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Han, Liang and Yin, Zhaozheng and Xia, Zhurong and Tang, Minqian and Jin, Rong},
	month = oct,
	year = {2020},
	pages = {2784--2792},
	file = {Han et al. - 2020 - Price Suggestion for Online Second-hand Items with.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XY2I2F3G\\Han et al. - 2020 - Price Suggestion for Online Second-hand Items with.pdf:application/pdf},
}

@inproceedings{hanVisionbasedPriceSuggestion2019,
	address = {Nice France},
	title = {Vision-based {Price} {Suggestion} for {Online} {Second}-hand {Items}},
	isbn = {978-1-4503-6889-6},
	url = {https://dl.acm.org/doi/10.1145/3343031.3350936},
	doi = {10.1145/3343031.3350936},
	abstract = {Different from shopping in physical stores, where people have the opportunity to closely check a product (e.g., touching the surface of a T-shirt or smelling the scent of perfume) before making a purchase decision, online shoppers rely greatly on the uploaded product images to make any purchase decision. The decision-making is challenging when selling or purchasing second-hand items online since estimating the items’ prices is not trivial. In this work, we present a vision-based price suggestion system for the online second-hand item shopping platform. The goal of vision-based price suggestion is to help sellers set effective prices for their second-hand listings with the images uploaded to the online platforms.},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Han, Liang and Yin, Zhaozheng and Xia, Zhurong and Guo, Li and Tang, Mingqian and Jin, Rong},
	month = oct,
	year = {2019},
	pages = {1988--1996},
	file = {Han et al. - 2019 - Vision-based Price Suggestion for Online Second-ha.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QFQ7VJZ8\\Han et al. - 2019 - Vision-based Price Suggestion for Online Second-ha.pdf:application/pdf},
}

@article{xuCoreInterestNetwork2021,
	title = {Core {Interest} {Network} for {Click}-{Through} {Rate} {Prediction}},
	volume = {15},
	issn = {1556-4681, 1556-472X},
	url = {https://dl.acm.org/doi/10.1145/3428079},
	doi = {10.1145/3428079},
	abstract = {In modern online advertising systems, the click-through rate (CTR) is an important index to measure the popularity of an item. It refers to the ratio of users who click on a specific advertisement to the number of total users who view it. Predicting the CTR of an item in advance can improve the accuracy of the advertisement recommendation. And it is commonly calculated based on users’ interests. Thus, extracting users’ interests is of great importance in CTR prediction tasks. In the literature, a lot of studies treat the interaction between users and items as sequential data and apply the recurrent neural network (RNN) model to extract users’ interests. However, these solutions cannot handle the case when the sequence length is relatively long, e.g., over 100. This is because of the vanishing gradient problem of RNN, i.e., the model cannot learn a users’ previous behaviors that are too far away from the current moment. To address this problem, we propose a new Core Interest Network (CIN) model to mitigate the problem of a long sequence in the CTR prediction task with sequential data. In brief, we first extract the core interests of users and then use the refined data as the input of subsequent learning tasks. Extensive evaluations on real dataset show that our CIN model can outperform the state-of-the-art solutions in terms of prediction accuracy.},
	language = {en},
	number = {2},
	urldate = {2021-06-03},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Xu, En and Yu, Zhiwen and Guo, Bin and Cui, Helei},
	month = apr,
	year = {2021},
	pages = {1--16},
	file = {Xu et al. - 2021 - Core Interest Network for Click-Through Rate Predi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9Q6NM7RG\\Xu et al. - 2021 - Core Interest Network for Click-Through Rate Predi.pdf:application/pdf},
}

@inproceedings{xiaoDeepMultiInterestNetwork2020,
	address = {Virtual Event Ireland},
	title = {Deep {Multi}-{Interest} {Network} for {Click}-through {Rate} {Prediction}},
	isbn = {978-1-4503-6859-9},
	url = {https://dl.acm.org/doi/10.1145/3340531.3412092},
	doi = {10.1145/3340531.3412092},
	abstract = {Click-through rate prediction plays an important role in many fields, such as recommender and advertising systems. It is one of the crucial parts to improve user experience and increase industry revenue. Recently, several deep learning-based models are successfully applied to this area. Some existing studies further model user representation based on user historical behavior sequence, in order to capture dynamic and evolving interests. We observe that users usually have multiple interests at a time and the latent dominant interest is expressed by the behavior. The switch of latent dominant interest results in the behavior changes. Thus, modeling and tracking latent multiple interests would be beneficial. In this paper, we propose a novel method named as Deep Multi-Interest Network (DMIN) which models user’s latent multiple interests for click-through rate prediction task. Specifically, we design a Behavior Refiner Layer using multi-head self-attention to capture better user historical item representations. Then the Multi-Interest Extractor Layer is applied to extract multiple user interests. We evaluate our method on three real-world datasets. Experimental results show that the proposed DMIN outperforms various state-of-the-art baselines in terms of click-through rate prediction task.},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {ACM},
	author = {Xiao, Zhibo and Yang, Luwei and Jiang, Wen and Wei, Yi and Hu, Yi and Wang, Hao},
	month = oct,
	year = {2020},
	keywords = {deepcvr},
	pages = {2265--2268},
	file = {Xiao et al. - 2020 - Deep Multi-Interest Network for Click-through Rate.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TXVIGVAB\\Xiao et al. - 2020 - Deep Multi-Interest Network for Click-through Rate.pdf:application/pdf},
}

@article{zhangDeepLearningClickThrough2021,
	title = {Deep {Learning} for {Click}-{Through} {Rate} {Estimation}},
	url = {http://arxiv.org/abs/2104.10584},
	abstract = {Click-through rate (CTR) estimation plays as a core function module in various personalized online services, including online advertising, recommender systems, and web search etc. From 2015, the success of deep learning started to beneﬁt CTR estimation performance and now deep CTR models have been widely applied in many industrial platforms. In this survey, we provide a comprehensive review of deep learning models for CTR estimation tasks. First, we take a review of the transfer from shallow to deep CTR models and explain why going deep is a necessary trend of development. Second, we concentrate on explicit feature interaction learning modules of deep CTR models. Then, as an important perspective on large platforms with abundant user histories, deep behavior models are discussed. Moreover, the recently emerged automated methods for deep CTR architecture design are presented. Finally, we summarize the survey and discuss the future prospects of this ﬁeld.},
	language = {en},
	urldate = {2021-06-03},
	journal = {arXiv:2104.10584 [cs]},
	author = {Zhang, Weinan and Qin, Jiarui and Guo, Wei and Tang, Ruiming and He, Xiuqiang},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.10584},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval, deepcvr},
	file = {Zhang et al. - 2021 - Deep Learning for Click-Through Rate Estimation.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L8PR8EKV\\Zhang et al. - 2021 - Deep Learning for Click-Through Rate Estimation.pdf:application/pdf},
}

@inproceedings{leiInteractivePathReasoning2020,
	address = {Virtual Event CA USA},
	title = {Interactive {Path} {Reasoning} on {Graph} for {Conversational} {Recommendation}},
	isbn = {978-1-4503-7998-4},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403258},
	doi = {10.1145/3394486.3403258},
	abstract = {Traditional recommendation systems estimate user preference on items from past interaction history, thus suffering from the limitations of obtaining fine-grained and dynamic user preference. Conversational recommendation system (CRS) brings revolutions to those limitations by enabling the system to directly ask users about their preferred attributes on items. However, existing CRS methods do not make full use of such advantage — they only use the attribute feedback in rather implicit ways such as updating the latent user representation. In this paper, we propose Conversational Path Reasoning (CPR), a generic framework that models conversational recommendation as an interactive path reasoning problem on a graph. It walks through the attribute vertices by following user feedback, utilizing the user preferred attributes in an explicit way. By leveraging on the graph structure, CPR is able to prune off many irrelevant candidate attributes, leading to better chance of hitting user preferred attributes. To demonstrate how CPR works, we propose a simple yet effective instantiation named SCPR (Simple CPR). We perform empirical studies on the multi-round conversational recommendation scenario, the most realistic CRS setting so far that considers multiple rounds of asking attributes and recommending items. Through extensive experiments on two datasets Yelp and LastFM, we validate the effectiveness of our SCPR, which significantly outperforms the state-of-the-art CRS methods EAR [13] and CRM [24]. In particular, we find that the more attributes there are, the more advantages our method can achieve.},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Lei, Wenqiang and Zhang, Gangyi and He, Xiangnan and Miao, Yisong and Wang, Xiang and Chen, Liang and Chua, Tat-Seng},
	month = aug,
	year = {2020},
	pages = {2073--2083},
	file = {Interactive Path Reasoning on Graph for Conversational Recommendation.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Notes\\Interactive Path Reasoning on Graph for Conversational Recommendation.md:text/plain;Lei et al. - 2020 - Interactive Path Reasoning on Graph for Conversati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JI2295AT\\Lei et al. - 2020 - Interactive Path Reasoning on Graph for Conversati.pdf:application/pdf;Lei et al. - 2020 - Interactive Path Reasoning on Graph for Conversati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6CZEMNYI\\Lei et al. - 2020 - Interactive Path Reasoning on Graph for Conversati.pdf:application/pdf;Lei et al. - 2020 - Interactive Path Reasoning on Graph for Conversati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3NNLCKLD\\Lei et al. - 2020 - Interactive Path Reasoning on Graph for Conversati.pdf:application/pdf},
}

@inproceedings{leExplainableRecommendationComparative2021,
	address = {Virtual Event Israel},
	title = {Explainable {Recommendation} with {Comparative} {Constraints} on {Product} {Aspects}},
	isbn = {978-1-4503-8297-7},
	url = {https://dl.acm.org/doi/10.1145/3437963.3441754},
	doi = {10.1145/3437963.3441754},
	abstract = {To aid users in choice-making, explainable recommendation models seek to provide not only accurate recommendations but also accompanying explanations that help to make sense of those recommendations. Most of the previous approaches rely on evaluative explanations, assessing the quality of an individual item along some aspects of interest to the user. In this work, we are interested in comparative explanations, the less studied problem of assessing a recommended item in comparison to another reference item.},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Le, Trung-Hoang and Lauw, Hady W.},
	month = mar,
	year = {2021},
	pages = {967--975},
	file = {Le and Lauw - 2021 - Explainable Recommendation with Comparative Constr.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2QLUBRN3\\Le and Lauw - 2021 - Explainable Recommendation with Comparative Constr.pdf:application/pdf;Le and Lauw - 2021 - Explainable Recommendation with Comparative Constr.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KP895XIW\\Le and Lauw - 2021 - Explainable Recommendation with Comparative Constr.pdf:application/pdf;Le and Lauw - 2021 - Explainable Recommendation with Comparative Constr.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T587LF72\\Le and Lauw - 2021 - Explainable Recommendation with Comparative Constr.pdf:application/pdf;leExplainableRecommendationComparative2021a-zotero.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Data Science\\Text Information Systems\\Recommendation Systems\\leExplainableRecommendationComparative2021a-zotero.md:text/plain},
}

@misc{SizingPotentialValue,
	title = {Sizing the potential value of {AI} and advanced analytics {\textbar} {McKinsey}},
	url = {https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and-value-of-deep-learning},
	urldate = {2021-06-03},
	file = {notes-from-the-ai-frontier-insights-from-hundreds-of-use-cases-discussion-paper.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\26PR4ZC6\\notes-from-the-ai-frontier-insights-from-hundreds-of-use-cases-discussion-paper.pdf:application/pdf;Sizing the potential value of AI and advanced analytics | McKinsey:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RLMQ43VS\\notes-from-the-ai-frontier-applications-and-value-of-deep-learning.html:text/html},
}

@misc{AmazonReviewData,
	title = {Amazon review data},
	url = {https://jmcauley.ucsd.edu/data/amazon/},
	urldate = {2021-06-03},
	file = {Amazon review data:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MQWYKXN2\\amazon.html:text/html},
}

@article{scannellDiagnosingDeclinePharmaceutical2012,
	title = {Diagnosing the decline in pharmaceutical {R}\&{D} efficiency},
	volume = {11},
	issn = {1474-1784},
	url = {https://doi.org/10.1038/nrd3681},
	doi = {10.1038/nrd3681},
	abstract = {The number of new drugs approved per billion US dollars spent on research and development (R\&D) has fallen around 80-fold in inflation-adjusted terms since 1950, despite advances in many of the scientific and technological inputs into the R\&D process. Given the apparent lack of impact of proposed solutions to declining R\&D efficiency so far, Scannell and colleagues ask whether the underlying problems have been correctly diagnosed and discuss factors that they consider to be the primary causes.},
	number = {3},
	journal = {Nature Reviews Drug Discovery},
	author = {Scannell, Jack W. and Blanckley, Alex and Boldon, Helen and Warrington, Brian},
	month = mar,
	year = {2012},
	pages = {191--200},
	file = {Diagnosing the decline in pharmaceutical R&D efficiency.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MH59JNBU\\Diagnosing the decline in pharmaceutical R&D efficiency.pdf:application/pdf},
}

@article{loMachineLearningStatistical2019,
	title = {Machine {Learning} with {Statistical} {Imputation} for {Predicting} {Drug} {Approval}},
	url = {https://hdsr.mitpress.mit.edu/pub/ct67j043},
	doi = {10.1162/99608f92.5c5f0525},
	abstract = {We apply machine-learning techniques to predict drug approvals using drug-development and clinicaltrial data from 2003 to 2015 involving several thousand drug-indication pairs with over 140 features across 15 disease groups. To deal with missing data, we use imputation methods that allow us to fully exploit the entire dataset, the largest of its kind. We show that our approach outperforms completecase analysis, which typically yields biased inferences. We achieve predictive measures of 0.78 and 0.81 AUC (“area under the receiver operating characteristic curve,” the estimated probability that a classifier will rank a positive outcome higher than a negative outcome) for predicting transitions from phase 2 to approval and phase 3 to approval, respectively. Using five-year rolling windows, we document an increasing trend in the predictive power of these models, a consequence of improving data quality and quantity. The most important features for predicting success are trial outcomes, trial status, trial accrual rates, duration, prior approval for another indication, and sponsor track records. We provide estimates of the probability of success for all drugs in the current pipeline.},
	language = {en},
	urldate = {2021-06-05},
	journal = {Harvard Data Science Review},
	author = {Lo, Andrew W. and Siah, Kien Wei and Wong, Chi Heem},
	month = jun,
	year = {2019},
	keywords = {machine learning},
	file = {Lo et al. - 2019 - Machine Learning with Statistical Imputation for P.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SDP5K8SA\\Lo et al. - 2019 - Machine Learning with Statistical Imputation for P.pdf:application/pdf},
}

@techreport{vergetisMachineLearningApproach2020,
	type = {preprint},
	title = {A {Machine} {Learning} approach for assessing drug development risk},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.10.08.331926},
	abstract = {Characterizing drug development risk – the probability that a drug will eventually receive regulatory approval – has been notoriously hard given the complexities of drug biology and clinical trials. This often leads to an inefficient allocation of resources, and an overall reduction in R\&D productivity. We propose a Machine Learning (ML) approach that provides a more accurate and unbiased estimate of drug development risk than traditional models.},
	language = {en},
	urldate = {2021-06-05},
	institution = {Bioinformatics},
	author = {Vergetis, Vangelis and Liaropoulos, Gerasimos and Georganaki, Maria and Dimakakos, Andreas and Skaltsas, Dimitrios and Gorgoulis, Vassilis G and Tsirigos, Aristotelis},
	month = oct,
	year = {2020},
	doi = {10.1101/2020.10.08.331926},
	file = {Vergetis et al. - 2020 - A Machine Learning approach for assessing drug dev.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y4TN8TEL\\Vergetis et al. - 2020 - A Machine Learning approach for assessing drug dev.pdf:application/pdf},
}

@techreport{heggePredictingSuccessPhase2020,
	type = {preprint},
	title = {Predicting {Success} of {Phase} {III} {Trials} in {Oncology}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2020.12.15.20248240},
	abstract = {We developed a model predicting the probability of success (PoS) for single planned or ongoing PhIII trials based on information available at trial initiation. Such a model is highly relevant for study sponsors to capture risk and opportunity on a trial to trial basis through trial optimization, and for investors to select drugs whose trial design match their investment strategy.
Objectives: To predict the outcome of planned or ongoing PhIII trials in oncology, given publicly available prior information
Design, Setting, Participants: Predictive modeling using publicly available data for 360 completed PhIII and 1240 PhII studies initiated between 2003 and 2012. Success and failure of PhIII studies were modeled using Bayesian logistic regression model.
Main Outcome Measures: Predicted PoS of individual PhIII trials based on a Bayesian model calibrated on publicly available data translated into 16 composite scores. Those scores cover aspects such as trial design, indication, number of patients, phase II (PhII) study outcomes, experience of sponsor at time of trial initiation, and others.
Results: The model allows to calculate the PoS distribution,  including credible intervals, for a PhIII trial in oncology. The predictive performance was determined using an area under the receiver operator curve (AUROC), resulting in an overall performance of 73\%oPP (mean AUROC). We identified two key factors contributing to the predictive performance of the model: quality and strength of PhII data and experience of the sponsor at the time of study initiation. 
Conclusion and Relevance: We describe the generation and application of a statistical model predicting the PoS for individual PhIII trials in oncological indications with unprecedented predictive performance. Compared to other approaches, this is the first study generating a fully transparent model resulting in trial specific PoS distributions. Moreover, we have shown that qualitative concepts such as PhII knowledge or sponsor R\&D strength can be captured in quantitative scores and that these scores have a high predictive power.},
	language = {en},
	urldate = {2021-06-05},
	institution = {Oncology},
	author = {Hegge, Stephan J and Thunecke, Markus Erik and Krings, Matthias and Ruedin, Leonard and Mueller, Jan Saputra and von Buenau, Paul},
	month = dec,
	year = {2020},
	doi = {10.1101/2020.12.15.20248240},
	keywords = {machine learning},
	file = {Hegge et al. - 2020 - Predicting Success of Phase III Trials in Oncology.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7T3KMTYX\\Hegge et al. - 2020 - Predicting Success of Phase III Trials in Oncology.pdf:application/pdf},
}

@article{loMachineLearningModelsPredicting2017,
	title = {Machine-{Learning} {Models} for {Predicting} {Drug} {Approvals} and {Clinical}-{Phase} {Transitions}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=2973611},
	doi = {10.2139/ssrn.2973611},
	abstract = {We apply machine-learning techniques to predict drug approvals and phase transitions using drug-development and clinical-trial data from 2003 to 2015 involving several thousand drug-indication pairs with over 140 features across 15 disease groups. Imputation methods are used to deal with missing data, allowing us to fully exploit the entire dataset, the largest of its kind. We achieve predictive measures of 0.74, 0.78, and 0.81 AUC for predicting transitions from phase 2 to phase 3, phase 2 to approval, and phase 3 to approval, respectively. Using five-year rolling windows, we document an increasing trend in the predictive power of these models, a consequence of improving data quality and quantity. The most important features for predicting success are trial outcomes, trial status, trial accrual rates, duration, prior approval for another indication, and sponsor track records. We provide estimates of the probability of success for all drugs in the current pipeline.},
	language = {en},
	urldate = {2021-06-05},
	journal = {SSRN Electronic Journal},
	author = {Lo, Andrew W. and Siah, Kien Wei and Wong, Chi Heem},
	year = {2017},
	keywords = {machine learning},
	file = {Lo et al. - 2017 - Machine-Learning Models for Predicting Drug Approv.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4HFEWQ8J\\Lo et al. - 2017 - Machine-Learning Models for Predicting Drug Approv.pdf:application/pdf},
}

@article{thuneckePredictingSuccessClinical,
	title = {Predicting {Success} of {Clinical} {Trials}},
	language = {en},
	author = {Thunecke, Markus},
	pages = {2},
	file = {Thunecke - Predicting Success of Clinical Trials.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z3E9GTJU\\Thunecke - Predicting Success of Clinical Trials.pdf:application/pdf},
}

@article{beinsePredictionDrugApproval2019,
	title = {Prediction of {Drug} {Approval} {After} {Phase} {I} {Clinical} {Trials} in {Oncology}: {RESOLVED2}},
	issn = {2473-4276},
	shorttitle = {Prediction of {Drug} {Approval} {After} {Phase} {I} {Clinical} {Trials} in {Oncology}},
	url = {https://ascopubs.org/doi/10.1200/CCI.19.00023},
	doi = {10.1200/CCI.19.00023},
	abstract = {PURPOSE Drug development in oncology currently is facing a conjunction of an increasing number of antineoplastic agents (ANAs) candidate for phase I clinical trials (P1CTs) and an important attrition rate for ﬁnal approval. We aimed to develop a machine learning algorithm (RESOLVED2) to predict drug development outcome, which could support early go/no-go decisions after P1CTs by better selection of drugs suitable for further development.
METHODS PubMed abstracts of P1CTs reporting on ANAs were used together with pharmacologic data from the DrugBank5.0 database to model time to US Food and Drug Administration (FDA) approval (FDA approval-free survival) since the ﬁrst P1CT publication. The RESOLVED2 model was trained with machine learning methods. Its performance was evaluated on an independent test set with weighted concordance index (IPCW).
RESULTS We identiﬁed 462 ANAs from PubMed that matched with DrugBank5.0 (P1CT publication dates 1972 to 2017). Among 1,411 variables, 28 were used by RESOLVED2 to model the FDA approval-free survival, with an IPCW of 0.89 on the independent test set. RESOLVED2 outperformed a model that was based on efﬁcacy/toxicity (IPCW, 0.69). In the test set at 6 years of follow-up, 73\% (95\% CI, 49\% to 86\%) of drugs predicted to be approved were approved, whereas 92\% (95\% CI, 87\% to 98\%) of drugs predicted to be nonapproved were still not approved (log-rank P , .001). A predicted approved drug was 16 times more likely to be approved than a predicted nonapproved drug (hazard ratio, 16.4; 95\% CI, 8.40 to 32.2).
CONCLUSION As soon as P1CT completion, RESOLVED2 can predict accurately the time to FDA approval. We provide the proof of concept that drug development outcome can be predicted by machine learning strategies.},
	language = {en},
	number = {3},
	urldate = {2021-06-05},
	journal = {JCO Clinical Cancer Informatics},
	author = {Beinse, Guillaume and Tellier, Virgile and Charvet, Valentin and Deutsch, Eric and Borget, Isabelle and Massard, Christophe and Hollebecque, Antoine and Verlingue, Loic},
	month = dec,
	year = {2019},
	keywords = {machine learning},
	pages = {1--10},
	file = {Beinse et al. - 2019 - Prediction of Drug Approval After Phase I Clinical.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VT49X7Q3\\Beinse et al. - 2019 - Prediction of Drug Approval After Phase I Clinical.pdf:application/pdf},
}

@article{redaMachineLearningApplications2020,
	title = {Machine learning applications in drug development},
	volume = {18},
	issn = {20010370},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2001037019303988},
	doi = {10.1016/j.csbj.2019.12.006},
	language = {en},
	urldate = {2021-06-05},
	journal = {Computational and Structural Biotechnology Journal},
	author = {Réda, Clémence and Kaufmann, Emilie and Delahaye-Duriez, Andrée},
	year = {2020},
	keywords = {machine learning},
	pages = {241--252},
	file = {Machine learning applications in drug development _ Enhanced Reader.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\47BRY8UQ\\Machine learning applications in drug development _ Enhanced Reader.pdf:application/pdf},
}

@article{vamathevanApplicationsMachineLearning2019,
	title = {Applications of machine learning in drug discovery and development},
	volume = {18},
	issn = {1474-1776, 1474-1784},
	url = {http://www.nature.com/articles/s41573-019-0024-5},
	doi = {10.1038/s41573-019-0024-5},
	language = {en},
	number = {6},
	urldate = {2021-06-05},
	journal = {Nature Reviews Drug Discovery},
	author = {Vamathevan, Jessica and Clark, Dominic and Czodrowski, Paul and Dunham, Ian and Ferran, Edgardo and Lee, George and Li, Bin and Madabhushi, Anant and Shah, Parantu and Spitzer, Michaela and Zhao, Shanrong},
	month = jun,
	year = {2019},
	pages = {463--477},
	file = {Applications of machine learning in drug discovery and development _ Enhanced Reader.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JSZFCM9V\\Applications of machine learning in drug discovery and development _ Enhanced Reader.pdf:application/pdf},
}

@inproceedings{tangAdoptingDataAnalysis2018,
	address = {Zhuhai, China},
	title = {Adopting {Data} {Analysis} and {Visualization} {Technology} to {Construct} {Clinical} {Research} {Data} {Management} and {Analysis} {System}},
	isbn = {978-1-4503-6127-9},
	url = {http://dl.acm.org/citation.cfm?doid=3301761.3301767},
	doi = {10.1145/3301761.3301767},
	abstract = {With the development of information technology, information systems have been widely used in medical institutions, and more and more clinical research data has been digitized, which provides the possibility to carry out clinical research with data as the source. However, the complexity and multi-dimensionality of clinical data make medical scientists' progress slow, and comprehensive use of various big data technologies is needed to help improve the efficiency of clinical research. Visualization technology can display data in an intuitive and easy -to-read way, helping medical researchers understand data, while parallel computing can greatly improve computing efficiency. Therefore, this paper explores the application strategies of data analysis technology and visualization technology in the management and analysis of clinical research data, and builds a set of clinical r esearch data management analysis system, which combines various technologies to help effectively promote medical clinical.},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {Proceedings of the 2018 2nd {International} {Conference} on {Software} and e-{Business}  - {ICSEB} '18},
	publisher = {ACM Press},
	author = {Tang, Haijing and Zhou, Yangdong and Yang, Xu and Gao, Keyan and Zheng, Wenhao and Zhao, Jinfeng},
	year = {2018},
	pages = {49--53},
	file = {Tang et al. - 2018 - Adopting Data Analysis and Visualization Technolog.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YCSLMEN8\\Tang et al. - 2018 - Adopting Data Analysis and Visualization Technolog.pdf:application/pdf},
}

@inproceedings{nobreReVISitLookingHood2021,
	address = {Yokohama Japan},
	title = {{reVISit}: {Looking} {Under} the {Hood} of {Interactive} {Visualization} {Studies}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {{reVISit}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445382},
	doi = {10.1145/3411764.3445382},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Nobre, Carolina and Wootton, Dylan and Cutler, Zach and Harrison, Lane and Pfister, Hanspeter and Lex, Alexander},
	month = may,
	year = {2021},
	pages = {1--13},
	file = {Nobre et al. - 2021 - reVISit Looking Under the Hood of Interactive Vis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\45XBBE47\\Nobre et al. - 2021 - reVISit Looking Under the Hood of Interactive Vis.pdf:application/pdf},
}

@inproceedings{dorierChallengesElasticSitu2019,
	address = {Denver Colorado USA},
	title = {The challenges of elastic in situ analysis and visualization},
	isbn = {978-1-4503-7723-2},
	url = {https://dl.acm.org/doi/10.1145/3364228.3364234},
	doi = {10.1145/3364228.3364234},
	abstract = {In situ analysis and visualization have been proposed in highperformance computing (HPC) to enable executing analysis tasks while a simulation is running, bypassing the parallel file system and avoiding the need for storing massive amounts of data. One aspect of in situ analysis that has not been extensively researched to date, however, is elasticity. Current in situ analysis frameworks use a fixed amount of resources and can hardly be scaled up or down dynamically throughout the simulation’s run time as a response to changes in the requirements.},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {Proceedings of the {Workshop} on {In} {Situ} {Infrastructures} for {Enabling} {Extreme}-{Scale} {Analysis} and {Visualization}},
	publisher = {ACM},
	author = {Dorier, Matthieu and Yildiz, Orcun and Peterka, Tom and Ross, Robert},
	month = nov,
	year = {2019},
	pages = {23--28},
	file = {Dorier et al. - 2019 - The challenges of elastic in situ analysis and vis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EHXNPN59\\Dorier et al. - 2019 - The challenges of elastic in situ analysis and vis.pdf:application/pdf},
}

@inproceedings{vaatajaInformationVisualizationHeuristics2016,
	address = {Baltimore, MD, USA},
	title = {Information {Visualization} {Heuristics} in {Practical} {Expert} {Evaluation}},
	isbn = {978-1-4503-4818-8},
	url = {http://dl.acm.org/citation.cfm?doid=2993901.2993918},
	doi = {10.1145/2993901.2993918},
	abstract = {While traditional HCI heuristics can be used to find usability issues also from information visualization systems, specialized heuristics tailored for the information visualization (InfoViz) domain can be more effective and focus on the special characteristics of these systems. In this study, we describe the application of ten information visualization heuristics from prior research and their testing in practical heuristic evaluation. We found that the selected heuristics were useful with good coverage in our application case. However, based on our observations, we argue that interaction, veracity, and aesthetics related heuristics should be added to the previously used set. The lack of domain knowledge made the evaluators somewhat uneasy with their capability to carry out the investigation in-depth. We suggest to train domain experts with understanding of the data and application domain to carry out the evaluation to get insightful feedback beyond usability issues.},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {Proceedings of the {Beyond} {Time} and {Errors} on {Novel} {Evaluation} {Methods} for {Visualization} - {BELIV} '16},
	publisher = {ACM Press},
	author = {Väätäjä, Heli and Varsaluoma, Jari and Heimonen, Tomi and Tiitinen, Katariina and Hakulinen, Jaakko and Turunen, Markku and Nieminen, Harri and Ihantola, Petri},
	year = {2016},
	pages = {36--43},
	file = {Väätäjä et al. - 2016 - Information Visualization Heuristics in Practical .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z7EB7Y3I\\Väätäjä et al. - 2016 - Information Visualization Heuristics in Practical .pdf:application/pdf},
}

@inproceedings{moneyLiveIntegratedVisualization2017,
	address = {New Orleans LA USA},
	title = {Live {Integrated} {Visualization} {Environment}: {An} {Experiment} in {Generalized} {Structured} {Frameworks} for {Visualization} and {Analysis}},
	isbn = {978-1-4503-5272-7},
	shorttitle = {Live {Integrated} {Visualization} {Environment}},
	url = {https://dl.acm.org/doi/10.1145/3093338.3093344},
	doi = {10.1145/3093338.3093344},
	abstract = {Many immersive visualization systems require custom coding and specialized hardware and software to function properly. A number of commercial products provide some of this functionality, but lack fully immersive tracking and controls. Additionally, these environments have been traditionally limited to real time data feeds and analysis. The Live Integrated Visualization Environment is a framework developed to address these limitations, while allowing for best of breed integration of commercial products, government software, and open source software. By combining a custom developed messaging bus with a web service implementation, a dynamic, interactive immersive environment is provided across a number of platforms including CAVEs, touch tables, single wall displays, and desktops. We provide an architecture discussion including driver capabilities to enable quick development of additional data sources with existing visualization applications. We conclude with a discussion of several projects that successfully utilize the framework for real-time big data and geospatial applications for a range of tasks. We include a brief introduction to recent work for an open source and cross platform replacement of the framework.},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {Proceedings of the {Practice} and {Experience} in {Advanced} {Research} {Computing} 2017 on {Sustainability}, {Success} and {Impact}},
	publisher = {ACM},
	author = {Money, James H. and Szewczyk, Thomas},
	month = jul,
	year = {2017},
	pages = {1--7},
	file = {Money and Szewczyk - 2017 - Live Integrated Visualization Environment An Expe.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U6RYQ93S\\Money and Szewczyk - 2017 - Live Integrated Visualization Environment An Expe.pdf:application/pdf},
}

@inproceedings{mccurdyActionDesignResearch2016,
	address = {Baltimore, MD, USA},
	title = {Action {Design} {Research} and {Visualization} {Design}},
	isbn = {978-1-4503-4818-8},
	url = {http://dl.acm.org/citation.cfm?doid=2993901.2993916},
	doi = {10.1145/2993901.2993916},
	abstract = {In applied visualization research, artifacts are shaped by a series of small design decisions, many of which are evaluated quickly and informally via methods that often go unreported and unveriﬁed. Such design decisions are inﬂuenced not only by visualization theory, but also by the people and context of the research. While existing applied visualization models support a level of reliability throughout the design process, they fail to explicitly account for the inﬂuence of the research context in shaping the resulting design artifacts. In this work, we look to action design research (ADR) for insight into addressing this issue. In particular, ADR oﬀers a framework along with a set of guiding principles for navigating and capitalizing on the disruptive, subjective, human-centered nature of applied design work, while aiming to ensure reliability of the process and design, and emphasizing opportunities for conducting research. We explore the utility of ADR in increasing the reliability of applied visualization design research by: describing ADR in the language and constructs developed within the visualization community; comparing ADR to existing visualization methodologies; and analyzing a recent design study retrospectively through the lens of ADR’s framework and principles.},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {Proceedings of the {Beyond} {Time} and {Errors} on {Novel} {Evaluation} {Methods} for {Visualization} - {BELIV} '16},
	publisher = {ACM Press},
	author = {McCurdy, Nina and Dykes, Jason and Meyer, Miriah},
	year = {2016},
	pages = {10--18},
	file = {McCurdy et al. - 2016 - Action Design Research and Visualization Design.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CTXD6X2V\\McCurdy et al. - 2016 - Action Design Research and Visualization Design.pdf:application/pdf},
}

@inproceedings{wangDrugTargetInteractionPrediction2018,
	address = {Tianjin, China},
	title = {Drug-{Target} {Interaction} {Prediction} {Based} on {Heterogeneous} {Networks}},
	isbn = {978-1-4503-6506-2},
	url = {http://dl.acm.org/citation.cfm?doid=3278198.3278204},
	doi = {10.1145/3278198.3278204},
	abstract = {Predicting drug-target interactions has gradually become a heated issue in medical research. However, identifying the drug-target interactions in clinical trials requires a lot of financial resources and time. More and more computational methods are currently used for drug-target interaction predictions. This paper proposes a drug-target interaction prediction method that can integrate information from different heterogeneous networks. This method constructs multiple drug and target similarity networks and applies the GraRep algorithm on the similarity networks after denoising in order to extract the features. Features obtained from heterogeneous networks are combined as a feature vector of DTIs, which is the input of Random Forest. The result of our experiment shows that our method in this paper increases the accuracy of prediction for DTIs, which is superior to other state-of-the-art approaches.},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Biomedical} {Engineering} and {Bioinformatics} - {ICBEB} 2018},
	publisher = {ACM Press},
	author = {Wang, Yingjie and Chang, Huiyou and Wang, Jihong and Shi, Yue},
	year = {2018},
	pages = {14--18},
	file = {Wang et al. - 2018 - Drug-Target Interaction Prediction Based on Hetero.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\65H8ZII3\\Wang et al. - 2018 - Drug-Target Interaction Prediction Based on Hetero.pdf:application/pdf},
}

@inproceedings{ninhClinicalTrialSimulation2019,
	address = {National Harbor, MD, USA},
	title = {Clinical {Trial} {Simulation}: {Modeling} and {Practical} {Considerations}},
	isbn = {978-1-72813-283-9},
	shorttitle = {Clinical {Trial} {Simulation}},
	url = {https://ieeexplore.ieee.org/document/9004916/},
	doi = {10.1109/WSC40007.2019.9004916},
	abstract = {As clinical trials are increasingly globalized with complex footprints over hundreds of sites worldwide, sponsors and contract research organizations constantly seek to make better and faster decisions on their investigational products, and drug supply planning must evolve to ensure efﬁcient, effective supply chain for every study. This endeavor is challenging due to unique characteristics of multi-center trials including randomization schemes for multiple treatment arms, ﬁnite recruitment target (that is, across all sites, only a ﬁnite number of subjects need be satisﬁed) and uncertainty in recruitment rates, etc. Simulation has great potential for being the ideal tool which companies can utilize to make better decisions with considerations to both supply chain risks and costs. To achieve this goal, it is important to understand the speciﬁcs of clinical trial supply chains. Built upon this knowledge, our paper provides an advanced tutorial on modeling and practical considerations in clinical supply simulation.},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {2019 {Winter} {Simulation} {Conference} ({WSC})},
	publisher = {IEEE},
	author = {Ninh, Anh and LeFew, Michael and Anisimov, Vladimir},
	month = dec,
	year = {2019},
	pages = {118--132},
	file = {Ninh et al. - 2019 - Clinical Trial Simulation Modeling and Practical .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6P96GWDC\\Ninh et al. - 2019 - Clinical Trial Simulation Modeling and Practical .pdf:application/pdf},
}

@inproceedings{elkinNetworkAnalysisRecommendation2019,
	address = {Niagara Falls NY USA},
	title = {Network {Analysis} and {Recommendation} for {Infectious} {Disease} {Clinical} {Trial} {Research}},
	isbn = {978-1-4503-6666-3},
	url = {https://dl.acm.org/doi/10.1145/3307339.3342156},
	doi = {10.1145/3307339.3342156},
	abstract = {Clinical trials are crucial for the advancement of treatment and knowledge within the medical community. Since 2007, US federal government took the initiative and required organizations sponsoring clinical trials with at least one site in the United States to submit information on these clinical trials to the ClinicalTrials.gov database, resulting in a rich source of information for clinical trial research. Nevertheless, only a handful of analytic studies have been carried out to understand this valuable data source. In this study, we propose to use network analysis to understand infectious disease clinical trial research. Our goal is to answer two important questions: (1) what are the concentrations and characteristics of infectious disease clinical trial research? and (2) how to accurately predict what type of clinical trials a sponsor (or an investigator) are interested in? The answers to the first question provide effective ways to summarize clinical trial research related to particular disease(s), and the answers to the second question help match clinical trial sponsors and investigators for information recommendation. By using 4,228 clinical trials as the test bed, our study involves 4,864 sponsors and 1,879 research areas characterized by Medical Subject Heading (MeSH) keywords. We extract a set of network measures to show patterns of infectious disease clinical trials, and design a new community based link prediction approach to predict sponsors’ interests, with significant improvement compared to baselines. This trans-formative study concludes that using network analysis can tremendously help the understanding of clinical trial research for effective summarization, characterization, and prediction.},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {Proceedings of the 10th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology} and {Health} {Informatics}},
	publisher = {ACM},
	author = {Elkin, Magdalyn E. and Andrews, Whitney A. and Zhu, Xingquan},
	month = sep,
	year = {2019},
	pages = {347--356},
	file = {Elkin et al. - 2019 - Network Analysis and Recommendation for Infectious.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JE3AKV2J\\Elkin et al. - 2019 - Network Analysis and Recommendation for Infectious.pdf:application/pdf},
}

@article{chumachenkoMACHINELEARNINGMETHODS,
	title = {{MACHINE} {LEARNING} {METHODS} {FOR} {MALWARE} {DETECTION} {AND} {CLASSIFICATION}},
	language = {en},
	author = {Chumachenko, Kateryna},
	pages = {93},
	file = {Chumachenko - MACHINE LEARNING METHODS FOR MALWARE DETECTION AND.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S7WFV4FE\\Chumachenko - MACHINE LEARNING METHODS FOR MALWARE DETECTION AND.pdf:application/pdf},
}

@article{mukherjeeBACHELORDEGREECOMPUTER,
	title = {{BACHELOR}’{S} {DEGREE} {IN} {COMPUTER} {SCIENCE} \& {COMMUNICATION} {ENGINEERING}},
	language = {en},
	author = {Mukherjee, Sagnik and Das, Toulik and Patra, Rupam},
	pages = {33},
	file = {Mukherjee et al. - BACHELOR’S DEGREE IN COMPUTER SCIENCE & COMMUNICAT.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UV6FSZQ5\\Mukherjee et al. - BACHELOR’S DEGREE IN COMPUTER SCIENCE & COMMUNICAT.pdf:application/pdf},
}

@article{shahiniMachineLearningPredict2019,
	title = {Machine {Learning} to {Predict} the {Likelihood} of a {Personal} {Computer} to {Be} {Infected} with {Malware}},
	volume = {2},
	abstract = {In this paper, we present a new model to predict the probability that a personal computer will become infected with malware. The dataset is selected from a Kaggle competition supported by Microsoft. The data includes computer conﬁguration, owner information, installed software, and conﬁguration information. In our research, several classiﬁcation models are utilized to assign a probability of a machine being infected with malware. The LightGBM classiﬁer is the optimum machine learning model by performing faster with higher eﬃciency and lower memory usage in this research. The LightGBM algorithm obtained a cross-validation ROC-AUC score of 74\%. Leading factors and feature importance are also identiﬁed by LightGBM technique. Our research revealed that variables related to location, ﬁrmware version, operating system, and anti-virus software are the most important variables that have the highest weight in predicting malware detection.},
	language = {en},
	number = {2},
	author = {Shahini, Maryam and Farhanian, Ramin and Ellis, Marcus},
	year = {2019},
	pages = {24},
	file = {Shahini et al. - 2019 - Machine Learning to Predict the Likelihood of a Pe.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5S2C9HPQ\\Shahini et al. - 2019 - Machine Learning to Predict the Likelihood of a Pe.pdf:application/pdf},
}

@article{zhuConvertingTabularData2021,
	title = {Converting tabular data into images for deep learning with convolutional neural networks},
	volume = {11},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-021-90923-y},
	doi = {10.1038/s41598-021-90923-y},
	abstract = {Abstract
            Convolutional neural networks (CNNs) have been successfully used in many applications where important information about data is embedded in the order of features, such as speech and imaging. However, most tabular data do not assume a spatial relationship between features, and thus are unsuitable for modeling using CNNs. To meet this challenge, we develop a novel algorithm, image generator for tabular data (IGTD), to transform tabular data into images by assigning features to pixel positions so that similar features are close to each other in the image. The algorithm searches for an optimized assignment by minimizing the difference between the ranking of distances between features and the ranking of distances between their assigned pixels in the image. We apply IGTD to transform gene expression profiles of cancer cell lines (CCLs) and molecular descriptors of drugs into their respective image representations. Compared with existing transformation methods, IGTD generates compact image representations with better preservation of feature neighborhood structure. Evaluated on benchmark drug screening datasets, CNNs trained on IGTD image representations of CCLs and drugs exhibit a better performance of predicting anti-cancer drug response than both CNNs trained on alternative image representations and prediction models trained on the original tabular data.},
	language = {en},
	number = {1},
	urldate = {2021-06-05},
	journal = {Scientific Reports},
	author = {Zhu, Yitan and Brettin, Thomas and Xia, Fangfang and Partin, Alexander and Shukla, Maulik and Yoo, Hyunseung and Evrard, Yvonne A. and Doroshow, James H. and Stevens, Rick L.},
	month = dec,
	year = {2021},
	pages = {11325},
	file = {Converting tabular data into images for deep learning with convolutional neural networks.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TMP4M4SU\\Converting tabular data into images for deep learning with convolutional neural networks.pdf:application/pdf},
}

@misc{MicrosoftMalwarePrediction,
	title = {Microsoft {Malware} {Prediction}},
	url = {https://kaggle.com/c/microsoft-malware-prediction},
	abstract = {Can you predict if a machine will soon be hit with malware?},
	language = {en},
	urldate = {2021-06-05},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LBBPEDTS\\data.html:text/html},
}

@article{krizhevskyImageNetClassificationDeep2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2021-06-05},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QADR3RTD\\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf},
}

@article{sunLightweightDeepNetwork2021,
	title = {A {Lightweight} {Deep} {Network} for {Efficient} {CSI} {Feedback} in {Massive} {MIMO} {Systems}},
	url = {http://arxiv.org/abs/2105.10283},
	abstract = {To fully exploit the advantages of massive multiple-input multiple-output (m-MIMO), accurate channel state information (CSI) is required at the transmitter. However, excessive CSI feedback for large antenna arrays is inefﬁcient and thus undesirable in practical applications. By exploiting the inherent correlation characteristics of complex-valued channel responses in the angulardelay domain, we propose a novel neural network (NN) architecture, namely ENet, for CSI compression and feedback in m-MIMO. Even if the ENet processes the real and imaginary parts of the CSI values separately, its special structure enables the network trained for the real part only to be reused for the imaginary part. The proposed ENet shows enhanced performance with the network size reduced by nearly an order of magnitude compared to the existing NN-based solutions. Experimental results verify the effectiveness of the proposed ENet.},
	language = {en},
	urldate = {2021-06-05},
	journal = {arXiv:2105.10283 [cs, eess, math]},
	author = {Sun, Yuyao and Xu, Wei and Liang, Le and Wang, Ning and Li, Geoffery Ye and You, Xiaohu},
	month = may,
	year = {2021},
	note = {arXiv: 2105.10283},
	keywords = {Computer Science - Information Theory, Electrical Engineering and Systems Science - Signal Processing},
	file = {Sun et al. - 2021 - A Lightweight Deep Network for Efficient CSI Feedb.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\X4DZWQRA\\Sun et al. - 2021 - A Lightweight Deep Network for Efficient CSI Feedb.pdf:application/pdf},
}

@article{vergetisAssessingDrugDevelopment2021,
	title = {Assessing {Drug} {Development} {Risk} {Using} {Big} {Data} and {Machine} {Learning}},
	volume = {81},
	issn = {0008-5472, 1538-7445},
	url = {http://cancerres.aacrjournals.org/lookup/doi/10.1158/0008-5472.CAN-20-0866},
	doi = {10.1158/0008-5472.CAN-20-0866},
	language = {en},
	number = {4},
	urldate = {2021-06-09},
	journal = {Cancer Research},
	author = {Vergetis, Vangelis and Skaltsas, Dimitrios and Gorgoulis, Vassilis G. and Tsirigos, Aristotelis},
	month = feb,
	year = {2021},
	pages = {816--819},
}

@article{malikPredictingSuccessRegulatory2014,
	title = {Predicting success in regulatory approval from {Phase} {I} results},
	volume = {74},
	issn = {0344-5704, 1432-0843},
	url = {http://link.springer.com/10.1007/s00280-014-2596-4},
	doi = {10.1007/s00280-014-2596-4},
	language = {en},
	number = {5},
	urldate = {2021-06-09},
	journal = {Cancer Chemotherapy and Pharmacology},
	author = {Malik, Laeeq and Mejia, Alex and Parsons, Helen and Ehler, Benjamin and Mahalingam, Devalingam and Brenner, Andrew and Sarantopoulos, John and Weitman, Steven},
	month = nov,
	year = {2014},
	keywords = {machine learning},
	pages = {1099--1103},
	file = {Predicting success in regulatory approval from Phase I results.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Predicting success in regulatory approval from Phase I results.pdf:application/pdf},
}

@techreport{wongEstimatingProbabilitiesSuccess2020,
	type = {preprint},
	title = {Estimating {Probabilities} of {Success} of {Clinical} {Trials} for {Vaccines} and {Other} {Anti}-{Infective} {Therapeutics}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2020.04.09.20059600},
	abstract = {A key driver in biopharmaceutical investment decisions is the probability of success of a drug development program. We estimate the probabilities of success (PoS) of clinical trials for vaccines and other anti-infective therapeutics using 43,414 unique triplets of clinical trial, drug, and disease between January 1, 2000, and January 7, 2020, yielding 2,544 vaccine programs and 6,829 non-vaccine programs targeting infectious diseases. The overall estimated PoS for an industry-sponsored vaccine program is 39.6\%, and 16.3\% for an industry-sponsored anti-infective therapeutic. Among industry-sponsored vaccines programs, only 12 out of 27 disease categories have seen at least one approval, with the most successful being against monkeypox (100\%), rotavirus (78.7\%), and Japanese encephalitis (67.6\%). The three infectious diseases with the highest PoS for industry-sponsored nonvaccine therapeutics are smallpox (100\%), CMV (31.8\%), and onychomycosis (29.8\%). Nonindustry-sponsored vaccine and non-vaccine development programs have lower overall PoSs: 6.8\% and 8.2\%, respectively. Viruses involved in recent outbreaks—MERS, SARS, Ebola, Zika—have had a combined total of only 45 non-vaccine development programs initiated over the past two decades, and no approved therapy to date (Note: our data was obtained just before the COVID-19 outbreak and do not contain information about the programs targeting this disease.) These estimates offer guidance both to biopharma investors as well as to policymakers seeking to identify areas most likely to be undeserved by private-sector engagement and in need of public-sector support.},
	language = {en},
	urldate = {2021-06-09},
	institution = {Health Economics},
	author = {Wong, Chi Heem and Siah, Kien Wei and Lo, Andrew W.},
	month = apr,
	year = {2020},
	doi = {10.1101/2020.04.09.20059600},
	keywords = {machine learning},
	file = {Wong et al. - 2020 - Estimating Probabilities of Success of Clinical Tr.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9NYU6VBG\\Wong et al. - 2020 - Estimating Probabilities of Success of Clinical Tr.pdf:application/pdf},
}

@article{serugaFailuresPhaseIII2015,
	title = {Failures in {Phase} {III}: {Causes} and {Consequences}},
	volume = {21},
	issn = {1078-0432, 1557-3265},
	shorttitle = {Failures in {Phase} {III}},
	url = {http://clincancerres.aacrjournals.org/lookup/doi/10.1158/1078-0432.CCR-15-0124},
	doi = {10.1158/1078-0432.CCR-15-0124},
	language = {en},
	number = {20},
	urldate = {2021-06-09},
	journal = {Clinical Cancer Research},
	author = {Seruga, Bostjan and Ocana, Alberto and Amir, Eitan and Tannock, Ian F.},
	month = oct,
	year = {2015},
	pages = {4552--4560},
	file = {Seruga et al. - 2015 - Failures in Phase III Causes and Consequences.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E4D2I8UP\\Seruga et al. - 2015 - Failures in Phase III Causes and Consequences.pdf:application/pdf},
}

@article{wuExploratoryAnalysisFactors2021,
	title = {Exploratory {Analysis} of the {Factors} {Associated} {With} {Success} {Rates} of {Confirmatory} {Randomized} {Controlled} {Trials} in {Cancer} {Drug} {Development}},
	volume = {14},
	issn = {1752-8054, 1752-8062},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cts.12852},
	doi = {10.1111/cts.12852},
	abstract = {This study examined the outcomes of recent confirmatory randomized controlled trials (RCTs) in phase 3 that were initiated between 2005 and 2017 for oncological drugs in the United States and identified several factors that were associated with the success of RCTs. Our regression analysis showed that studies with progression-free survival or response rate as primary endpoint were more likely to succeed than studies with overall survival (odds ratio (OR)=2.94 and 6.23, respectively). The status of development was also linked with success rates. Studies for non-lead indication tended to have lower success rates than studies for lead indication (OR=0.68). Studies for first-line therapy were observed to have low success rates compared with studies for post second-line therapies (OR=0.37). Studies for which strong prior evidence was not listed in their publication tended to be more successful than studies that followed rigorous RCTs or single arm studies for the indication. These results suggest that historical success rates may reflect not only the important features of trials, which can be observed directly from study design and results, but also the background status of trials in clinical development pathways.},
	language = {en},
	number = {1},
	urldate = {2021-06-09},
	journal = {Clinical and Translational Science},
	author = {Wu, Can and Ono, Shunsuke},
	month = jan,
	year = {2021},
	pages = {260--267},
	file = {Wu and Ono - 2021 - Exploratory Analysis of the Factors Associated Wit.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6NQHU2HB\\Wu and Ono - 2021 - Exploratory Analysis of the Factors Associated Wit.pdf:application/pdf},
}

@article{yamaguchiApprovalSuccessRates2021,
	title = {Approval success rates of drug candidates based on target, action, modality, application, and their combinations},
	issn = {1752-8054, 1752-8062},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cts.12980},
	doi = {10.1111/cts.12980},
	abstract = {The current success rate of a drug candidate, from the beginning of the clinical trial to receiving marketing approval, is about 10\%–2­ 0\%, and it has not changed during the past few decades. Therefore, pharmaceutical companies are under pressure to select one compound, among many others, with a high probability of success. The differences in drug features affect their probabilities of approval success. In this study, we examined the approval success rates of drug candidates, developed in the United States, the European Union, or Japan, by focusing on four parameters (“drug target,” “drug action,” “drug modality,” and “drug application”) and their combinations, and identified factors that conditioned the outcome of the drug development process. We obtained a total success rate of 12.8\%, after evaluating 3999 compounds. Moreover, after analyzing the combinations of these parameters, the approval success rates of drugs that corresponded to the following categories—­a stimulant in drug action or an enzyme in drug target and biologics (excluding monoclonal antibody) in drug modality—­were high (34.1\% and 31.3\%, respectively). Univariate and multivariate logistic regression analyses revealed that stimulant in drug action, and “B” (blood and blood forming organs), “G” (genito-u­ rinary system and sex), and “J” (anti-i­nfectives for systemic use) in drug application were statistically associated with high approval success rates. We found several parameters and their combinations that affected drug approval success rates. Our results could assist pharmaceutical companies in evaluating the probability of success of their drug candidates and, thus, in efficiently conducting the clinical development process.},
	language = {en},
	urldate = {2021-06-09},
	journal = {Clinical and Translational Science},
	author = {Yamaguchi, Shingo and Kaneko, Masayuki and Narukawa, Mamoru},
	month = apr,
	year = {2021},
	pages = {cts.12980},
	file = {Yamaguchi et al. - 2021 - Approval success rates of drug candidates based on.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GGJYPQ8F\\Yamaguchi et al. - 2021 - Approval success rates of drug candidates based on.pdf:application/pdf},
}

@article{shnaydmanIndustryDrugDevelopment2020,
	title = {Industry drug development portfolio forecasting: productivity, risk, innovation, sustainability},
	volume = {25},
	issn = {1478-565X, 1462-8732},
	shorttitle = {Industry drug development portfolio forecasting},
	url = {http://www.commercialbiotechnology.com/index.php/jcb/article/view/879},
	doi = {10.5912/jcb879},
	abstract = {Multiple factors may a ect industry performance, risk, innovation and sustainability in coming years such as long-term economic trends, patents expiration, demographic shifts, and regulatory issues. Currently available industry-wide prediction frameworks have limited capabilities. They are based on: (1) analysts’ consensus; (2) extrapolation of current trends; (3) nancial performance of big pharma only; (4) empirical formulas, etc. Therefore, the need of robust forecasting methodology based on simulation modeling and covering the entire industry portfolio predictions could not be underestimated. The simulation model utilizes available data about each drug/indication in the industry R\&D pipeline, and transforms it into a set of metrics characterizing future industry performance. Industry-wide portfolio simulation model was developed to address short- and longterm portfolio productivity forecasting challenges. The model is drug–centric, it simulates drug development work ow process. The model also incorporates multiple business rules related to drugs interdependence. The model predicted 2016 drop in NME approvals based on 2014 data. Other modeling experiments include analysis of industry sustainability and innovation strategy, impact of approval rates and likelihood on the variations of clinical trials cycle time, probabilities of success, and FDA approval cycle time.},
	language = {en},
	number = {2},
	urldate = {2021-06-09},
	journal = {Journal of Commercial Biotechnology},
	author = {Shnaydman, Vladimir},
	month = feb,
	year = {2020},
	file = {Shnaydman - 2020 - Industry drug development portfolio forecasting p.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\36M2IFUC\\Shnaydman - 2020 - Industry drug development portfolio forecasting p.pdf:application/pdf},
}

@article{pammolliEndlessFrontierRecent2020,
	title = {The endless frontier? {The} recent increase of {R}\&{D} productivity in pharmaceuticals},
	volume = {18},
	issn = {1479-5876},
	shorttitle = {The endless frontier?},
	url = {https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-020-02313-z},
	doi = {10.1186/s12967-020-02313-z},
	abstract = {Background:  Studies on the early 2000s documented increasing attrition rates and duration of clinical trials, leading to a representation of a “productivity crisis” in pharmaceutical research and development (R\&D). In this paper, we produce a new set of analyses for the last decade and report a recent increase of R\&D productivity within the industry.
Methods:  We use an extensive data set on the development history of more than 50,000 projects between 1990 and 2017, which we integrate with data on sales, patents, and anagraphical information on each institution involved. We devise an indicator to quantify the novelty of each project, based on its set of mechanisms of action.
Results:  First, we investigate how R\&D projects are allocated across therapeutic areas and find a polarization towards high uncertainty/high potential reward indications, with a strong focus on oncology. Second, we find that attrition rates have been decreasing at all stages of clinical research in recent years. In parallel, for each phase, we observe a significant reduction of time required to identify projects to be discontinued. Moreover, our analysis shows that more recent successful R\&D projects are increasingly based on novel mechanisms of action and target novel indications, which are characterized by relatively small patient populations. Third, we find that the number of R\&D projects on advanced therapies is also growing. Finally, we investigate the relative contribution to productivity variations of different types of institutions along the drug development process, with a specific focus on the distinction between the roles of Originators and Developers of R\&D projects. We document that in the last decade Originator–Developer collaborations in which biotech companies act as Developers have been growing in importance. Moreover, we show that biotechnology companies have reached levels of productivity in project development that are equivalent to those of large pharmaceutical companies.
Conclusions:  Our study reports on the state of R\&D productivity in the bio-pharmaceutical industry, finding several signals of an improving performance, with R\&D projects becoming more targeted and novel in terms of indications and mechanisms of action.},
	language = {en},
	number = {1},
	urldate = {2021-06-09},
	journal = {Journal of Translational Medicine},
	author = {Pammolli, Fabio and Righetto, Lorenzo and Abrignani, Sergio and Pani, Luca and Pelicci, Pier Giuseppe and Rabosio, Emanuele},
	month = dec,
	year = {2020},
	pages = {162},
	file = {Pammolli et al. - 2020 - The endless frontier The recent increase of R&D p.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4JWEJUCV\\Pammolli et al. - 2020 - The endless frontier The recent increase of R&D p.pdf:application/pdf},
}

@article{woutersEstimatedResearchDevelopment2020,
	title = {Estimated {Research} and {Development} {Investment} {Needed} to {Bring} a {New} {Medicine} to {Market}, 2009-2018},
	volume = {323},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2762311},
	doi = {10.1001/jama.2020.1166},
	language = {en},
	number = {9},
	urldate = {2021-06-09},
	journal = {JAMA},
	author = {Wouters, Olivier J. and McKee, Martin and Luyten, Jeroen},
	month = mar,
	year = {2020},
	pages = {844},
	file = {Estimated Research and Development Investment Needed to Bring a New Medicine to.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Estimated Research and Development Investment Needed to Bring a New Medicine to.pdf:application/pdf;Estimated Research and Development Investment Needed to Bring a New Medicine to.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Estimated Research and Development Investment Needed to Bring a New Medicine to2.pdf:application/pdf},
}

@misc{2021ClinicalDevelopment,
	title = {2021 {Clinical} {Development} {Success} {Rates} l {Pharma} {Intelligence}},
	url = {https://pharmaintelligence.informa.com/resources/product-content/2021-clinical-development-success-rates},
	urldate = {2021-06-09},
	file = {2021 Clinical Development Success Rates 2011-2020 v17.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EF4ENXAV\\2021 Clinical Development Success Rates 2011-2020 v17.pdf:application/pdf;2021 Clinical Development Success Rates l Pharma Intelligence:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RCZ2S9MS\\2021-clinical-development-success-rates.html:text/html},
}

@article{wongEstimationClinicalTrial2019,
	title = {Estimation of clinical trial success rates and related parameters},
	volume = {20},
	issn = {1465-4644, 1468-4357},
	url = {https://academic.oup.com/biostatistics/article/20/2/273/4817524},
	doi = {10.1093/biostatistics/kxx069},
	language = {en},
	number = {2},
	urldate = {2021-06-09},
	journal = {Biostatistics},
	author = {Wong, Chi Heem and Siah, Kien Wei and Lo, Andrew W},
	month = apr,
	year = {2019},
	keywords = {machine learning},
	pages = {273--286},
	file = {Estimation of clinical trial success rates and related parameters.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Estimation of clinical trial success rates and related parameters.pdf:application/pdf;Estimation of clinical trial success rates and related parameters.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Estimation of clinical trial success rates and related parameters2.pdf:application/pdf},
}

@misc{EstimatingProbabilityRegulatory,
	title = {Estimating the {Probability} of {Regulatory} {Registration} {Success}},
	url = {https://www.raps.org/news-and-articles/news-articles/2020/2/estimating-the-probability-of-regulatory-registrat},
	abstract = {This article discusses how to estimate the probability of regulatory/registration success for pharmaceutical products in development.},
	urldate = {2021-06-09},
	keywords = {machine learning},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PVSEFNZK\\estimating-the-probability-of-regulatory-registrat.html:text/html},
}

@article{lathuiliereComprehensiveAnalysisDeep2020,
	title = {A {Comprehensive} {Analysis} of {Deep} {Regression}},
	volume = {42},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/1803.08450},
	doi = {10.1109/TPAMI.2019.2910523},
	abstract = {Deep learning revolutionized data science, and recently its popularity has grown exponentially, as did the amount of papers employing deep networks. Vision tasks, such as human pose estimation, did not escape from this trend. There is a large number of deep models, where small changes in the network architecture, or in the data pre-processing, together with the stochastic nature of the optimization procedures, produce notably different results, making extremely difﬁcult to sift methods that signiﬁcantly outperform others. This situation motivates the current study, in which we perform a systematic evaluation and statistical analysis of vanilla deep regression, i.e. convolutional neural networks with a linear regression top layer. This is the ﬁrst comprehensive analysis of deep regression techniques. We perform experiments on four vision problems, and report conﬁdence intervals for the median performance as well as the statistical signiﬁcance of the results, if any. Surprisingly, the variability due to different data pre-processing procedures generally eclipses the variability due to modiﬁcations in the network architecture. Our results reinforce the hypothesis according to which, in general, a general-purpose network (e.g. VGG-16 or ResNet-50) adequately tuned can yield results close to the state-of-the-art without having to resort to more complex and ad-hoc regression models.},
	language = {en},
	number = {9},
	urldate = {2021-06-09},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Lathuilière, Stéphane and Mesejo, Pablo and Alameda-Pineda, Xavier and Horaud, Radu},
	month = sep,
	year = {2020},
	note = {arXiv: 1803.08450},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {2065--2081},
	file = {Lathuilière et al. - 2020 - A Comprehensive Analysis of Deep Regression.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WCKA9MV8\\Lathuilière et al. - 2020 - A Comprehensive Analysis of Deep Regression.pdf:application/pdf},
}

@article{lauerEstimatingProbabilitySuccess2013,
	title = {Estimating the probability of success of a simple algorithm for switched linear regression},
	volume = {8},
	issn = {1751570X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1751570X12000532},
	doi = {10.1016/j.nahs.2012.10.001},
	abstract = {This paper deals with the switched linear regression problem inherent in hybrid system identiﬁcation. In particular, we discuss k-LinReg, a straightforward and easy to implement algorithm in the spirit of k-means for the nonconvex optimization problem at the core of switched linear regression, and focus on the question of its accuracy on large data sets and its ability to reach global optimality. To this end, we emphasize the relationship between the sample size and the probability of obtaining a local minimum close to the global one with a random initialization. This is achieved through the estimation of a model of the behavior of this probability with respect to the problem dimensions. This model can then be used to tune the number of restarts required to obtain a global solution with high probability. Experiments show that the model can accurately predict the probability of success and that, despite its simplicity, the resulting algorithm can outperform more complicated approaches in both speed and accuracy.},
	language = {en},
	urldate = {2021-06-09},
	journal = {Nonlinear Analysis: Hybrid Systems},
	author = {Lauer, Fabien},
	month = may,
	year = {2013},
	pages = {31--47},
	file = {Lauer - 2013 - Estimating the probability of success of a simple .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GJYQCYFQ\\Lauer - 2013 - Estimating the probability of success of a simple .pdf:application/pdf},
}

@article{arikTabNetAttentiveInterpretable2020,
	title = {{TabNet}: {Attentive} {Interpretable} {Tabular} {Learning}},
	shorttitle = {{TabNet}},
	url = {http://arxiv.org/abs/1908.07442},
	abstract = {We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efﬁcient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into its global behavior. Finally, we demonstrate self-supervised learning for tabular data, signiﬁcantly improving performance when unlabeled data is abundant.},
	language = {en},
	urldate = {2021-06-09},
	journal = {arXiv:1908.07442 [cs, stat]},
	author = {Arik, Sercan O. and Pfister, Tomas},
	month = dec,
	year = {2020},
	note = {arXiv: 1908.07442},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Arik and Pfister - 2020 - TabNet Attentive Interpretable Tabular Learning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AAH3U52S\\Arik and Pfister - 2020 - TabNet Attentive Interpretable Tabular Learning.pdf:application/pdf;Arik and Pfister - 2020 - TabNet Attentive Interpretable Tabular Learning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JWLQ5PXW\\Arik and Pfister - 2020 - TabNet Attentive Interpretable Tabular Learning.pdf:application/pdf},
}

@misc{EstimatingClassMembership,
	title = {Estimating {Class} {Membership} {Probabilities} {Using} {Classifier} {Learners}.pdf},
	file = {Estimating Class Membership Probabilities Using Classifier Learners.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F3FAXB6D\\Estimating Class Membership Probabilities Using Classifier Learners.pdf:application/pdf},
}

@inproceedings{niculescu-mizilPredictingGoodProbabilities2005,
	address = {Bonn, Germany},
	title = {Predicting good probabilities with supervised learning},
	isbn = {978-1-59593-180-1},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102430},
	doi = {10.1145/1102351.1102430},
	abstract = {We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.},
	language = {en},
	urldate = {2021-06-09},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning  - {ICML} '05},
	publisher = {ACM Press},
	author = {Niculescu-Mizil, Alexandru and Caruana, Rich},
	year = {2005},
	pages = {625--632},
	file = {Niculescu-Mizil and Caruana - 2005 - Predicting good probabilities with supervised lear.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZUJIGX3Y\\Niculescu-Mizil and Caruana - 2005 - Predicting good probabilities with supervised lear.pdf:application/pdf},
}

@article{jospinHandsonBayesianNeural2020,
	title = {Hands-on {Bayesian} {Neural} {Networks} -- a {Tutorial} for {Deep} {Learning} {Users}},
	url = {http://arxiv.org/abs/2007.06823},
	abstract = {Modern deep learning methods have equipped researchers and engineers with incredibly powerful tools to tackle problems that previously seemed impossible. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural networks predictions. This paper provides a tutorial for researchers and scientists who are using machine learning, especially deep learning, with an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian neural networks. CCS Concepts: • Mathematics of computing → Probability and statistics; • Computing methodologies → Neural networks; Bayesian network models; Ensemble methods; Regularization.},
	language = {en},
	urldate = {2021-06-09},
	journal = {arXiv:2007.06823 [cs, stat]},
	author = {Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.06823},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 62-02 (Primary), G.3, I.2.6},
	file = {Jospin et al. - 2020 - Hands-on Bayesian Neural Networks -- a Tutorial fo.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S5K9HZV2\\Jospin et al. - 2020 - Hands-on Bayesian Neural Networks -- a Tutorial fo.pdf:application/pdf},
}

@article{qiPredictingPhaseClinical,
	title = {Predicting {Phase} 3 {Clinical} {Trial} {Results} by {Modeling} {Phase} 2 {Clinical} {Trial} {Subject} {Level} {Data} {Using} {Deep} {Learning}},
	abstract = {Predicting Phase 3 clinical trial results is a critical step of Go/No-Go decision making and Phase 3 trial design optimization. To predict the overall treatment eﬀect for patients enrolled into a Phase 3 trial, we propose a framework consisting of two models. First, an individual trough pharmacokinetic concentration (Ctrough) model is developed to predict the trough pharmacokinetic concentration for a potentially new treatment regime planned for Phase 3. Second, an individual treatment eﬀect model is built to model the relationship between patient baseline characteristics, Ctrough and clinical outcomes. These two models are combined together to predict Phase 3 clinical trial results. Since the clinical outcomes to be predicted are longitudinal and the predictors are a mix of time-invariant and timevariant variables, a novel neural network, Residual Semi-Recurrent Neural Network, is developed for both models. The proposed framework is applied in a post-hoc prediction of Phase 3 clinical trial results, and it outperforms the traditional method.},
	language = {en},
	author = {Qi, Youran and Tang, Qi},
	keywords = {machine learning},
	pages = {15},
	file = {Qi and Tang - Predicting Phase 3 Clinical Trial Results by Model.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ALD6I4Z8\\Qi and Tang - Predicting Phase 3 Clinical Trial Results by Model.pdf:application/pdf},
}

@article{kesselheimHighCostPrescription2016,
	title = {The {High} {Cost} of {Prescription} {Drugs} in the {United} {States}: {Origins} and {Prospects} for {Reform}},
	volume = {316},
	issn = {0098-7484},
	shorttitle = {The {High} {Cost} of {Prescription} {Drugs} in the {United} {States}},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2016.11237},
	doi = {10.1001/jama.2016.11237},
	language = {en},
	number = {8},
	urldate = {2021-06-10},
	journal = {JAMA},
	author = {Kesselheim, Aaron S. and Avorn, Jerry and Sarpatwari, Ameet},
	month = aug,
	year = {2016},
	pages = {858},
}

@article{morganCostDrugDevelopment2011,
	title = {The cost of drug development: {A} systematic review},
	volume = {100},
	issn = {01688510},
	shorttitle = {The cost of drug development},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168851010003659},
	doi = {10.1016/j.healthpol.2010.12.002},
	language = {en},
	number = {1},
	urldate = {2021-06-10},
	journal = {Health Policy},
	author = {Morgan, Steve and Grootendorst, Paul and Lexchin, Joel and Cunningham, Colleen and Greyson, Devon},
	month = apr,
	year = {2011},
	pages = {4--17},
}

@article{dimasiPriceInnovationNew2003,
	title = {The price of innovation: new estimates of drug development costs},
	volume = {22},
	issn = {01676296},
	shorttitle = {The price of innovation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167629602001261},
	doi = {10.1016/S0167-6296(02)00126-1},
	language = {en},
	number = {2},
	urldate = {2021-06-10},
	journal = {Journal of Health Economics},
	author = {DiMasi, Joseph A and Hansen, Ronald W and Grabowski, Henry G},
	month = mar,
	year = {2003},
	pages = {151--185},
	file = {The price of innovation.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\The price of innovation.pdf:application/pdf},
}

@article{dimasiInnovationPharmaceuticalIndustry2016,
	title = {Innovation in the pharmaceutical industry: {New} estimates of {R}\&{D} costs},
	volume = {47},
	issn = {01676296},
	shorttitle = {Innovation in the pharmaceutical industry},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167629616000291},
	doi = {10.1016/j.jhealeco.2016.01.012},
	language = {en},
	urldate = {2021-06-10},
	journal = {Journal of Health Economics},
	author = {DiMasi, Joseph A. and Grabowski, Henry G. and Hansen, Ronald W.},
	month = may,
	year = {2016},
	pages = {20--33},
	file = {Innovation in the pharmaceutical industry.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Innovation in the pharmaceutical industry.pdf:application/pdf},
}

@article{jayasundaraEstimatingClinicalCost2019,
	title = {Estimating the clinical cost of drug development for orphan versus non-orphan drugs},
	volume = {14},
	issn = {1750-1172},
	url = {https://ojrd.biomedcentral.com/articles/10.1186/s13023-018-0990-4},
	doi = {10.1186/s13023-018-0990-4},
	language = {en},
	number = {1},
	urldate = {2021-06-10},
	journal = {Orphanet Journal of Rare Diseases},
	author = {Jayasundara, Kavisha and Hollis, Aidan and Krahn, Murray and Mamdani, Muhammad and Hoch, Jeffrey S. and Grootendorst, Paul},
	month = dec,
	year = {2019},
	pages = {12},
	file = {Estimating the clinical cost of drug development for orphan versus non-orphan.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Estimating the clinical cost of drug development for orphan versus non-orphan.pdf:application/pdf},
}

@article{adamsEstimatingCostNew2006,
	title = {Estimating {The} {Cost} {Of} {New} {Drug} {Development}: {Is} {It} {Really} \$802 {Million}?},
	volume = {25},
	issn = {0278-2715, 1544-5208},
	shorttitle = {Estimating {The} {Cost} {Of} {New} {Drug} {Development}},
	url = {http://www.healthaffairs.org/doi/10.1377/hlthaff.25.2.420},
	doi = {10.1377/hlthaff.25.2.420},
	language = {en},
	number = {2},
	urldate = {2021-06-10},
	journal = {Health Affairs},
	author = {Adams, Christopher P. and Brantner, Van V.},
	month = mar,
	year = {2006},
	pages = {420--428},
}

@article{adamsSpendingNewDrug2010,
	title = {Spending on new drug development},
	volume = {19},
	issn = {10579230, 10991050},
	url = {http://doi.wiley.com/10.1002/hec.1454},
	doi = {10.1002/hec.1454},
	language = {en},
	number = {2},
	urldate = {2021-06-10},
	journal = {Health Economics},
	author = {Adams, Christopher Paul and Brantner, Van Vu},
	month = feb,
	year = {2010},
	pages = {130--141},
	file = {Spending on new drug development.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Spending on new drug development.pdf:application/pdf},
}

@article{dimasiCostBiopharmaceuticalBiotech2007,
	title = {The cost of biopharmaceutical {R}\&{D}: is biotech different?},
	volume = {28},
	issn = {01436570, 10991468},
	shorttitle = {The cost of biopharmaceutical {R}\&{D}},
	url = {http://doi.wiley.com/10.1002/mde.1360},
	doi = {10.1002/mde.1360},
	language = {en},
	number = {4-5},
	urldate = {2021-06-10},
	journal = {Managerial and Decision Economics},
	author = {DiMasi, Joseph A. and Grabowski, Henry G.},
	month = jun,
	year = {2007},
	pages = {469--479},
}

@article{dimasiCostsReturnsTherapeutic2004,
	title = {R\&{D} {Costs} and {Returns} by {Therapeutic} {Category}},
	volume = {38},
	issn = {0092-8615, 2164-9200},
	url = {http://link.springer.com/10.1177/009286150403800301},
	doi = {10.1177/009286150403800301},
	language = {en},
	number = {3},
	urldate = {2021-06-10},
	journal = {Drug Information Journal},
	author = {DiMasi, Joseph A. and Grabowski, Henry G. and Vernon, John},
	month = jul,
	year = {2004},
	pages = {211--223},
}

@article{paulHowImproveProductivity2010,
	title = {How to improve {R}\&{D} productivity: the pharmaceutical industry's grand challenge},
	volume = {9},
	issn = {1474-1776, 1474-1784},
	shorttitle = {How to improve {R}\&{D} productivity},
	url = {http://www.nature.com/articles/nrd3078},
	doi = {10.1038/nrd3078},
	language = {en},
	number = {3},
	urldate = {2021-06-10},
	journal = {Nature Reviews Drug Discovery},
	author = {Paul, Steven M. and Mytelka, Daniel S. and Dunwiddie, Christopher T. and Persinger, Charles C. and Munos, Bernard H. and Lindborg, Stacy R. and Schacht, Aaron L.},
	month = mar,
	year = {2010},
	pages = {203--214},
	file = {How to improve R&D productivity.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\How to improve R&D productivity.pdf:application/pdf;How to improve R&D productivity.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\How to improve R&D productivity2.pdf:application/pdf;Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2VZDLKYR\\nrd3078.html:text/html},
}

@article{prasadResearchDevelopmentSpending2017,
	title = {Research and {Development} {Spending} to {Bring} a {Single} {Cancer} {Drug} to {Market} and {Revenues} {After} {Approval}},
	volume = {177},
	issn = {2168-6106},
	url = {http://archinte.jamanetwork.com/article.aspx?doi=10.1001/jamainternmed.2017.3601},
	doi = {10.1001/jamainternmed.2017.3601},
	language = {en},
	number = {11},
	urldate = {2021-06-10},
	journal = {JAMA Internal Medicine},
	author = {Prasad, Vinay and Mailankody, Sham},
	month = nov,
	year = {2017},
	pages = {1569},
}

@article{lanthierImprovedApproachMeasuring2013,
	title = {An {Improved} {Approach} {To} {Measuring} {Drug} {Innovation} {Finds} {Steady} {Rates} {Of} {First}-{In}-{Class} {Pharmaceuticals}, 1987–2011},
	volume = {32},
	issn = {0278-2715, 1544-5208},
	url = {http://www.healthaffairs.org/doi/10.1377/hlthaff.2012.0541},
	doi = {10.1377/hlthaff.2012.0541},
	language = {en},
	number = {8},
	urldate = {2021-06-10},
	journal = {Health Affairs},
	author = {Lanthier, Michael and Miller, Kathleen L. and Nardinelli, Clark and Woodcock, Janet},
	month = aug,
	year = {2013},
	pages = {1433--1439},
}

@article{darrowDrugDevelopmentFDA2014,
	title = {Drug {Development} and {FDA} {Approval}, 1938–2013},
	volume = {370},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/10.1056/NEJMp1402114},
	doi = {10.1056/NEJMp1402114},
	language = {en},
	number = {26},
	urldate = {2021-06-10},
	journal = {New England Journal of Medicine},
	author = {Darrow, Jonathan J. and Kesselheim, Aaron S.},
	month = jun,
	year = {2014},
	pages = {e39},
}

@article{hayClinicalDevelopmentSuccess2014,
	title = {Clinical development success rates for investigational drugs},
	volume = {32},
	issn = {1087-0156, 1546-1696},
	url = {http://www.nature.com/articles/nbt.2786},
	doi = {10.1038/nbt.2786},
	language = {en},
	number = {1},
	urldate = {2021-06-10},
	journal = {Nature Biotechnology},
	author = {Hay, Michael and Thomas, David W and Craighead, John L and Economides, Celia and Rosenthal, Jesse},
	month = jan,
	year = {2014},
	pages = {40--51},
}

@article{chitOpportunityCostCapital2015,
	title = {The {Opportunity} {Cost} of {Capital}: {Development} of {New} {Pharmaceuticals}},
	volume = {52},
	issn = {0046-9580, 1945-7243},
	shorttitle = {The {Opportunity} {Cost} of {Capital}},
	url = {http://journals.sagepub.com/doi/10.1177/0046958015584641},
	doi = {10.1177/0046958015584641},
	abstract = {The opportunity cost of the capital invested in pharmaceutical research and development (R\&D) to bring a new drug to market makes up as much as half the total cost. However, the literature on the cost of pharmaceutical R\&D is mixed on how, exactly, one should calculate this “hidden” cost. Some authors attempt to adopt models from the field of finance, whereas other prominent authors dismiss this practice as biased, arguing that it artificially inflates the R\&D cost to justify higher prices for pharmaceuticals. In this article, we examine the arguments made by both sides of the debate and then explain the cost of capital concept and describe in detail how this value is calculated. Given the significant contribution of the cost of capital to the overall cost of new drug R\&D, a clear understanding of the concept is critical for policy makers, investors, and those involved directly in the R\&D.},
	language = {en},
	urldate = {2021-06-10},
	journal = {INQUIRY: The Journal of Health Care Organization, Provision, and Financing},
	author = {Chit, Ayman and Chit, Ahmad and Papadimitropoulos, Manny and Krahn, Murray and Parker, Jayson and Grootendorst, Paul},
	month = jan,
	year = {2015},
	pages = {004695801558464},
	file = {The Opportunity Cost of Capital.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\The Opportunity Cost of Capital.pdf:application/pdf},
}

@article{vandergrondeAssessingPharmaceuticalResearch2018,
	title = {Assessing {Pharmaceutical} {Research} and {Development} {Costs}},
	volume = {178},
	issn = {2168-6106},
	url = {http://archinte.jamanetwork.com/article.aspx?doi=10.1001/jamainternmed.2017.8706},
	doi = {10.1001/jamainternmed.2017.8706},
	language = {en},
	number = {4},
	urldate = {2021-06-10},
	journal = {JAMA Internal Medicine},
	author = {van der Gronde, Toon and Pieters, Toine},
	month = apr,
	year = {2018},
	pages = {587},
}

@article{dimasiAssessingPharmaceuticalResearch2018,
	title = {Assessing {Pharmaceutical} {Research} and {Development} {Costs}},
	volume = {178},
	issn = {2168-6106},
	url = {http://archinte.jamanetwork.com/article.aspx?doi=10.1001/jamainternmed.2017.8703},
	doi = {10.1001/jamainternmed.2017.8703},
	language = {en},
	number = {4},
	urldate = {2021-06-10},
	journal = {JAMA Internal Medicine},
	author = {DiMasi, Joseph A.},
	month = apr,
	year = {2018},
	pages = {587},
}

@article{prasadAssessingPharmaceuticalResearch2018,
	title = {Assessing {Pharmaceutical} {Research} and {Development} {Costs}—{Reply}},
	volume = {178},
	issn = {2168-6106},
	url = {http://archinte.jamanetwork.com/article.aspx?doi=10.1001/jamainternmed.2017.8737},
	doi = {10.1001/jamainternmed.2017.8737},
	language = {en},
	number = {4},
	urldate = {2021-06-10},
	journal = {JAMA Internal Medicine},
	author = {Prasad, Vinay and Mailankody, Sham},
	month = apr,
	year = {2018},
	pages = {588},
}

@article{lightExtraordinaryClaimsRequire2005,
	title = {Extraordinary claims require extraordinary evidence},
	volume = {24},
	issn = {01676296},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016762960500055X},
	doi = {10.1016/j.jhealeco.2005.07.001},
	language = {en},
	number = {5},
	urldate = {2021-06-10},
	journal = {Journal of Health Economics},
	author = {Light, Donald W. and Warburton, Rebecca N.},
	month = sep,
	year = {2005},
	pages = {1030--1033},
}

@article{gooznerMuchNeededCorrectiveDrug2017,
	title = {A {Much}-{Needed} {Corrective} on {Drug} {Development} {Costs}},
	volume = {177},
	issn = {2168-6106},
	url = {http://archinte.jamanetwork.com/article.aspx?doi=10.1001/jamainternmed.2017.4997},
	doi = {10.1001/jamainternmed.2017.4997},
	language = {en},
	number = {11},
	urldate = {2021-06-10},
	journal = {JAMA Internal Medicine},
	author = {Goozner, Merrill},
	month = nov,
	year = {2017},
	pages = {1575},
}

@article{avornBillionPillMethodologic2015,
	title = {The \$2.6 {Billion} {Pill} — {Methodologic} and {Policy} {Considerations}},
	volume = {372},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/10.1056/NEJMp1500848},
	doi = {10.1056/NEJMp1500848},
	language = {en},
	number = {20},
	urldate = {2021-06-10},
	journal = {New England Journal of Medicine},
	author = {Avorn, Jerry},
	month = may,
	year = {2015},
	pages = {1877--1879},
}

@article{dimasiTrendsRisksAssociated2010,
	title = {Trends in {Risks} {Associated} {With} {New} {Drug} {Development}: {Success} {Rates} for {Investigational} {Drugs}},
	volume = {87},
	issn = {0009-9236, 1532-6535},
	shorttitle = {Trends in {Risks} {Associated} {With} {New} {Drug} {Development}},
	url = {http://doi.wiley.com/10.1038/clpt.2009.295},
	doi = {10.1038/clpt.2009.295},
	number = {3},
	urldate = {2021-06-10},
	journal = {Clinical Pharmacology \& Therapeutics},
	author = {DiMasi, J A and Feldman, L and Seckler, A and Wilson, A},
	month = mar,
	year = {2010},
	pages = {272--277},
}

@article{lloydPharmaAnnualReview2021,
	title = {Pharma {R}\&{D} {Annual} {Review} 2021},
	language = {en},
	author = {Lloyd, Ian},
	year = {2021},
	pages = {45},
	file = {Lloyd - 2021 - Pharma R&D Annual Review 2021.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9T7VUW7L\\Lloyd - 2021 - Pharma R&D Annual Review 2021.pdf:application/pdf},
}

@article{feijooKeyIndicatorsPhase2020,
	title = {Key indicators of phase transition for clinical trials through machine learning},
	volume = {25},
	issn = {1359-6446},
	url = {https://www.sciencedirect.com/science/article/pii/S1359644620300052},
	doi = {10.1016/j.drudis.2019.12.014},
	abstract = {A significant number of drugs fail during the clinical testing stage. To understand the attrition of drugs through the regulatory process, here we review and advance machine-learning (ML) and natural language-processing algorithms to investigate the importance of factors in clinical trials that are linked with failure in Phases II and III. We find that clinical trial phase transitions can be predicted with an average accuracy of 80\%. Identifying these trials provides information to sponsors facing difficult decisions about whether these higher risk trials should be modified or halted. We also find common protocol characteristics across therapeutic areas that are linked to phase success, including the number of endpoints and the complexity of the eligibility criteria.},
	language = {en},
	number = {2},
	urldate = {2021-06-10},
	journal = {Drug Discovery Today},
	author = {Feijoo, Felipe and Palopoli, Michele and Bernstein, Jen and Siddiqui, Sauleh and Albright, Tenley E.},
	month = feb,
	year = {2020},
	keywords = {machine learning},
	pages = {414--421},
	file = {Key indicators of phase transition for clinical trials through machine learning.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Key indicators of phase transition for clinical trials through machine learning.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5IJF8X2U\\S1359644620300052.html:text/html},
}

@misc{AACTDatabaseClinical,
	title = {{AACT} {Database} {\textbar} {Clinical} {Trials} {Transformation} {Initiative}},
	url = {https://aact.ctti-clinicaltrials.org/},
	urldate = {2021-06-16},
	file = {AACT Database | Clinical Trials Transformation Initiative:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4NP8VSHF\\aact.ctti-clinicaltrials.org.html:text/html},
}

@misc{DrugsFDAFDAApproved,
	title = {Drugs@{FDA}: {FDA}-{Approved} {Drugs}},
	shorttitle = {Drugs@{FDA}},
	url = {https://www.accessdata.fda.gov/scripts/cder/daf/},
	urldate = {2021-06-17},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\URR898H3\\daf.html:text/html},
}

@article{avramDrugCentral2021Supports2021,
	title = {{DrugCentral} 2021 supports drug discovery and repositioning},
	volume = {49},
	issn = {0305-1048, 1362-4962},
	url = {https://academic.oup.com/nar/article/49/D1/D1160/5957163},
	doi = {10.1093/nar/gkaa997},
	abstract = {Abstract
            DrugCentral is a public resource (http://drugcentral.org) that serves the scientific community by providing up-to-date drug information, as described in previous papers. The current release includes 109 newly approved (October 2018 through March 2020) active pharmaceutical ingredients in the US, Europe, Japan and other countries; and two molecular entities (e.g. mefuparib) of interest for COVID19. New additions include a set of pharmacokinetic properties for ∼1000 drugs, and a sex-based separation of side effects, processed from FAERS (FDA Adverse Event Reporting System); as well as a drug repositioning prioritization scheme based on the market availability and intellectual property rights forFDA approved drugs. In the context of the COVID19 pandemic, we also incorporated REDIAL-2020, a machine learning platform that estimates anti-SARS-CoV-2 activities, as well as the ‘drugs in news’ feature offers a brief enumeration of the most interesting drugs at the present moment. The full database dump and data files are available for download from the DrugCentral web portal.},
	language = {en},
	number = {D1},
	urldate = {2021-06-17},
	journal = {Nucleic Acids Research},
	author = {Avram, Sorin and Bologa, Cristian G and Holmes, Jayme and Bocci, Giovanni and Wilson, Thomas B and Nguyen, Dac-Trung and Curpan, Ramona and Halip, Liliana and Bora, Alina and Yang, Jeremy J and Knockel, Jeffrey and Sirimulla, Suman and Ursu, Oleg and Oprea, Tudor I},
	month = jan,
	year = {2021},
	pages = {D1160--D1169},
	file = {DrugCentral 2021 supports drug discovery and repositioning.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\DrugCentral 2021 supports drug discovery and repositioning.pdf:application/pdf},
}

@article{fawcettIntroductionROCAnalysis2006,
	title = {An introduction to {ROC} analysis},
	volume = {27},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S016786550500303X},
	doi = {10.1016/j.patrec.2005.10.010},
	abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.},
	number = {8},
	journal = {ROC Analysis in Pattern Recognition},
	author = {Fawcett, Tom},
	month = jun,
	year = {2006},
	keywords = {Classifier evaluation, Evaluation metrics, ROC analysis},
	pages = {861--874},
}

@article{piccininniDirectedAcyclicGraphs2020,
	title = {Directed acyclic graphs and causal thinking in clinical risk prediction modeling},
	volume = {20},
	issn = {1471-2288},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01058-z},
	doi = {10.1186/s12874-020-01058-z},
	language = {en},
	number = {1},
	urldate = {2021-06-22},
	journal = {BMC Medical Research Methodology},
	author = {Piccininni, Marco and Konigorski, Stefan and Rohmann, Jessica L. and Kurth, Tobias},
	month = dec,
	year = {2020},
	pages = {179},
	file = {Full Text:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\83GH2WAM\\Piccininni et al. - 2020 - Directed acyclic graphs and causal thinking in cli.pdf:application/pdf},
}

@article{opgen-rheinCorrelationCausationNetworks2007,
	title = {From correlation to causation networks: a simple approximate learning algorithm and its application to high-dimensional plant gene expression data},
	volume = {1},
	issn = {1752-0509},
	shorttitle = {From correlation to causation networks},
	url = {https://bmcsystbiol.biomedcentral.com/articles/10.1186/1752-0509-1-37},
	doi = {10.1186/1752-0509-1-37},
	language = {en},
	number = {1},
	urldate = {2021-06-22},
	journal = {BMC Systems Biology},
	author = {Opgen-Rhein, Rainer and Strimmer, Korbinian},
	month = dec,
	year = {2007},
	pages = {37},
	file = {Full Text:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\M5WC3I87\\Opgen-Rhein and Strimmer - 2007 - From correlation to causation networks a simple a.pdf:application/pdf},
}

@article{vowelsYaDAGsSurvey2021,
	title = {D'ya like {DAGs}? {A} {Survey} on {Structure} {Learning} and {Causal} {Discovery}},
	shorttitle = {D'ya like {DAGs}?},
	url = {http://arxiv.org/abs/2103.02582},
	abstract = {Causal reasoning is a crucial part of science and human intelligence. In order to discover causal relationships from data, we need structure discovery methods. We provide a review of background theory and a survey of methods for structure discovery. We primarily focus on modern, continuous optimization methods, and provide reference to further resources such as benchmark datasets and software packages. Finally, we discuss the assumptive leap required to take us from structure to causality.},
	urldate = {2021-06-22},
	journal = {arXiv:2103.02582 [cs, stat]},
	author = {Vowels, Matthew J. and Camgoz, Necati Cihan and Bowden, Richard},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.02582},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\53MIAZED\\Vowels et al. - 2021 - D'ya like DAGs A Survey on Structure Learning and.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XDUQAQGP\\2103.html:text/html},
}

@misc{ExecutiveGuideMachine,
	title = {An executive’s guide to machine learning {\textbar} {McKinsey}},
	url = {https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/an-executives-guide-to-machine-learning},
	urldate = {2021-06-24},
	file = {An executive’s guide to machine learning | McKinsey:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8M539T74\\an-executives-guide-to-machine-learning.html:text/html},
}

@article{gaultonChEMBLDatabase20172017,
	title = {The {ChEMBL} database in 2017},
	volume = {45},
	issn = {0305-1048, 1362-4962},
	url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkw1074},
	doi = {10.1093/nar/gkw1074},
	language = {en},
	number = {D1},
	urldate = {2021-06-24},
	journal = {Nucleic Acids Research},
	author = {Gaulton, Anna and Hersey, Anne and Nowotka, Michał and Bento, A. Patrícia and Chambers, Jon and Mendez, David and Mutowo, Prudence and Atkinson, Francis and Bellis, Louisa J. and Cibrián-Uhalte, Elena and Davies, Mark and Dedman, Nathan and Karlsson, Anneli and Magariños, María Paula and Overington, John P. and Papadatos, George and Smit, Ines and Leach, Andrew R.},
	month = jan,
	year = {2017},
	pages = {D945--D954},
	file = {Full Text:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C7MYGRZS\\Gaulton et al. - 2017 - The ChEMBL database in 2017.pdf:application/pdf},
}

@misc{OpenFDA,
	title = {{openFDA}},
	url = {https://open.fda.gov/apis/drug/label/},
	urldate = {2021-06-27},
	file = {openFDA:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CNEMLUDK\\label.html:text/html},
}

@article{wishartDrugBankMajorUpdate2018,
	title = {{DrugBank} 5.0: a major update to the {DrugBank} database for 2018},
	volume = {46},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{DrugBank} 5.0},
	url = {http://academic.oup.com/nar/article/46/D1/D1074/4602867},
	doi = {10.1093/nar/gkx1037},
	language = {en},
	number = {D1},
	urldate = {2021-06-27},
	journal = {Nucleic Acids Research},
	author = {Wishart, David S and Feunang, Yannick D and Guo, An C and Lo, Elvis J and Marcu, Ana and Grant, Jason R and Sajed, Tanvir and Johnson, Daniel and Li, Carin and Sayeeda, Zinat and Assempour, Nazanin and Iynkkaran, Ithayavani and Liu, Yifeng and Maciejewski, Adam and Gale, Nicola and Wilson, Alex and Chin, Lucy and Cummings, Ryan and Le, Diana and Pon, Allison and Knox, Craig and Wilson, Michael},
	month = jan,
	year = {2018},
	pages = {D1074--D1082},
	file = {Full Text:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZQTYNRS9\\Wishart et al. - 2018 - DrugBank 5.0 a major update to the DrugBank datab.pdf:application/pdf},
}

@misc{OpenTargetsPlatform,
	title = {Open {Targets} {Platform}},
	url = {https://platform.opentargets.org/downloads},
	urldate = {2021-06-27},
	file = {Open Targets Platform:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LD4IATHR\\downloads.html:text/html},
}

@misc{PurpleBookDatabase,
	title = {Purple {Book}: {Database}},
	url = {https://purplebooksearch.fda.gov/advanced-search},
	urldate = {2021-06-27},
	file = {Purple Book\: Database:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GFGCWMRW\\advanced-search.html:text/html},
}

@misc{researchCenterDrugEvaluation2020,
	title = {Center for {Drug} {Evaluation} and {Research} {\textbar} {CDER}},
	url = {https://www.fda.gov/about-fda/fda-organization/center-drug-evaluation-and-research-cder},
	abstract = {The Center for Drug Evaluation and Research (CDER) regulates over-the-counter and prescription drugs, including biological therapeutics and generic drugs.},
	language = {en},
	urldate = {2021-06-27},
	journal = {FDA},
	author = {Research, Center for Drug Evaluation and},
	month = aug,
	year = {2020},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BS2DYXMI\\center-drug-evaluation-and-research-cder.html:text/html},
}

@article{tasneemDatabaseAggregateAnalysis2012,
	title = {The {Database} for {Aggregate} {Analysis} of {ClinicalTrials}.gov ({AACT}) and {Subsequent} {Regrouping} by {Clinical} {Specialty}},
	volume = {7},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0033677},
	doi = {10.1371/journal.pone.0033677},
	language = {en},
	number = {3},
	urldate = {2021-07-12},
	journal = {PLoS ONE},
	author = {Tasneem, Asba and Aberle, Laura and Ananth, Hari and Chakraborty, Swati and Chiswell, Karen and McCourt, Brian J. and Pietrobon, Ricardo},
	editor = {Gagnier, Joel Joseph},
	month = mar,
	year = {2012},
	pages = {e33677},
	file = {Full Text:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CFFXK4CV\\Tasneem et al. - 2012 - The Database for Aggregate Analysis of ClinicalTri.pdf:application/pdf},
}

@article{mendezChEMBLDirectDeposition2019,
	title = {{ChEMBL}: towards direct deposition of bioassay data},
	volume = {47},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{ChEMBL}},
	url = {https://academic.oup.com/nar/article/47/D1/D930/5162468},
	doi = {10.1093/nar/gky1075},
	language = {en},
	number = {D1},
	urldate = {2021-07-13},
	journal = {Nucleic Acids Research},
	author = {Mendez, David and Gaulton, Anna and Bento, A Patrícia and Chambers, Jon and De Veij, Marleen and Félix, Eloy and Magariños, María Paula and Mosquera, Juan F and Mutowo, Prudence and Nowotka, Michał and Gordillo-Marañón, María and Hunter, Fiona and Junco, Laura and Mugumbate, Grace and Rodriguez-Lopez, Milagros and Atkinson, Francis and Bosc, Nicolas and Radoux, Chris J and Segura-Cabrera, Aldo and Hersey, Anne and Leach, Andrew R},
	month = jan,
	year = {2019},
	pages = {D930--D940},
	file = {Full Text:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UI4INGUG\\Mendez et al. - 2019 - ChEMBL towards direct deposition of bioassay data.pdf:application/pdf},
}

@book{wilkinsonGrammarGraphics1999,
	address = {New York, NY},
	series = {Statistics and {Computing}},
	title = {The {Grammar} of {Graphics}},
	isbn = {978-1-4757-3102-6 978-1-4757-3100-2},
	url = {http://link.springer.com/10.1007/978-1-4757-3100-2},
	language = {en},
	urldate = {2021-08-06},
	publisher = {Springer New York},
	author = {Wilkinson, Leland},
	editor = {Chambers, J. and Eddy, W. and Härdle, W. and Sheather, S. and Tierney, L.},
	year = {1999},
	doi = {10.1007/978-1-4757-3100-2},
	file = {Wilkinson - 1999 - The Grammar of Graphics.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z8RRVRPW\\Wilkinson - 1999 - The Grammar of Graphics.pdf:application/pdf;Wilkinson - 1999 - The Grammar of Graphics.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KAZAGN5F\\Wilkinson - 1999 - The Grammar of Graphics.pdf:application/pdf},
}

@article{heerAnimatedTransitionsStatistical2007,
	title = {Animated {Transitions} in {Statistical} {Data} {Graphics}},
	volume = {13},
	issn = {1077-2626},
	url = {http://ieeexplore.ieee.org/document/4376146/},
	doi = {10.1109/TVCG.2007.70539},
	abstract = {In this paper we investigate the effectiveness of animated transitions between common statistical data graphics such as bar charts, pie charts, and scatter plots. We extend theoretical models of data graphics to include such transitions, introducing a taxonomy of transition types. We then propose design principles for creating effective transitions and illustrate the application of these principles in DynaVis, a visualization system featuring animated data graphics. Two controlled experiments were conducted to assess the efficacy of various transition types, finding that animated transitions can significantly improve graphical perception.},
	language = {en},
	number = {6},
	urldate = {2021-08-06},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Heer, Jeffrey and Robertson, George},
	month = nov,
	year = {2007},
	pages = {1240--1247},
	file = {Heer and Robertson - 2007 - Animated Transitions in Statistical Data Graphics.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G2N7JTMU\\Heer and Robertson - 2007 - Animated Transitions in Statistical Data Graphics.pdf:application/pdf;Heer and Robertson - 2007 - Animated Transitions in Statistical Data Graphics.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N53B5MCS\\Heer and Robertson - 2007 - Animated Transitions in Statistical Data Graphics.pdf:application/pdf},
}

@article{segelNarrativeVisualizationTelling2010,
	title = {Narrative {Visualization}: {Telling} {Stories} with {Data}},
	volume = {16},
	issn = {1077-2626},
	shorttitle = {Narrative {Visualization}},
	url = {http://ieeexplore.ieee.org/document/5613452/},
	doi = {10.1109/TVCG.2010.179},
	abstract = {Data visualization is regularly promoted for its ability to reveal stories within data, yet these “data stories” differ in important ways from traditional forms of storytelling. Storytellers, especially online journalists, have increasingly been integrating visualizations into their narratives, in some cases allowing the visualization to function in place of a written story. In this paper, we systematically review the design space of this emerging class of visualizations. Drawing on case studies from news media to visualization research, we identify distinct genres of narrative visualization. We characterize these design differences, together with interactivity and messaging, in terms of the balance between the narrative ﬂow intended by the author (imposed by graphical elements and the interface) and story discovery on the part of the reader (often through interactive exploration). Our framework suggests design strategies for narrative visualization, including promising under-explored approaches to journalistic storytelling and educational media.},
	language = {en},
	number = {6},
	urldate = {2021-08-06},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Segel, E and Heer, J},
	month = nov,
	year = {2010},
	pages = {1139--1148},
	file = {Segel and Heer - 2010 - Narrative Visualization Telling Stories with Data.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\W2Y8CH68\\Segel and Heer - 2010 - Narrative Visualization Telling Stories with Data.pdf:application/pdf;Segel and Heer - 2010 - Narrative Visualization Telling Stories with Data.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UA2Z7DP7\\Segel and Heer - 2010 - Narrative Visualization Telling Stories with Data.pdf:application/pdf},
}

@article{mackinlayAutomatingDesignGraphical1986,
	title = {Automating the design of graphical presentations of relational information},
	volume = {5},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/22949.22950},
	doi = {10.1145/22949.22950},
	abstract = {The goal of the research described in this paper is to develop an application-independent presentation tool that automatically designs effective graphical presentations (such as bar charts, scatter plots, and connected graphs) of relational information. Two problems are raised by this goal: The codification of graphic design criteria in a form that can be used by the presentation tool, and the generation of a wide variety of designs so that the presentation tool can accommodate a wide variety of information. The approach described in this paper is based on the view that graphical presentations are sentences of graphical languages. The graphic design issues are codified as expressiveness and effectiveness criteria for graphical languages. Expressiveness criteria determine whether a graphical language can express the desired information. Effectiveness criteria determine whether a graphical language exploits the capabilities of the output medium and the human visual system. A wide variety of designs can be systematically generated by using a composition algebra that composes a small set of primitive graphical languages. Artificial intelligence techniques are used to implement a prototype presentation tool called APT (A Presentation Tool), which is based on the composition algebra and the graphic design criteria.},
	language = {en},
	number = {2},
	urldate = {2021-08-06},
	journal = {ACM Transactions on Graphics},
	author = {Mackinlay, Jock},
	month = apr,
	year = {1986},
	pages = {110--141},
	file = {Mackinlay - 1986 - Automating the design of graphical presentations o.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PM3KAFXQ\\Mackinlay - 1986 - Automating the design of graphical presentations o.pdf:application/pdf;Mackinlay - 1986 - Automating the design of graphical presentations o.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\75GEEMDK\\Mackinlay - 1986 - Automating the design of graphical presentations o.pdf:application/pdf},
}

@article{heerInteractiveDynamicsVisual,
	title = {Interactive {Dynamics} for {Visual} {Analysis}},
	language = {en},
	author = {Heer, Jeffrey and Shneiderman, Ben},
	pages = {26},
	file = {Heer and Shneiderman - Interactive Dynamics for Visual Analysis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\B8U324GX\\Heer and Shneiderman - Interactive Dynamics for Visual Analysis.pdf:application/pdf;Heer and Shneiderman - Interactive Dynamics for Visual Analysis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3F9F3PSP\\Heer and Shneiderman - Interactive Dynamics for Visual Analysis.pdf:application/pdf},
}

@article{gershonWhatStorytellingCan2001,
	title = {What storytelling can do for information visualization},
	volume = {44},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/381641.381653},
	doi = {10.1145/381641.381653},
	language = {en},
	number = {8},
	urldate = {2021-08-06},
	journal = {Communications of the ACM},
	author = {Gershon, Nahum and Page, Ward},
	month = aug,
	year = {2001},
	pages = {31--37},
	file = {Gershon and Page - 2001 - What storytelling can do for information visualiza.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AG4Y96Q5\\Gershon and Page - 2001 - What storytelling can do for information visualiza.pdf:application/pdf;Gershon and Page - 2001 - What storytelling can do for information visualiza.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IDCNPIAT\\Gershon and Page - 2001 - What storytelling can do for information visualiza.pdf:application/pdf},
}

@article{berinatoVisualizationsThatReally,
	title = {Visualizations {That} {Really} {Work}},
	language = {en},
	author = {Berinato, Scott},
	pages = {17},
	file = {Berinato - Visualizations That Really Work.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NCAMVLAS\\Berinato - Visualizations That Really Work.pdf:application/pdf},
}

@article{mostellerLandmarkBookWondeiful,
	title = {(({A} landmark book, a wondeiful book. 1},
	language = {en},
	author = {Mosteller, Frederick},
	pages = {208},
	file = {Mosteller - ((A landmark book, a wondeiful book. 1.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\J4CWEWVQ\\Mosteller - ((A landmark book, a wondeiful book. 1.pdf:application/pdf},
}

@article{knaflicStorytellingData,
	title = {Storytelling with {Data}},
	language = {en},
	author = {Knaflic, Cole Nussbaumer},
	pages = {284},
	file = {Knaflic - Storytelling with Data.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\29UTZIH5\\Knaflic - Storytelling with Data.pdf:application/pdf},
}

@book{healyDataVisualizationPractical2018,
	address = {Princeton, NJ},
	title = {Data visualization: a practical introduction},
	isbn = {978-0-691-18161-5 978-0-691-18162-2},
	shorttitle = {Data visualization},
	language = {en},
	publisher = {Princeton University Press},
	author = {Healy, Kieran},
	year = {2018},
	file = {Healy - 2018 - Data visualization a practical introduction.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GRDEC7JZ\\Healy - 2018 - Data visualization a practical introduction.pdf:application/pdf},
}

@book{berinatoGoodChartsWorkbook2019,
	address = {Boston, Massachusetts},
	title = {Good charts workbook: tips, tools, and exercises for making better data visualizations},
	isbn = {978-1-63369-617-4},
	shorttitle = {Good charts workbook},
	abstract = {"You know right away when you see an effective chart or graphic. It hits you with an immediate sense of its meaning and impact. But what actually makes it clearer, sharper, and more effective? If you're ready to create your own "good charts"--data visualizations that powerfully communicate your ideas and research and that advance your career--the Good Charts Workbook is the hands-on guide you've been looking for. The original Good Charts changed the landscape by helping readers understand how to think visually and by laying out a process for creating powerful data visualizations. Now, the Good Charts Workbook provides tools, exercises, and practical insights to help people in all kinds of enterprises gain the skills they need to get started." -- Provided by publisher},
	language = {en},
	publisher = {Harvard Business Review Press},
	author = {Berinato, Scott},
	year = {2019},
	note = {OCLC: on1030759134},
	keywords = {Business presentations, Charts, diagrams, etc, Communication in management, Diagramm, Visual communication, Visualisierung},
	file = {Berinato - 2019 - Good charts workbook tips, tools, and exercises f.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EMRS43CD\\Berinato - 2019 - Good charts workbook tips, tools, and exercises f.pdf:application/pdf},
}

@article{wilkeFundamentalsDataVisualization,
	title = {Fundamentals of {Data} {Visualization}},
	language = {en},
	author = {Wilke, Claus O},
	pages = {389},
	file = {Wilke - Fundamentals of Data Visualization.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BZYRV2HD\\Wilke - Fundamentals of Data Visualization.pdf:application/pdf},
}

@article{wareInformationVisualization,
	title = {Information {Visualization}},
	abstract = {This is a book about what the science of perception can tell us about visualization. There is a gold mine of information about how we see to be found in more than a century of work by vision researchers. The purpose of this book is to extract from that large body of research literature those design principles that apply to displaying information effectively” – Provided by publisher. Includes bibliographical references and index. ISBN 978-0-12-381464-7 (hardback) 1. Visual perception. 2. Visualization. 3. Information visualization. I. Title. BF241.W34 2012 152.14–dc23 2012009489 British Library Cataloguing-in-Publication Data A catalogue record for this book is available from the British Library.},
	language = {en},
	author = {Ware, Colin},
	pages = {537},
	file = {Ware - Information Visualization.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\86C8BC6Y\\Ware - Information Visualization.pdf:application/pdf;Ware - Information Visualization.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CI2ESFI7\\Ware - Information Visualization.pdf:application/pdf},
}

@article{primodecarvalhoalvesMajorDepressiveDisorder2017,
	title = {The {Major} {Depressive} {Disorder} {Hierarchy}: {Rasch} {Analysis} of 6 items of the {Hamilton} {Depression} {Scale} {Covering} the {Continuum} of {Depressive} {Syndrome}},
	volume = {12},
	issn = {1932-6203},
	shorttitle = {The {Major} {Depressive} {Disorder} {Hierarchy}},
	url = {https://dx.plos.org/10.1371/journal.pone.0170000},
	doi = {10.1371/journal.pone.0170000},
	language = {en},
	number = {1},
	urldate = {2021-08-07},
	journal = {PLOS ONE},
	author = {Primo de Carvalho Alves, Lucas and Pio de Almeida Fleck, Marcelo and Boni, Aline and Sica da Rocha, Neusa},
	editor = {Vrana, Kent E.},
	month = jan,
	year = {2017},
	pages = {e0170000},
	file = {Full Text:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U29YKG6E\\Primo de Carvalho Alves et al. - 2017 - The Major Depressive Disorder Hierarchy Rasch Ana.pdf:application/pdf},
}

@article{hamiltonRATINGSCALEDEPRESSION1960,
	title = {A {RATING} {SCALE} {FOR} {DEPRESSION}},
	volume = {23},
	issn = {0022-3050},
	url = {https://jnnp.bmj.com/lookup/doi/10.1136/jnnp.23.1.56},
	doi = {10.1136/jnnp.23.1.56},
	language = {en},
	number = {1},
	urldate = {2021-08-12},
	journal = {Journal of Neurology, Neurosurgery \& Psychiatry},
	author = {Hamilton, M.},
	month = feb,
	year = {1960},
	pages = {56--62},
	file = {Hamilton - 1960 - A RATING SCALE FOR DEPRESSION.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6CFL4KJ3\\Hamilton - 1960 - A RATING SCALE FOR DEPRESSION.pdf:application/pdf},
}

@article{sharpHamiltonRatingScale2015,
	title = {The {Hamilton} {Rating} {Scale} for {Depression}},
	volume = {65},
	issn = {0962-7480, 1471-8405},
	url = {https://academic.oup.com/occmed/article-lookup/doi/10.1093/occmed/kqv043},
	doi = {10.1093/occmed/kqv043},
	language = {en},
	number = {4},
	urldate = {2021-08-12},
	journal = {Occupational Medicine},
	author = {Sharp, Rachel},
	month = jun,
	year = {2015},
	pages = {340--340},
	file = {Full Text:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H6ZDUVDD\\Sharp - 2015 - The Hamilton Rating Scale for Depression.pdf:application/pdf},
}

@article{hamiltonDevelopmentRatingScale1967,
	title = {Development of a {Rating} {Scale} for {Primary} {Depressive} {Illness}},
	volume = {6},
	issn = {00071293},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2044-8260.1967.tb00530.x},
	doi = {10.1111/j.2044-8260.1967.tb00530.x},
	language = {en},
	number = {4},
	urldate = {2021-08-12},
	journal = {British Journal of Social and Clinical Psychology},
	author = {Hamilton, Max},
	month = dec,
	year = {1967},
	pages = {278--296},
}

@article{zimmermanSeverityClassificationHamilton2013,
	title = {Severity classification on the {Hamilton} depression rating scale},
	volume = {150},
	issn = {01650327},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165032713003017},
	doi = {10.1016/j.jad.2013.04.028},
	language = {en},
	number = {2},
	urldate = {2021-08-12},
	journal = {Journal of Affective Disorders},
	author = {Zimmerman, Mark and Martinez, Jennifer H. and Young, Diane and Chelminski, Iwona and Dalrymple, Kristy},
	month = sep,
	year = {2013},
	pages = {384--388},
}

@article{corderoConversationalRecommenderSystem2020,
	title = {A conversational recommender system for diagnosis using fuzzy rules},
	volume = {154},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417420302736},
	doi = {10.1016/j.eswa.2020.113449},
	language = {en},
	urldate = {2021-08-14},
	journal = {Expert Systems with Applications},
	author = {Cordero, P. and Enciso, M. and López, D. and Mora, A.},
	month = sep,
	year = {2020},
	pages = {113449},
	file = {Cordero et al. - 2020 - A conversational recommender system for diagnosis .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4FV3G89Z\\Cordero et al. - 2020 - A conversational recommender system for diagnosis .pdf:application/pdf;Cordero et al. - 2020 - A conversational recommender system for diagnosis .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CA4JHKMU\\Cordero et al. - 2020 - A conversational recommender system for diagnosis .pdf:application/pdf},
}

@inproceedings{pecuneModelSocialExplanations2019,
	address = {Kyoto Japan},
	title = {A {Model} of {Social} {Explanations} for a {Conversational} {Movie} {Recommendation} {System}},
	isbn = {978-1-4503-6922-0},
	url = {https://dl.acm.org/doi/10.1145/3349537.3351899},
	doi = {10.1145/3349537.3351899},
	abstract = {A critical aspect of any recommendation process is explaining the reasoning behind each recommendation. These explanations can not only improve users’ experiences, but also change their perception of the recommendation quality. This work describes our human-centered design for our conversational movie recommendation agent, which explains its decisions as humans would. After exploring and analyzing a corpus of dyadic interactions, we developed a computational model of explanations. We then incorporated this model in the architecture of a conversational agent and evaluated the resulting system via a user experiment. Our results show that social explanations can improve the perceived quality of both the system and the interaction, regardless of the intrinsic quality of the recommendations.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Human}-{Agent} {Interaction}},
	publisher = {ACM},
	author = {Pecune, Florian and Murali, Shruti and Tsai, Vivian and Matsuyama, Yoichi and Cassell, Justine},
	month = sep,
	year = {2019},
	pages = {135--143},
	file = {Pecune et al. - 2019 - A Model of Social Explanations for a Conversationa.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UGCU9ZIZ\\Pecune et al. - 2019 - A Model of Social Explanations for a Conversationa.pdf:application/pdf;Pecune et al. - 2019 - A Model of Social Explanations for a Conversationa.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NL3A7WHX\\Pecune et al. - 2019 - A Model of Social Explanations for a Conversationa.pdf:application/pdf},
}

@inproceedings{polatoPreliminaryStudyRecommender2016,
	address = {Boston, Massachusetts},
	title = {A preliminary study on a recommender system for the job recommendation challenge},
	isbn = {978-1-4503-4801-0},
	url = {http://dl.acm.org/citation.cfm?doid=2987538.2987549},
	doi = {10.1145/2987538.2987549},
	abstract = {In this paper we present our method used in the RecSys ’16 Challenge. In particular, we propose a general collaborative ﬁltering framework where many predictors can be cast. The framework is able to incorporate information about the content but in a collaborative fashion. Using this framework we instantiate a set of diﬀerent predictors that consider diﬀerent aspects of the dataset provided for the challenge. In order to merge all these aspects together, we also provide a method able to linearly combine the predictors. This method learns the weights of the predictors by solving a quadratic optimization problem.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the {Recommender} {Systems} {Challenge} on - {RecSys} {Challenge} '16},
	publisher = {ACM Press},
	author = {Polato, Mirko and Aiolli, Fabio},
	year = {2016},
	pages = {1--4},
	file = {Polato and Aiolli - 2016 - A preliminary study on a recommender system for th.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FSZQGAUM\\Polato and Aiolli - 2016 - A preliminary study on a recommender system for th.pdf:application/pdf;Polato and Aiolli - 2016 - A preliminary study on a recommender system for th.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8QTUAIGQ\\Polato and Aiolli - 2016 - A preliminary study on a recommender system for th.pdf:application/pdf},
}

@inproceedings{yagciRankerEnsembleMultiobjective2017,
	address = {Como, Italy},
	title = {A {Ranker} {Ensemble} for {Multi}-objective {Job} {Recommendation} in an {Item} {Cold} {Start} {Setting}},
	isbn = {978-1-4503-5391-5},
	url = {http://dl.acm.org/citation.cfm?doid=3124791.3124798},
	doi = {10.1145/3124791.3124798},
	abstract = {Real-life recommender systems often have multiple objectives, and considering only a single stakeholder’s perspective can be inadequate. ACM Recsys 2017 challenge requires such a multi-objective job recommender system taking into account job seeker satisfaction, recruiter satisfaction, balance between relevance and revenue, and scalability in a primarily item cold start setting. In this paper, we discuss our ndings, and a possible solution to this problem. Making use of interaction pro les and content data, a hybrid of ranking algorithms is proposed to optimize the required objectives. This solution was able to achieve 8th position during A/B testing, and some of the ideas can be useful in a more complex ensemble.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the {Recommender} {Systems} {Challenge} 2017 on {ZZZ} - {RecSys} {Challenge} '17},
	publisher = {ACM Press},
	author = {Yagci, Murat and Gurgen, Fikret},
	year = {2017},
	pages = {1--4},
	file = {Yagci and Gurgen - 2017 - A Ranker Ensemble for Multi-objective Job Recommen.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F29MGHIC\\Yagci and Gurgen - 2017 - A Ranker Ensemble for Multi-objective Job Recommen.pdf:application/pdf;Yagci and Gurgen - 2017 - A Ranker Ensemble for Multi-objective Job Recommen.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ULKCYQB4\\Yagci and Gurgen - 2017 - A Ranker Ensemble for Multi-objective Job Recommen.pdf:application/pdf},
}

@inproceedings{luRecommenderSystemJob2013,
	address = {Rio de Janeiro, Brazil},
	title = {A recommender system for job seeking and recruiting website},
	isbn = {978-1-4503-2038-2},
	url = {http://dl.acm.org/citation.cfm?doid=2487788.2488092},
	doi = {10.1145/2487788.2488092},
	abstract = {In this paper, a hybrid recommender system for job seeking and recruiting websites is presented. The various interaction features designed on the website help the users organize the resources they need as well as express their interest. The hybrid recommender system exploits the job and user profiles and the actions undertaken by users in order to generate personalized recommendations of candidates and jobs. The data collected from the website is modeled using a directed, weighted, and multirelational graph, and the 3A ranking algorithm is exploited to rank items according to their relevance to the target user. A preliminary evaluation is conducted based on simulated data and production data from a job hunting website in Switzerland.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {World} {Wide} {Web} - {WWW} '13 {Companion}},
	publisher = {ACM Press},
	author = {Lu, Yao and El Helou, Sandy and Gillet, Denis},
	year = {2013},
	pages = {963--966},
	file = {Lu et al. - 2013 - A recommender system for job seeking and recruitin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CLW3SBBS\\Lu et al. - 2013 - A recommender system for job seeking and recruitin.pdf:application/pdf;Lu et al. - 2013 - A recommender system for job seeking and recruitin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2T8B6CGP\\Lu et al. - 2013 - A recommender system for job seeking and recruitin.pdf:application/pdf},
}

@article{guptaSurveyRecommenderSystem2019,
	title = {A {Survey} on {Recommender} {System}},
	volume = {14},
	abstract = {Recommender systems have gained its importance because of the availability of enormous online information. In current time, deep learning has gained appreciable attention in many researches such as natural language processing, artificial intelligence due to high performance and great learning feature representations. The effect of deep learning is also persistent, lately showing its usefulness when put to retrieval of information and recommenders work which eventually have resulted in the flourish of deep learning approaches in recommender system. Hybrid approaches for designing recommender models have been gaining popularity in recent years. The paper aims in giving a comprehensive insight of recent research works on recommender systems.},
	language = {en},
	number = {14},
	author = {Gupta, Koyel Datta},
	year = {2019},
	pages = {4},
	file = {Gupta - 2019 - A Survey on Recommender System.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MQTGDHB8\\Gupta - 2019 - A Survey on Recommender System.pdf:application/pdf;Gupta - 2019 - A Survey on Recommender System.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SF2XW6MY\\Gupta - 2019 - A Survey on Recommender System.pdf:application/pdf;Gupta - 2019 - A Survey on Recommender System.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\J99SXFRS\\Gupta - 2019 - A Survey on Recommender System.pdf:application/pdf;Gupta - 2019 - A Survey on Recommender System.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JHCUD24K\\Gupta - 2019 - A Survey on Recommender System.pdf:application/pdf},
}

@article{jannachSurveyConversationalRecommender2021,
	title = {A {Survey} on {Conversational} {Recommender} {Systems}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3453154},
	doi = {10.1145/3453154},
	abstract = {Recommender systems are software applications that help users to find items of interest in situations of information overload. Current research often assumes a one-shot interaction paradigm, where the users’ preferences are estimated based on past observed behavior and where the presentation of a ranked list of suggestions is the main, one-directional form of user interaction. Conversational recommender systems (CRS) take a different approach and support a richer set of interactions. These interactions can, for example, help to improve the preference elicitation process or allow the user to ask questions about the recommendations and to give feedback. The interest in CRS has significantly increased in the past few years. This development is mainly due to the significant progress in the area of natural language processing, the emergence of new voice-controlled home assistants, and the increased use of chatbot technology. With this article, we provide a detailed survey of existing approaches to conversational recommendation. We categorize these approaches in various dimensions, e.g., in terms of the supported user intents or the knowledge they use in the background. Moreover, we discuss technological approaches, review how CRS are evaluated, and finally identify a number of gaps that deserve more research in the future.},
	language = {en},
	number = {5},
	urldate = {2021-08-14},
	journal = {ACM Computing Surveys},
	author = {Jannach, Dietmar and Manzoor, Ahtsham and Cai, Wanling and Chen, Li},
	month = jun,
	year = {2021},
	pages = {1--36},
	file = {Jannach et al. - 2021 - A Survey on Conversational Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ALZYWGG3\\Jannach et al. - 2021 - A Survey on Conversational Recommender Systems.pdf:application/pdf;Jannach et al. - 2021 - A Survey on Conversational Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZATLU5AM\\Jannach et al. - 2021 - A Survey on Conversational Recommender Systems.pdf:application/pdf},
}

@article{misirAlorsAlgorithmRecommender2017,
	title = {Alors: {An} algorithm recommender system},
	volume = {244},
	issn = {00043702},
	shorttitle = {Alors},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370216301436},
	doi = {10.1016/j.artint.2016.12.001},
	abstract = {Algorithm selection (AS), selecting the algorithm best suited for a particular problem instance, is acknowledged to be a key issue to make the best out of algorithm portfolios. While most AS approaches proceed by learning the performance model, this paper presents a collaborative ﬁltering approach to AS.},
	language = {en},
	urldate = {2021-08-14},
	journal = {Artificial Intelligence},
	author = {Mısır, Mustafa and Sebag, Michèle},
	month = mar,
	year = {2017},
	pages = {291--314},
	file = {Mısır and Sebag - 2017 - Alors An algorithm recommender system.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MVUFQTQ7\\Mısır and Sebag - 2017 - Alors An algorithm recommender system.pdf:application/pdf;Mısır and Sebag - 2017 - Alors An algorithm recommender system.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XV48A59Y\\Mısır and Sebag - 2017 - Alors An algorithm recommender system.pdf:application/pdf},
}

@article{xingshengguoAnalysisFrameworkContentbased2014,
	title = {An {Analysis} {Framework} for {Content}-based {Job} {Recommendation}},
	url = {http://rgdoi.net/10.13140/2.1.1090.4328},
	doi = {10.13140/2.1.1090.4328},
	abstract = {In this paper, we focus on the task of job recommendation. In particular, we consider several personalised content-based and case-based approaches to recommendation. We investigate a number of feature-based item representations, along with a variety of feature weighting schemes. A comparative evaluation of the various approaches is performed using a realworld, open source dataset.},
	language = {en},
	urldate = {2021-08-14},
	author = {Xingsheng Guo and Houssem Jerbi and Mahony, Michael P. O. '},
	year = {2014},
	note = {Publisher: Unpublished},
	file = {Xingsheng Guo et al. - 2014 - An Analysis Framework for Content-based Job Recomm.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KAIM8T5R\\Xingsheng Guo et al. - 2014 - An Analysis Framework for Content-based Job Recomm.pdf:application/pdf;Xingsheng Guo et al. - 2014 - An Analysis Framework for Content-based Job Recomm.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\B9XEBUSM\\Xingsheng Guo et al. - 2014 - An Analysis Framework for Content-based Job Recomm.pdf:application/pdf},
}

@inproceedings{zhangEnsembleMethodJob2016,
	address = {Boston, Massachusetts},
	title = {An ensemble method for job recommender systems},
	isbn = {978-1-4503-4801-0},
	url = {http://dl.acm.org/citation.cfm?doid=2987538.2987545},
	doi = {10.1145/2987538.2987545},
	abstract = {In this paper, we present an ensemble method for job recommendation to ACM RecSys Challenge 2016. Given a user, the goal of a job recommendation system is to predict those job postings that are likely to be relevant to the user1.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the {Recommender} {Systems} {Challenge} on - {RecSys} {Challenge} '16},
	publisher = {ACM Press},
	author = {Zhang, Chenrui and Cheng, Xueqi},
	year = {2016},
	pages = {1--4},
	file = {Zhang and Cheng - 2016 - An ensemble method for job recommender systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RXS7HG3F\\Zhang and Cheng - 2016 - An ensemble method for job recommender systems.pdf:application/pdf;Zhang and Cheng - 2016 - An ensemble method for job recommender systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TDPNAZ6V\\Zhang and Cheng - 2016 - An ensemble method for job recommender systems.pdf:application/pdf},
}

@article{nayebzadehInvestigationSocialNetwork,
	title = {An {Investigation} on {Social} {Network} {Recommender} {Systems} and {Collaborative} {Filtering} {Techniques}},
	abstract = {Nowadays, with the remarkable expansion of the information through the internet, users prefer to receive the exact information that they need through some suggestions from their friends or profiles to save their time and money. Recommend systems based on different algorithms as one of the basic ways to reach this goal through the internet have been proposed but each of them has their own advantages and disadvantages. In this study, we have selected and implemented two approaches which are Collaborative Filtering (CF) and Social Network Recommendations System (SNRS). Based on some limitations to finding a dataset which covers friendship, rating and item categories we generated it for 10 categories, 10 items, and 100 users and compared two approaches. We used Mean Absolute Error (MAE) and accuracy to compare the result of two mentioned approaches and found that the SNRS method as it is claimed to be improved version of CF works more efficiency.},
	language = {en},
	author = {Nayebzadeh, Maryam and Moazzam, Akbar and Saba, Amir Mohammad and Abdolrahimpour, Hadi and Shahab, Elham},
	pages = {6},
	file = {Nayebzadeh et al. - An Investigation on Social Network Recommender Sys.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WP8QZSXP\\Nayebzadeh et al. - An Investigation on Social Network Recommender Sys.pdf:application/pdf;Nayebzadeh et al. - An Investigation on Social Network Recommender Sys.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Q4VSI7W5\\Nayebzadeh et al. - An Investigation on Social Network Recommender Sys.pdf:application/pdf},
}

@inproceedings{chenCoAttentiveMultiTaskLearning2019,
	address = {Macao, China},
	title = {Co-{Attentive} {Multi}-{Task} {Learning} for {Explainable} {Recommendation}},
	isbn = {978-0-9992411-4-1},
	url = {https://www.ijcai.org/proceedings/2019/296},
	doi = {10.24963/ijcai.2019/296},
	abstract = {Despite widespread adoption, recommender systems remain mostly black boxes. Recently, providing explanations about why items are recommended has attracted increasing attention due to its capability to enhance user trust and satisfaction. In this paper, we propose a co-attentive multitask learning model for explainable recommendation. Our model improves both prediction accuracy and explainability of recommendation by fully exploiting the correlations between the recommendation task and the explanation task. In particular, we design an encoder-selector-decoder architecture inspired by human’s information-processing model in cognitive psychology. We also propose a hierarchical co-attentive selector to effectively model the cross knowledge transferred for both tasks. Our model not only enhances prediction accuracy of the recommendation task, but also generates linguistic explanations that are ﬂuent, useful, and highly personalized. Experiments on three public datasets demonstrate the effectiveness of our model.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Chen, Zhongxia and Wang, Xiting and Xie, Xing and Wu, Tong and Bu, Guoqing and Wang, Yining and Chen, Enhong},
	month = aug,
	year = {2019},
	pages = {2137--2143},
	file = {Chen et al. - 2019 - Co-Attentive Multi-Task Learning for Explainable R.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VXWULM2P\\Chen et al. - 2019 - Co-Attentive Multi-Task Learning for Explainable R.pdf:application/pdf;Chen et al. - 2019 - Co-Attentive Multi-Task Learning for Explainable R.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BPPXFUNA\\Chen et al. - 2019 - Co-Attentive Multi-Task Learning for Explainable R.pdf:application/pdf},
}

@article{yangCombiningContentbasedCollaborative2017,
	title = {Combining content-based and collaborative filtering for job recommendation system: {A} cost-sensitive {Statistical} {Relational} {Learning} approach},
	volume = {136},
	issn = {09507051},
	shorttitle = {Combining content-based and collaborative filtering for job recommendation system},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095070511730374X},
	doi = {10.1016/j.knosys.2017.08.017},
	language = {en},
	urldate = {2021-08-14},
	journal = {Knowledge-Based Systems},
	author = {Yang, Shuo and Korayem, Mohammed and AlJadda, Khalifeh and Grainger, Trey and Natarajan, Sriraam},
	month = nov,
	year = {2017},
	pages = {37--45},
	file = {Yang et al. - 2017 - Combining content-based and collaborative filterin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8JU8W2WS\\Yang et al. - 2017 - Combining content-based and collaborative filterin.pdf:application/pdf;Yang et al. - 2017 - Combining content-based and collaborative filterin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z5SW9EEZ\\Yang et al. - 2017 - Combining content-based and collaborative filterin.pdf:application/pdf},
}

@inproceedings{iovineConversationalAgentsRecommender2020,
	address = {Virtual Event Brazil},
	title = {Conversational {Agents} for {Recommender} {Systems}},
	isbn = {978-1-4503-7583-2},
	url = {https://dl.acm.org/doi/10.1145/3383313.3411453},
	doi = {10.1145/3383313.3411453},
	abstract = {In my Ph.D. work, my objective is to improve the state of the art in Conversational Recommender Systems, by proposing a model that closely follows the process that people enact when searching for products and services. Rich user profiles are elicited using natural language dialogue. Item descriptions will be extracted from a combination of structured and unstructured data such as user reviews. Natural language explanations will ensure that users can quickly understand the reasoning behind the recommendations. Interactive explanation will then allow them to further compare several alternatives. This extended abstract presents the motivations of my work, it details the research plan, and the research questions. Finally, it shows some preliminary results, and outlines the next steps for my Ph.D. program.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Fourteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Iovine, Andrea},
	month = sep,
	year = {2020},
	pages = {758--763},
	file = {Iovine - 2020 - Conversational Agents for Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZJH6REWM\\Iovine - 2020 - Conversational Agents for Recommender Systems.pdf:application/pdf;Iovine - 2020 - Conversational Agents for Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V3EQU2ZL\\Iovine - 2020 - Conversational Agents for Recommender Systems.pdf:application/pdf},
}

@inproceedings{sunConversationalRecommenderSystem2018,
	address = {Ann Arbor MI USA},
	title = {Conversational {Recommender} {System}},
	isbn = {978-1-4503-5657-2},
	url = {https://dl.acm.org/doi/10.1145/3209978.3210002},
	doi = {10.1145/3209978.3210002},
	abstract = {A personalized conversational sales agent could have much commercial potential. E-commerce companies such as Amazon, eBay, JD, Alibaba etc. are piloting such kind of agents with their users. However, the research on this topic is very limited and existing solutions are either based on single round adhoc search engine or traditional multi round dialog system. They usually only utilize user inputs in the current session, ignoring users’ long term preferences. On the other hand, it is well known that sales conversion rate can be greatly improved based on recommender systems, which learn user preferences based on past purchasing behavior and optimize business oriented metrics such as conversion rate or expected revenue. In this work, we propose to integrate research in dialog systems and recommender systems into a novel and unified deep reinforcement learning framework to build a personalized conversational recommendation agent that optimizes a per session based utility function.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {The 41st {International} {ACM} {SIGIR} {Conference} on {Research} \& {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Sun, Yueming and Zhang, Yi},
	month = jun,
	year = {2018},
	pages = {235--244},
	file = {Sun and Zhang - 2018 - Conversational Recommender System.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N5A3IVZH\\Sun and Zhang - 2018 - Conversational Recommender System.pdf:application/pdf;Sun and Zhang - 2018 - Conversational Recommender System.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DAX3F84J\\Sun and Zhang - 2018 - Conversational Recommender System.pdf:application/pdf},
}

@article{iovineConversationalRecommenderSystems2020,
	title = {Conversational {Recommender} {Systems} and natural language:},
	volume = {131},
	issn = {01679236},
	shorttitle = {Conversational {Recommender} {Systems} and natural language},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167923620300051},
	doi = {10.1016/j.dss.2020.113250},
	abstract = {Digital Assistants (DA) such as Amazon Alexa, Siri, or Google Assistant are now gaining great diffusion, since they allow users to execute a wide range of actions through messages in natural language. Even though DAs are able to complete tasks such as sending texts, making phone calls, or playing songs, they do not yet implement recommendation facilities. In this paper, we investigate the combination of Digital Assistants and Conversational Recommender Systems (CoRSs) by designing and implementing a framework named ConveRSE (Conversational Recommender System framEwork), for building chatbots that can recommend items from different domains and interact with the user through natural language. Since a CoRS architecture is generally composed of different elements, we performed an in-vitro experiment with two synthetic datasets, to investigate the impact that each component has on the CoRS in terms of recommendation accuracy. Additionally, an in-vivo experiment was carried out to understand how natural language influences both the cost of interaction and recommendation accuracy of a CoRS. Experimental results have revealed the most critical components in a CoRS architecture, especially in cold-start situations, and the main issues of the natural-language-based interaction. All the dialogues have been collected in a public available dataset.},
	language = {en},
	urldate = {2021-08-14},
	journal = {Decision Support Systems},
	author = {Iovine, Andrea and Narducci, Fedelucio and Semeraro, Giovanni},
	month = apr,
	year = {2020},
	pages = {113250},
	file = {Iovine et al. - 2020 - Conversational Recommender Systems and natural lan.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZHS8JDFI\\Iovine et al. - 2020 - Conversational Recommender Systems and natural lan.pdf:application/pdf;Iovine et al. - 2020 - Conversational Recommender Systems and natural lan.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\B24C67HY\\Iovine et al. - 2020 - Conversational Recommender Systems and natural lan.pdf:application/pdf},
}

@article{renCRSALConversationalRecommender2020,
	title = {{CRSAL}: {Conversational} {Recommender} {Systems} with {Adversarial} {Learning}},
	volume = {38},
	issn = {1046-8188, 1558-2868},
	shorttitle = {{CRSAL}},
	url = {https://dl.acm.org/doi/10.1145/3394592},
	doi = {10.1145/3394592},
	abstract = {Recommender systems have been attracting much attention from both academia and industry because of their ability to capture user interests and generate personalized item recommendations. As the life pace in contemporary society speeds up, traditional recommender systems are inevitably limited by their disconnected interaction styles and low adaptivity to users’ evolving demands. Consequently, conversational recommender systems emerge as a prospective research area, where an intelligent dialogue agent is integrated with a recommender system. Conversational recommender systems possess the ability to accurately understand end-users’ intent or request and generate human-like dialogue responses when performing recommendations. However, existing conversational recommender systems only allow the systems to ask users for more preference information, while users’ further questions and concerns about the recommended items (e.g., enquiring the location of a recommended restaurant) can hardly be addressed. Though the recent task-oriented dialogue systems allow for two-way communications, they are not easy to train because of their high dependence on human guidance in terms of user intent recognition and system response generation. Hence, to enable two-way human-machine communications and tackle the challenges brought by manually crafted rules, we propose Conversational Recommender System with Adversarial Learning (CRSAL), a novel end-to-end system to tackle the task of conversational recommendation. In CRSAL, we innovatively design a fully statistical dialogue state tracker coupled with a neural policy agent to precisely capture each user’s intent from limited dialogue data and generate conversational recommendation actions. We further develop an adversarial Actor-Critic reinforcement learning approach to adaptively refine the quality of generated system actions, thus ensuring coherent human-like dialogue responses. Extensive experiments on two benchmark datasets fully demonstrate the superiority of CRSAL on conversational recommendation tasks.},
	language = {en},
	number = {4},
	urldate = {2021-08-14},
	journal = {ACM Transactions on Information Systems},
	author = {Ren, Xuhui and Yin, Hongzhi and Chen, Tong and Wang, Hao and Hung, Nguyen Quoc Viet and Huang, Zi and Zhang, Xiangliang},
	month = oct,
	year = {2020},
	pages = {1--40},
	file = {Ren et al. - 2020 - CRSAL Conversational Recommender Systems with Adv.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ESBZB6K7\\Ren et al. - 2020 - CRSAL Conversational Recommender Systems with Adv.pdf:application/pdf;Ren et al. - 2020 - CRSAL Conversational Recommender Systems with Adv.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6FGXMQVN\\Ren et al. - 2020 - CRSAL Conversational Recommender Systems with Adv.pdf:application/pdf},
}

@inproceedings{luoDeepCritiquingVAEbased2020,
	address = {Virtual Event China},
	title = {Deep {Critiquing} for {VAE}-based {Recommender} {Systems}},
	isbn = {978-1-4503-8016-4},
	url = {https://dl.acm.org/doi/10.1145/3397271.3401091},
	doi = {10.1145/3397271.3401091},
	abstract = {Providing explanations for recommended items not only allows users to understand the reason for receiving recommendations but also provides users with an opportunity to re�ne recommendations by critiquing undesired parts of the explanation. While much research focuses on improving the explanation of recommendations, less e�ort has focused on interactive recommendation by allowing a user to critique explanations. Aside from traditional constraint- and utility-based critiquing systems, the only end-to-end deep learning based critiquing approach in the literature so far, CE-VNCF, su�ers from unstable and ine�cient training performance. In this paper, we propose a Variational Autoencoder (VAE) based critiquing system to mitigate these issues and improve overall performance. The proposed model generates keyphrase-based explanations of recommendations and allows users to critique the generated explanations to re�ne their personalized recommendations. Our experiments show promising results: (1) The proposed model is competitive in terms of general performance in comparison to state-of-the-art recommenders, despite having an augmented loss function to support explanation and critiquing. (2) The proposed model can generate high-quality explanations compared to user or item keyphrase popularity baselines. (3) The proposed model is more e�ective in re�ning recommendations based on critiquing than CE-VNCF, where the rank of critiquing-a�ected items drops while general recommendation performance remains stable. In summary, this paper presents a signi�cantly improved method for multi-step deep critiquing based recommender systems based on the VAE framework.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Luo, Kai and Yang, Hojin and Wu, Ga and Sanner, Scott},
	month = jul,
	year = {2020},
	pages = {1269--1278},
	file = {Luo et al. - 2020 - Deep Critiquing for VAE-based Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\72KVGBAA\\Luo et al. - 2020 - Deep Critiquing for VAE-based Recommender Systems.pdf:application/pdf;Luo et al. - 2020 - Deep Critiquing for VAE-based Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KLB5MWK7\\Luo et al. - 2020 - Deep Critiquing for VAE-based Recommender Systems.pdf:application/pdf},
}

@article{zhangDeepLearningBased2019,
	title = {Deep {Learning} based {Recommender} {System}: {A} {Survey} and {New} {Perspectives}},
	volume = {52},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Deep {Learning} based {Recommender} {System}},
	url = {http://arxiv.org/abs/1707.07435},
	doi = {10.1145/3285029},
	abstract = {With the ever-growing volume of online information, recommender systems have been an effective strategy to overcome such information overload. The utility of recommender systems cannot be overstated, given its widespread adoption in many web applications, along with its potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. Evidently, the field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning based recommender systems. More concretely, we provide and devise a taxonomy of deep learning based recommendation models, along with providing a comprehensive summary of the state-of-the-art. Finally, we expand on current trends and provide new perspectives pertaining to this new exciting development of the field.},
	language = {en},
	number = {1},
	urldate = {2021-08-14},
	journal = {ACM Computing Surveys},
	author = {Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
	month = feb,
	year = {2019},
	note = {arXiv: 1707.07435},
	keywords = {Computer Science - Information Retrieval},
	pages = {1--38},
	file = {Zhang et al. - 2019 - Deep Learning based Recommender System A Survey a.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KXS8VN9A\\Zhang et al. - 2019 - Deep Learning based Recommender System A Survey a.pdf:application/pdf;Zhang et al. - 2019 - Deep Learning based Recommender System A Survey a.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L53KDQNK\\Zhang et al. - 2019 - Deep Learning based Recommender System A Survey a.pdf:application/pdf;Zhang et al. - 2019 - Deep Learning based Recommender System A Survey a.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\R4FGVIJJ\\Zhang et al. - 2019 - Deep Learning based Recommender System A Survey a.pdf:application/pdf;Zhang et al. - 2019 - Deep Learning based Recommender System A Survey a.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FINTIXNI\\Zhang et al. - 2019 - Deep Learning based Recommender System A Survey a.pdf:application/pdf},
}

@inproceedings{longDeepNaturalLanguage2020,
	address = {Virtual Event China},
	title = {Deep {Natural} {Language} {Processing} for {Search} and {Recommendation}},
	isbn = {978-1-4503-8016-4},
	url = {https://dl.acm.org/doi/10.1145/3397271.3401465},
	doi = {10.1145/3397271.3401465},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Long, Bo and Ye, Jieping and Li, Zang and Gao, Huiji and Jha, Sandeep Kumar},
	month = jul,
	year = {2020},
	pages = {2461--2463},
	file = {Long et al. - 2020 - Deep Natural Language Processing for Search and Re.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IN66BJJS\\Long et al. - 2020 - Deep Natural Language Processing for Search and Re.pdf:application/pdf;Long et al. - 2020 - Deep Natural Language Processing for Search and Re.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V7ZTLHLI\\Long et al. - 2020 - Deep Natural Language Processing for Search and Re.pdf:application/pdf},
}

@inproceedings{liDeepTimeAwareItem2020,
	address = {Virtual Event Ireland},
	title = {Deep {Time}-{Aware} {Item} {Evolution} {Network} for {Click}-{Through} {Rate} {Prediction}},
	isbn = {978-1-4503-6859-9},
	url = {https://dl.acm.org/doi/10.1145/3340531.3411952},
	doi = {10.1145/3340531.3411952},
	abstract = {For better user satisfaction and business effectiveness, Click-Through Rate (CTR) prediction is one of the most important tasks in Ecommerce. It is often the case that users’ interests different from their past routines may emerge or impressions such as promotional items may burst in a very short period. In essence, such changes relate to item evolution problem, which has not been investigated by previous studies. The state-of-the-art methods in the sequential recommendation, which use simple user behaviors, are incapable of modeling these changes sufficiently. It is because, in the user behaviors, outdated interests may exist and the popularity of an item over time is not well represented. To address these limitations, we introduce time-aware item behaviors for addressing the recommendation of emerging preference. The time-aware item behavior for an item is a set of users who interact with this item with timestamps. The rich interaction information of users for an item may help to model its evolution. In this work, we propose a CTR prediction model TIEN based on the time-aware item behavior. In TIEN, by leveraging the interaction time intervals, information of similar users in a short time interval helps identify the emerging user interest of the target user. By using the sequential time intervals, the item’s popularity over time can be captured in evolutionary item dynamics. Noisy users who interact with items accidentally are further eliminated thus learning robust personalized item dynamics. To the best of our knowledge, this is the first study to the item evolution problem for E-commerce CTR prediction. We conduct extensive experiments on five real-world CTR prediction datasets. The results show that the TIEN model consistently achieves remarkable improvements to the state-of-the-art methods.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {ACM},
	author = {Li, Xiang and Wang, Chao and Tong, Bin and Tan, Jiwei and Zeng, Xiaoyi and Zhuang, Tao},
	month = oct,
	year = {2020},
	pages = {785--794},
	file = {Li et al. - 2020 - Deep Time-Aware Item Evolution Network for Click-T.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VABTSTDX\\Li et al. - 2020 - Deep Time-Aware Item Evolution Network for Click-T.pdf:application/pdf;Li et al. - 2020 - Deep Time-Aware Item Evolution Network for Click-T.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2B2K8ZH3\\Li et al. - 2020 - Deep Time-Aware Item Evolution Network for Click-T.pdf:application/pdf},
}

@inproceedings{leiEstimationActionReflectionDeepInteraction2020,
	address = {Houston TX USA},
	title = {Estimation-{Action}-{Reflection}: {Towards} {Deep} {Interaction} {Between} {Conversational} and {Recommender} {Systems}},
	isbn = {978-1-4503-6822-3},
	shorttitle = {Estimation-{Action}-{Reflection}},
	url = {https://dl.acm.org/doi/10.1145/3336191.3371769},
	doi = {10.1145/3336191.3371769},
	abstract = {Recommender systems are embracing conversational technologies to obtain user preferences dynamically, and to overcome inherent limitations of their static models. A successful Conversational Recommender System (CRS) requires proper handling of interactions between conversation and recommendation. We argue that three fundamental problems need to be solved: 1) what questions to ask regarding item attributes, 2) when to recommend items, and 3) how to adapt to the users’ online feedback. To the best of our knowledge, there lacks a unified framework that addresses these problems.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Lei, Wenqiang and He, Xiangnan and Miao, Yisong and Wu, Qingyun and Hong, Richang and Kan, Min-Yen and Chua, Tat-Seng},
	month = jan,
	year = {2020},
	pages = {304--312},
	file = {Lei et al. - 2020 - Estimation-Action-Reflection Towards Deep Interac.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RJLDR56E\\Lei et al. - 2020 - Estimation-Action-Reflection Towards Deep Interac.pdf:application/pdf;Lei et al. - 2020 - Estimation-Action-Reflection Towards Deep Interac.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LBQX2C8B\\Lei et al. - 2020 - Estimation-Action-Reflection Towards Deep Interac.pdf:application/pdf},
}

@article{zhangExplainableRecommendationSurvey2020,
	title = {Explainable {Recommendation}: {A} {Survey} and {New} {Perspectives}},
	volume = {14},
	issn = {1554-0669, 1554-0677},
	shorttitle = {Explainable {Recommendation}},
	url = {http://www.nowpublishers.com/article/Details/INR-066},
	doi = {10.1561/1500000066},
	language = {en},
	number = {1},
	urldate = {2021-08-14},
	journal = {Foundations and Trends® in Information Retrieval},
	author = {Zhang, Yongfeng and Chen, Xu},
	year = {2020},
	pages = {1--101},
	file = {Explainable Recommendation A Survey and New Perspectives.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Notes\\Explainable Recommendation A Survey and New Perspectives.md:text/plain;Zhang and Chen - 2020 - Explainable Recommendation A Survey and New Persp.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CTZ8Q483\\Zhang and Chen - 2020 - Explainable Recommendation A Survey and New Persp.pdf:application/pdf;Zhang and Chen - 2020 - Explainable Recommendation A Survey and New Persp.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IMWKQ5T4\\Zhang and Chen - 2020 - Explainable Recommendation A Survey and New Persp.pdf:application/pdf;Zhang and Chen - 2020 - Explainable Recommendation A Survey and New Persp.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NJ4HKEZY\\Zhang and Chen - 2020 - Explainable Recommendation A Survey and New Persp.pdf:application/pdf},
}

@inproceedings{gutierrezExplainingExploringJob2019,
	address = {Copenhagen Denmark},
	title = {Explaining and exploring job recommendations: a user-driven approach for interacting with knowledge-based job recommender systems},
	isbn = {978-1-4503-6243-6},
	shorttitle = {Explaining and exploring job recommendations},
	url = {https://dl.acm.org/doi/10.1145/3298689.3347001},
	doi = {10.1145/3298689.3347001},
	abstract = {The dynamics of the labor market and the tasks with which jobs are being composed are continuously evolving. Job mobility is not evident, and providing efective recommendations in this context has also been found to be particularly challenging. In this paper, we present Labor Market Explorer, an interactive dashboard that enables job seekers to explore the labor market in a personalized way based on their skills and competences. Through a user-centered design process involving job seekers and job mediators, we developed this dashboard to enable job seekers to explore job recommendations and their required competencies, as well as how these competencies map to their profle. Evaluation results indicate the dashboard empowers job seekers to explore, understand, and fnd relevant vacancies, mostly independent of their background and age.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the 13th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Gutiérrez, Francisco and Charleer, Sven and De Croon, Robin and Htun, Nyi Nyi and Goetschalckx, Gerd and Verbert, Katrien},
	month = sep,
	year = {2019},
	pages = {60--68},
	file = {Gutiérrez et al. - 2019 - Explaining and exploring job recommendations a us.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I8IFMCEK\\Gutiérrez et al. - 2019 - Explaining and exploring job recommendations a us.pdf:application/pdf;Gutiérrez et al. - 2019 - Explaining and exploring job recommendations a us.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SRJ2Z6DN\\Gutiérrez et al. - 2019 - Explaining and exploring job recommendations a us.pdf:application/pdf},
}

@inproceedings{zhouImprovingConversationalRecommender2020,
	address = {Virtual Event CA USA},
	title = {Improving {Conversational} {Recommender} {Systems} via {Knowledge} {Graph} based {Semantic} {Fusion}},
	isbn = {978-1-4503-7998-4},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403143},
	doi = {10.1145/3394486.3403143},
	abstract = {Conversational recommender systems (CRS) aim to recommend high-quality items to users through interactive conversations. Although several efforts have been made for CRS, two major issues still remain to be solved. First, the conversation data itself lacks of sufficient contextual information for accurately understanding users’ preference. Second, there is a semantic gap between natural language expression and item-level user preference.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Zhou, Kun and Zhao, Wayne Xin and Bian, Shuqing and Zhou, Yuanhang and Wen, Ji-Rong and Yu, Jingsong},
	month = aug,
	year = {2020},
	pages = {1006--1014},
	file = {Zhou et al. - 2020 - Improving Conversational Recommender Systems via K.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EAXEWRST\\Zhou et al. - 2020 - Improving Conversational Recommender Systems via K.pdf:application/pdf;Zhou et al. - 2020 - Improving Conversational Recommender Systems via K.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KVGL5J2T\\Zhou et al. - 2020 - Improving Conversational Recommender Systems via K.pdf:application/pdf},
}

@inproceedings{shanthakumarItemBasedRecommendation2021,
	address = {Virtual Event USA},
	title = {Item based recommendation using matrix-factorization-like embeddings from deep networks},
	isbn = {978-1-4503-8068-3},
	url = {https://dl.acm.org/doi/10.1145/3409334.3452041},
	doi = {10.1145/3409334.3452041},
	abstract = {In this paper we describe a method for computing item based recommendations using matrix-factorization-like embeddings of the items computed using a neural network. Matrix factorizations (MF) compute near optimal item embeddings by minimizing a loss that measures the discrepancy between the predicted and known values of a sparse user-item rating matrix. Though useful for recommendation tasks, they are computationally intensive and hard to compute for large sets of users and items. Hence there is need to compute MF-like embeddings using other less computationally intensive methods, which can be substituted for the actual ones. In this work we explore the possibility of doing the same using a deep neural network (DNN). Our network is trained to learn matrix-factorizationlike embeddings from easy to compute natural language processing (NLP) based semantic embeddings. The resulting MF-like embeddings are used to compute recommendations using an anonymized user product engagement dataset from the online retail company Overstock.com. We present the results of using our embeddings for computing recommendations with the Overstock.com production dataset consisting of ∼3.5 million items and ∼6 million users. Recommendations from Overstock.com’s own recommendation system is compared against those obtained by using our MF-like embeddings, by comparing the results from both to the ground truth, which in our case is actual user co-clicks data. Our results show that it is possible to use DNNs for efficiently computing MFlike embeddings which can then be used in conjunction with the NLP based embeddings to improve the recommendations obtained from the NLP based embeddings.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the 2021 {ACM} {Southeast} {Conference}},
	publisher = {ACM},
	author = {Shanthakumar, Vaidyanath Areyur and Barnett, Clark and Warnick, Keith and Sudyanti, Putu Ayu and Gerbuz, Vitalii and Mukherjee, Tathagata},
	month = apr,
	year = {2021},
	pages = {71--78},
	file = {Shanthakumar et al. - 2021 - Item based recommendation using matrix-factorizati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I6JR2H5B\\Shanthakumar et al. - 2021 - Item based recommendation using matrix-factorizati.pdf:application/pdf;Shanthakumar et al. - 2021 - Item based recommendation using matrix-factorizati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7ENALVCL\\Shanthakumar et al. - 2021 - Item based recommendation using matrix-factorizati.pdf:application/pdf},
}

@inproceedings{sidanaKASANDRLargeScaleDataset2017,
	address = {Shinjuku Tokyo Japan},
	title = {{KASANDR}: {A} {Large}-{Scale} {Dataset} with {Implicit} {Feedback} for {Recommendation}},
	isbn = {978-1-4503-5022-8},
	shorttitle = {{KASANDR}},
	url = {https://dl.acm.org/doi/10.1145/3077136.3080713},
	doi = {10.1145/3077136.3080713},
	abstract = {In this paper, we describe a novel, publicly available collection for recommendation systems that records the behavior of customers of the European leader in eCommerce advertising, Kelkoo¹, during one month. This dataset gathers implicit feedback, in form of clicks, of users that have interacted with over 56 million offers displayed by Kelkoo, along with a rich set of contextual features regarding both customers and offers. In conjunction with a detailed description of the dataset, we show the performance of six stateof-the-art recommender models and raise some questions on how to encompass the existing contextual information in the system.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the 40th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Sidana, Sumit and Laclau, Charlotte and Amini, Massih R. and Vandelle, Gilles and Bois-Crettez, André},
	month = aug,
	year = {2017},
	pages = {1245--1248},
	file = {Sidana et al. - 2017 - KASANDR A Large-Scale Dataset with Implicit Feedb.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8SQNLRHH\\Sidana et al. - 2017 - KASANDR A Large-Scale Dataset with Implicit Feedb.pdf:application/pdf;Sidana et al. - 2017 - KASANDR A Large-Scale Dataset with Implicit Feedb.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZIK3278Y\\Sidana et al. - 2017 - KASANDR A Large-Scale Dataset with Implicit Feedb.pdf:application/pdf},
}

@article{kabburDISSERTATIONSUBMITTEDFACULTY,
	title = {A {DISSERTATION} {SUBMITTED} {TO} {THE} {FACULTY} {OF} {THE} {GRADUATE} {SCHOOL} {OF} {THE} {UNIVERSITY} {OF} {MINNESOTA}},
	language = {en},
	author = {Kabbur, Santosh},
	pages = {100},
	file = {Kabbur - A DISSERTATION SUBMITTED TO THE FACULTY OF THE GRA.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YCH7EXBF\\Kabbur - A DISSERTATION SUBMITTED TO THE FACULTY OF THE GRA.pdf:application/pdf;Kabbur - A DISSERTATION SUBMITTED TO THE FACULTY OF THE GRA.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9BA3QGI2\\Kabbur - A DISSERTATION SUBMITTED TO THE FACULTY OF THE GRA.pdf:application/pdf},
}

@inproceedings{erdenizMatrixFactorizationBased2019,
	address = {Limassol Cyprus},
	title = {Matrix factorization based heuristics for constraint-based recommenders},
	isbn = {978-1-4503-5933-7},
	url = {https://dl.acm.org/doi/10.1145/3297280.3297441},
	doi = {10.1145/3297280.3297441},
	abstract = {The main challenges for recommender systems are: producing high quality recommendations and performing many real-time recommendations per second for millions of customers and products. This paper addresses both challenges in the context of constraintbased recommenders where users specify their requirements and the system recommends a solution. We propose a novel approach to determine value ordering heuristics on the basis of matrix factorization. As far as we are aware, no researches exist in constraint-based recommendation domain which exploit matrix factorization techniques. The main idea of our approach consists in the prediction of value ordering heuristics based on historical transactions which can either represent past customer purchases or requirements. Thereby, value ordering heuristics are computed which are specific to each user’s requirements. A series of experiments on real-world datasets for calculating constraint-based recommendations has shown that our approach outperforms compared methods in terms of runtime efficiency and prediction quality.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Erdeniz, Seda Polat and Felfernig, Alexander and Samer, Ralph and Atas, Muesluem},
	month = apr,
	year = {2019},
	pages = {1655--1662},
	file = {Erdeniz et al. - 2019 - Matrix factorization based heuristics for constrai.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ETI3HZNV\\Erdeniz et al. - 2019 - Matrix factorization based heuristics for constrai.pdf:application/pdf;Erdeniz et al. - 2019 - Matrix factorization based heuristics for constrai.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9VL42R7Q\\Erdeniz et al. - 2019 - Matrix factorization based heuristics for constrai.pdf:application/pdf},
}

@inproceedings{argyriouMicrosoftRecommendersBest2020,
	address = {Taipei Taiwan},
	title = {Microsoft {Recommenders}: {Best} {Practices} for {Production}-{Ready} {Recommendation} {Systems}},
	isbn = {978-1-4503-7024-0},
	shorttitle = {Microsoft {Recommenders}},
	url = {https://dl.acm.org/doi/10.1145/3366424.3382692},
	doi = {10.1145/3366424.3382692},
	abstract = {Recommendation algorithms have been widely applied in various contemporary business areas, however the process of implementing them in production systems is complex and has to address significant challenges. We present Microsoft Recommenders, an opensource Github repository for helping researchers, developers and non-experts in general to prototype, experiment with and bring to production both classic and state-of-the-art recommendation algorithms. A focus of this repository is on best practices in development of recommendation systems. We have also incorporated learnings from our experience with recommendation systems in production, in order to enhance ease of use; speed of implementation and deployment; scalability and performance.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2020},
	publisher = {ACM},
	author = {Argyriou, Andreas and González-Fierro, Miguel and Zhang, Le},
	month = apr,
	year = {2020},
	pages = {50--51},
	file = {Argyriou et al. - 2020 - Microsoft Recommenders Best Practices for Product.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IAVSI8SZ\\Argyriou et al. - 2020 - Microsoft Recommenders Best Practices for Product.pdf:application/pdf;Argyriou et al. - 2020 - Microsoft Recommenders Best Practices for Product.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9WTKCECY\\Argyriou et al. - 2020 - Microsoft Recommenders Best Practices for Product.pdf:application/pdf},
}

@article{institutodepesquisastecnologicasdoestadodespcidadeuniversitariabutantasaopaulobrasil.ModularArchitectureRecommender2016,
	title = {Modular {Architecture} for {Recommender} {Systems} {Applied} in a {Brazilian} e-{Commerce}},
	volume = {11},
	issn = {1796217X},
	url = {http://www.jsoftware.us/index.php?m=content&c=index&a=show&catid=172&id=2677},
	doi = {10.17706/jsw.11.9.912-923},
	abstract = {Over the last decade, recommender systems have been widely applied by major e-commerce websites for personalized user experience. However, few efforts have been focused so far on recommender systems architecture. In addition, Big Data technologies present opportunities to create unprecedented business advantage and better service delivery at low cost.},
	language = {en},
	number = {9},
	urldate = {2021-08-14},
	journal = {Journal of Software},
	author = {{Instituto de Pesquisas Tecnológicas do Estado de SP, Cidade Universitária, Butantã, São Paulo, Brasil.} and Vidotti Prando, Allan and Nice Alves de Souza, Solange},
	month = sep,
	year = {2016},
	pages = {912--923},
	file = {Instituto de Pesquisas Tecnológicas do Estado de SP, Cidade Universitária, Butantã, São Paulo, Brasil. et al. - 2016 - Modular Architecture for Recommender Systems Appli.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7KHHWKBE\\Instituto de Pesquisas Tecnológicas do Estado de SP, Cidade Universitária, Butantã, São Paulo, Brasil. et al. - 2016 - Modular Architecture for Recommender Systems Appli.pdf:application/pdf;Instituto de Pesquisas Tecnológicas do Estado de SP, Cidade Universitária, Butantã, São Paulo, Brasil. et al. - 2016 - Modular Architecture for Recommender Systems Appli.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GZHC7P5Y\\Instituto de Pesquisas Tecnológicas do Estado de SP, Cidade Universitária, Butantã, São Paulo, Brasil. et al. - 2016 - Modular Architecture for Recommender Systems Appli.pdf:application/pdf},
}

@article{guoOnlineProductFeature2020,
	title = {Online {Product} {Feature} {Recommendations} with {Interpretable} {Machine} {Learning}},
	abstract = {Product feature recommendations are critical for online customers to purchase the right products based on the right features. For a customer, selecting the product that has the best trade-off between price and functionality is a time-consuming step in an online shopping experience, and customers can be overwhelmed by the available choices. However, determining the set of product features that most differentiate a particular product is still an open question in online recommender systems. In this paper, we focus on using interpretable machine learning methods to tackle this problem. First, we identify this unique product feature recommendation problem from a business perspective on a major US e-commerce site. Second, we formulate the problem into a price-driven supervised learning problem to discover the product features that could best explain the price of a product in a given product category. We build machine learning models with a model-agnostic method Shapley Values to understand the importance of each feature, rank and recommend the most essential features. Third, we leverage human experts to evaluate its relevancy. The results show that our method is superior to a strong baseline method based on customer behavior and significantly boosts the coverage by 45\%. Finally, our proposed method shows comparable conversion rate against the baseline in online A/B tests.},
	language = {en},
	author = {Guo, Mingming and Yan, Nian and Cui, Xiquan and Hughes, Simon and Jadda, Khalifeh Al},
	year = {2020},
	pages = {7},
	file = {Guo et al. - 2020 - Online Product Feature Recommendations with Interp.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H87YAXAL\\Guo et al. - 2020 - Online Product Feature Recommendations with Interp.pdf:application/pdf;Guo et al. - 2020 - Online Product Feature Recommendations with Interp.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7SBPGVN9\\Guo et al. - 2020 - Online Product Feature Recommendations with Interp.pdf:application/pdf;Guo et al. - 2020 - Online Product Feature Recommendations with Interp.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VR38HD64\\Guo et al. - 2020 - Online Product Feature Recommendations with Interp.pdf:application/pdf},
}

@inproceedings{kenthapadiPersonalizedJobRecommendation2017,
	address = {Como Italy},
	title = {Personalized {Job} {Recommendation} {System} at {LinkedIn}: {Practical} {Challenges} and {Lessons} {Learned}},
	isbn = {978-1-4503-4652-8},
	shorttitle = {Personalized {Job} {Recommendation} {System} at {LinkedIn}},
	url = {https://dl.acm.org/doi/10.1145/3109859.3109921},
	doi = {10.1145/3109859.3109921},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Proceedings of the {Eleventh} {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Kenthapadi, Krishnaram and Le, Benjamin and Venkataraman, Ganesh},
	month = aug,
	year = {2017},
	pages = {346--347},
	file = {Kenthapadi et al. - 2017 - Personalized Job Recommendation System at LinkedIn.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UIVKAWBJ\\Kenthapadi et al. - 2017 - Personalized Job Recommendation System at LinkedIn.pdf:application/pdf;Kenthapadi et al. - 2017 - Personalized Job Recommendation System at LinkedIn.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\A48PS8AD\\Kenthapadi et al. - 2017 - Personalized Job Recommendation System at LinkedIn.pdf:application/pdf},
}

@book{chevalierCollaborativeSocialInformation2009,
	title = {Collaborative and {Social} {Information} {Retrieval} and {Access}: {Techniques} for {Improved} {User} {Modeling}},
	isbn = {978-1-60566-306-7 978-1-60566-307-4},
	shorttitle = {Collaborative and {Social} {Information} {Retrieval} and {Access}},
	url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60566-306-7},
	abstract = {The aim of Recommender Systems is to help users to find items that they should appreciate from huge catalogues. In that field, collaborative filtering approaches can be distinguished from content-based ones. The former is based on a set of user ratings on items while the latter uses item content descriptions and user thematic profiles.},
	language = {en},
	urldate = {2021-08-14},
	publisher = {IGI Global},
	editor = {Chevalier, Max and Julien, Christine and Soule-Dupuy, Chantal},
	year = {2009},
	doi = {10.4018/978-1-60566-306-7},
	file = {Chevalier et al. - 2009 - Collaborative and Social Information Retrieval and.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NF32LVF5\\Chevalier et al. - 2009 - Collaborative and Social Information Retrieval and.pdf:application/pdf},
}

@article{trivediCognitiveDeficitsPsychiatric2006,
	title = {Cognitive deficits in psychiatric disorders: {Current} status},
	volume = {48},
	issn = {0019-5545},
	shorttitle = {Cognitive deficits in psychiatric disorders},
	url = {http://www.indianjpsychiatry.org/text.asp?2006/48/1/10/31613},
	doi = {10.4103/0019-5545.31613},
	language = {en},
	number = {1},
	urldate = {2021-08-19},
	journal = {Indian Journal of Psychiatry},
	author = {Trivedi, Jk},
	year = {2006},
	pages = {10},
}

@article{pennDrugsDonWork2012,
	title = {The drugs don’t work? antidepressants and the current and future pharmacological management of depression},
	volume = {2},
	issn = {2045-1253, 2045-1261},
	shorttitle = {The drugs don’t work?},
	url = {http://journals.sagepub.com/doi/10.1177/2045125312445469},
	doi = {10.1177/2045125312445469},
	abstract = {Depression is a potentially life-threatening disorder affecting millions of people across the globe. It is a huge burden to both the individual and society, costing over £9 billion in 2000 alone: the World Health Organisation (WHO) cited it as the third leading cause of global disability in 2004 (first in the developed world), and project it will be the leading cause by 2030. The serendipitous discovery of antidepressants has revolutionized both our understanding and management of depression: however, their efficacy in the treatment of depression has long been debated and recently been brought very much into the public limelight by a controversial publication by Kirsch, in which the role of placebo response in antidepressant efficacy trials is highlighted. Whilst antidepressants offer benefits in both the short and long term, important problems persist such as intolerability, delayed therapeutic onset, limited efficacy in milder depression and the existence of treatment-resistant depression.},
	language = {en},
	number = {5},
	urldate = {2021-08-19},
	journal = {Therapeutic Advances in Psychopharmacology},
	author = {Penn, Elizabeth and Tracy, Derek K.},
	month = oct,
	year = {2012},
	pages = {179--188},
	file = {Penn_Tracy_2012_The drugs don’t work.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S78SKMW2\\Penn_Tracy_2012_The drugs don’t work.pdf:application/pdf},
}

@article{kurianStrategiesEnhanceTherapeutic2009,
	title = {Strategies to enhance the therapeutic efficacy of antidepressants: targeting residual symptoms},
	volume = {9},
	issn = {1473-7175, 1744-8360},
	shorttitle = {Strategies to enhance the therapeutic efficacy of antidepressants},
	url = {http://www.tandfonline.com/doi/full/10.1586/ern.09.53},
	doi = {10.1586/ern.09.53},
	language = {en},
	number = {7},
	urldate = {2021-08-19},
	journal = {Expert Review of Neurotherapeutics},
	author = {Kurian, Benji T and Greer, Tracy L and Trivedi, Madhukar H},
	month = jul,
	year = {2009},
	pages = {975--984},
	file = {Kurian et al_2009_Strategies to enhance the therapeutic efficacy of antidepressants.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NMG5YIJ7\\Kurian et al_2009_Strategies to enhance the therapeutic efficacy of antidepressants.pdf:application/pdf},
}

@inproceedings{zhangExplicitFactorModels2014,
	address = {Gold Coast Queensland Australia},
	title = {Explicit factor models for explainable recommendation based on phrase-level sentiment analysis},
	isbn = {978-1-4503-2257-7},
	url = {https://dl.acm.org/doi/10.1145/2600428.2609579},
	doi = {10.1145/2600428.2609579},
	abstract = {Collaborative Filtering(CF)-based recommendation algorithms, such as Latent Factor Models (LFM), work well in terms of prediction accuracy. However, the latent features make it diﬃculty to explain the recommendation results to the users. Fortunately, with the continuous growth of online user reviews, the information available for training a recommender system is no longer limited to just numerical star ratings or user/item features. By extracting explicit user opinions about various aspects of a product from the reviews, it is possible to learn more details about what aspects a user cares, which further sheds light on the possibility to make explainable recommendations.},
	language = {en},
	urldate = {2021-09-04},
	booktitle = {Proceedings of the 37th international {ACM} {SIGIR} conference on {Research} \& development in information retrieval},
	publisher = {ACM},
	author = {Zhang, Yongfeng and Lai, Guokun and Zhang, Min and Zhang, Yi and Liu, Yiqun and Ma, Shaoping},
	month = jul,
	year = {2014},
	keywords = {xrec},
	pages = {83--92},
	file = {Zhang et al. - 2014 - Explicit factor models for explainable recommendat.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GTHRRJCF\\Zhang et al. - 2014 - Explicit factor models for explainable recommendat.pdf:application/pdf;Zhang et al. - 2014 - Explicit factor models for explainable recommendat.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WXSC46KS\\Zhang et al. - 2014 - Explicit factor models for explainable recommendat.pdf:application/pdf;Zhang et al. - 2014 - Explicit factor models for explainable recommendat.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XY2QZ6GJ\\Zhang et al. - 2014 - Explicit factor models for explainable recommendat.pdf:application/pdf},
}

@article{chenDynamicExplainableRecommendation2019,
	title = {Dynamic {Explainable} {Recommendation} {Based} on {Neural} {Attentive} {Models}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/3768},
	doi = {10.1609/aaai.v33i01.330153},
	abstract = {Providing explanations in a recommender system is getting more and more attention in both industry and research communities. Most existing explainable recommender models regard user preferences as invariant to generate static explanations. However, in real scenarios, a user’s preference is always dynamic, and she may be interested in different product features at different states. The mismatching between the explanation and user preference may degrade costumers’ satisfaction, confidence and trust for the recommender system.},
	language = {en},
	urldate = {2021-09-04},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Xu and Zhang, Yongfeng and Qin, Zheng},
	month = jul,
	year = {2019},
	pages = {53--60},
	file = {Chen et al. - 2019 - Dynamic Explainable Recommendation Based on Neural.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\J8IW4KM5\\Chen et al. - 2019 - Dynamic Explainable Recommendation Based on Neural.pdf:application/pdf;Chen et al. - 2019 - Dynamic Explainable Recommendation Based on Neural.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UTA6A7RQ\\Chen et al. - 2019 - Dynamic Explainable Recommendation Based on Neural.pdf:application/pdf;Chen et al. - 2019 - Dynamic Explainable Recommendation Based on Neural.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YBHPCH5U\\Chen et al. - 2019 - Dynamic Explainable Recommendation Based on Neural.pdf:application/pdf},
}

@inproceedings{luWhyItMultitask2018,
	address = {Vancouver British Columbia Canada},
	title = {Why {I} like it: multi-task learning for recommendation and explanation},
	isbn = {978-1-4503-5901-6},
	shorttitle = {Why {I} like it},
	url = {https://dl.acm.org/doi/10.1145/3240323.3240365},
	doi = {10.1145/3240323.3240365},
	abstract = {We describe a novel, multi-task recommendation model, which jointly learns to perform rating prediction and recommendation explanation by combining matrix factorization, for rating prediction, and adversarial sequence to sequence learning for explanation generation. The result is evaluated using real-world datasets to demonstrate improved rating prediction performance, compared to state-of-the-art alternatives, while producing effective, personalized explanations.},
	language = {en},
	urldate = {2021-09-05},
	booktitle = {Proceedings of the 12th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Lu, Yichao and Dong, Ruihai and Smyth, Barry},
	month = sep,
	year = {2018},
	pages = {4--12},
	file = {Lu et al. - 2018 - Why I like it multi-task learning for recommendat.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RMK2BISI\\Lu et al. - 2018 - Why I like it multi-task learning for recommendat.pdf:application/pdf;Lu et al. - 2018 - Why I like it multi-task learning for recommendat.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SAUTSTU8\\Lu et al. - 2018 - Why I like it multi-task learning for recommendat.pdf:application/pdf},
}

@inproceedings{luCoevolutionaryRecommendationModel2018,
	address = {Lyon, France},
	title = {Coevolutionary {Recommendation} {Model}: {Mutual} {Learning} between {Ratings} and {Reviews}},
	isbn = {978-1-4503-5639-8},
	shorttitle = {Coevolutionary {Recommendation} {Model}},
	url = {http://dl.acm.org/citation.cfm?doid=3178876.3186158},
	doi = {10.1145/3178876.3186158},
	abstract = {Collaborative filtering (CF) is a common recommendation approach that relies on user-item ratings. However, the natural sparsity of user-item rating data can be problematic in many domains and settings, limiting the ability to generate accurate predictions and effective recommendations. Moreover, in some CF approaches latent features are often used to represent users and items, which can lead to a lack of recommendation transparency and explainability. User-generated, customer reviews are now commonplace on many websites, providing users with an opportunity to convey their experiences and opinions of products and services. As such, these reviews have the potential to serve as a useful source of recommendation data, through capturing valuable sentiment information about particular product features. In this paper, we present a novel deep learning recommendation model, which co-learns user and item information from ratings and customer reviews, by optimizing matrix factorization and an attention-based GRU network. Using real-world datasets we show a significant improvement in recommendation performance, compared to a variety of alternatives. Furthermore, the approach is useful when it comes to assigning intuitive meanings to latent features to improve the transparency and explainability of recommender systems.},
	language = {en},
	urldate = {2021-09-05},
	booktitle = {Proceedings of the 2018 {World} {Wide} {Web} {Conference} on {World} {Wide} {Web} - {WWW} '18},
	publisher = {ACM Press},
	author = {Lu, Yichao and Dong, Ruihai and Smyth, Barry},
	year = {2018},
	pages = {773--782},
	file = {Lu et al. - 2018 - Coevolutionary Recommendation Model Mutual Learni.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YHSEWUDL\\Lu et al. - 2018 - Coevolutionary Recommendation Model Mutual Learni.pdf:application/pdf;Lu et al. - 2018 - Coevolutionary Recommendation Model Mutual Learni.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZARXQLQU\\Lu et al. - 2018 - Coevolutionary Recommendation Model Mutual Learni.pdf:application/pdf},
}

@inproceedings{chenNeuralAttentionalRating2018,
	address = {Lyon, France},
	title = {Neural {Attentional} {Rating} {Regression} with {Review}-level {Explanations}},
	isbn = {978-1-4503-5639-8},
	url = {http://dl.acm.org/citation.cfm?doid=3178876.3186070},
	doi = {10.1145/3178876.3186070},
	abstract = {Reviews information is dominant for users to make online purchasing decisions in e-commerces. However, the usefulness of reviews is varied. We argue that less-useful reviews hurt model’s performance, and are also less meaningful for user’s reference. While some existing models utilize reviews for improving the performance of recommender systems, few of them consider the usefulness of reviews for recommendation quality. In this paper, we introduce a novel attention mechanism to explore the usefulness of reviews, and propose a Neural Attentional Regression model with Review-level Explanations (NARRE) for recommendation. Speci cally, NARRE can not only predict precise ratings, but also learn the usefulness of each review simultaneously. Therefore, the highly-useful reviews are obtained which provide review-level explanations to help users make better and faster decisions. Extensive experiments on benchmark datasets of Amazon and Yelp on di erent domains show that the proposed NARRE model consistently outperforms the state-ofthe-art recommendation approaches, including PMF, NMF, SVD++, HFT, and DeepCoNN in terms of rating prediction, by the proposed attention model that takes review usefulness into consideration. Furthermore, the selected reviews are shown to be e ective when taking existing review-usefulness ratings in the system as ground truth. Besides, crowd-sourcing based evaluations reveal that in most cases, NARRE achieves equal or even better performances than system’s usefulness rating method in selecting reviews. And it is exible to o er great help on the dominant cases in real ecommerce scenarios when the ratings on review-usefulness are not available in the system.},
	language = {en},
	urldate = {2021-09-05},
	booktitle = {Proceedings of the 2018 {World} {Wide} {Web} {Conference} on {World} {Wide} {Web} - {WWW} '18},
	publisher = {ACM Press},
	author = {Chen, Chong and Zhang, Min and Liu, Yiqun and Ma, Shaoping},
	year = {2018},
	pages = {1583--1592},
	file = {Chen et al. - 2018 - Neural Attentional Rating Regression with Review-l.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6H858SE4\\Chen et al. - 2018 - Neural Attentional Rating Regression with Review-l.pdf:application/pdf;Chen et al. - 2018 - Neural Attentional Rating Regression with Review-l.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AJPWY6CB\\Chen et al. - 2018 - Neural Attentional Rating Regression with Review-l.pdf:application/pdf},
}

@inproceedings{covingtonDeepNeuralNetworks2016,
	address = {Boston Massachusetts USA},
	title = {Deep {Neural} {Networks} for {YouTube} {Recommendations}},
	isbn = {978-1-4503-4035-9},
	url = {https://dl.acm.org/doi/10.1145/2959100.2959190},
	doi = {10.1145/2959100.2959190},
	abstract = {YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: ﬁrst, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous userfacing impact.},
	language = {en},
	urldate = {2021-09-05},
	booktitle = {Proceedings of the 10th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Covington, Paul and Adams, Jay and Sargin, Emre},
	month = sep,
	year = {2016},
	pages = {191--198},
	file = {Covington et al. - 2016 - Deep Neural Networks for YouTube Recommendations.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\W7WEZJQG\\Covington et al. - 2016 - Deep Neural Networks for YouTube Recommendations.pdf:application/pdf;Covington et al. - 2016 - Deep Neural Networks for YouTube Recommendations.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZDWYVSSH\\Covington et al. - 2016 - Deep Neural Networks for YouTube Recommendations.pdf:application/pdf},
}

@article{korenFactorizationMeetsNeighborhood,
	title = {Factorization {Meets} the {Neighborhood}: a {Multifaceted} {Collaborative} {Filtering} {Model}},
	language = {en},
	author = {Koren, Yehuda and Labs, T},
	pages = {24},
	file = {Koren and Labs - Factorization Meets the Neighborhood a Multifacet.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N49U5AAY\\Koren and Labs - Factorization Meets the Neighborhood a Multifacet.pdf:application/pdf;Koren and Labs - Factorization Meets the Neighborhood a Multifacet.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ITBGIDX4\\Koren and Labs - Factorization Meets the Neighborhood a Multifacet.pdf:application/pdf},
}

@inproceedings{seoInterpretableConvolutionalNeural2017a,
	address = {Como Italy},
	title = {Interpretable {Convolutional} {Neural} {Networks} with {Dual} {Local} and {Global} {Attention} for {Review} {Rating} {Prediction}},
	isbn = {978-1-4503-4652-8},
	url = {https://dl.acm.org/doi/10.1145/3109859.3109890},
	doi = {10.1145/3109859.3109890},
	abstract = {Recently, many e-commerce websites have encouraged their users to rate shopping items and write review texts. This review information has been very useful for understanding user preferences and item properties, as well as enhancing the capability to make personalized recommendations of these websites. In this paper, we propose to model user preferences and item properties using convolutional neural networks (CNNs) with dual local and global attention, motivated by the superiority of CNNs to extract complex features. By using aggregated review texts from a user and aggregated review text for an item, our model can learn the unique features (embedding) of each user and each item. These features are then used to predict ratings. We train these user and item networks jointly which enable the interaction between users and items in a similar way as matrix factorization. The local attention provides us insight on a user’s preferences or an item’s properties. The global attention helps CNNs focus on the semantic meaning of the whole review text. Thus, the combined local and global attentions enable an interpretable and better-learned representation of users and items. We validate the proposed models by testing on popular review datasets in Yelp and Amazon and compare the results with matrix factorization (MF), the hidden factor and topical (HFT) model, and the recently proposed convolutional matrix factorization (ConvMF+). Our proposed CNNs with dual attention model outperforms HFT and ConvMF+ in terms of mean square errors (MSE). In addition, we compare the user/item embeddings learned from these models for classi cation and recommendation. These results also con rm the superior quality of user/item embeddings learned from our model.},
	language = {en},
	urldate = {2021-09-05},
	booktitle = {Proceedings of the {Eleventh} {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Seo, Sungyong and Huang, Jing and Yang, Hao and Liu, Yan},
	month = aug,
	year = {2017},
	pages = {297--305},
	file = {Seo et al. - 2017 - Interpretable Convolutional Neural Networks with D.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VBX3VK22\\Seo et al. - 2017 - Interpretable Convolutional Neural Networks with D.pdf:application/pdf;Seo et al. - 2017 - Interpretable Convolutional Neural Networks with D.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H3QAGNKN\\Seo et al. - 2017 - Interpretable Convolutional Neural Networks with D.pdf:application/pdf},
}

@article{brownAssociationDepressiveSymptoms2014,
	title = {Association of {Depressive} {Symptoms} with {Hippocampal} {Volume} in 1936 {Adults}},
	volume = {39},
	issn = {0893-133X, 1740-634X},
	url = {http://www.nature.com/articles/npp2013271},
	doi = {10.1038/npp.2013.271},
	language = {en},
	number = {3},
	urldate = {2021-09-15},
	journal = {Neuropsychopharmacology},
	author = {Brown, E Sherwood and Hughes, Carroll W and McColl, Roderick and Peshock, Ronald and King, Kevin S and Rush, A John},
	month = feb,
	year = {2014},
	pages = {770--779},
	file = {Brown et al_2014_Association of Depressive Symptoms with Hippocampal Volume in 1936 Adults.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SSQYUILI\\Brown et al_2014_Association of Depressive Symptoms with Hippocampal Volume in 1936 Adults.pdf:application/pdf},
}

@article{bremnerHippocampalVolumeReduction2000,
	title = {Hippocampal {Volume} {Reduction} in {Major} {Depression}},
	volume = {157},
	issn = {0002-953X, 1535-7228},
	url = {http://psychiatryonline.org/doi/abs/10.1176/ajp.157.1.115},
	doi = {10.1176/ajp.157.1.115},
	language = {en},
	number = {1},
	urldate = {2021-09-15},
	journal = {American Journal of Psychiatry},
	author = {Bremner, J. Douglas and Narayan, Meena and Anderson, Eric R. and Staib, Lawrence H. and Miller, Helen L. and Charney, Dennis S.},
	month = jan,
	year = {2000},
	pages = {115--118},
}

@article{zhangBrainStructureAlterations2018,
	title = {Brain structure alterations in depression: {Psychoradiological} evidence},
	volume = {24},
	issn = {17555930},
	shorttitle = {Brain structure alterations in depression},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cns.12835},
	doi = {10.1111/cns.12835},
	language = {en},
	number = {11},
	urldate = {2021-09-15},
	journal = {CNS Neuroscience \& Therapeutics},
	author = {Zhang, Fei-Fei and Peng, Wei and Sweeney, John A. and Jia, Zhi-Yun and Gong, Qi-Yong},
	month = nov,
	year = {2018},
	pages = {994--1003},
	file = {Zhang et al_2018_Brain structure alterations in depression.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RY6TL7VU\\Zhang et al_2018_Brain structure alterations in depression.pdf:application/pdf},
}

@article{scheepensLinkStructuralFunctional2020,
	title = {The {Link} {Between} {Structural} and {Functional} {Brain} {Abnormalities} in {Depression}: {A} {Systematic} {Review} of {Multimodal} {Neuroimaging} {Studies}},
	volume = {11},
	issn = {1664-0640},
	shorttitle = {The {Link} {Between} {Structural} and {Functional} {Brain} {Abnormalities} in {Depression}},
	url = {https://www.frontiersin.org/article/10.3389/fpsyt.2020.00485/full},
	doi = {10.3389/fpsyt.2020.00485},
	urldate = {2021-09-15},
	journal = {Frontiers in Psychiatry},
	author = {Scheepens, Dominique S. and van Waarde, Jeroen A. and Lok, Anja and de Vries, Glenn and Denys, Damiaan A. J. P. and van Wingen, Guido A.},
	month = jun,
	year = {2020},
	pages = {485},
	file = {Scheepens et al_2020_The Link Between Structural and Functional Brain Abnormalities in Depression.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\32JLBRTN\\Scheepens et al_2020_The Link Between Structural and Functional Brain Abnormalities in Depression.pdf:application/pdf},
}

@misc{NationalCenterHealth2021,
	title = {National {Center} for {Health} {Statistics}. {Health}, {United} {States}, 2019; {Table} 029. {Hyattsville}, {MD} 2021. {Available} from: https://www.cdc.gov/nchs/hus/contents2019.htm.},
	url = {https://www.cdc.gov/nchs/hus/contents2019.htm},
	abstract = {Delay or nonreceipt of needed medical care, nonreceipt of needed prescription drugs, and nonreceipt of needed dental care during the past 12 months due to cost, by selected characteristics: United States, selected years 1997–2018},
	language = {en-us},
	urldate = {2021-09-15},
	journal = {Health, United States, 2019 – Data Finder},
	month = may,
	year = {2021},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BABKWD9P\\contents2019.html:text/html},
}

@misc{2019NationalHealthcare,
	title = {2019 {National} {Healthcare} {Quality} and {Disparities} {Report}},
	url = {http://www.ahrq.gov/research/findings/nhqrdr/nhqdr19/index.html},
	abstract = {For the 17th year in a row, AHRQ is reporting on healthcare quality and disparities. The annual National Healthcare Quality and Disparities Report is mandated by Congress to provide a comprehensive overview of the quality of healthcare received by the general U.S. population and disparities in care experienced by different racial and socioeconomic groups. The report is produced with the help of an Interagency Work Group led by AHRQ.},
	language = {en-us},
	urldate = {2021-09-15},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DEHIL3AQ\\index.html:text/html},
}

@article{tikkanenHealthCareGlobal,
	title = {U.{S}. {Health} {Care} from a {Global} {Perspective}, 2019: {Higher} {Spending}, {Worse} {Outcomes}?},
	shorttitle = {U.{S}. {Health} {Care} from a {Global} {Perspective}, 2019},
	url = {https://www.commonwealthfund.org/publications/issue-briefs/2020/jan/us-health-care-global-perspective-2019},
	doi = {10.26099/7AVY-FC29},
	abstract = {Americans are living shorter, unhealthier lives. Yet, the United States outspends other wealthy nations when it comes to health care, according to a new Commonwealth Fund report. This analysis compares the U.S. to 10 other high-income nations on spending, outcomes, risk factors, and quality.},
	urldate = {2021-09-15},
	author = {Tikkanen, Roosa and Abrams, Melinda K.},
	note = {Publisher: Commonwealth Fund},
}

@misc{TermsConditionsOffice,
	title = {Terms \& {Conditions} – {Office} of {Student} {Financial} {Aid}},
	url = {https://osfa.illinois.edu/process/checking-your-status-2/terms-conditions/},
	language = {en-US},
	urldate = {2021-09-22},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V6BVYA64\\terms-conditions.html:text/html},
}

@misc{MaximumMinimumEnrollment,
	title = {Maximum \& {Minimum} {Enrollment} {Levels} – {Office} of the {Registrar}},
	url = {https://registrar.illinois.edu/registration/registration-process/max-min-enrollment-levels/},
	language = {en-US},
	urldate = {2021-09-22},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TZYZM4Y7\\max-min-enrollment-levels.html:text/html},
}

@misc{communicationsOnlineMasterComputer,
	title = {Online {Master} of {Computer} {Science}},
	url = {https://cs.illinois.edu/academics/graduate/professional-mcs/online-master-computer-science},
	abstract = {Online Master of Computer Science},
	language = {en},
	urldate = {2021-09-22},
	author = {Communications, Grainger Engineering Office of Marketing and},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ATX27HLX\\online-master-computer-science.html:text/html},
}

@misc{HandbookPoliciesGraduate,
	title = {Handbook \& {Policies} {\textbar} {The} {Graduate} {College} at the {University} of {Illinois} at {Urbana}-{Champaign}},
	url = {https://grad.illinois.edu/handbooks-policies},
	urldate = {2021-09-23},
	file = {Handbook & Policies | The Graduate College at the University of Illinois at Urbana-Champaign:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2R4S87SY\\handbooks-policies.html:text/html},
}

@misc{FastStats2020,
	title = {{FastStats}},
	url = {https://www.cdc.gov/nchs/fastats/electronic-medical-records.htm},
	abstract = {FastStats is an official application from the Centers for Disease Control and Prevention’s (CDC) National Center for Health Statistics (NCHS) and puts access to topic-specific statistics at your fingertips.},
	language = {en-us},
	urldate = {2021-09-23},
	month = aug,
	year = {2020},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PGHEW8IF\\electronic-medical-records.html:text/html},
}

@article{thiriouxRelationEmpathyInsight2020,
	title = {The {Relation} {Between} {Empathy} and {Insight} in {Psychiatric} {Disorders}: {Phenomenological}, {Etiological}, and {Neuro}-{Functional} {Mechanisms}},
	volume = {10},
	issn = {1664-0640},
	shorttitle = {The {Relation} {Between} {Empathy} and {Insight} in {Psychiatric} {Disorders}},
	url = {https://www.frontiersin.org/article/10.3389/fpsyt.2019.00966},
	doi = {10.3389/fpsyt.2019.00966},
	abstract = {Lack of insight, i.e., unawareness of one’s mental illness, is frequently encountered in psychiatric conditions. Insight is the capacity to recognize (psychical insight) and accept one’s mental illness (emotional insight). Insight growth necessitates developing an objective perspective on one’s subjective pathological experiences. Therefore, insight has been posited to require undamaged self-reflexion and cognitive perspective-taking capacities. These enable patients to look objectively at themselves from the imagined perspective of someone else. Preserved theory-of-mind performances have been reported to positively impact insight in psychosis. However, some patients with schizophrenia or obsessive-compulsive disorders, although recognizing their mental disease, are still not convinced of this and do not accept it. Hence, perspective-taking explains psychical insight (recognition) but not emotional insight (acceptance). Here, we propose a new conceptual model. We hypothesize that insight growth relies upon the association of intact self-reflexion and empathic capacities. Empathy (feeling into someone else) integrates heterocentered visuo-spatial perspective (feeling into), embodiment, affective (feeling into) and cognitive processes, leading to internally experience the other’s thought. We posit that this subjective experience enables to better understand the other’s thought about oneself and to affectively adhere to this. We propose that the process of objectification, resulting from empathic heterocentered, embodiment, and cognitive processes, generates an objective viewpoint on oneself. It enables to recognize one’s mental illness and positively impacts psychical insight. The process of subjectification, resulting from empathic affective processes, enables to accept one’s illness and positively impacts emotional insight. That is, affectively experiencing the thought of another person about oneself reinforces the adhesion of the emotional system to the objective recognition of the disease. Applying our model to different psychiatric disorders, we predict that the negative effect of impaired self-reflexion and empathic capacities on insight is a transnosographic state and that endophenotypical differences modulate this common state, determining a psychiatric disease as specific.},
	urldate = {2021-09-23},
	journal = {Frontiers in Psychiatry},
	author = {Thirioux, Bérangère and Harika-Germaneau, Ghina and Langbour, Nicolas and Jaafari, Nematollah},
	year = {2020},
	pages = {966},
	file = {Thirioux et al_2020_The Relation Between Empathy and Insight in Psychiatric Disorders.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WQWTS378\\Thirioux et al_2020_The Relation Between Empathy and Insight in Psychiatric Disorders.pdf:application/pdf},
}

@article{newman-tokerRateDiagnosticErrors2021,
	title = {Rate of diagnostic errors and serious misdiagnosis-related harms for major vascular events, infections, and cancers: toward a national incidence estimate using the “{Big} {Three}”},
	volume = {8},
	issn = {2194-802X},
	shorttitle = {Rate of diagnostic errors and serious misdiagnosis-related harms for major vascular events, infections, and cancers},
	url = {https://www.degruyter.com/document/doi/10.1515/dx-2019-0104/html},
	doi = {10.1515/dx-2019-0104},
	abstract = {Background Missed vascular events, infections, and cancers account for {\textasciitilde}75\% of serious harms from diagnostic errors. Just 15 diseases from these “Big Three” categories account for nearly half of all serious misdiagnosis-related harms in malpractice claims. As part of a larger project estimating total US burden of serious misdiagnosis-related harms, we performed a focused literature review to measure diagnostic error and harm rates for these 15 conditions. Methods We searched PubMed, Google, and cited references. For errors, we selected high-quality, modern, US-based studies, if available, and best available evidence otherwise. For harms, we used literature-based estimates of the generic (disease-agnostic) rate of serious harms (morbidity/mortality) per diagnostic error and applied claims-based severity weights to construct disease-specific rates. Results were validated via expert review and comparison to prior literature that used different methods. We used Monte Carlo analysis to construct probabilistic plausible ranges (PPRs) around estimates. Results Rates for the 15 diseases were drawn from 28 published studies representing 91,755 patients. Diagnostic error (false negative) rates ranged from 2.2\% (myocardial infarction) to 62.1\% (spinal abscess), with a median of 13.6\% [interquartile range (IQR) 9.2–24.7] and an aggregate mean of 9.7\% (PPR 8.2–12.3). Serious misdiagnosis-related harm rates per incident disease case ranged from 1.2\% (myocardial infarction) to 35.6\% (spinal abscess), with a median of 5.5\% (IQR 4.6–13.6) and an aggregate mean of 5.2\% (PPR 4.5–6.7). Rates were considered face valid by domain experts and consistent with prior literature reports. Conclusions Diagnostic improvement initiatives should focus on dangerous conditions with higher diagnostic error and misdiagnosis-related harm rates.},
	language = {en},
	number = {1},
	urldate = {2021-09-23},
	journal = {Diagnosis},
	author = {Newman-Toker, David E. and Wang, Zheyu and Zhu, Yuxin and Nassery, Najlla and Tehrani, Ali S. Saber and Schaffer, Adam C. and Yu-Moe, Chihwen Winnie and Clemens, Gwendolyn D. and Fanai, Mehdi and Siegal, Dana},
	month = feb,
	year = {2021},
	note = {Publisher: De Gruyter},
	keywords = {diagnosis, diagnostic errors, health services research, medical errors, misdiagnosis-related harms},
	pages = {67--84},
	file = {Newman-Toker et al_2021_Rate of diagnostic errors and serious misdiagnosis-related harms for major.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TSX8H9ZF\\Newman-Toker et al_2021_Rate of diagnostic errors and serious misdiagnosis-related harms for major.pdf:application/pdf},
}

@misc{OfficebasedPhysicianElectronic,
	title = {Office-based {Physician} {Electronic} {Health} {Record} {Adoption} {\textbar} {HealthIT}.gov},
	url = {https://www.healthit.gov/data/quickstats/office-based-physician-electronic-health-record-adoption},
	urldate = {2021-09-23},
	file = {Office-based Physician Electronic Health Record Adoption | HealthIT.gov:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BIU5HPR8\\office-based-physician-electronic-health-record-adoption.html:text/html},
}

@misc{ERDoctorsCommonly,
	title = {{ER} {Doctors} {Commonly} {Miss} {More} {Strokes} {Among} {Women}, {Minorities} and {Younger} {Patients} - 04/03/2014},
	url = {https://www.hopkinsmedicine.org/news/media/releases/er_doctors_commonly_miss_more_strokes_among_women_minorities_and_younger_patients},
	abstract = {Analyzing federal health care data, a team of researchers led by a Johns Hopkins specialist concluded that doctors overlook or discount the early signs of potentially disabling strokes in tens of thousands of American each year, a large number of them visitors to emergency rooms complaining of dizziness or headaches.},
	language = {en},
	urldate = {2021-09-23},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YNEBD6H4\\er_doctors_commonly_miss_more_strokes_among_women_minorities_and_younger_patients.html:text/html},
}

@article{raghavanPublicHealthInnovation2021,
	title = {Public {Health} {Innovation} through {Cloud} {Adoption}: {A} {Comparative} {Analysis} of {Drivers} and {Barriers} in {Japan}, {South} {Korea}, and {Singapore}},
	volume = {18},
	issn = {1661-7827},
	shorttitle = {Public {Health} {Innovation} through {Cloud} {Adoption}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7794833/},
	doi = {10.3390/ijerph18010334},
	abstract = {Governments are increasingly using cloud computing to reduce cost, increase access, improve quality, and create innovations in healthcare. Existing literature is primarily based on successful examples from developed western countries, and there is a lack of similar evidence from Asia. With a population close to 4.5 billion people, Asia faces healthcare challenges that pose an immense burden on economic growth and policymaking. Cloud computing in healthcare can potentially help increase the quality of healthcare delivery and reduce the economic burden, enabling governments to address healthcare challenges effectively and within a short timeframe. Advanced Asian countries such as Japan, South Korea, and Singapore provide successful examples of how cloud computing can be used to develop nationwide databases of electronic health records; real-time health monitoring for the elderly population; genetic database to support advanced research and cancer treatment; telemedicine; and health cities that drive the economy through medical industry, tourism, and research. This article examines these countries and identifies the drivers and barriers of cloud adoption in healthcare and makes policy recommendations to enable successful public health innovations through cloud adoption.},
	number = {1},
	urldate = {2021-09-23},
	journal = {International Journal of Environmental Research and Public Health},
	author = {Raghavan, Aarthi and Demircioglu, Mehmet Akif and Taeihagh, Araz},
	month = jan,
	year = {2021},
	pmid = {33466338},
	pmcid = {PMC7794833},
	pages = {334},
	file = {Raghavan et al_2021_Public Health Innovation through Cloud Adoption.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KQ4M6VNN\\Raghavan et al_2021_Public Health Innovation through Cloud Adoption.pdf:application/pdf},
}

@article{mullardLandmarkAlzheimerDrug2021,
	title = {Landmark {Alzheimer}’s drug approval confounds research community},
	volume = {594},
	copyright = {2021 Nature},
	url = {https://www.nature.com/articles/d41586-021-01546-2},
	doi = {10.1038/d41586-021-01546-2},
	abstract = {Many scientists say there is not enough evidence that Biogen’s aducanumab is an effective therapy for the disease.},
	language = {en},
	number = {7863},
	urldate = {2021-09-23},
	journal = {Nature},
	author = {Mullard, Asher},
	month = jun,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: News
Number: 7863
Publisher: Nature Publishing Group
Subject\_term: Drug discovery, Alzheimer's disease},
	pages = {309--310},
	file = {Full Text PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZAYWYC9B\\Mullard - 2021 - Landmark Alzheimer’s drug approval confounds resea.pdf:application/pdf;Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RDPYF737\\d41586-021-01546-2.html:text/html},
}

@techreport{biogenPhaseMulticenterRandomized2021,
	type = {Clinical trial registration},
	title = {A {Phase} 3 {Multicenter}, {Randomized}, {Double}-{Blind}, {Placebo}-{Controlled}, {Parallel}-{Group} {Study} to {Evaluate} the {Efficacy} and {Safety} of {Aducanumab} ({BIIB037}) in {Subjects} {With} {Early} {Alzheimer}'s {Disease}},
	shorttitle = {{221AD302} {Phase} 3 {Study} of {Aducanumab} ({BIIB037}) in {Early} {Alzheimer}'s {Disease}},
	url = {https://clinicaltrials.gov/ct2/show/NCT02484547},
	abstract = {The primary objective of the study is to evaluate the efficacy of monthly doses of aducanumab in slowing cognitive and functional impairment as measured by changes in the Clinical Dementia Rating-Sum of Boxes (CDR-SB) score as compared with placebo in participants with early AD. Secondary objectives are to assess the effect of monthly doses of aducanumab as compared with placebo on clinical progression as measured by Mini-Mental State Examination (MMSE), AD Assessment Scale-Cognitive Subscale (13 items) [ADAS-Cog 13], and AD Cooperative Study-Activities of Daily Living Inventory (Mild Cognitive Impairment version) [ADCS-ADL-MCI].},
	number = {NCT02484547},
	urldate = {2021-09-22},
	institution = {clinicaltrials.gov},
	author = {{Biogen}},
	month = aug,
	year = {2021},
	note = {submitted: June 18, 2015},
}

@techreport{biogenRandomizedDoubleBlindedPlaceboControlled2020,
	type = {Clinical trial registration},
	title = {A {Randomized}, {Double}-{Blinded}, {Placebo}-{Controlled} {Multiple} {Dose} {Study} to {Assess} the {Safety}, {Tolerability}, {Pharmacokinetics}, and {Pharmacodynamics} of {BIIB037} in {Subjects} {With} {Prodromal} or {Mild} {Alzheimer}'s {Disease}},
	shorttitle = {Multiple {Dose} {Study} of {Aducanumab} ({BIIB037}) ({Recombinant}, {Fully} {Human} {Anti}-{Aβ} {IgG1} {mAb}) in {Participants} {With} {Prodromal} or {Mild} {Alzheimer}'s {Disease}},
	url = {https://clinicaltrials.gov/ct2/show/NCT01677572},
	abstract = {The primary objective of this study is to evaluate the safety and tolerability of multiple doses of Aducanumab (recombinant, fully human anti-Aβ IgG1 mAb) in participants with prodromal or mild Alzheimer's Disease (AD). The secondary objectives of this study are to assess the effect on cerebral amyloid plaque content as measured by florbetapir-fluorine-18 (18F-AV-45F-AV-45) positron emission tomography (PET) imaging, to assess the multiple dose serum concentrations of Aducanumab and to evaluate the immunogenicity of Aducanumab after multiple dose administration in this population.},
	number = {NCT01677572},
	urldate = {2021-09-22},
	institution = {clinicaltrials.gov},
	author = {{Biogen}},
	month = jul,
	year = {2020},
	note = {submitted: August 30, 2012},
}

@misc{FactsFigures,
	title = {Facts and {Figures}},
	url = {https://www.alz.org/alzheimers-dementia/facts-figures},
	abstract = {Alzheimer's Disease Facts and Figures report – get the latest statistics on the impact of Alzheimer's and dementia in the United States.},
	language = {en-US},
	urldate = {2021-09-23},
	journal = {Alzheimer's Disease and Dementia},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V98I3J2S\\facts-figures.html:text/html},
}

@article{matsunamiClinicalMeaningAcademic2013,
	title = {[{The} clinical meaning and academic significance of "endogenous depression" as an ideal type]},
	volume = {115},
	issn = {0033-2658},
	abstract = {The assumption that a core group of depressive disorders is due to a discontinuous change in the function of the brain system, suggests that the symptoms of the core group of depressive disorders should be discerned from those of non-core depression. Core depression is thought to correspond to depression of an endogenous nature, which has recently been disregarded in diagnosing mood disorders. However, in diagnosing endogenous depression, we can identify its characteristic symptoms by referring to a traditional symptomatology. Therefore, the idea of Verstehen (Jaspers, K) becomes essential, but has been neglected in academic journals in English-speaking countries. The depressive mood in endogenous depression may be an inhibition of various kinds of emotion, which can never be experienced within a normal emotional state. Thus, it is thought to be 'unverstaendlich' (incomprehensible) in nature. The "anhedonia hypothesis" of depression from DSM-IV permits the inclusion of the non-core group of depression into major depressive disorder, because patients with the endogenous type suffer not only from a loss of pleasure but also from a loss of sadness. A new type of depression that has recently been debated in Japan is diagnosed as major depressive disorder in DSM-IV, but many suspected cases are thought to be due to a psychogenic state or neurotic condition, because their symptoms are thought to be 'verstaendlich' (comprehensible). Endogenous depression can manifest as manic-depressive disorder in almost all cases, but psychiatric practitioners have taken the necessary precautions against the risk of a shift to a manic state even when treating pure depression which seemingly appears to have no manic component. According to recent studies on bipolar disorder, pure mania is not empirically found. Thus, the manic pole may not exist, and we may be able to think of endogenous depression as manic-depressive disorder based on its genetic entity. Additionally, from the viewpoint of its symptomatology there is one pole of depression modified by manic tendencies based on various intensities. The current study presents the hypothesis that there is one entity for disease, manic-depression, but whether the disorder manifests itself as uni- or bipolar disorder depends upon the regulation of manic manifestations by a meticulous, obsessive-compulsive personality which has been considered to be the premorbid personality for unipolar depression.},
	language = {jpn},
	number = {3},
	journal = {Seishin Shinkeigaku Zasshi = Psychiatria Et Neurologia Japonica},
	author = {Matsunami, Katsufumi},
	year = {2013},
	pmid = {23691813},
	keywords = {Humans, Bipolar Disorder, Depressive Disorder, Depressive Disorder, Major, Diagnosis, Differential, Emotions, Japan},
	pages = {267--276},
}

@book{coulourisDistributedSystemsConcepts2012,
	address = {Boston},
	edition = {5th ed},
	title = {Distributed systems: concepts and design},
	isbn = {978-0-13-214301-1},
	shorttitle = {Distributed systems},
	publisher = {Addison-Wesley},
	editor = {Coulouris, George F.},
	year = {2012},
	keywords = {Distributed processing, Electronic data processing},
	file = {Distributed systems concepts and design.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\Distributed systems concepts and design.md:text/plain;Distributed systems concepts and design.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Computer Science\\Distributed systems concepts and design.md:text/plain;Distributed systems concepts and design.md:C\:\\Users\\John\\Documents\\LearningCentre\\reference\\Distributed systems concepts and design.md:text/plain},
}

@incollection{massachusettscomputerassociatesinc.TimeClocksOrdering2019,
	title = {Time, clocks, and the ordering of events in a distributed system},
	isbn = {978-1-4503-7270-1},
	url = {https://dl.acm.org/citation.cfm?id=3335934},
	urldate = {2021-09-29},
	booktitle = {Concurrency: the {Works} of {Leslie} {Lamport}},
	publisher = {Association for Computing Machinery},
	author = {{Massachusetts Computer Associates, Inc.} and Lamport, Leslie},
	editor = {Malkhi, Dahlia},
	collaborator = {{VMware Research and Calibra}},
	month = oct,
	year = {2019},
	doi = {10.1145/3335772.3335934},
	file = {Massachusetts Computer Associates, Inc._Lamport_2019_Time, clocks, and the ordering of events in a distributed system.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S7RP8ZWZ\\Massachusetts Computer Associates, Inc._Lamport_2019_Time, clocks, and the ordering of events in a distributed system.pdf:application/pdf;Time, clocks, and the ordering of events in a distributed system.md:C\:\\Users\\John\\Documents\\LearningCentre\\reference\\Time, clocks, and the ordering of events in a distributed system.md:text/plain},
}

@techreport{beckBeckDepressionInventory2011,
	title = {Beck {Depression} {Inventory}–{II}},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/t00742-000},
	abstract = {The BDI-II is a widely used 21-item self-report inventory measuring the severity of depression in adolescents and adults. The BDI-II was revised in 1996 to be more consistent with DSM-IV criteria for depression. For example, individuals are asked to respond to each question based on a two-week time period rather than the one-week timeframe on the BDI. The BDI-II is widely used as an indicator of the severity of depression, but not as a diagnostic tool, and numerous studies provide evidence for its reliability and validity across different populations and cultural groups. It has also been used in numerous treatment outcome studies and in numerous studies with trauma-exposed individuals. (APA PsycTests Database Record (c) 2019 APA, all rights reserved)},
	language = {en},
	urldate = {2021-10-04},
	institution = {American Psychological Association},
	author = {Beck, Aaron T. and Steer, R. A. and Brown, G.},
	month = sep,
	year = {2011},
	doi = {10.1037/t00742-000},
	note = {Type: dataset},
}

@book{americanpsychiatricassociationDiagnosticStatisticalManual2013,
	address = {Washington, D.C},
	edition = {5th ed},
	title = {Diagnostic and statistical manual of mental disorders: {DSM}-5},
	isbn = {978-0-89042-554-1 978-0-89042-555-8},
	shorttitle = {Diagnostic and statistical manual of mental disorders},
	language = {en},
	publisher = {American Psychiatric Association},
	editor = {American Psychiatric Association and American Psychiatric Association},
	year = {2013},
	keywords = {classification, Classification, diagnosis, Diagnosis, Diagnostic and statistical manual of mental disorders, Mental Disorders, Mental illness},
	file = {American Psychiatric Association and American Psychiatric Association - 2013 - Diagnostic and statistical manual of mental disord.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PR5W3ZJ9\\American Psychiatric Association and American Psychiatric Association - 2013 - Diagnostic and statistical manual of mental disord.pdf:application/pdf},
}

@incollection{bogdukDiagnosticStatisticalManual2013,
	address = {Berlin, Heidelberg},
	title = {Diagnostic and {Statistical} {Manual} of {Mental} {Disorders}},
	isbn = {978-3-642-28752-7 978-3-642-28753-4},
	url = {http://link.springer.com/10.1007/978-3-642-28753-4_1094},
	language = {en},
	urldate = {2021-10-05},
	booktitle = {Encyclopedia of {Pain}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bogduk, Nikolai},
	editor = {Gebhart, Gerald F. and Schmidt, Robert F.},
	year = {2013},
	doi = {10.1007/978-3-642-28753-4_1094},
	pages = {979--982},
}

@article{hayesOperationsStrategyTechnology,
	title = {Operations, {Strategy}, and {Technology}: {Pursuing} the {Competitive} {Edge}},
	language = {en},
	author = {Hayes, Robert H},
	pages = {1},
	file = {Hayes - Operations, Strategy, and Technology Pursuing the.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DW46YIMK\\Hayes - Operations, Strategy, and Technology Pursuing the.pdf:application/pdf;Hayes - Operations, Strategy, and Technology Pursuing the.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4LH82GWD\\Hayes - Operations, Strategy, and Technology Pursuing the.pdf:application/pdf},
}

@article{levinEntryMarketStructure,
	title = {Entry and {Market} {Structure}},
	language = {en},
	author = {Levin, Jonathan},
	pages = {40},
	file = {Levin - Entry and Market Structure.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\K7C48HMP\\Levin - Entry and Market Structure.pdf:application/pdf;Levin - Entry and Market Structure.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DT9QJ2U8\\Levin - Entry and Market Structure.pdf:application/pdf},
}

@book{evansInvisibleEnginesHow2006,
	address = {Cambridge, Mass},
	title = {Invisible engines: how software platforms drive innovation and transform industries},
	isbn = {978-0-262-05085-2},
	shorttitle = {Invisible engines},
	language = {en},
	publisher = {MIT Press},
	author = {Evans, David S. and Hagiu, Andrei and Schmalensee, Richard},
	year = {2006},
	note = {OCLC: ocm70232384},
	keywords = {Application program interfaces (Computer software), Data processing, Industries, Software ecosystems},
	file = {Evans et al. - 2006 - Invisible engines how software platforms drive in.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DEV9G9AH\\Evans et al. - 2006 - Invisible engines how software platforms drive in.pdf:application/pdf;Evans et al. - 2006 - Invisible engines how software platforms drive in.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\92CUB4TJ\\Evans et al. - 2006 - Invisible engines how software platforms drive in.pdf:application/pdf},
}

@article{hanmer-lloydManagingRoutesMarket,
	title = {Managing {Routes} to {Market}: {A} {Glimpse} into the {Future}},
	abstract = {The importance of managing the B2B channel relationship is increasingly being recognised by suppliers. For example, Carlo Fiorini, CEO of Hewlett Packard, pointed to poor channel relationships as a root cause for poor business performance. Gary Frazier1 stated “As the world economy evolves, more and more companies are highlighting channel management as among their very top priorities. The opportunities for channel researchers to contribute to knowledge creation in the marketing discipline and, at the same time, affect business practice are almost endless”. Research was conducted via nineteen in-depth interviews of senior channel managers from a range of organisations and sectors to identify key issues that are likely to influence their channel strategies in the future. The subsequent interviews were analysed and a number of key strategic issues were identified as future challenges for channel managers. Also, over twenty ‘trends’ emerged from the interviews, which would undoubtedly influence the way channels would be managed in the future.},
	language = {en},
	author = {Hanmer-Lloyd, Stuart and Canning, Louise},
	pages = {20},
	file = {Hanmer-Lloyd and Canning - Managing Routes to Market A Glimpse into the Futu.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9UQJ5W2E\\Hanmer-Lloyd and Canning - Managing Routes to Market A Glimpse into the Futu.pdf:application/pdf;Hanmer-Lloyd and Canning - Managing Routes to Market A Glimpse into the Futu.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9X6CTYGM\\Hanmer-Lloyd and Canning - Managing Routes to Market A Glimpse into the Futu.pdf:application/pdf},
}

@book{hooleyMarketingStrategyCompetitive2017,
	address = {Harlow, UK},
	edition = {Sixth edition},
	title = {Marketing strategy \& competitive positioning},
	isbn = {978-1-292-01731-0},
	language = {en},
	publisher = {Pearson Educational Limited},
	author = {Hooley, Graham and Hooley, Graham J. and Piercy, Nigel and Nicoulaud, Brigitte M. and Rudd, John M. and Nicoulaud, Brigitte},
	year = {2017},
	file = {Hooley et al. - 2017 - Marketing strategy & competitive positioning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UUWQFGXW\\Hooley et al. - 2017 - Marketing strategy & competitive positioning.pdf:application/pdf;Hooley et al. - 2017 - Marketing strategy & competitive positioning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IDDX2YBQ\\Hooley et al. - 2017 - Marketing strategy & competitive positioning.pdf:application/pdf;Marketing strategy & competitive positioning.md:C\:\\Users\\John\\Documents\\Daelmann\\Knowledge Centre, Daelmann\\Strategy\\Marketing strategy & competitive positioning.md:text/plain;Marketing strategy & competitive positioning.md:C\:\\Users\\John\\Documents\\LearningCentre\\Business\\Resources\\references\\Marketing strategy & competitive positioning.md:text/plain},
}

@book{geronHandsonMachineLearning2019,
	address = {Beijing [China] ; Sebastopol, CA},
	edition = {Second edition},
	title = {Hands-on machine learning with {Scikit}-{Learn}, {Keras}, and {TensorFlow}: concepts, tools, and techniques to build intelligent systems},
	isbn = {978-1-4920-3264-9},
	shorttitle = {Hands-on machine learning with {Scikit}-{Learn}, {Keras}, and {TensorFlow}},
	publisher = {O'Reilly Media, Inc},
	author = {Géron, Aurélien},
	year = {2019},
	keywords = {Artificial intelligence, Machine learning, Python (Computer program language), TensorFlow},
	file = {Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow concepts, tools, and techniques to build intelligent systems.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow concepts, tools, and techniques to build intelligent systems.md:text/plain},
}

@article{erramilliExperienceFactorForeign1991,
	title = {The {Experience} {Factor} in {Foreign} {Market} {Entry} {Behavior} of {Service} {Firms}},
	volume = {22},
	issn = {0047-2506, 1478-6990},
	url = {http://link.springer.com/10.1057/palgrave.jibs.8490312},
	doi = {10.1057/palgrave.jibs.8490312},
	abstract = {The paper examines the effect of international experience on service firms' selection of foreign markets and entry modes. The investigation utilizes survey data from 151 United States-based service firms. Results on market selection suggest that, as their experience increases and becomes geographically more diversifled, service firms tend to choose markets that are culturally less similar to their home country. On entry mode choice, the paper departs from traditional linear conceptualizations and hypothesizes a U-shaped relationship between experience and propensity for integrated entry modes. Results generally support the hypothesis. The paper explains these findings and describes how service firms resemble and differ from manufacturing firms in their foreign market entry behavior.},
	language = {en},
	number = {3},
	urldate = {2021-10-13},
	journal = {Journal of International Business Studies},
	author = {Erramilli, M. Krishna},
	month = sep,
	year = {1991},
	pages = {479--501},
	file = {Erramilli - 1991 - The Experience Factor in Foreign Market Entry Beha.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VW5J3DNF\\Erramilli - 1991 - The Experience Factor in Foreign Market Entry Beha.pdf:application/pdf},
}

@article{hollenbach34McKinseyQuarterly,
	title = {34 {The} {McKinsey} {Quarterly} 2005 {Number} 4},
	language = {en},
	author = {Hollenbach, David},
	pages = {12},
	file = {Hollenbach - 34 The McKinsey Quarterly 2005 Number 4.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3X6TQ52M\\Hollenbach - 34 The McKinsey Quarterly 2005 Number 4.pdf:application/pdf},
}

@article{morsePRODUCTMARKETDEFINITION2021,
	title = {{PRODUCT} {MARKET} {DEFINITION} {IN} {THE} {PHARMACEUTICAL}                             {INDUSTRY}},
	language = {en},
	author = {Morse, M Howard},
	year = {2021},
	pages = {45},
	file = {Morse - 2021 - PRODUCT MARKET DEFINITION IN THE PHARMACEUTICAL   .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\82CAAU92\\Morse - 2021 - PRODUCT MARKET DEFINITION IN THE PHARMACEUTICAL   .pdf:application/pdf},
}

@book{porterCompetitiveAdvantageCreating1998,
	address = {New York},
	edition = {1st Free Press ed},
	title = {Competitive advantage: creating and sustaining superior performance: with a new introduction},
	isbn = {978-0-684-84146-5},
	shorttitle = {Competitive advantage},
	publisher = {Free Press},
	author = {Porter, Michael E.},
	year = {1998},
	keywords = {Competition, Industrial management},
	file = {Competitive advantage creating and sustaining superior performance with a new introduction.md:C\:\\Users\\John\\Documents\\LearningCentre\\Business\\Resources\\references\\Competitive advantage creating and sustaining superior performance with a new introduction.md:text/plain},
}

@book{millerStrategicSellingUnique1986,
	address = {New York},
	edition = {1. ed. repr},
	title = {Strategic selling: the unique sales system proven successful by {America}'s best companies},
	isbn = {978-0-446-38627-2},
	shorttitle = {Strategic selling},
	language = {eng},
	publisher = {Warner},
	author = {Miller, Robert B. and Heiman, Stephen E.},
	year = {1986},
	file = {Strategic selling the unique sales system proven successful by America's best companies.md:C\:\\Users\\John\\Documents\\LearningCentre\\Business\\Resources\\references\\Strategic selling the unique sales system proven successful by America's best companies.md:text/plain},
}

@article{vissakForeignMarketEntries2020,
	title = {Foreign market entries, exits and re-entries: {The} role of knowledge, network relationships and decision-making logic},
	volume = {29},
	issn = {09695931},
	shorttitle = {Foreign market entries, exits and re-entries},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0969593117308594},
	doi = {10.1016/j.ibusrev.2019.101592},
	language = {en},
	number = {1},
	urldate = {2021-10-21},
	journal = {International Business Review},
	author = {Vissak, Tiia and Francioni, Barbara and Freeman, Susan},
	month = feb,
	year = {2020},
	pages = {101592},
}

@inproceedings{niJustifyingRecommendationsUsing2019,
	address = {Hong Kong, China},
	title = {Justifying {Recommendations} using {Distantly}-{Labeled} {Reviews} and {Fine}-{Grained} {Aspects}},
	url = {https://www.aclweb.org/anthology/D19-1018},
	doi = {10.18653/v1/D19-1018},
	abstract = {Several recent works have considered the problem of generating reviews (or ‘tips’) as a form of explanation as to why a recommendation might match a user’s interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justiﬁcations that are relevant to users’ decision-making process. We seek to introduce new datasets and methods to address this recommendation justiﬁcation task. In terms of data, we ﬁrst propose an ‘extractive’ approach to identify review segments which justify users’ intentions; this approach is then used to distantly label massive review corpora and construct largescale personalized recommendation justiﬁcation datasets. In terms of generation, we design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justiﬁcations covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justiﬁcations based on templates extracted from justiﬁcation histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justiﬁcations.},
	language = {en},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Ni, Jianmo and Li, Jiacheng and McAuley, Julian},
	year = {2019},
	keywords = {xrec},
	pages = {188--197},
	file = {Ni et al. - 2019 - Justifying Recommendations using Distantly-Labeled.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QHE2XMQG\\Ni et al. - 2019 - Justifying Recommendations using Distantly-Labeled.pdf:application/pdf;Ni et al. - 2019 - Justifying Recommendations using Distantly-Labeled.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\X526JLI3\\Ni et al. - 2019 - Justifying Recommendations using Distantly-Labeled.pdf:application/pdf},
}

@book{sammutEncyclopediaMachineLearning2010,
	address = {Boston, MA},
	title = {Encyclopedia of {Machine} {Learning}},
	isbn = {978-0-387-30768-8 978-0-387-30164-8},
	url = {http://link.springer.com/10.1007/978-0-387-30164-8},
	language = {en},
	urldate = {2021-11-05},
	publisher = {Springer US},
	editor = {Sammut, Claude and Webb, Geoffrey I.},
	year = {2010},
	doi = {10.1007/978-0-387-30164-8},
}

@article{chenAttributeAwareRecommenderSystem2020,
	title = {Attribute-{Aware} {Recommender} {System} {Based} on {Collaborative} {Filtering}: {Survey} and {Classification}},
	volume = {2},
	issn = {2624-909X},
	shorttitle = {Attribute-{Aware} {Recommender} {System} {Based} on {Collaborative} {Filtering}},
	url = {https://www.frontiersin.org/article/10.3389/fdata.2019.00049/full},
	doi = {10.3389/fdata.2019.00049},
	urldate = {2021-11-05},
	journal = {Frontiers in Big Data},
	author = {Chen, Wen-Hao and Hsu, Chin-Chi and Lai, Yi-An and Liu, Vincent and Yeh, Mi-Yen and Lin, Shou-De},
	month = jan,
	year = {2020},
	pages = {49},
	file = {Chen et al_2020_Attribute-Aware Recommender System Based on Collaborative Filtering.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NZ9HRLA6\\Chen et al_2020_Attribute-Aware Recommender System Based on Collaborative Filtering.pdf:application/pdf;Chen et al. - 2020 - Attribute-Aware Recommender System Based on Collab.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MYHILHQT\\Chen et al. - 2020 - Attribute-Aware Recommender System Based on Collab.pdf:application/pdf},
}

@book{lehmannAnalysisMarketingPlanning2008,
	address = {Boston},
	edition = {7th ed},
	title = {Analysis for marketing planning},
	isbn = {978-0-07-352984-4},
	publisher = {McGraw-Hill Irwin},
	author = {Lehmann, Donald R. and Winer, Russell S.},
	year = {2008},
	note = {OCLC: ocn173509457},
	keywords = {Management, Marketing, United States},
	file = {Analysis for marketing planning.md:C\:\\Users\\John\\Documents\\LearningCentre\\Business\\Resources\\references\\Analysis for marketing planning.md:text/plain},
}

@book{wassermanAllStatisticsConcise2010,
	address = {New York Berlin Heidelberg},
	edition = {Corr. 2. print., [repr.]},
	series = {Springer texts in statistics},
	title = {All of statistics: a concise course in statistical inference},
	isbn = {978-1-4419-2322-6},
	shorttitle = {All of statistics},
	language = {eng},
	publisher = {Springer},
	author = {Wasserman, Larry},
	year = {2010},
	file = {All of statistics a concise course in statistical inference.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\All of statistics a concise course in statistical inference.md:text/plain},
}

@book{forthoferBiostatisticsGuideDesign2007,
	address = {Burlington, MA},
	edition = {2nd ed},
	title = {Biostatistics: a guide to design, analysis, and discovery},
	isbn = {978-0-12-369492-8},
	shorttitle = {Biostatistics},
	publisher = {Elsevier Academic Press},
	author = {Forthofer, Ron N. and Lee, Eun Sul and Hernandez, Mike},
	year = {2007},
	keywords = {Biometry, Medicine, Research Statistical methods},
	file = {Biostatistics a guide to design, analysis, and discovery.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\Biostatistics a guide to design, analysis, and discovery.md:text/plain},
}

@misc{mitelectiondataandsciencelabCountyPresidentialElection2018,
	title = {County {Presidential} {Election} {Returns} 2000-2020},
	url = {https://dataverse.harvard.edu/citation?persistentId=doi:10.7910/DVN/VOQCHQ},
	abstract = {This dataset contains county-level returns for presidential elections from 2000 to 2020.},
	urldate = {2021-11-22},
	publisher = {Harvard Dataverse},
	author = {MIT Election Data {And} Science Lab},
	collaborator = {Curiel, John A},
	year = {2018},
	doi = {10.7910/DVN/VOQCHQ},
	note = {Type: dataset},
}

@book{chernickBootstrapMethodsGuide2008,
	address = {Hoboken, N.J},
	title = {Bootstrap methods: a guide for practitioners and researchers},
	isbn = {978-0-471-75621-7 978-0-470-19257-3},
	shorttitle = {Bootstrap methods},
	language = {English},
	publisher = {Wiley},
	author = {Chernick, Michael R},
	year = {2008},
	note = {OCLC: 890560889},
	file = {Bootstrap methods a guide for practitioners and researchers.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\Bootstrap methods a guide for practitioners and researchers.md:text/plain},
}

@book{efronIntroductionBootstrap1993,
	address = {Boston, MA},
	title = {An {Introduction} to the {Bootstrap}},
	isbn = {978-0-412-04231-7 978-1-4899-4541-9},
	url = {http://link.springer.com/10.1007/978-1-4899-4541-9},
	language = {en},
	urldate = {2021-11-22},
	publisher = {Springer US},
	author = {Efron, Bradley and Tibshirani, Robert J.},
	year = {1993},
	doi = {10.1007/978-1-4899-4541-9},
	file = {An Introduction to the Bootstrap.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\An Introduction to the Bootstrap.md:text/plain;Efron_Tibshirani_1993_An Introduction to the Bootstrap.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\M889M5ZB\\Efron_Tibshirani_1993_An Introduction to the Bootstrap.pdf:application/pdf},
}

@misc{enwiki:1049500973,
	title = {Empirical distribution function — {Wikipedia}, the free encyclopedia},
	url = {https://en.wikipedia.org/w/index.php?title=Empirical<sub>d</sub>istribution<sub>f</sub>unction&oldid=1049500973},
	author = {{Wikipedia contributors}},
	year = {2021},
	file = {Empirical distribution function — Wikipedia, the free encyclopedia.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\Empirical distribution function — Wikipedia, the free encyclopedia.md:text/plain},
}

@incollection{shaniEvaluatingRecommendationSystems2011,
	address = {Boston, MA},
	title = {Evaluating {Recommendation} {Systems}},
	isbn = {978-0-387-85819-7 978-0-387-85820-3},
	url = {http://link.springer.com/10.1007/978-0-387-85820-3_8},
	abstract = {Recommender systems are now popular both commercially and in the research community, where many approaches have been suggested for providing recommendations. In many cases a system designer that wishes to employ a recommendation system must choose between a set of candidate approaches. A ﬁrst step towards selecting an appropriate algorithm is to decide which properties of the application to focus upon when making this choice. Indeed, recommendation systems have a variety of properties that may affect user experience, such as accuracy, robustness, scalability, and so forth. In this paper we discuss how to compare recommenders based on a set of properties that are relevant for the application. We focus on comparative studies, where a few algorithms are compared using some evaluation metric, rather than absolute benchmarking of algorithms. We describe experimental settings appropriate for making choices between algorithms. We review three types of experiments, starting with an ofﬂine setting, where recommendation approaches are compared without user interaction, then reviewing user studies, where a small group of subjects experiment with the system and report on the experience, and ﬁnally describe large scale online experiments, where real user populations interact with the system. In each of these cases we describe types of questions that can be answered, and suggest protocols for experimentation. We also discuss how to draw trustworthy conclusions from the conducted experiments. We then review a large set of properties, and explain how to evaluate systems given relevant properties. We also survey a large set of evaluation metrics in the context of the property that they evaluate.},
	language = {en},
	urldate = {2021-12-04},
	booktitle = {Recommender {Systems} {Handbook}},
	publisher = {Springer US},
	author = {Shani, Guy and Gunawardana, Asela},
	editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
	year = {2011},
	doi = {10.1007/978-0-387-85820-3_8},
	pages = {257--297},
	file = {Shani and Gunawardana - 2011 - Evaluating Recommendation Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\22PHGTM3\\Shani and Gunawardana - 2011 - Evaluating Recommendation Systems.pdf:application/pdf;Shani and Gunawardana - 2011 - Evaluating Recommendation Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T2AL8X97\\Shani and Gunawardana - 2011 - Evaluating Recommendation Systems.pdf:application/pdf},
}

@book{shannonMathematicalTheoryCommunication1998,
	address = {Urbana},
	edition = {21. print},
	title = {The mathematical theory of communication},
	isbn = {978-0-252-72548-7},
	language = {eng},
	publisher = {Univ. of Illinois Press},
	author = {Shannon, Claude Elwood and Weaver, Warren},
	year = {1998},
}

@book{booleInvestigationLawsThought2009,
	title = {An investigation of the laws of thought: on which are founded the mathematical theories of logic and probabilities},
	isbn = {978-0-511-69309-0 978-1-108-00153-3},
	shorttitle = {An investigation of the laws of thought},
	abstract = {Self-taught mathematician and father of Boolean algebra, George Boole (1815-1864) published An Investigation of the Laws of Thought in 1854. In this highly original investigation of the fundamental laws of human reasoning, a sequel to ideas he had explored in earlier writings, Boole uses the symbolic language of mathematics to establish a method to examine the nature of the human mind using logic and the theory of probabilities. Boole considers language not just as a mode of expression, but as a system one can use to understand the human mind. In the first 12 chapters, he sets down the rules necessary to represent logic in this unique way. Then he analyses a variety of arguments and propositions of various writers from Aristotle to Spinoza. One of history's most insightful mathematicians, Boole is compelling reading for today's student of intellectual history and the science of the mind.},
	language = {English},
	author = {Boole, George},
	year = {2009},
	note = {OCLC: 967688612},
}

@article{shannonSymbolicAnalysisRelay1938,
	title = {A symbolic analysis of relay and switching circuits},
	volume = {57},
	issn = {0096-3860},
	url = {http://ieeexplore.ieee.org/document/5057767/},
	doi = {10.1109/T-AIEE.1938.5057767},
	number = {12},
	urldate = {2021-12-04},
	journal = {Transactions of the American Institute of Electrical Engineers},
	author = {Shannon, Claude E.},
	month = dec,
	year = {1938},
	pages = {713--723},
	file = {Shannon_1938_A symbolic analysis of relay and switching circuits.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VXQF3YCV\\Shannon_1938_A symbolic analysis of relay and switching circuits.pdf:application/pdf},
}

@book{brockwellIntroductionTimeSeries2016,
	address = {Cham},
	series = {Springer {Texts} in {Statistics}},
	title = {Introduction to {Time} {Series} and {Forecasting}},
	isbn = {978-3-319-29852-8 978-3-319-29854-2},
	url = {http://link.springer.com/10.1007/978-3-319-29854-2},
	language = {en},
	urldate = {2021-12-08},
	publisher = {Springer International Publishing},
	author = {Brockwell, Peter J. and Davis, Richard A.},
	year = {2016},
	doi = {10.1007/978-3-319-29854-2},
	file = {Brockwell and Davis - 2016 - Introduction to Time Series and Forecasting.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XF6XEZD2\\Brockwell and Davis - 2016 - Introduction to Time Series and Forecasting.pdf:application/pdf;Introduction to Time Series and Forecasting.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\Introduction to Time Series and Forecasting.md:text/plain},
}

@book{boxTimeSeriesAnalysis2016,
	address = {Hoboken, New Jersey},
	edition = {Fifth edition},
	series = {Wiley series in probabilit and statistics},
	title = {Time series analysis: forecasting and control},
	isbn = {978-1-118-67491-8 978-1-118-67492-5},
	shorttitle = {Time series analysis},
	publisher = {John Wiley \& Sons, Inc},
	author = {Box, George E. P. and Jenkins, Gwilym M. and Reinsel, Gregory C. and Ljung, Greta M.},
	year = {2016},
	keywords = {Feedback control systems, Mathematical models, Prediction theory, Time-series analysis, Transfer functions},
	file = {Time series analysis forecasting and control.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\Time series analysis forecasting and control.md:text/plain},
}

@article{xiaAttentionNeuralCollaboration2021,
	title = {Attention neural collaboration filtering based on {GRU} for recommender systems},
	volume = {7},
	issn = {2199-4536, 2198-6053},
	url = {https://link.springer.com/10.1007/s40747-021-00274-4},
	doi = {10.1007/s40747-021-00274-4},
	abstract = {The collaborative filtering method is widely used in the traditional recommendation system. The collaborative filtering method based on matrix factorization treats the user’s preference for the item as a linear combination of the user and the item latent vectors, and cannot learn a deeper feature representation. In addition, the cold start and data sparsity remain major problems for collaborative filtering. To tackle these problems, some scholars have proposed to use deep neural network to extract text information, but did not consider the impact of long-distance dependent information and key information on their models. In this paper, we propose a neural collaborative filtering recommender method that integrates user and item auxiliary information. This method fully integrates user-item rating information, user assistance information and item text assistance information for feature extraction. First, Stacked Denoising Auto Encoder is used to extract user features, and Gated Recurrent Unit with auxiliary information is used to extract items’ latent vectors, respectively. The attention mechanism is used to learn key information when extracting text features. Second, the latent vectors learned by deep learning techniques are used in multi-layer nonlinear networks to learn more abstract and deeper feature representations to predict user preferences. According to the verification results on the MovieLens data set, the proposed model outperforms other traditional approaches and deep learning models making it state of the art.},
	language = {en},
	number = {3},
	urldate = {2021-12-09},
	journal = {Complex \& Intelligent Systems},
	author = {Xia, Hongbin and Luo, Yang and Liu, Yuan},
	month = jun,
	year = {2021},
	pages = {1367--1379},
	file = {Xia et al. - 2021 - Attention neural collaboration filtering based on .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ALFXY5ZL\\Xia et al. - 2021 - Attention neural collaboration filtering based on .pdf:application/pdf},
}

@article{aljuhaniComparisonSentimentAnalysis2019,
	title = {A {Comparison} of {Sentiment} {Analysis} {Methods} on {Amazon} {Reviews} of {Mobile} {Phones}},
	volume = {10},
	issn = {21565570, 2158107X},
	url = {http://thesai.org/Publications/ViewPaper?Volume=10&Issue=6&Code=IJACSA&SerialNo=78},
	doi = {10.14569/IJACSA.2019.0100678},
	abstract = {The consumer reviews serve as feedback for businesses in terms of performance, product quality, and consumer service. In this research, we predict consumer opinion based on mobile phone reviews, in addition to providing an analysis of the most important factors behind reviews being classiﬁed as either positive, negative, or neutral. This insight could help companies improve their products as well as helping potential buyers to make the right decision. The research presented in this paper was carried out as follows: the data was pre-processed, before being converted from text to vector representation using a range of feature extraction techniques such as bag-of-words, TF-IDF, Glove, and word2vec. We study the performance of different machine learning algorithms, such as logistic regression, stochastic gradient descent, naive Bayes and convolutional neural networks. In addition, we evaluate our models using accuracy, F1score, precision, recall and log loss function. Moreover, we apply Lime technique to provide analytical reasons for the reviews being classiﬁed as either positive, negative or neutral. Our experiments revealed that convolutional neural network with word2vec as a feature extraction technique provides the best results for both the unbalanced and balanced versions of the dataset.},
	language = {en},
	number = {6},
	urldate = {2021-12-09},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Aljuhani, Sara Ashour and Saleh, Norah},
	year = {2019},
	file = {Aljuhani and Saleh - 2019 - A Comparison of Sentiment Analysis Methods on Amaz.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9MWXXT92\\Aljuhani and Saleh - 2019 - A Comparison of Sentiment Analysis Methods on Amaz.pdf:application/pdf},
}

@article{mohseniMultidisciplinarySurveyFramework2021,
	title = {A {Multidisciplinary} {Survey} and {Framework} for {Design} and {Evaluation} of {Explainable} {AI} {Systems}},
	volume = {11},
	issn = {2160-6455, 2160-6463},
	url = {https://dl.acm.org/doi/10.1145/3387166},
	doi = {10.1145/3387166},
	abstract = {The need for interpretable and accountable intelligent systems grows along with the prevalence of
              artificial intelligence
              (
              AI
              ) applications used in everyday life.
              Explainable AI
              (
              XAI
              ) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
	language = {en},
	number = {3-4},
	urldate = {2021-12-09},
	journal = {ACM Transactions on Interactive Intelligent Systems},
	author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
	month = dec,
	year = {2021},
	pages = {1--45},
	file = {Mohseni et al. - 2021 - A Multidisciplinary Survey and Framework for Desig.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZNGTG4XG\\Mohseni et al. - 2021 - A Multidisciplinary Survey and Framework for Desig.pdf:application/pdf},
}

@inproceedings{tianSurveyPersonalizedRecommendation2020,
	address = {Xiamen China},
	title = {A {Survey} of {Personalized} {Recommendation} {Based} on {Machine} {Learning} {Algorithms}},
	isbn = {978-1-4503-8781-1},
	url = {https://dl.acm.org/doi/10.1145/3443467.3444711},
	doi = {10.1145/3443467.3444711},
	abstract = {Personalized recommendation is a key technology to effectively solve the overload of online information and eliminate information islands. It is widely known as an important way to improve the quality of information services. However, cold start, data sparseness, algorithm performance, recommendation accuracy and surprise are still the key issues that restrict users' personalized recommendations. Firstly, we review the development trend of personalized information recommendation algorithms in the past 15 years. And then we propose a new classification method for users’ personalized recommendation based on machine learning algorithms with cold start, data sparseness, and the performance of the algorithm as the main goals. On this basis, we summarize and compare the ideas, practices and conclusions of related machine learning algorithms. Finally, we further summarize the main advantages and disadvantages of the 10 kinds of personalized recommendation algorithms from the perspective of classification proposed, and look forward to the development directions, difficulties, focus and methods of personalized recommendation algorithms.},
	language = {en},
	urldate = {2021-12-09},
	booktitle = {Proceedings of the 2020 4th {International} {Conference} on {Electronic} {Information} {Technology} and {Computer} {Engineering}},
	publisher = {ACM},
	author = {Tian, Luogeng and Yang, Bailong and Yin, Xinli and Su, Yang},
	month = nov,
	year = {2020},
	pages = {602--610},
	file = {Tian et al. - 2020 - A Survey of Personalized Recommendation Based on M.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L34K7CHY\\Tian et al. - 2020 - A Survey of Personalized Recommendation Based on M.pdf:application/pdf},
}

@article{usictggsipudelhi110078indiaAnalysisAmazonProduct2019,
	title = {Analysis of {Amazon} {Product} {Reviews} {Using} {Big} {Data}- {Apache} {Pig} {Tool}},
	volume = {11},
	issn = {20749023, 20749031},
	url = {http://www.mecs-press.org/ijieeb/ijieeb-v11-n1/v11n1-2.html},
	doi = {10.5815/ijieeb.2019.01.02},
	abstract = {We live in the era of digital technologies where data is increasing day by day at a very high rate. The data is further popularly classified as ‘Big Data’ because of its velocity, veracity, variety and its huge volume. This data could be unstructured, semi-structured or structured as it is divergent in nature. In this work, we would assess various categories of Amazon Product Reviews, the large datasets that contain around 144 million reviews in total. The datasets consists of Product reviews collected from Amazon, each having various numbers of attributes of 11 different categories. The motive of this work is to find and compare the ratings of the products during the lifespan of the product reviews. Another goal of this work is to help Amazon regarding the listing of the products in their database.},
	language = {en},
	number = {1},
	urldate = {2021-12-09},
	journal = {International Journal of Information Engineering and Electronic Business},
	author = {{USICT, GGSIPU, Delhi, 110078, India} and Pal Singh, Amrit and Singh, Gurvinder},
	month = jan,
	year = {2019},
	pages = {11--18},
	file = {USICT, GGSIPU, Delhi, 110078, India et al. - 2019 - Analysis of Amazon Product Reviews Using Big Data-.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GD7T6RJR\\USICT, GGSIPU, Delhi, 110078, India et al. - 2019 - Analysis of Amazon Product Reviews Using Big Data-.pdf:application/pdf},
}

@article{mongiaDeepLatentFactor2020,
	title = {Deep latent factor model for collaborative filtering},
	volume = {169},
	issn = {01651684},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165168419304190},
	doi = {10.1016/j.sigpro.2019.107366},
	abstract = {Latent factor models have been used widely in collaborative filtering based recommender systems. In recent years, deep learning has been successful in solving a wide variety of machine learning problems. Motivated by the success of deep learning, we propose a deeper version of latent factor model. Experiments on benchmark datasets shows that our proposed technique significantly outperforms all stateof-the-art collaborative filtering techniques.},
	language = {en},
	urldate = {2021-12-09},
	journal = {Signal Processing},
	author = {Mongia, Aanchal and Jhamb, Neha and Chouzenoux, Emilie and Majumdar, Angshul},
	month = apr,
	year = {2020},
	pages = {107366},
	file = {Mongia et al. - 2020 - Deep latent factor model for collaborative filteri.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2RDE3I7P\\Mongia et al. - 2020 - Deep latent factor model for collaborative filteri.pdf:application/pdf},
}

@incollection{xuExplainableAIBrief2019,
	address = {Cham},
	title = {Explainable {AI}: {A} {Brief} {Survey} on {History}, {Research} {Areas}, {Approaches} and {Challenges}},
	volume = {11839},
	isbn = {978-3-030-32235-9 978-3-030-32236-6},
	shorttitle = {Explainable {AI}},
	url = {http://link.springer.com/10.1007/978-3-030-32236-6_51},
	abstract = {Deep learning has made signiﬁcant contribution to the recent progress in artiﬁcial intelligence. In comparison to traditional machine learning methods such as decision trees and support vector machines, deep learning methods have achieved substantial improvement in various prediction tasks. However, deep neural networks (DNNs) are comparably weak in explaining their inference processes and ﬁnal results, and they are typically treated as a black-box by both developers and users. Some people even consider DNNs (deep neural networks) in the current stage rather as alchemy, than as real science. In many real-world applications such as business decision, process optimization, medical diagnosis and investment recommendation, explainability and transparency of our AI systems become particularly essential for their users, for the people who are aﬀected by AI decisions, and furthermore, for the researchers and developers who create the AI solutions. In recent years, the explainability and explainable AI have received increasing attention by both research community and industry. This paper ﬁrst introduces the history of Explainable AI, starting from expert systems and traditional machine learning approaches to the latest progress in the context of modern deep learning, and then describes the major research areas and the state-ofart approaches in recent years. The paper ends with a discussion on the challenges and future directions.},
	language = {en},
	urldate = {2021-12-09},
	booktitle = {Natural {Language} {Processing} and {Chinese} {Computing}},
	publisher = {Springer International Publishing},
	author = {Xu, Feiyu and Uszkoreit, Hans and Du, Yangzhou and Fan, Wei and Zhao, Dongyan and Zhu, Jun},
	editor = {Tang, Jie and Kan, Min-Yen and Zhao, Dongyan and Li, Sujian and Zan, Hongying},
	year = {2019},
	doi = {10.1007/978-3-030-32236-6_51},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {563--574},
	file = {Xu et al. - 2019 - Explainable AI A Brief Survey on History, Researc.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7XV8KB7E\\Xu et al. - 2019 - Explainable AI A Brief Survey on History, Researc.pdf:application/pdf},
}

@article{talExplainableNeuralAttention,
	title = {Explainable {Neural} {Attention} {Recommender} {Systems}},
	language = {en},
	author = {Tal, Omer},
	pages = {98},
	file = {Tal - Explainable Neural Attention Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FWZN3H5P\\Tal - Explainable Neural Attention Recommender Systems.pdf:application/pdf},
}

@inproceedings{hanafiExploitMultiLayer2019,
	address = {Yogyakarta, Indonesia},
	title = {Exploit {Multi} {Layer} {Deep} {Learning} and {Latent} {Factor} to {Handle} {Sparse} {Data} for {E}-commerce {Recommender} {System}:},
	isbn = {978-989-758-453-4},
	shorttitle = {Exploit {Multi} {Layer} {Deep} {Learning} and {Latent} {Factor} to {Handle} {Sparse} {Data} for {E}-commerce {Recommender} {System}},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0009910603430351},
	doi = {10.5220/0009910603430351},
	language = {en},
	urldate = {2021-12-09},
	booktitle = {Proceedings of the {International} {Conferences} on {Information} {System} and {Technology}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Hanafi, . and Basari, Abdul Samad Hasan},
	year = {2019},
	pages = {343--351},
	file = {Hanafi and Basari - 2019 - Exploit Multi Layer Deep Learning and Latent Facto.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZYKT44SJ\\Hanafi and Basari - 2019 - Exploit Multi Layer Deep Learning and Latent Facto.pdf:application/pdf},
}

@article{chenLatentFactorModels,
	title = {Latent {Factor} {Models} for {Web} {Recommender} {Systems}},
	language = {en},
	author = {Chen, Bee-Chung and Agarwal, Deepak and Elango, Pradheep and Ramakrishnan, Raghu},
	pages = {40},
	file = {Chen et al. - Latent Factor Models for Web Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RI3AAAET\\Chen et al. - Latent Factor Models for Web Recommender Systems.pdf:application/pdf},
}

@inproceedings{patinoLeveragingUserEmbeddings2020,
	address = {Virtual Event Brazil},
	title = {Leveraging {User} {Embeddings} and {Text} to {Improve} {CTR} {Predictions} {With} {Deep} {Recommender} {Systems}},
	isbn = {978-1-4503-8835-1},
	url = {https://dl.acm.org/doi/10.1145/3415959.3415995},
	doi = {10.1145/3415959.3415995},
	abstract = {Predicting user engagement, often framed as a CTR prediction problem, is important to maximize user satisfaction in social networks. The 2020 Recsys Challenge was sponsored by Twitter and set the goal of predicting four types of user engagement using a dataset with 160 million tweets. Our approach extracted information from the tweet’s text tokens and built optimized user embeddings. We designed our model based on ideas from recommender systems and deep learning that had been successful in CTR prediction tasks. We show that our modifications to existing state-ofthe-art architectures and feature engineering improved the model’s ability to predict user engagement. Factored’s team was called Los Trinadores and had the 6th best submission of the challenge with an overall score of 22. The code for our solution is available at https://github.com/factoredai/recsys20-challenge/.},
	language = {en},
	urldate = {2021-12-09},
	booktitle = {Proceedings of the {Recommender} {Systems} {Challenge} 2020},
	publisher = {ACM},
	author = {Patiño, Carlos Miguel and Velásquez, Camilo and Muñoz, Juan Manuel and Gutiérrez, Juan Manuel and Valencia, David Ricardo and Bartolome Aramburu, Cristian},
	month = sep,
	year = {2020},
	pages = {11--15},
	file = {Patiño et al. - 2020 - Leveraging User Embeddings and Text to Improve CTR.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YMZEH63H\\Patiño et al. - 2020 - Leveraging User Embeddings and Text to Improve CTR.pdf:application/pdf},
}

@inproceedings{guesmiOndemandPersonalizedExplanation2021,
	address = {Utrecht Netherlands},
	title = {On-demand {Personalized} {Explanation} for {Transparent} {Recommendation}},
	isbn = {978-1-4503-8367-7},
	url = {https://dl.acm.org/doi/10.1145/3450614.3464479},
	doi = {10.1145/3450614.3464479},
	abstract = {The literature on explainable recommendations is already rich. In this paper, we aim to shed light on an aspect that remains underexplored in this area of research, namely providing personalized explanations. To address this gap, we developed a transparent Recommendation and Interest Modeling Application (RIMA) that provides on-demand personalized explanations with varying levels of detail to meet the demands of different types of end-users. The results of a preliminary qualitative user study demonstrated potential benefits in terms of user satisfaction with the explainable recommender system. Our work would contribute to the literature on explainable recommendation by exploring the potential of on-demand personalized explanations, and contribute to the practice by offering suggestions for the design and appropriate use of personalized explanation interfaces in recommender systems.},
	language = {en},
	urldate = {2021-12-09},
	booktitle = {Adjunct {Proceedings} of the 29th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {ACM},
	author = {Guesmi, Mouadh and Chatti, Mohamed Amine and Vorgerd, Laura and Joarder, Shoeb and Zumor, Shadi and Sun, Yiqi and Ji, Fangzheng and Muslim, Arham},
	month = jun,
	year = {2021},
	pages = {246--252},
	file = {Guesmi et al. - 2021 - On-demand Personalized Explanation for Transparent.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\J983CN7G\\Guesmi et al. - 2021 - On-demand Personalized Explanation for Transparent.pdf:application/pdf},
}

@inproceedings{liDeleteRetrieveGenerate2018,
	address = {New Orleans, Louisiana},
	title = {Delete, {Retrieve}, {Generate}: a {Simple} {Approach} to {Sentiment} and {Style} {Transfer}},
	shorttitle = {Delete, {Retrieve}, {Generate}},
	url = {http://aclweb.org/anthology/N18-1169},
	doi = {10.18653/v1/N18-1169},
	abstract = {We consider the task of text attribute transfer: transforming a sentence to alter a speciﬁc attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., changing “screen is just the right size” to “screen is too small”). Our training data includes only sentences labeled with their attribute (e.g., positive or negative), but not pairs of sentences that differ only in their attributes, so we must learn to disentangle attributes from attributeindependent content in an unsupervised way. Previous work using adversarial methods has struggled to produce high-quality outputs. In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., “too small”). Our strongest method extracts content words by deleting phrases associated with the sentence’s original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to ﬂuently combine these into a ﬁnal output. On human evaluation, our best method generates grammatical and appropriate responses on 22\% more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous.},
	language = {en},
	urldate = {2021-12-09},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Human} {Language}           {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Juncen and Jia, Robin and He, He and Liang, Percy},
	year = {2018},
	keywords = {xrec},
	pages = {1865--1874},
	file = {Li et al. - 2018 - Delete, Retrieve, Generate a Simple Approach to S.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YG9DATL2\\Li et al. - 2018 - Delete, Retrieve, Generate a Simple Approach to S.pdf:application/pdf},
}

@misc{mansimovGeneralizedFrameworkSequence2020,
	title = {A {Generalized} {Framework} of {Sequence} {Generation} with {Application} to {Undirected} {Sequence} {Models}},
	url = {http://arxiv.org/abs/1905.12790},
	abstract = {Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as questionanswering and natural language inference. The problem of generating sequences directly from these models has received relatively little attention, in part because generating from undirected models departs signiﬁcantly from conventional monotonic generation in directed sequence models. We investigate this problem by proposing a generalized model of sequence generation that uniﬁes decoding in directed and undirected models. The proposed framework models the process of generation rather than the resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and reﬁnementbased non-autoregressive models. This uniﬁcation enables us to adapt decoding algorithms originally developed for directed sequence models to undirected sequence models. We demonstrate this by evaluating various handcrafted and learned decoding strategies on a BERT-like machine translation model (Lample \& Conneau, 2019). The proposed approach achieves constant-time translation results on par with linear-time translation results from the same undirected sequence model, while both are competitive with the state-of-theart on WMT’14 English-German translation.},
	language = {en},
	urldate = {2021-12-09},
	author = {Mansimov, Elman and Wang, Alex and Welleck, Sean and Cho, Kyunghyun},
	month = feb,
	year = {2020},
	note = {arXiv: 1905.12790},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, xrec},
	file = {Mansimov et al. - 2020 - A Generalized Framework of Sequence Generation wit.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EYBMR62B\\Mansimov et al. - 2020 - A Generalized Framework of Sequence Generation wit.pdf:application/pdf},
}

@article{erkanLexRankGraphbasedLexical2004,
	title = {{LexRank}: {Graph}-based {Lexical} {Centrality} as {Salience} in {Text} {Summarization}},
	volume = {22},
	issn = {1076-9757},
	shorttitle = {{LexRank}},
	url = {https://jair.org/index.php/jair/article/view/10396},
	doi = {10.1613/jair.1523},
	abstract = {We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically deﬁned in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in ﬁrst place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.},
	language = {en},
	urldate = {2021-12-09},
	journal = {Journal of Artificial Intelligence Research},
	author = {Erkan, G. and Radev, D. R.},
	month = dec,
	year = {2004},
	keywords = {xrec},
	pages = {457--479},
	file = {Erkan and Radev - 2004 - LexRank Graph-based Lexical Centrality as Salienc.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TPSWGM7Q\\Erkan and Radev - 2004 - LexRank Graph-based Lexical Centrality as Salienc.pdf:application/pdf},
}

@inproceedings{dongLearningGenerateProduct2017,
	address = {Valencia, Spain},
	title = {Learning to {Generate} {Product} {Reviews} from {Attributes}},
	url = {http://aclweb.org/anthology/E17-1059},
	doi = {10.18653/v1/E17-1059},
	abstract = {Automatically generating product reviews is a meaningful, yet not well-studied task in sentiment analysis. Traditional natural language generation methods rely extensively on hand-crafted rules and predeﬁned templates. This paper presents an attention-enhanced attribute-to-sequence model to generate product reviews for given attribute information, such as user, product, and rating. The attribute encoder learns to represent input attributes as vectors. Then, the sequence decoder generates reviews by conditioning its output on these vectors. We also introduce an attention mechanism to jointly generate reviews and align words with input attributes. The proposed model is trained end-to-end to maximize the likelihood of target product reviews given the attributes. We build a publicly available dataset for the review generation task by leveraging the Amazon book reviews and their metadata. Experiments on the dataset show that our approach outperforms baseline methods and the attention mechanism significantly improves the performance of our model.},
	language = {en},
	urldate = {2021-12-09},
	booktitle = {Proceedings of the 15th {Conference} of the {European} {Chapter} of the           {Association} for {Computational} {Linguistics}: {Volume} 1, {Long} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Dong, Li and Huang, Shaohan and Wei, Furu and Lapata, Mirella and Zhou, Ming and Xu, Ke},
	year = {2017},
	keywords = {xrec},
	pages = {623--632},
	file = {Dong et al. - 2017 - Learning to Generate Product Reviews from Attribut.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7HRNP7MR\\Dong et al. - 2017 - Learning to Generate Product Reviews from Attribut.pdf:application/pdf},
}

@inproceedings{liDiversityPromotingObjectiveFunction2016,
	address = {San Diego, California},
	title = {A {Diversity}-{Promoting} {Objective} {Function} for {Neural} {Conversation} {Models}},
	url = {http://aclweb.org/anthology/N16-1014},
	doi = {10.18653/v1/N16-1014},
	abstract = {Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., I don’t know) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations.},
	language = {en},
	urldate = {2021-12-09},
	booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
	year = {2016},
	keywords = {xrec},
	pages = {110--119},
	file = {Li et al. - 2016 - A Diversity-Promoting Objective Function for Neura.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\23KQZ2V6\\Li et al. - 2016 - A Diversity-Promoting Objective Function for Neura.pdf:application/pdf},
}

@inproceedings{xianEX3ExplainableAttributeaware2021,
	address = {Amsterdam Netherlands},
	title = {{EX3}: {Explainable} {Attribute}-aware {Item}-set {Recommendations}},
	isbn = {978-1-4503-8458-2},
	shorttitle = {{EX3}},
	url = {https://dl.acm.org/doi/10.1145/3460231.3474240},
	doi = {10.1145/3460231.3474240},
	abstract = {Existing recommender systems in the e-commerce domain primarily focus on generating a set of relevant items as recommendations; however, few existing systems utilize underlying item attributes as a key organizing principle in presenting recommendations to users. Mining important attributes of items from customer perspectives and presenting them along with item sets as recommendations can provide users more explainability and help them make better purchase decision. In this work, we generalize the attribute-aware item-set recommendation problem, and develop a new approach to generate sets of items (recommendations) with corresponding important attributes (explanations) that can best justify why the items are recommended to users. In particular, we propose a system that learns important attributes from historical user behavior to derive item set recommendations, so that an organized view of recommendations and their attribute-driven explanations can help users more easily understand how the recommendations relate to their preferences. Our approach is geared towards real world scenarios: we expect a solution to be scalable to billions of items, and be able to learn item and attribute relevance automatically from user behavior without human annotations. To this end, we propose a multi-step learning-based framework called Extract-Expect-Explain (EX3), which is able to adaptively select recommended items and important attributes for users. We experiment on a large-scale realworld benchmark and the results show that our model outperforms state-of-the-art baselines by an 11.35\% increase on NDCG with adaptive explainability for item set recommendation.},
	language = {en},
	urldate = {2021-12-09},
	booktitle = {Fifteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Xian, Yikun and Zhao, Tong and Li, Jin and Chan, Jim and Kan, Andrey and Ma, Jun and Dong, Xin Luna and Faloutsos, Christos and Karypis, George and Muthukrishnan, S. and Zhang, Yongfeng},
	month = sep,
	year = {2021},
	pages = {484--494},
	file = {Xian et al. - 2021 - EX3 Explainable Attribute-aware Item-set Recommen.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HRXY2SHQ\\Xian et al. - 2021 - EX3 Explainable Attribute-aware Item-set Recommen.pdf:application/pdf},
}

@misc{ValueGettingPersonalization,
	title = {The value of getting personalization right--or wrong--is multiplying {\textbar} {McKinsey}},
	url = {https://www.mckinsey.com/business-functions/marketing-and-sales/our-insights/the-value-of-getting-personalization-right-or-wrong-is-multiplying#},
	urldate = {2021-12-11},
	file = {The value of getting personalization right--or wrong--is multiplying | McKinsey:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T5BQY68H\\the-value-of-getting-personalization-right-or-wrong-is-multiplying.html:text/html},
}

@misc{zotero-5307,
}

@inproceedings{sofiauniversityst.klimentohridskibulgariaOverviewNLPTechniques2019,
	title = {Overview on {NLP} {Techniques} for {Content}-{Based} {Recommender} {Systems} for {Books}},
	url = {https://acl-bg.org/proceedings/2019/RANLPStud%202019/pdf/RANLPStud009.pdf},
	doi = {10.26615/issn.2603-2821.2019_009},
	abstract = {Recommender systems are an essential part of today’s largest websites. Without them, it would be hard for users to ﬁnd the right products and content. One of the most popular methods for recommendations is contentbased ﬁltering. It relies on analysing product metadata, a great part of which is textual data. Despite their frequent use, there is still no standard procedure for developing and evaluating content-based recommenders. In this paper, we ﬁrst examine current approaches for designing, training and evaluating recommender systems based on textual data for books recommendations for the GoodReads website. We examine critically existing methods and suggest how natural language techniques could be employed for the improvement of content-based recommenders.},
	language = {en},
	urldate = {2021-12-12},
	booktitle = {Proceedings of the {Student} {Research} {Workshop} {Associated} with {RANLP} 2019},
	publisher = {Incoma Ltd.},
	author = {{Sofia University ”St. Kliment Ohridski”, Bulgaria} and Berbatova, Melania},
	month = sep,
	year = {2019},
	pages = {55--61},
	file = {Sofia University ”St. Kliment Ohridski”, Bulgaria and Berbatova - 2019 - Overview on NLP Techniques for Content-Based Recom.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XSL989LK\\Sofia University ”St. Kliment Ohridski”, Bulgaria and Berbatova - 2019 - Overview on NLP Techniques for Content-Based Recom.pdf:application/pdf},
}

@inproceedings{heUnsupervisedNeuralAttention2017,
	address = {Vancouver, Canada},
	title = {An {Unsupervised} {Neural} {Attention} {Model} for {Aspect} {Extraction}},
	url = {http://aclweb.org/anthology/P17-1036},
	doi = {10.18653/v1/P17-1036},
	abstract = {Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.},
	language = {en},
	urldate = {2021-12-12},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for           {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
	year = {2017},
	pages = {388--397},
	file = {He et al. - 2017 - An Unsupervised Neural Attention Model for Aspect .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LWQIARWM\\He et al. - 2017 - An Unsupervised Neural Attention Model for Aspect .pdf:application/pdf},
}

@article{sunUnsupervisedAspectAwareRecommendation2022,
	title = {An {Unsupervised} {Aspect}-{Aware} {Recommendation} {Model} with {Explanation} {Text} {Generation}},
	volume = {40},
	issn = {1046-8188, 1558-2868},
	url = {https://dl.acm.org/doi/10.1145/3483611},
	doi = {10.1145/3483611},
	abstract = {Review based recommendation utilizes both users’ rating records and the associated reviews for recommendation. Recently, with the rapid demand for explanations of recommendation results, reviews are used to train the encoder–decoder models for explanation text generation. As most of the reviews are general text without detailed evaluation, some researchers leveraged auxiliary information of users or items to enrich the generated explanation text. Nevertheless, the auxiliary data is not available in most scenarios and may suffer from data privacy problems. In this article, we argue that the reviews contain abundant semantic information to express the users’ feelings for various aspects of items, while these information are not fully explored in current explanation text generation task. To this end, we study how to generate more fine-grained explanation text in review based recommendation without any auxiliary data. Though the idea is simple, it is non-trivial since the aspect is hidden and unlabeled. Besides, it is also very challenging to inject aspect information for generating explanation text with noisy review input. To solve these challenges, we first leverage an advanced unsupervised neural aspect extraction model to learn the aspect-aware representation of each review sentence. Thus, users and items can be represented in the aspect space based on their historical associated reviews. After that, we detail how to better predict ratings and generate explanation text with the user and item representations in the aspect space. We further dynamically assign review sentences which contain larger proportion of aspect words with larger weights to control the text generation process, and jointly optimize rating prediction accuracy and explanation text generation quality with a multi-task learning framework. Finally, extensive experimental results on three real-world datasets demonstrate the superiority of our proposed model for both recommendation accuracy and explainability.},
	language = {en},
	number = {3},
	urldate = {2021-12-12},
	journal = {ACM Transactions on Information Systems},
	author = {Sun, Peijie and Wu, Le and Zhang, Kun and Su, Yu and Wang, Meng},
	month = jul,
	year = {2022},
	pages = {1--29},
	file = {Sun et al. - 2022 - An Unsupervised Aspect-Aware Recommendation Model .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FISPEIC5\\Sun et al. - 2022 - An Unsupervised Aspect-Aware Recommendation Model .pdf:application/pdf},
}

@inproceedings{panExplainableRecommendationInterpretable2020,
	address = {Yokohama, Japan},
	title = {Explainable {Recommendation} via {Interpretable} {Feature} {Mapping} and {Evaluation} of {Explainability}},
	isbn = {978-0-9992411-6-5},
	url = {https://www.ijcai.org/proceedings/2020/373},
	doi = {10.24963/ijcai.2020/373},
	abstract = {Latent factor collaborative ﬁltering (CF) has been a widely used technique for recommender system by learning the semantic representations of users and items. Recently, explainable recommendation has attracted much attention from research community. However, trade-off exists between explainability and performance of the recommendation where metadata is often needed to alleviate the dilemma. We present a novel feature mapping approach that maps the uninterpretable general features onto the interpretable aspect features, achieving both satisfactory accuracy and explainability in the recommendations by simultaneous minimization of rating prediction loss and interpretation loss. To evaluate the explainability, we propose two new evaluation metrics speciﬁcally designed for aspect-level explanation using surrogate ground truth. Experimental results demonstrate a strong performance in both recommendation and explaining explanation, eliminating the need for metadata. Code is available from https://github.com/pd90506/AMCF.},
	language = {en},
	urldate = {2021-12-12},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Pan, Deng and Li, Xiangrui and Li, Xin and Zhu, Dongxiao},
	month = jul,
	year = {2020},
	pages = {2690--2696},
	file = {Pan et al. - 2020 - Explainable Recommendation via Interpretable Featu.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6IRZ3FS9\\Pan et al. - 2020 - Explainable Recommendation via Interpretable Featu.pdf:application/pdf},
}

@inproceedings{zhuFaithfullyExplainableRecommendation2021,
	address = {Online},
	title = {Faithfully {Explainable} {Recommendation} via {Neural} {Logic} {Reasoning}},
	url = {https://www.aclweb.org/anthology/2021.naacl-main.245},
	doi = {10.18653/v1/2021.naacl-main.245},
	language = {en},
	urldate = {2021-12-12},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Yaxin and Xian, Yikun and Fu, Zuohui and de Melo, Gerard and Zhang, Yongfeng},
	year = {2021},
	pages = {3083--3090},
	file = {Zhu et al. - 2021 - Faithfully Explainable Recommendation via Neural L.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CVNELHJW\\Zhu et al. - 2021 - Faithfully Explainable Recommendation via Neural L.pdf:application/pdf},
}

@book{montgomeryIntroductionTimeSeries2015,
	address = {Hoboken, New Jersey},
	edition = {Second edition},
	series = {Wiley series in probability and statistics},
	title = {Introduction to time series analysis and forecasting},
	isbn = {978-1-118-74511-3},
	publisher = {Wiley},
	author = {Montgomery, Douglas C. and Jennings, Cheryl L. and Kulahci, Murat},
	year = {2015},
	keywords = {Forecasting, Time-series analysis},
	file = {Introduction to time series analysis and forecasting.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\Introduction to time series analysis and forecasting.md:text/plain},
}

@inproceedings{NIPS2000_728f206c,
	title = {A neural probabilistic language model},
	volume = {13},
	url = {https://proceedings.neurips.cc/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {MIT Press},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal},
	editor = {Leen, T. and Dietterich, T. and Tresp, V.},
	year = {2001},
	file = {A neural probabilistic language model.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\A neural probabilistic language model.md:text/plain},
}

@article{collobertNaturalLanguageProcessing2011,
	title = {Natural {Language} {Processing} (almost) from {Scratch}},
	url = {http://arxiv.org/abs/1103.0398},
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
	urldate = {2021-12-13},
	journal = {arXiv:1103.0398 [cs]},
	author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	month = mar,
	year = {2011},
	note = {arXiv: 1103.0398},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LDQFZUTK\\1103.html:text/html;Collobert et al_2011_Natural Language Processing (almost) from Scratch.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BSFWQT33\\Collobert et al_2011_Natural Language Processing (almost) from Scratch.pdf:application/pdf;Natural Language Processing (almost) from Scratch.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\Natural Language Processing (almost) from Scratch.md:text/plain},
}

@book{jurafskySpeechLanguageProcessing2009,
	address = {Upper Saddle River, N.J},
	edition = {2nd ed},
	series = {Prentice {Hall} series in artificial intelligence},
	title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
	isbn = {978-0-13-187321-6},
	shorttitle = {Speech and language processing},
	publisher = {Pearson Prentice Hall},
	author = {Jurafsky, Dan and Martin, James H.},
	year = {2009},
	note = {OCLC: 213375806},
	keywords = {Automatic speech recognition, Computational linguistics},
	file = {Speech and Language Processing (3rd ed. draft).pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Data Science\\Natural Language Processing\\Speech and Language Processing (3rd ed. draft).pdf:application/pdf;Speech and language processing an introduction to natural language processing, computational linguistics, and speech recognition.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\Speech and language processing an introduction to natural language processing, computational linguistics, and speech recognition.md:text/plain},
}

@inproceedings{guoEnhancedDoublyRobust2021,
	address = {Virtual Event Canada},
	title = {Enhanced {Doubly} {Robust} {Learning} for {Debiasing} {Post}-{Click} {Conversion} {Rate} {Estimation}},
	isbn = {978-1-4503-8037-9},
	url = {https://dl.acm.org/doi/10.1145/3404835.3462917},
	doi = {10.1145/3404835.3462917},
	language = {en},
	urldate = {2021-12-15},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Guo, Siyuan and Zou, Lixin and Liu, Yiding and Ye, Wenwen and Cheng, Suqi and Wang, Shuaiqiang and Chen, Hechang and Yin, Dawei and Chang, Yi},
	month = jul,
	year = {2021},
	pages = {275--284},
	file = {Guo et al_2021_Enhanced Doubly Robust Learning for Debiasing Post-Click Conversion Rate.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6T7K54QI\\Guo et al_2021_Enhanced Doubly Robust Learning for Debiasing Post-Click Conversion Rate.pdf:application/pdf;Guo et al. - 2021 - Enhanced Doubly Robust Learning for Debiasing Post.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BQXFCKKD\\Guo et al. - 2021 - Enhanced Doubly Robust Learning for Debiasing Post.pdf:application/pdf},
}

@inproceedings{baoGMCMGraphbasedMicrobehavior2020,
	address = {Virtual Event China},
	title = {{GMCM}: {Graph}-based {Micro}-behavior {Conversion} {Model} for {Post}-click {Conversion} {Rate} {Estimation}},
	isbn = {978-1-4503-8016-4},
	shorttitle = {{GMCM}},
	url = {https://dl.acm.org/doi/10.1145/3397271.3401425},
	doi = {10.1145/3397271.3401425},
	language = {en},
	urldate = {2021-12-15},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Bao, Wentian and Wen, Hong and Li, Sha and Liu, Xiao-Yang and Lin, Quan and Yang, Keping},
	month = jul,
	year = {2020},
	pages = {2201--2210},
	file = {Bao et al. - 2020 - GMCM Graph-based Micro-behavior Conversion Model .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9PBGGMRT\\Bao et al. - 2020 - GMCM Graph-based Micro-behavior Conversion Model .pdf:application/pdf},
}

@book{kingsnorthDigitalMarketingStrategy2019,
	address = {London ; New York},
	edition = {Second edition},
	title = {Digital marketing strategy: an integrated approach to online marketing},
	isbn = {978-0-7494-9808-5 978-0-7494-8422-4},
	shorttitle = {Digital marketing strategy},
	publisher = {Kogan Page Ltd},
	author = {Kingsnorth, Simon},
	year = {2019},
	keywords = {Management, Electronic commerce, Internet marketing, Strategic planning},
	file = {Digital marketing strategy an integrated approach to online marketing.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\Digital marketing strategy an integrated approach to online marketing.md:text/plain},
}

@article{gharibshahUserResponsePrediction2021,
	title = {User {Response} {Prediction} in {Online} {Advertising}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3446662},
	doi = {10.1145/3446662},
	abstract = {Online advertising, as a vast market, has gained significant attention in various platforms ranging from search engines, third-party websites, social media, and mobile apps. The prosperity of online campaigns is a challenge in online marketing and is usually evaluated by user response through different metrics, such as clicks on advertisement (ad) creatives, subscriptions to products, purchases of items, or explicit user feedback through online surveys. Recent years have witnessed a significant increase in the number of studies using computational approaches, including machine learning methods, for user response prediction. However, existing literature mainly focuses on algorithmic-driven designs to solve specific challenges, and no comprehensive review exists to answer many important questions. What are the parties involved in the online digital advertising eco-systems? What type of data are available for user response prediction? How do we predict user response in a reliable and/or transparent way? In this survey, we provide a comprehensive review of user response prediction in online advertising and related recommender applications. Our essential goal is to provide a thorough understanding of online advertising platforms, stakeholders, data availability, and typical ways of user response prediction. We propose a taxonomy to categorize state-of-the-art user response prediction methods, primarily focusing on the current progress of machine learning methods used in different online platforms. In addition, we also review applications of user response prediction, benchmark datasets, and open source codes in the field.},
	language = {en},
	number = {3},
	urldate = {2021-12-16},
	journal = {ACM Computing Surveys},
	author = {Gharibshah, Zhabiz and Zhu, Xingquan},
	month = jun,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, deepcvr},
	pages = {1--43},
	file = {Gharibshah and Zhu - 2021 - User Response Prediction in Online Advertising.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2GSLDY3T\\Gharibshah and Zhu - 2021 - User Response Prediction in Online Advertising.pdf:application/pdf;Gharibshah and Zhu - 2021 - User Response Prediction in Online Advertising.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LN2L7VDK\\Gharibshah and Zhu - 2021 - User Response Prediction in Online Advertising.pdf:application/pdf;Gharibshah_Zhu_2021_User Response Prediction in Online Advertising.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FUKAN29E\\Gharibshah_Zhu_2021_User Response Prediction in Online Advertising.pdf:application/pdf;User Response Prediction in Online Advertising.md:C\:\\Users\\John\\Documents\\LearningCentre\\Data Science\\references\\User Response Prediction in Online Advertising.md:text/plain},
}

@inproceedings{hungInvestigationEffectiveUse2020,
	address = {Taipei Taiwan},
	title = {Investigation of the {Effective} {Use} of {Ensemble} {Learning} {Algorithms} for {Cyber} {Data} {Analytics} –{The} {Prediction} of the {Customer} {Revenue} on the {Google} {Merchandise} {Store} ({GStore})},
	isbn = {978-1-4503-8877-1},
	url = {https://dl.acm.org/doi/10.1145/3421682.3421690},
	doi = {10.1145/3421682.3421690},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {2020 {The} 4th {International} {Conference} on {E}-{Society}, {E}-{Education} and {E}-{Technology}},
	publisher = {ACM},
	author = {Hung, Yu-Hsin and Wang, Yi-Jie and Chang, Ray-I},
	month = aug,
	year = {2020},
	pages = {76--82},
	file = {Hung et al. - 2020 - Investigation of the Effective Use of Ensemble Lea.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QMYQ9XCG\\Hung et al. - 2020 - Investigation of the Effective Use of Ensemble Lea.pdf:application/pdf},
}

@article{chapelleSimpleScalableResponse2015,
	title = {Simple and {Scalable} {Response} {Prediction} for {Display} {Advertising}},
	volume = {5},
	issn = {2157-6904, 2157-6912},
	url = {https://dl.acm.org/doi/10.1145/2532128},
	doi = {10.1145/2532128},
	abstract = {Clickthrough and conversation rates estimation are two core predictions tasks in display advertising. We present in this article a machine learning framework based on logistic regression that is specifically designed to tackle the specifics of display advertising. The resulting system has the following characteristics: It is easy to implement and deploy, it is highly scalable (we have trained it on terabytes of data), and it provides models with state-of-the-art accuracy.},
	language = {en},
	number = {4},
	urldate = {2021-12-19},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Chapelle, Olivier and Manavoglu, Eren and Rosales, Romer},
	month = jan,
	year = {2015},
	keywords = {deepcvr},
	pages = {1--34},
	file = {Chapelle et al. - 2015 - Simple and Scalable Response Prediction for Displa.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZBIRWDUW\\Chapelle et al. - 2015 - Simple and Scalable Response Prediction for Displa.pdf:application/pdf},
}

@inproceedings{liFollowProphetAccurate2021,
	address = {Virtual Event Canada},
	title = {Follow the {Prophet}: {Accurate} {Online} {Conversion} {Rate} {Prediction} in the {Face} of {Delayed} {Feedback}},
	isbn = {978-1-4503-8037-9},
	shorttitle = {Follow the {Prophet}},
	url = {https://dl.acm.org/doi/10.1145/3404835.3463045},
	doi = {10.1145/3404835.3463045},
	abstract = {The delayed feedback problem is one of the imperative challenges in online advertising, which is caused by the highly diversified feedback delay of a conversion varying from a few minutes to several days. It is hard to design an appropriate online learning system under these non-identical delay for different types of ads and users. In this paper, we propose to tackle the delayed feedback problem in online advertising by “Following the Prophet” (FTP for short). The key insight is that, if the feedback came instantly for all the logged samples, we could get a model without delayed feedback, namely the “prophet”. Although the prophet cannot be obtained during online learning, we show that we could predict the prophet’s predictions by an aggregation policy on top of a set of multi-task predictions, where each task captures the feedback patterns of different periods. We propose the objective and optimization approach for the policy, and use the logged data to imitate the prophet. Extensive experiments on three real-world advertising datasets show that our method outperforms the previous state-of-the-art baselines.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Li, Haoming and Pan, Feiyang and Ao, Xiang and Yang, Zhao and Lu, Min and Pan, Junwei and Liu, Dapeng and Xiao, Lei and He, Qing},
	month = jul,
	year = {2021},
	keywords = {deepcvr},
	pages = {1915--1919},
	file = {Li et al. - 2021 - Follow the Prophet Accurate Online Conversion Rat.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\55F2T4J5\\Li et al. - 2021 - Follow the Prophet Accurate Online Conversion Rat.pdf:application/pdf},
}

@article{yangCapturingDelayedFeedback,
	title = {Capturing {Delayed} {Feedback} in {Conversion} {Rate} {Prediction} via {Elapsed}-{Time} {Sampling}},
	abstract = {Conversion rate (CVR) prediction is one of the most critical tasks for digital display advertising. Commercial systems often require to update models in an online learning manner to catch up with the evolving data distribution. However, conversions usually do not happen immediately after user clicks. This may result in inaccurate labeling, which is called delayed feedback problem. In previous studies, delayed feedback problem is handled either by waiting positive label for a long period of time, or by consuming the negative sample on its arrival and then insert a positive duplicate when conversion happens later. Indeed, there is a trade-off between waiting for more accurate labels and utilizing fresh data, which is not considered in existing works. To strike a balance in this trade-off, we propose Elapsed-Time Sampling Delayed Feedback Model (ES-DFM), which models the relationship between the observed conversion distribution and the true conversion distribution. Then we optimize the expectation of true conversion distribution via importance sampling under the elapsed-time sampling distribution. We further estimate the importance weight for each instance, which is used as the weight of loss function in CVR prediction. To demonstrate the effectiveness of ES-DFM, we conduct extensive experiments on a public data and a private industrial dataset. Experimental results conﬁrm that our method consistently outperforms the previous state-of-the-art results.},
	language = {en},
	author = {Yang, Jia-Qi and Li, Xiang and Han, Shuguang and Zhuang, Tao and Zhan, De-Chuan and Zeng, Xiaoyi and Tong, Bin},
	keywords = {Computer Science - Machine Learning},
	pages = {8},
	file = {Yang et al. - 2021 - Capturing Delayed Feedback in Conversion Rate Pred.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9SKA4DWW\\Yang et al. - 2021 - Capturing Delayed Feedback in Conversion Rate Pred.pdf:application/pdf;Yang et al. - Capturing Delayed Feedback in Conversion Rate Pred.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\67FVXTWA\\Yang et al. - Capturing Delayed Feedback in Conversion Rate Pred.pdf:application/pdf},
}

@inproceedings{zadroznyLearningEvaluatingClassifiers2004,
	address = {Banff, Alberta, Canada},
	title = {Learning and evaluating classifiers under sample selection bias},
	url = {http://portal.acm.org/citation.cfm?doid=1015330.1015425},
	doi = {10.1145/1015330.1015425},
	abstract = {Classiﬁer learning methods commonly assume that the training data consist of randomly drawn examples from the same distribution as the test examples about which the learned model is expected to make predictions. In many practical situations, however, this assumption is violated, in a problem known in econometrics as sample selection bias. In this paper, we formalize the sample selection bias problem in machine learning terms and study analytically and experimentally how a number of well-known classiﬁer learning methods are aﬀected by it. We also present a bias correction method that is particularly useful for classiﬁer evaluation under sample selection bias.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Twenty-first international conference on {Machine} learning  - {ICML} '04},
	publisher = {ACM Press},
	author = {Zadrozny, Bianca},
	year = {2004},
	keywords = {deepcvr},
	pages = {114},
	file = {Zadrozny - 2004 - Learning and evaluating classifiers under sample s.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YT3K82XM\\Zadrozny - 2004 - Learning and evaluating classifiers under sample s.pdf:application/pdf},
}

@inproceedings{wenEntireSpaceMultiTask2020,
	address = {Virtual Event China},
	title = {Entire {Space} {Multi}-{Task} {Modeling} via {Post}-{Click} {Behavior} {Decomposition} for {Conversion} {Rate} {Prediction}},
	isbn = {978-1-4503-8016-4},
	url = {https://dl.acm.org/doi/10.1145/3397271.3401443},
	doi = {10.1145/3397271.3401443},
	abstract = {Recommender system, as an essential part of modern e-commerce, consists of two fundamental modules, namely Click-Through Rate (CTR) and Conversion Rate (CVR) prediction. While CVR has a direct impact on the purchasing volume, its prediction is wellknown challenging due to the Sample Selection Bias (SSB) and Data Sparsity (DS) issues. Although existing methods, typically built on the user sequential behavior path “impression→click→purchase”, is effective for dealing with SSB issue, they still struggle to address the DS issue due to rare purchase training samples. Observing that users always take several purchase-related actions after clicking, we propose a novel idea of post-click behavior decomposition. Specifically, disjoint purchase-related Deterministic Action (DAction) and Other Action (OAction) are inserted between click and purchase in parallel, forming a novel user sequential behavior graph “impression→click→D(O)Action→purchase”. Defining model on this graph enables to leverage all the impression samples over the entire space and extra abundant supervised signals from D(O)Action, which will effectively address the SSB and DS issues together. To this end, we devise a novel deep recommendation model named Elaborated Entire Space Supervised Multi-task Model (𝐸𝑆𝑀2). According to the conditional probability rule defined on the graph, it employs multi-task learning to predict some decomposed sub-targets in parallel and compose them sequentially to formulate the final CVR. Extensive experiments on both offline and online environments demonstrate the superiority of 𝐸𝑆𝑀2 over state-ofthe-art models. The source code and dataset will be released.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Wen, Hong and Zhang, Jing and Wang, Yuan and Lv, Fuyu and Bao, Wentian and Lin, Quan and Yang, Keping},
	month = jul,
	year = {2020},
	keywords = {deepcvr, ali-ccp},
	pages = {2377--2386},
	file = {Wen et al. - 2020 - Entire Space Multi-Task Modeling via Post-Click Be.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4QIU6GY3\\Wen et al. - 2020 - Entire Space Multi-Task Modeling via Post-Click Be.pdf:application/pdf;Wen et al. - 2020 - Entire Space Multi-Task Modeling via Post-Click Be.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XGE6UXTQ\\Wen et al. - 2020 - Entire Space Multi-Task Modeling via Post-Click Be.pdf:application/pdf},
}

@inproceedings{suAttentionbasedModelConversion2020,
	address = {Yokohama, Japan},
	title = {An {Attention}-based {Model} for {Conversion} {Rate} {Prediction} with {Delayed} {Feedback} via {Post}-click {Calibration}},
	isbn = {978-0-9992411-6-5},
	url = {https://www.ijcai.org/proceedings/2020/487},
	doi = {10.24963/ijcai.2020/487},
	abstract = {Conversion rate (CVR) prediction is becoming increasingly important in the multi-billion dollar online display advertising industry. It has two major challenges: ﬁrstly, the scarce user history data is very complicated and non-linear; secondly, the time delay between the clicks and the corresponding conversions can be very large, e.g., ranging from seconds to weeks. Existing models usually suffer from such scarce and delayed conversion behaviors. In this paper, we propose a novel deep learning framework to tackle the two challenges. Speciﬁcally, we extract the pre-trained embedding from impressions/clicks to assist in conversion models and propose an inner/self-attention mechanism to capture the ﬁne-grained personalized product purchase interests from the sequential click data. Besides, to overcome the time-delay issue, we calibrate the delay model by learning dynamic hazard function with the abundant post-click data more in line with the real distribution. Empirical experiments with real-world user behavior data prove the effectiveness of the proposed method.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Su, Yumin and Zhang, Liang and Dai, Quanyu and Zhang, Bo and Yan, Jinyao and Wang, Dan and Bao, Yongjun and Xu, Sulong and He, Yang and Yan, Weipeng},
	month = jul,
	year = {2020},
	pages = {3522--3528},
	file = {Su et al. - 2020 - An Attention-based Model for Conversion Rate Predi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N5ADW9TX\\Su et al. - 2020 - An Attention-based Model for Conversion Rate Predi.pdf:application/pdf},
}

@inproceedings{zhangLargescaleCausalApproaches2020,
	address = {Taipei Taiwan},
	title = {Large-scale {Causal} {Approaches} to {Debiasing} {Post}-click {Conversion} {Rate} {Estimation} with {Multi}-task {Learning}},
	isbn = {978-1-4503-7023-3},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380037},
	doi = {10.1145/3366423.3380037},
	abstract = {Post-click conversion rate (CVR) estimation is a critical task in e-commerce recommender systems. This task is deemed quite challenging under industrial setting with two major issues: 1) selection bias caused by user self-selection, and 2) data sparsity due to the rare click events. A successful conversion typically has the following sequential events: "exposure → click → conversion". Conventional CVR estimators are trained in the click space, but inference is done in the entire exposure space. They fail to account for the causes of the missing data and treat them as missing at random. Hence, their estimations are highly likely to deviate from the real values by large. In addition, the data sparsity issue can also handicap many industrial CVR estimators which usually have large parameter spaces.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {ACM},
	author = {Zhang, Wenhao and Bao, Wentian and Liu, Xiao-Yang and Yang, Keping and Lin, Quan and Wen, Hong and Ramezani, Ramin},
	month = apr,
	year = {2020},
	pages = {2775--2781},
	file = {Zhang et al. - 2020 - Large-scale Causal Approaches to Debiasing Post-cl.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4GFF272C\\Zhang et al. - 2020 - Large-scale Causal Approaches to Debiasing Post-cl.pdf:application/pdf},
}

@book{ryanBestDigitalMarketing2014,
	address = {London ; Philadelphia},
	edition = {Second edition]},
	title = {The best digital marketing campaigns in the world {II}},
	isbn = {978-0-7494-6968-9},
	language = {en},
	publisher = {Kogan Page},
	author = {Ryan, Damian},
	year = {2014},
	keywords = {Social media, Management, Marketing, Internet marketing, Advertising campaigns, BUSINESS \& ECONOMICS / Advertising \& Promotion, BUSINESS \& ECONOMICS / E-Commerce / Internet Marketing, BUSINESS \& ECONOMICS / Marketing / General, Case studies},
	file = {Ryan - 2014 - The best digital marketing campaigns in the world .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GYX5QVME\\Ryan - 2014 - The best digital marketing campaigns in the world .pdf:application/pdf},
}

@book{kingsnorthDigitalMarketingStrategy2019a,
	address = {New York},
	edition = {2nd Edition},
	title = {Digital marketing strategy: an integrated approach to online marketing},
	isbn = {978-0-7494-8423-1},
	shorttitle = {Digital marketing strategy},
	language = {en},
	publisher = {Kogan Page Ltd},
	author = {Kingsnorth, Simon},
	year = {2019},
	keywords = {Management, Electronic commerce, Internet marketing, Strategic planning},
	file = {Kingsnorth - 2019 - Digital marketing strategy an integrated approach.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\735877PM\\Kingsnorth - 2019 - Digital marketing strategy an integrated approach.pdf:application/pdf},
}

@article{tallisReactingVariationsProduct2018,
	title = {Reacting to {Variations} in {Product} {Demand}: {An} {Application} for {Conversion} {Rate} ({CR}) {Prediction} in {Sponsored} {Search}},
	shorttitle = {Reacting to {Variations} in {Product} {Demand}},
	url = {http://arxiv.org/abs/1806.08211},
	abstract = {In online internet advertising, machine learning models are widely used to compute the likelihood of a user engaging with product related advertisements. However, the performance of traditional machine learning models is often impacted due to variations in user and advertiser behavior. For example, search engine traffic for florists usually tends to peak around Valentine’s day, Mother’s day, etc. To overcome, this challenge, in this manuscript we propose three models which are able to incorporate the effects arising due to variations in product demand. The proposed models are a combination of product demand features, specialized data sampling methodologies and ensemble techniques. We demonstrate the performance of our proposed models on datasets obtained from a real-world setting. Our results show that the proposed models more accurately predict the outcome of users interactions with product related advertisements while simultaneously being robust to fluctuations in user and advertiser behaviors.},
	language = {en},
	urldate = {2021-12-19},
	journal = {arXiv:1806.08211 [cs]},
	author = {Tallis, Marcelo and Yadav, Pranjul},
	month = may,
	year = {2018},
	note = {arXiv: 1806.08211},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval, Computer Science - Social and Information Networks},
	file = {Tallis and Yadav - 2018 - Reacting to Variations in Product Demand An Appli.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y2Q9XA86\\Tallis and Yadav - 2018 - Reacting to Variations in Product Demand An Appli.pdf:application/pdf},
}

@article{liAttentiveCapsuleNetwork2021,
	title = {Attentive capsule network for click-through rate and conversion rate prediction in online advertising},
	volume = {211},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705120306511},
	doi = {10.1016/j.knosys.2020.106522},
	abstract = {Estimating Click-through Rate (CTR) and Conversion Rate (CVR) are two essential user response prediction tasks in computing advertising and recommendation systems. The mainstream methods map sparse, high-dimensional categorical features (e.g., user id, item id) into low-dimensional representations with neural networks. Although they have achieved significant advancement in recent years, how to capture user’s diverse interests effectively from past behaviors is still challenging. Recently some works try using attention-based methods to learn the representation from user behavior history adaptively. However, it is insufficient to capture the diversity of user’s interests. As a step forward to improve this goal, we propose a method named Attentive Capsule Network (ACN). It uses Transformers for feature interaction and leverages capsule networks to capture multiple interests from user behavior history. To precisely obtain sequence representation related to the current advertisement, we further design a modified dynamic routing algorithm integrating with an attention mechanism. Experimental results on real-world datasets demonstrate the effectiveness of our proposed ACN with significant improvement over state-of-the-art approaches. Moreover, it also offers good explainability when extracting diverse interest points of users from behavior history.},
	language = {en},
	urldate = {2021-12-19},
	journal = {Knowledge-Based Systems},
	author = {Li, Dongfang and Hu, Baotian and Chen, Qingcai and Wang, Xiao and Qi, Quanchang and Wang, Liubin and Liu, Haishan},
	month = jan,
	year = {2021},
	pages = {106522},
	file = {Li et al. - 2021 - Attentive capsule network for click-through rate a.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NBTDT5VP\\Li et al. - 2021 - Attentive capsule network for click-through rate a.pdf:application/pdf},
}

@inproceedings{luPracticalFrameworkConversion2017,
	address = {Halifax NS Canada},
	title = {A {Practical} {Framework} of {Conversion} {Rate} {Prediction} for {Online} {Display} {Advertising}},
	isbn = {978-1-4503-5194-2},
	url = {https://dl.acm.org/doi/10.1145/3124749.3124750},
	doi = {10.1145/3124749.3124750},
	abstract = {Cost-per-action (CPA), or cost-per-acquisition, has become the primary campaign performance objective in online advertising industry. As a result, accurate conversion rate (CVR) prediction is crucial for any real-time bidding (RTB) platform. However, CVR prediction is quite challenging due to several factors, including extremely sparse conversions, delayed feedback, a ribution gaps between the platform and the third party, etc. In order to tackle these challenges, we proposed a practical framework that has been successfully deployed on Yahoo! BrightRoll, one of the largest RTB ad buying platforms. In this paper, we rst show that over-prediction and the resulted over-bidding are fundamental challenges for CPA campaigns in a real RTB environment. We then propose a safe prediction framework with conversion a ribution adjustment to handle over-predictions and to further alleviate over-bidding at di erent levels. At last, we illustrate both o ine and online experimental results to demonstrate the e ectiveness of the framework.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the {ADKDD}'17},
	publisher = {ACM},
	author = {Lu, Quan and Pan, Shengjun and Wang, Liang and Pan, Junwei and Wan, Fengdan and Yang, Hongxia},
	month = aug,
	year = {2017},
	pages = {1--9},
	file = {Lu et al. - 2017 - A Practical Framework of Conversion Rate Predictio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DKCZEWPW\\Lu et al. - 2017 - A Practical Framework of Conversion Rate Predictio.pdf:application/pdf},
}

@article{wangBillionscaleCommodityEmbedding2018,
	title = {Billion-scale {Commodity} {Embedding} for {E}-commerce {Recommendation} in {Alibaba}},
	url = {http://arxiv.org/abs/1803.02349},
	abstract = {Recommender systems (RSs) have been the most important technology for increasing the business in Taobao, the largest online consumer-to-consumer (C2C) platform in China. There are three major challenges facing RS in Taobao: scalability, sparsity and cold start. In this paper, we present our technical solutions to address these three challenges. The methods are based on a wellknown graph embedding framework. We first construct an item graph from users’ behavior history, and learn the embeddings of all items in the graph. The item embeddings are employed to compute pairwise similarities between all items, which are then used in the recommendation process. To alleviate the sparsity and cold start problems, side information is incorporated into the graph embedding framework. We propose two aggregation methods to integrate the embeddings of items and the corresponding side information. Experimental results from offline experiments show that methods incorporating side information are superior to those that do not. Further, we describe the platform upon which the embedding methods are deployed and the workflow to process the billion-scale data in Taobao. Using A/B test, we show that the online Click-Through-Rates (CTRs) are improved comparing to the previous collaborative filtering based methods widely used in Taobao, further demonstrating the effectiveness and feasibility of our proposed methods in Taobao’s live production environment.},
	language = {en},
	urldate = {2021-12-19},
	journal = {arXiv:1803.02349 [cs]},
	author = {Wang, Jizhe and Huang, Pipei and Zhao, Huan and Zhang, Zhibo and Zhao, Binqiang and Lee, Dik Lun},
	month = may,
	year = {2018},
	note = {arXiv: 1803.02349},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Wang et al. - 2018 - Billion-scale Commodity Embedding for E-commerce R.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9I37W5R7\\Wang et al. - 2018 - Billion-scale Commodity Embedding for E-commerce R.pdf:application/pdf},
}

@inproceedings{wenHierarchicallyModelingMicro2021,
	address = {Virtual Event Canada},
	title = {Hierarchically {Modeling} {Micro} and {Macro} {Behaviors} via {Multi}-{Task} {Learning} for {Conversion} {Rate} {Prediction}},
	isbn = {978-1-4503-8037-9},
	url = {https://dl.acm.org/doi/10.1145/3404835.3463053},
	doi = {10.1145/3404835.3463053},
	abstract = {Conversion Rate (CVR) prediction in modern industrial e-commerce platforms is becoming increasingly important, which directly contributes to the final revenue. In order to address the wellknown sample selection bias (SSB) and data sparsity (DS) issues encountered during CVR modeling, the abundant labeled macro behaviors (𝑖.𝑒., user’s interactions with items) are used. Nonetheless, we observe that several purchase-related micro behaviors (𝑖.𝑒., user’s interactions with specific components on the item detail page) can supplement fine-grained cues for CVR prediction. Motivated by this observation, we propose a novel CVR prediction method by Hierarchically Modeling both Micro and Macro behaviors (𝐻 𝑀3). Specifically, we first construct a complete user sequential behavior graph to hierarchically represent micro behaviors and macro behaviors as one-hop and two-hop post-click nodes. Then, we embody 𝐻 𝑀3 as a multi-head deep neural network, which predicts six probability variables corresponding to explicit sub-paths in the graph. They are further combined into the prediction targets of four auxiliary tasks as well as the final 𝐶𝑉 𝑅 according to the conditional probability rule defined on the graph. By employing multi-task learning and leveraging the abundant supervisory labels from micro and macro behaviors, 𝐻 𝑀3 can be trained end-to-end and address the SSB and DS issues. Extensive experiments on both offline and online settings demonstrate the superiority of the proposed 𝐻 𝑀3 over representative state-of-the-art methods.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Wen, Hong and Zhang, Jing and Lv, Fuyu and Bao, Wentian and Wang, Tianyi and Chen, Zulong},
	month = jul,
	year = {2021},
	pages = {2187--2191},
	file = {Wen et al. - 2021 - Hierarchically Modeling Micro and Macro Behaviors .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TBWJITGX\\Wen et al. - 2021 - Hierarchically Modeling Micro and Macro Behaviors .pdf:application/pdf},
}

@article{maEntireSpaceMultiTask2018,
	title = {Entire {Space} {Multi}-{Task} {Model}: {An} {Effective} {Approach} for {Estimating} {Post}-{Click} {Conversion} {Rate}},
	shorttitle = {Entire {Space} {Multi}-{Task} {Model}},
	url = {http://arxiv.org/abs/1804.07931},
	abstract = {Estimating post-click conversion rate (CVR) accurately is crucial for ranking systems in industrial applications such as recommendation and advertising. Conventional CVR modeling applies popular deep learning methods and achieves state-of-the-art performance. However it encounters several task-specific problems in practice, making CVR modeling challenging. For example, conventional CVR models are trained with samples of clicked impressions while utilized to make inference on the entire space with samples of all impressions. This causes a sample selection bias problem. Besides, there exists an extreme data sparsity problem, making the model fitting rather difficult. In this paper, we model CVR in a brand-new perspective by making good use of sequential pattern of user actions, i.e., impression → click → conversion. The proposed Entire Space Multi-task Model (ESMM) can eliminate the two problems simultaneously by i) modeling CVR directly over the entire space, ii) employing a feature representation transfer learning strategy. Experiments on dataset gathered from traffic logs of Taobao’s recommender system demonstrate that ESMM significantly outperforms competitive methods. We also release a sampling version of this dataset to enable future research. To the best of our knowledge, this is the first public dataset which contains samples with sequential dependence of click and conversion labels for CVR modeling.},
	language = {en},
	urldate = {2021-12-19},
	journal = {arXiv:1804.07931 [cs, stat]},
	author = {Ma, Xiao and Zhao, Liqin and Huang, Guan and Wang, Zhi and Hu, Zelin and Zhu, Xiaoqiang and Gai, Kun},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.07931},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval, deepcvr, ali-ccp},
	file = {Ma et al. - 2018 - Entire Space Multi-Task Model An Effective Approa.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PUQH22MM\\Ma et al. - 2018 - Entire Space Multi-Task Model An Effective Approa.pdf:application/pdf},
}

@misc{TianchiDataSets,
	title = {Tianchi: {Data} {Sets}},
	url = {https://tianchi.aliyun.com/dataset/dataDetail?dataId=408},
	urldate = {2021-12-19},
	keywords = {deepcvr},
	file = {Tianchi\: Data Sets:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\APN2VPC4\\dataDetail.html:text/html},
}

@article{wangDelayedFeedbackModeling2020,
	title = {Delayed {Feedback} {Modeling} for the {Entire} {Space} {Conversion} {Rate} {Prediction}},
	url = {http://arxiv.org/abs/2011.11826},
	abstract = {Estimating post-click conversion rate (CVR) accurately is crucial in E-commerce. However, CVR prediction usually suffers from three major challenges in practice: i) data sparsity: compared with impressions, conversion samples are often extremely scarce; ii) sample selection bias: conventional CVR models are trained with clicked impressions while making inference on the entire space of all impressions; iii) delayed feedback: many conversions can only be observed after a relatively long and random delay since clicks happened, resulting in many false negative labels during training. Previous studies mainly focus on one or two issues while ignoring the others. In this paper, we propose a novel neural network framework ESDF to tackle the above three challenges simultaneously. Unlike existing methods, ESDF models the CVR prediction from a perspective of entire space, and combines the advantage of user sequential behavior pattern and the time delay factor. Speciﬁcally, ESDF utilizes sequential behavior of user actions on the entire space with all impressions to alleviate the sample selection bias problem. By sharing the embedding parameters between CTR and CVR networks, data sparsity problem is greatly relieved. Different from conventional delayed feedback methods, ESDF does not make any special assumption about the delay distribution. We discretize the delay time by day slot and model the probability based on survival analysis with deep neural network, which is more practical and suitable for industrial situations. Extensive experiments are conducted to evaluate the effectiveness of our method. To the best of our knowledge, ESDF is the ﬁrst attempt to unitedly solve the above three challenges in CVR prediction area. More than that, we release a sampling version of industrial dataset to enable the future research, which is the ﬁrst public dataset that comprises impression, click and delayed conversion labels for CVR modeling.},
	language = {en},
	urldate = {2021-12-21},
	journal = {arXiv:2011.11826 [cs]},
	author = {Wang, Yanshi and Zhang, Jie and Da, Qing and Zeng, Anxiang},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.11826},
	keywords = {Computer Science - Machine Learning, deepcvr},
	file = {Wang et al. - 2020 - Delayed Feedback Modeling for the Entire Space Con.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YRYALFWW\\Wang et al. - 2020 - Delayed Feedback Modeling for the Entire Space Con.pdf:application/pdf;Wang et al. - 2020 - Delayed Feedback Modeling for the Entire Space Con.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6VYN3ANA\\Wang et al. - 2020 - Delayed Feedback Modeling for the Entire Space Con.pdf:application/pdf},
}

@article{yoshikawaNonparametricDelayedFeedback2018,
	title = {A {Nonparametric} {Delayed} {Feedback} {Model} for {Conversion} {Rate} {Prediction}},
	url = {http://arxiv.org/abs/1802.00255},
	abstract = {Predicting conversion rates (CVRs) in display advertising (e.g., predicting the proportion of users who purchase an item (i.e., a conversion) after its corresponding ad is clicked) is important when measuring the effects of ads shown to users and to understanding the interests of the users. There is generally a time delay (i.e., so-called delayed feedback) between the ad click and conversion. Owing to the delayed feedback, samples that are converted after an observation period may be treated as negative. To overcome this drawback, CVR prediction assuming that the time delay follows an exponential distribution has been proposed. In practice, however, there is no guarantee that the delay is generated from the exponential distribution, and the best distribution with which to represent the delay depends on the data. In this paper, we propose a nonparametric delayed feedback model for CVR prediction that represents the distribution of the time delay without assuming a parametric distribution, such as an exponential or Weibull distribution. Because the distribution of the time delay is modeled depending on the content of an ad and the features of a user, various shapes of the distribution can be represented potentially. In experiments, we show that the proposed model can capture the distribution for the time delay on a synthetic dataset, even when the distribution is complicated. Moreover, on a real dataset, we show that the proposed model outperforms the existing method that assumes an exponential distribution for the time delay in terms of conversion rate prediction.},
	language = {en},
	urldate = {2021-12-21},
	journal = {arXiv:1802.00255 [cs, stat]},
	author = {Yoshikawa, Yuya and Imai, Yusaku},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.00255},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Applications},
	file = {Yoshikawa and Imai - 2018 - A Nonparametric Delayed Feedback Model for Convers.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BGGHJHV2\\Yoshikawa and Imai - 2018 - A Nonparametric Delayed Feedback Model for Convers.pdf:application/pdf},
}

@inproceedings{chapelleModelingDelayedFeedback2014,
	address = {New York New York USA},
	title = {Modeling delayed feedback in display advertising},
	isbn = {978-1-4503-2956-9},
	url = {https://dl.acm.org/doi/10.1145/2623330.2623634},
	doi = {10.1145/2623330.2623634},
	abstract = {In performance display advertising a key metric of a campaign eﬀectiveness is its conversion rate – the proportion of users who take a predeﬁned action on the advertiser website, such as a purchase. Predicting this conversion rate is thus essential for estimating the value of an impression and can be achieved via machine learning. One diﬃculty however is that the conversions can take place long after the impression – up to a month – and this delayed feedback hinders the conversion modeling. We tackle this issue by introducing an additional model that captures the conversion delay. Intuitively, this probabilistic model helps determining whether a user that has not converted should be treated as a negative sample – when the elapsed time is larger than the predicted delay – or should be discarded from the training set – when it is too early to tell. We provide experimental results on real traﬃc logs that demonstrate the eﬀectiveness of the proposed model.},
	language = {en},
	urldate = {2021-12-21},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Chapelle, Olivier},
	month = aug,
	year = {2014},
	keywords = {deepcvr},
	pages = {1097--1105},
	file = {Chapelle - 2014 - Modeling delayed feedback in display advertising.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YXYCQMGU\\Chapelle - 2014 - Modeling delayed feedback in display advertising.pdf:application/pdf},
}

@inproceedings{forkanMobDLFrameworkProfiling2020,
	address = {Darmstadt Germany},
	title = {{MobDL}: {A} {Framework} for {Profiling} {Deep} {Learning} {Models}: {A} {Case} {Study} using {Mobile} {Digital} {Health} {Applications}},
	isbn = {978-1-4503-8840-5},
	shorttitle = {{MobDL}},
	url = {https://dl.acm.org/doi/10.1145/3448891.3448896},
	doi = {10.1145/3448891.3448896},
	abstract = {Smart mobile devices coupled with the Internet of Things (IoT) and Artificial Intelligence (AI) have emerged as a key enabler of modern digital health applications. While cloud computing is now a well established paradigm for analysing IoT captured data in mobile health applications, on-board analysis of data using AI approaches such as Deep Learning (DL) is gaining significant momentum. This is driven primarily by advances in on-board resources enabling modern mobile devices to execute complex DL models, while also offering improved response time and accuracy for rapid decision-making, and enhanced user privacy. While the number of mobile digital health applications that use IoT and DL is increasing, progress is currently impeded by a lack of framework for profiling and evaluating the performance of DL models on mobile devices. To this end, we propose MobDL, a framework for profiling and evaluating DL models running on smart mobile devices. We present the architecture of this framework and devise a novel evaluation methodology for conducting quantitative comparisons of various DL models running on mobile devices. Three diverse digital health applications using heterogeneous data (e.g. image, time series) are introduced. We conduct extensive experimental evaluations using several DL models that have been developed using the data sets obtained for the three digital health applications to validate the effectiveness of the proposed MobDL framework.},
	language = {en},
	urldate = {2021-12-21},
	booktitle = {{MobiQuitous} 2020 - 17th {EAI} {International} {Conference} on {Mobile} and {Ubiquitous} {Systems}: {Computing}, {Networking} and {Services}},
	publisher = {ACM},
	author = {Forkan, Abdur Rahim Mohammad and Jayaraman, Prem Prakash and Kaul, Rohit and Zhang, Yuxin and McCarthy, Chris and Delir Haghighi, Pari and Ranjan, Rajiv},
	month = dec,
	year = {2020},
	pages = {405--414},
	file = {Forkan et al. - 2020 - MobDL A Framework for Profiling Deep Learning Mod.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HG78H7QP\\Forkan et al. - 2020 - MobDL A Framework for Profiling Deep Learning Mod.pdf:application/pdf},
}

@inproceedings{yangHybridNeuralNetwork2021,
	address = {Kuala Lumpur Malaysia},
	title = {A {Hybrid} {Neural} {Network} based on {Particle} {Swarm} {Optimization} for {Predicting} the {Diabetes}},
	isbn = {978-1-4503-8882-5},
	url = {https://dl.acm.org/doi/10.1145/3457784.3457831},
	doi = {10.1145/3457784.3457831},
	abstract = {In recent years, more and more studies have applied hybrid models in order to improve the performance of traditional neural networks. By combining particle swarm optimization and neural network, this research proposes a new hybrid neural prediction algorithm named as PSONN. The algorithm was applied to Pima Indians Diabetes Database and compared with eight other algorithms including Logistic regression, Ridge regression, Lasso regression, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Random Forest, Gradient Boosting Machine, and Adam (a neural network algorithm). The findings indicated that the proposed algorithm had higher accuracy and stability, but it took more time to execute. It is suggested that, future research could apply parallelization technology for reducing execution time.},
	language = {en},
	urldate = {2021-12-21},
	booktitle = {2021 10th {International} {Conference} on {Software} and {Computer} {Applications}},
	publisher = {ACM},
	author = {Yang, Heng-Li and Li, Bo-Yi},
	month = feb,
	year = {2021},
	pages = {302--306},
	file = {Yang and Li - 2021 - A Hybrid Neural Network based on Particle Swarm Op.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ADBE7KCA\\Yang and Li - 2021 - A Hybrid Neural Network based on Particle Swarm Op.pdf:application/pdf},
}

@inproceedings{chenTaskwiseSplitGradient2021,
	address = {Virtual Event Singapore},
	title = {Task-wise {Split} {Gradient} {Boosting} {Trees} for {Multi}-center {Diabetes} {Prediction}},
	isbn = {978-1-4503-8332-5},
	url = {https://dl.acm.org/doi/10.1145/3447548.3467123},
	doi = {10.1145/3447548.3467123},
	abstract = {Diabetes prediction is an important data science application in the social healthcare domain. There exist two main challenges in the diabetes prediction task: data heterogeneity since demographic and metabolic data are of different types, data insufficiency since the number of diabetes cases in a single medical center is usually limited. To tackle the above challenges, we employ gradient boosting decision trees (GBDT) to handle data heterogeneity and introduce multi-task learning (MTL) to solve data insufficiency. To this end, Task-wise Split Gradient Boosting Trees (TSGB) is proposed for the multi-center diabetes prediction task. Specifically, we firstly introduce task gain to evaluate each task separately during tree construction, with a theoretical analysis of GBDT’s learning objective. Secondly, we reveal a problem when directly applying GBDT in MTL, i.e., the negative task gain problem. Finally, we propose a novel split method for GBDT in MTL based on the task gain statistics, named task-wise split, as an alternative to standard feature-wise split to overcome the mentioned negative task gain problem. Extensive experiments on a large-scale real-world diabetes dataset and a commonly used benchmark dataset demonstrate TSGB achieves superior performance against several state-of-the-art methods. Detailed case studies further support our analysis of negative task gain problems and provide insightful findings. The proposed TSGB method has been deployed as an online diabetes risk assessment software for early diagnosis.},
	language = {en},
	urldate = {2021-12-21},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Mingcheng and Wang, Zhenghui and Zhao, Zhiyun and Zhang, Weinan and Guo, Xiawei and Shen, Jian and Qu, Yanru and Lu, Jieli and Xu, Min and Xu, Yu and Wang, Tiange and Li, Mian and Tu, Weiwei and Yu, Yong and Bi, Yufang and Wang, Weiqing and Ning, Guang},
	month = aug,
	year = {2021},
	pages = {2663--2673},
	file = {Chen et al. - 2021 - Task-wise Split Gradient Boosting Trees for Multi-.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TGT9FL66\\Chen et al. - 2021 - Task-wise Split Gradient Boosting Trees for Multi-.pdf:application/pdf},
}

@inproceedings{marquesDiabetesDiseaseMachine2020,
	address = {Zhuhai China},
	title = {Diabetes {Disease} through {Machine} {Learning}: {A} comparative study},
	isbn = {978-1-4503-8843-6},
	shorttitle = {Diabetes {Disease} through {Machine} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3445815.3445828},
	doi = {10.1145/3445815.3445828},
	abstract = {Diabetes is a critical problem in developed and developing countries. The early detection of this disease is crucial for efficient and effective treatment. Moreover, the application of machine learning for disease detection is a trending topic. There are numerous machine learning methods available in the literature. The main contribution of this paper is to present a preliminary study on the application of machine learning methods on a public and widely used diabetes dataset. The authors have applied eight different machine learning techniques using PIMA diabetes dataset. The data have been normalized, and Neural Networks, SGD, Random Forest, kNN, Naïve Bayes, AdaBoost, Decision Tree and SVM methods have been applied. First, the techniques have been validated using stratified 10-fold cross-validation. Second, the confusion matrix has been extracted for each method, and the accuracy, recall, precision and F1-score have been calculated. The three methods with better accuracy are Neural Networks, SGD and kNN. These methods report 77.47\%, 76.43\% and 73.96\% of average accuracy between classes.},
	language = {en},
	urldate = {2021-12-21},
	booktitle = {2020 4th {International} {Conference} on {Computer} {Science} and {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Marques, Gonçalo and Pires, Ivan Miguel and Garcia, Nuno M.},
	month = dec,
	year = {2020},
	pages = {74--79},
	file = {Marques et al. - 2020 - Diabetes Disease through Machine Learning A compa.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QFRSNELE\\Marques et al. - 2020 - Diabetes Disease through Machine Learning A compa.pdf:application/pdf},
}

@misc{DiabetesStatistics,
	title = {Diabetes {Statistics}},
	url = {https://www.diabetesresearch.org/diabetes-statistics},
	urldate = {2021-12-21},
	file = {Diabetes Statistics:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N47KWK4P\\diabetes-statistics.html:text/html;Diabetes Statistics:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YS7S9UBT\\diabetes-statistics.html:text/html},
}

@misc{2020DiabetesFacts,
	title = {2020 {Diabetes} {Facts} \& {Statistics}},
	url = {https://wallethub.com/blog/diabetes-statistics/41253},
	language = {en},
	urldate = {2021-12-21},
	journal = {WalletHub},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HSXHEPPJ\\41253.html:text/html},
}

@article{herlandBigDataFraud2018,
	title = {Big {Data} fraud detection using multiple medicare data sources},
	volume = {5},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-018-0138-3},
	doi = {10.1186/s40537-018-0138-3},
	abstract = {In the United States, advances in technology and medical sciences continue to improve the general well-being of the population. With this continued progress, programs such as Medicare are needed to help manage the high costs associated with quality healthcare. Unfortunately, there are individuals who commit fraud for nefarious reasons and personal gain, limiting Medicare’s ability to effectively provide for the healthcare needs of the elderly and other qualifying people. To minimize fraudulent activities, the Centers for Medicare and Medicaid Services (CMS) released a number of “Big Data” datasets for different parts of the Medicare program. In this paper, we focus on the detection of Medicare fraud using the following CMS datasets: (1) Medicare Provider Utilization and Payment Data: Physician and Other Supplier (Part B), (2) Medicare Provider Utilization and Payment Data: Part D Prescriber (Part D), and (3) Medicare Provider Utilization and Payment Data: Referring Durable Medical Equipment, Prosthetics, Orthotics and Supplies (DMEPOS). Additionally, we create a fourth dataset which is a combination of the three primary datasets. We discuss data processing for all four datasets and the mapping of real-world provider fraud labels using the List of Excluded Individuals and Entities (LEIE) from the Office of the Inspector General. Our exploratory analysis on Medicare fraud detection involves building and assessing three learners on each dataset. Based on the Area under the Receiver Operating Characteristic (ROC) Curve performance metric, our results show that the Combined dataset with the Logistic Regression (LR) learner yielded the best overall score at 0.816, closely followed by the Part B dataset with LR at 0.805. Overall, the Combined and Part B datasets produced the best fraud detection performance with no statistical difference between these datasets, over all the learners. Therefore, based on our results and the assumption that there is no way to know within which part of Medicare a physician will commit fraud, we suggest using the Combined dataset for detecting fraudulent behavior when a physician has submitted payments through any or all Medicare parts evaluated in our study.},
	language = {en},
	number = {1},
	urldate = {2021-12-21},
	journal = {Journal of Big Data},
	author = {Herland, Matthew and Khoshgoftaar, Taghi M. and Bauder, Richard A.},
	month = dec,
	year = {2018},
	pages = {29},
	file = {Herland et al. - 2018 - Big Data fraud detection using multiple medicare d.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\45GNUU33\\Herland et al. - 2018 - Big Data fraud detection using multiple medicare d.pdf:application/pdf},
}

@misc{FraudStatisticsBcbsm,
	title = {Fraud {Statistics} {\textbar} bcbsm.com},
	url = {https://www.bcbsm.com/health-care-fraud/fraud-statistics.html},
	urldate = {2021-12-21},
	file = {Fraud Statistics | bcbsm.com:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\428285HV\\fraud-statistics.html:text/html},
}

@article{breunigLOFIdentifyingDensitybased2000,
	title = {{LOF}: identifying density-based local outliers},
	volume = {29},
	issn = {0163-5808},
	shorttitle = {{LOF}},
	url = {https://dl.acm.org/doi/10.1145/335191.335388},
	doi = {10.1145/335191.335388},
	abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a
              degree
              of being an outlier. This degree is called the
              local outlier factor
              (LOF) of an object. It is
              local
              in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
	language = {en},
	number = {2},
	urldate = {2022-01-12},
	journal = {ACM SIGMOD Record},
	author = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, Jörg},
	month = jun,
	year = {2000},
	keywords = {deepcvr},
	pages = {93--104},
}

@misc{adgateAgenciesAgree2021,
	title = {Agencies {Agree}; 2021 {Was} {A} {Record} {Year} {For} {Ad} {Spending}, {With} {More} {Growth} {Expected} {In} 2022},
	url = {https://www.forbes.com/sites/bradadgate/2021/12/08/agencies-agree-2021-was-a-record-year-for-ad-spending-with-more-growth-expected-in-2022/},
	abstract = {Ad agencies agree led by digital media, the ad market in 2021 was bullish, with year-over-year increases expected to be over 20\%. More double-digit growth are forecast for 2022.},
	language = {en},
	urldate = {2022-01-12},
	journal = {Forbes},
	author = {Adgate, Brad},
	note = {Section: Media},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3ZXKJR7A\\agencies-agree-2021-was-a-record-year-for-ad-spending-with-more-growth-expected-in-2022.html:text/html},
}

@misc{marketsGlobalDigitalAdvertising2021,
	title = {Global {Digital} {Advertising} {Market} {Analysis} \& {Forecasts}, 2015-2020, 2021-{2025F} \& {2030F}},
	url = {https://www.globenewswire.com/news-release/2021/11/18/2337022/28124/en/Global-Digital-Advertising-Market-Analysis-Forecasts-2015-2020-2021-2025F-2030F.html},
	abstract = {Dublin, Nov.  18, 2021  (GLOBE NEWSWIRE) -- The "Digital Advertising Global Market Opportunities And Strategies To 2030: COVID-19 Growth and Change" report...},
	language = {en},
	urldate = {2022-01-25},
	journal = {GlobeNewswire News Room},
	author = {Markets, Research and},
	month = nov,
	year = {2021},
	keywords = {deepcvr},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\464T6566\\Global-Digital-Advertising-Market-Analysis-Forecasts-2015-2020-2021-2025F-2030F.html:text/html},
}

@misc{mothMostCompaniesSpend2013,
	title = {Most companies spend less than 5\% of marketing budgets on conversion optimization},
	url = {https://econsultancy.com/most-companies-spend-less-than-5-of-marketing-budgets-on-conversion-optimization/},
	abstract = {A majority of companies (53\%) spend less than 5\% of their total marketing budgets on optimization activities, despite the fact that a small uplift in conversion rates can translate into millions of dollars of extra revenue.
The findings come from a new survey by Adobe which also found that companies spending more on optimization are reaping the benefits.
The Adobe 2013 Digital Marketing Optimization Survey, with analysis carried out by Econsultancy, received global responses from more than 1,800 digital marketers across North America, Europe and Asia.
It explores the key areas in which digital marketers need to excel to ensure success, including mobile, social, personalisation and customer experience.},
	language = {en},
	urldate = {2022-01-25},
	journal = {Econsultancy},
	author = {Moth, David},
	month = apr,
	year = {2013},
	note = {Section: Conversion Rate Optimisation},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\B6RCWGDI\\most-companies-spend-less-than-5-of-marketing-budgets-on-conversion-optimization.html:text/html},
}

@article{roserInternet2015,
	title = {Internet},
	url = {https://ourworldindata.org/internet},
	abstract = {For many, the internet is now essential for work, finding information, and connecting with others.
How did half the world get online in just one generation? And what are the challenges ahead?},
	urldate = {2022-01-25},
	journal = {Our World in Data},
	author = {Roser, Max and Ritchie, Hannah and Ortiz-Ospina, Esteban},
	month = jul,
	year = {2015},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YTACU7YJ\\internet.html:text/html},
}

@misc{TopOnlineAdvertisers,
	title = {Top {U}.{S}. online advertisers by impressions 2020},
	url = {https://www.statista.com/statistics/266882/us-online-advertisers-ranked-by-impressions/},
	abstract = {In a recent study of the online advertising universe in the United States conducted in the first quarter of 2020, it was found that Disney was the largest advertiser in the measured period, having amassed approximately 29.7 billion impressions on U.S.},
	language = {en},
	urldate = {2022-01-25},
	journal = {Statista},
	keywords = {deepcvr},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C56XF6Z9\\us-online-advertisers-ranked-by-impressions.html:text/html},
}

@misc{simpsonCouncilPostFinding,
	title = {Council {Post}: {Finding} {Brand} {Success} {In} {The} {Digital} {World}},
	shorttitle = {Council {Post}},
	url = {https://www.forbes.com/sites/forbesagencycouncil/2017/08/25/finding-brand-success-in-the-digital-world/},
	abstract = {In doing the proper research and strategy to build a brand, the challenge in distinguishing your brand becomes simpler.},
	language = {en},
	urldate = {2022-01-25},
	journal = {Forbes},
	author = {Simpson, Jon},
	note = {Section: Business},
	keywords = {deepcvr},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7KVJPKLZ\\finding-brand-success-in-the-digital-world.html:text/html},
}

@misc{EcommerceConversionRates2022,
	title = {E-commerce conversion rates benchmarks 2022 - {How} do yours compare?},
	url = {https://www.smartinsights.com/ecommerce/ecommerce-analytics/ecommerce-conversion-rates/},
	abstract = {E-commerce conversion rate and lead generation stats compilation desktop and mobile devices benchmarking averages across different industries},
	language = {en-US},
	urldate = {2022-01-25},
	journal = {Smart Insights},
	month = jan,
	year = {2022},
	keywords = {deepcvr},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QF56S5G8\\ecommerce-conversion-rates.html:text/html},
}

@misc{WhatGoodConversion2014,
	title = {What {Is} a {Good} {Conversion} {Rate}? {It}'s {Higher} {Than} {You} {Think}!},
	shorttitle = {What {Is} a {Good} {Conversion} {Rate}?},
	url = {https://www.wordstream.com/blog/ws/2014/03/17/what-is-a-good-conversion-rate},
	abstract = {What is a good conversion rate? Conversion is a key element in your paid search strategy. In this post, you'll learn a step-by-step, replicable process for boosting your conversion rates.},
	language = {en-US},
	urldate = {2022-01-25},
	journal = {WordStream},
	month = mar,
	year = {2014},
	keywords = {deepcvr},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G2VPD5GP\\what-is-a-good-conversion-rate.html:text/html},
}

@article{ktenaAddressingDelayedFeedback2021,
	title = {Addressing {Delayed} {Feedback} for {Continuous} {Training} with {Neural} {Networks} in {CTR} prediction},
	url = {http://arxiv.org/abs/1907.06558},
	abstract = {One of the challenges in display advertising is that the distribution of features and click through rate (CTR) can exhibit large shifts over time due to seasonality, changes to ad campaigns and other factors. The predominant strategy to keep up with these shifts is to train predictive models continuously, on fresh data, in order to prevent them from becoming stale. However, in many ad systems positive labels are only observed after a possibly long and random delay. These delayed labels pose a challenge to data freshness in continuous training: fresh data may not have complete label information at the time they are ingested by the training algorithm. Naive strategies which consider any data point a negative example until a positive label becomes available tend to underestimate CTR, resulting in inferior user experience and suboptimal performance for advertisers. The focus of this paper is to identify the best combination of loss functions and models that enable large-scale learning from a continuous stream of data in the presence of delayed labels. In this work, we compare 5 different loss functions, 3 of them applied to this problem for the first time. We benchmark their performance in offline settings on both public and proprietary datasets in conjunction with shallow and deep model architectures. We also discuss the engineering cost associated with implementing each loss function in a production environment. Finally, we carried out online experiments with the top performing methods, in order to validate their performance in a continuous training scheme. While training on 668 million in-house data points offline, our proposed methods outperform previous state-of-the-art by 3\% relative cross entropy (RCE). During online experiments, we observed 55\% gain in revenue per thousand requests (RPMq) against naive log loss.},
	language = {en},
	urldate = {2022-01-25},
	journal = {arXiv:1907.06558 [cs, stat]},
	author = {Ktena, Sofia Ira and Tejani, Alykhan and Theis, Lucas and Myana, Pranay Kumar and Dilipkumar, Deepak and Huszar, Ferenc and Yoo, Steven and Shi, Wenzhe},
	month = apr,
	year = {2021},
	note = {arXiv: 1907.06558},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Ktena et al. - 2021 - Addressing Delayed Feedback for Continuous Trainin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TFMXVSCK\\Ktena et al. - 2021 - Addressing Delayed Feedback for Continuous Trainin.pdf:application/pdf},
}

@article{beraDimensionalityReductionCategorical2021,
	title = {Dimensionality {Reduction} for {Categorical} {Data}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {http://arxiv.org/abs/2112.00362},
	doi = {10.1109/TKDE.2021.3132373},
	abstract = {Categorical attributes are those that can take a discrete set of values, e.g., colours. This work is about compressing vectors over categorical attributes to low-dimension discrete vectors. The current hash-based methods compressing vectors over categorical attributes to low-dimension discrete vectors do not provide any guarantee on the Hamming distances between the compressed representations. Here we present FSketch to create sketches for sparse categorical data and an estimator to estimate the pairwise Hamming distances among the uncompressed data only from their sketches. We claim that these sketches can be used in the usual data mining tasks in place of the original data without compromising the quality of the task. For that, we ensure that the sketches also are categorical, sparse, and the Hamming distance estimates are reasonably precise. Both the sketch construction and the Hamming distance estimation algorithms require just a single-pass; furthermore, changes to a data point can be incorporated into its sketch in an efficient manner. The compressibility depends upon how sparse the data is and is independent of the original dimension -- making our algorithm attractive for many real-life scenarios. Our claims are backed by rigorous theoretical analysis of the properties of FSketch and supplemented by extensive comparative evaluations with related algorithms on some real-world datasets. We show that FSketch is significantly faster, and the accuracy obtained by using its sketches are among the top for the standard unsupervised tasks of RMSE, clustering and similarity search.},
	urldate = {2022-01-31},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Bera, Debajyoti and Pratap, Rameshwar and Verma, Bhisham Dev},
	year = {2021},
	note = {arXiv: 2112.00362},
	keywords = {Computer Science - Machine Learning},
	pages = {1--1},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JVKHG79Q\\2112.html:text/html;Bera et al_2021_Dimensionality Reduction for Categorical Data.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TLDQG6B2\\Bera et al_2021_Dimensionality Reduction for Categorical Data.pdf:application/pdf},
}

@article{nguyenTenQuickTips2019,
	title = {Ten quick tips for effective dimensionality reduction},
	volume = {15},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1006907},
	doi = {10.1371/journal.pcbi.1006907},
	language = {en},
	number = {6},
	urldate = {2022-01-31},
	journal = {PLOS Computational Biology},
	author = {Nguyen, Lan Huong and Holmes, Susan},
	editor = {Ouellette, Francis},
	month = jun,
	year = {2019},
	pages = {e1006907},
	file = {Nguyen_Holmes_2019_Ten quick tips for effective dimensionality reduction.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DPGRCYS5\\Nguyen_Holmes_2019_Ten quick tips for effective dimensionality reduction.pdf:application/pdf},
}

@article{merrittCoursEconomiePolitique1898,
	title = {\textit{{Cours} d'{Economie} {Politique}} . {Vilfredo} {Pareto}},
	volume = {6},
	issn = {0022-3808, 1537-534X},
	url = {https://www.journals.uchicago.edu/doi/10.1086/250536},
	doi = {10.1086/250536},
	language = {en},
	number = {4},
	urldate = {2022-01-31},
	journal = {Journal of Political Economy},
	author = {Merritt, Fred D},
	month = sep,
	year = {1898},
	keywords = {deepcvr},
	pages = {549--552},
	file = {Merritt_1898_iCours d'Economie Politique-i.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WXCWU87G\\Merritt_1898_iCours d'Economie Politique-i.pdf:application/pdf},
}

@book{zotero-5470,
}

@article{bainHUMANBEHAVIORPRINCIPLE1950,
	title = {{HUMAN} {BEHAVIOR} {AND} {THE} {PRINCIPLE} {OF} {LEAST} {EFFORT}: {AN} {INTRODUCTION} {TO} {HUMAN} {ECOLOGY}. {By} {George} {Kingsley} {Zipf}. {Cambridge}, {Mass}.: {Addison}-{Wesley} {Press}, {Inc}., 1949. 573 pp. \$6.50},
	volume = {28},
	issn = {0037-7732, 1534-7605},
	shorttitle = {{HUMAN} {BEHAVIOR} {AND} {THE} {PRINCIPLE} {OF} {LEAST} {EFFORT}},
	url = {https://academic.oup.com/sf/article-lookup/doi/10.2307/2572028},
	doi = {10.2307/2572028},
	language = {en},
	number = {3},
	urldate = {2022-01-31},
	journal = {Social Forces},
	author = {Bain, R.},
	month = mar,
	year = {1950},
	pages = {340--341},
}

@inproceedings{guoDeepFMFactorizationMachineBased2017,
	address = {Melbourne, Australia},
	title = {{DeepFM}: {A} {Factorization}-{Machine} based {Neural} {Network} for {CTR} {Prediction}},
	isbn = {978-0-9992411-0-3},
	shorttitle = {{DeepFM}},
	url = {https://www.ijcai.org/proceedings/2017/239},
	doi = {10.24963/ijcai.2017/239},
	abstract = {Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and highorder feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \& Deep model from Google, DeepFM has a shared input to its “wide” and “deep” parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efﬁciency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Guo, Huifeng and Tang, Ruiming and Ye, Yunming and Li, Zhenguo and He, Xiuqiang},
	month = aug,
	year = {2017},
	keywords = {deepcvr},
	pages = {1725--1731},
	file = {Guo et al. - 2017 - DeepFM A Factorization-Machine based Neural Netwo.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N4THY7MR\\Guo et al. - 2017 - DeepFM A Factorization-Machine based Neural Netwo.pdf:application/pdf},
}

@inproceedings{zhouDeepInterestNetwork2018,
	address = {London United Kingdom},
	title = {Deep {Interest} {Network} for {Click}-{Through} {Rate} {Prediction}},
	isbn = {978-1-4503-5552-0},
	url = {https://dl.acm.org/doi/10.1145/3219819.3219823},
	doi = {10.1145/3219819.3219823},
	abstract = {Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\&MLP methods to capture user’s diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Zhou, Guorui and Zhu, Xiaoqiang and Song, Chenru and Fan, Ying and Zhu, Han and Ma, Xiao and Yan, Yanghui and Jin, Junqi and Li, Han and Gai, Kun},
	month = jul,
	year = {2018},
	keywords = {deepcvr},
	pages = {1059--1068},
	file = {Zhou et al. - 2018 - Deep Interest Network for Click-Through Rate Predi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YGKULDHV\\Zhou et al. - 2018 - Deep Interest Network for Click-Through Rate Predi.pdf:application/pdf},
}

@inproceedings{fengDeepSessionInterest2019,
	address = {Macao, China},
	title = {Deep {Session} {Interest} {Network} for {Click}-{Through} {Rate} {Prediction}},
	isbn = {978-0-9992411-4-1},
	url = {https://www.ijcai.org/proceedings/2019/319},
	doi = {10.24963/ijcai.2019/319},
	abstract = {Click-Through Rate (CTR) prediction plays an important role in many industrial applications, such as online advertising and recommender systems. How to capture users’ dynamic and evolving interests from their behavior sequences remains a continuous research topic in the CTR prediction. However, most existing studies overlook the intrinsic structure of the sequences: the sequences are composed of sessions, where sessions are user behaviors separated by their occurring time. We observe that user behaviors are highly homogeneous in each session, and heterogeneous cross sessions. Based on this observation, we propose a novel CTR model named Deep Session Interest Network (DSIN) that leverages users’ multiple historical sessions in their behavior sequences. We ﬁrst use self-attention mechanism with bias encoding to extract users’ interests in each session. Then we apply Bi-LSTM to model how users’ interests evolve and interact among sessions. Finally, we employ the local activation unit to adaptively learn the inﬂuences of various session interests on the target item. Experiments are conducted on both advertising and production recommender datasets and DSIN outperforms other stateof-the-art models on both datasets.},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Feng, Yufei and Lv, Fuyu and Shen, Weichen and Wang, Menghan and Sun, Fei and Zhu, Yu and Yang, Keping},
	month = aug,
	year = {2019},
	keywords = {deepcvr},
	pages = {2301--2307},
	file = {Feng et al. - 2019 - Deep Session Interest Network for Click-Through Ra.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H6C7NF6C\\Feng et al. - 2019 - Deep Session Interest Network for Click-Through Ra.pdf:application/pdf},
}

@inproceedings{xiaoAttentionalFactorizationMachines2017,
	address = {Melbourne, Australia},
	title = {Attentional {Factorization} {Machines}: {Learning} the {Weight} of {Feature} {Interactions} via {Attention} {Networks}},
	isbn = {978-0-9992411-0-3},
	shorttitle = {Attentional {Factorization} {Machines}},
	url = {https://www.ijcai.org/proceedings/2017/435},
	doi = {10.24963/ijcai.2017/435},
	abstract = {Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a 8.6\% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide\&Deep [Cheng et al., 2016] and DeepCross [Shan et al., 2016] with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: https://github.com/hexiangnan/attentional\_factorization\_machine},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Xiao, Jun and Ye, Hao and He, Xiangnan and Zhang, Hanwang and Wu, Fei and Chua, Tat-Seng},
	month = aug,
	year = {2017},
	pages = {3119--3125},
	file = {Xiao et al. - 2017 - Attentional Factorization Machines Learning the W.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KBKT5SHR\\Xiao et al. - 2017 - Attentional Factorization Machines Learning the W.pdf:application/pdf},
}

@article{sunFMFieldmatrixedFactorization2021,
	title = {\${FM}{\textasciicircum}2\$: {Field}-matrixed {Factorization} {Machines} for {Recommender} {Systems}},
	shorttitle = {\${FM}{\textasciicircum}2\$},
	url = {http://arxiv.org/abs/2102.12994},
	doi = {10.1145/3442381.3449930},
	abstract = {Click-through rate (CTR) prediction plays a critical role in recommender systems and online advertising. The data used in these applications are multi-field categorical data, where each feature belongs to one field. Field information is proved to be important and there are several works considering fields in their models. In this paper, we proposed a novel approach to model the field information effectively and efficiently. The proposed approach is a direct improvement of FwFM, and is named as Field-matrixed Factorization Machines (FmFM, or 𝐹 𝑀2). We also proposed a new explanation of FM and FwFM within the FmFM framework, and compared it with the FFM. Besides pruning the cross terms, our model supports field-specific variable dimensions of embedding vectors, which acts as a soft pruning. We also proposed an efficient way to minimize the dimension while keeping the model performance. The FmFM model can also be optimized further by caching the intermediate vectors, and it only takes thousands floating-point operations (FLOPs) to make a prediction. Our experiment results show that it can out-perform the FFM, which is more complex. The FmFM model’s performance is also comparable to DNN models which require much more FLOPs in runtime.},
	language = {en},
	urldate = {2022-02-04},
	journal = {Proceedings of the Web Conference 2021},
	author = {Sun, Yang and Pan, Junwei and Zhang, Alex and Flores, Aaron},
	month = apr,
	year = {2021},
	note = {arXiv: 2102.12994},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	pages = {2828--2837},
	file = {Sun et al. - 2021 - \$FM^2\$ Field-matrixed Factorization Machines for .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BJDTL2XX\\Sun et al. - 2021 - \$FM^2\$ Field-matrixed Factorization Machines for .pdf:application/pdf},
}

@misc{200909931FieldEmbedded,
	title = {[2009.09931] {Field}-{Embedded} {Factorization} {Machines} for {Click}-through rate prediction},
	url = {https://arxiv.org/abs/2009.09931},
	urldate = {2022-02-04},
}

@article{pandeFieldEmbeddedFactorizationMachines2021,
	title = {Field-{Embedded} {Factorization} {Machines} for {Click}-through rate prediction},
	url = {http://arxiv.org/abs/2009.09931},
	abstract = {Click-through rate (CTR) prediction models are common in many online applications such as digital advertising and recommender systems. Field-Aware Factorization Machine (FFM) and Field-weighted Factorization Machine (FwFM) are state-of-the-art among the shallow models for CTR prediction. Recently, many deep learning-based models have also been proposed. Among deeper models, DeepFM, xDeepFM, AutoInt+, and FiBiNet are state-of-the-art models. The deeper models combine a core architectural component, which learns explicit feature interactions, with a deep neural network (DNN) component. We propose a novel shallow Field-Embedded Factorization Machine (FEFM) and its deep counterpart Deep Field-Embedded Factorization Machine (DeepFEFM). FEFM learns symmetric matrix embeddings for each ﬁeld pair along with the usual single vector embeddings for each feature. FEFM has signiﬁcantly lower model complexity than FFM and roughly the same complexity as FwFM. FEFM also has insightful mathematical properties about important ﬁelds and ﬁeld interactions. DeepFEFM combines the FEFM interaction vectors learned by the FEFM component with a DNN and is thus able to learn higher order interactions. We conducted comprehensive experiments over a wide range of hyperparameters on two large publicly available real-world datasets. When comparing test AUC and log loss, the results show that FEFM and DeepFEFM outperform the existing state-of-the-art shallow and deep models for CTR prediction tasks. We have made the code of FEFM and DeepFEFM available in the DeepCTR library(https: //github.com/shenweichen/DeepCTR).},
	language = {en},
	urldate = {2022-02-04},
	journal = {arXiv:2009.09931 [cs, stat]},
	author = {Pande, Harshit},
	month = jun,
	year = {2021},
	note = {arXiv: 2009.09931},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval, cvr},
	file = {Pande - 2021 - Field-Embedded Factorization Machines for Click-th.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LD3CD877\\Pande - 2021 - Field-Embedded Factorization Machines for Click-th.pdf:application/pdf},
}

@article{abiodunStateoftheartArtificialNeural2018,
	title = {State-of-the-art in artificial neural network applications: {A} survey},
	volume = {4},
	issn = {24058440},
	shorttitle = {State-of-the-art in artificial neural network applications},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2405844018332067},
	doi = {10.1016/j.heliyon.2018.e00938},
	language = {en},
	number = {11},
	urldate = {2022-02-04},
	journal = {Heliyon},
	author = {Abiodun, Oludare Isaac and Jantan, Aman and Omolara, Abiodun Esther and Dada, Kemi Victoria and Mohamed, Nachaat AbdElatif and Arshad, Humaira},
	month = nov,
	year = {2018},
	pages = {e00938},
	file = {Abiodun et al. - 2018 - State-of-the-art in artificial neural network appl.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G9WY25RP\\Abiodun et al. - 2018 - State-of-the-art in artificial neural network appl.pdf:application/pdf},
}

@inproceedings{juanFieldawareFactorizationMachines2016,
	address = {Boston Massachusetts USA},
	title = {Field-aware {Factorization} {Machines} for {CTR} {Prediction}},
	isbn = {978-1-4503-4035-9},
	url = {https://dl.acm.org/doi/10.1145/2959100.2959134},
	doi = {10.1145/2959100.2959134},
	abstract = {Click-through rate (CTR) prediction plays an important role in computational advertising. Models based on degree-2 polynomial mappings and factorization machines (FMs) are widely used for this task. Recently, a variant of FMs, ﬁeldaware factorization machines (FFMs), outperforms existing models in some world-wide CTR-prediction competitions. Based on our experiences in winning two of them, in this paper we establish FFMs as an eﬀective method for classifying large sparse data including those from CTR prediction. First, we propose eﬃcient implementations for training FFMs. Then we comprehensively analyze FFMs and compare this approach with competing models. Experiments show that FFMs are very useful for certain classiﬁcation problems. Finally, we have released a package of FFMs for public use.},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {Proceedings of the 10th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Juan, Yuchin and Zhuang, Yong and Chin, Wei-Sheng and Lin, Chih-Jen},
	month = sep,
	year = {2016},
	keywords = {cvr},
	pages = {43--50},
	file = {Juan et al. - 2016 - Field-aware Factorization Machines for CTR Predict.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z767J9DW\\Juan et al. - 2016 - Field-aware Factorization Machines for CTR Predict.pdf:application/pdf},
}

@article{panFieldweightedFactorizationMachines2018,
	title = {Field-weighted {Factorization} {Machines} for {Click}-{Through} {Rate} {Prediction} in {Display} {Advertising}},
	url = {http://arxiv.org/abs/1806.03514},
	doi = {10.1145/3178876.3186040},
	abstract = {Click-through rate (CTR) prediction is a critical task in online display advertising. The data involved in CTR prediction are typically multi-field categorical data, i.e., every feature is categorical and belongs to one and only one field. One of the interesting characteristics of such data is that features from one field often interact differently with features from different other fields. Recently, Fieldaware Factorization Machines (FFMs) have been among the best performing models for CTR prediction by explicitly modeling such difference. However, the number of parameters in FFMs is in the order of feature number times field number, which is unacceptable in the real-world production systems. In this paper, we propose Field-weighted Factorization Machines (FwFMs) to model the different feature interactions between different fields in a much more memory-efficient way. Our experimental evaluations show that FwFMs can achieve competitive prediction performance with only as few as 4\% parameters of FFMs. When using the same number of parameters, FwFMs can bring 0.92\% and 0.47\% AUC lift over FFMs on two real CTR prediction data sets.},
	language = {en},
	urldate = {2022-02-04},
	journal = {Proceedings of the 2018 World Wide Web Conference on World Wide Web  - WWW '18},
	author = {Pan, Junwei and Xu, Jian and Ruiz, Alfonso Lobos and Zhao, Wenliang and Pan, Shengjun and Sun, Yu and Lu, Quan},
	year = {2018},
	note = {arXiv: 1806.03514},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, cvr},
	pages = {1349--1357},
	file = {Pan et al. - 2018 - Field-weighted Factorization Machines for Click-Th.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CBKX25IW\\Pan et al. - 2018 - Field-weighted Factorization Machines for Click-Th.pdf:application/pdf},
}

@article{songAutoIntAutomaticFeature2019,
	title = {{AutoInt}: {Automatic} {Feature} {Interaction} {Learning} via {Self}-{Attentive} {Neural} {Networks}},
	shorttitle = {{AutoInt}},
	url = {http://arxiv.org/abs/1810.11921},
	doi = {10.1145/3357384.3357925},
	abstract = {Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking on an ad or an item, is critical to many online applications such as online advertising and recommender systems. The problem is very challenging since (1) the input features (e.g., the user id, user age, item id, item category) are usually sparse and high-dimensional, and (2) an effective prediction relies on highorder combinatorial features (a.k.a. cross features), which are very time-consuming to hand-craft by domain experts and are impossible to be enumerated. Therefore, there have been efforts in finding lowdimensional representations of the sparse and high-dimensional raw features and their meaningful combinations. In this paper, we propose an effective and efficient method called the AutoInt to automatically learn the high-order feature interactions of input features. Our proposed algorithm is very general, which can be applied to both numerical and categorical input features. Specifically, we map both the numerical and categorical features into the same low-dimensional space. Afterwards, a multihead self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the lowdimensional space. With different layers of the multi-head selfattentive neural networks, different orders of feature combinations of input features can be modeled. The whole model can be efficiently fit on large-scale raw data in an end-to-end fashion. Experimental results on four real-world datasets show that our proposed approach not only outperforms existing state-of-the-art approaches for prediction but also offers good explainability. Code is available at: https://github.com/DeepGraphLearning/RecommenderSystems.},
	language = {en},
	urldate = {2022-02-04},
	journal = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
	author = {Song, Weiping and Shi, Chence and Xiao, Zhiping and Duan, Zhijian and Xu, Yewen and Zhang, Ming and Tang, Jian},
	month = nov,
	year = {2019},
	note = {arXiv: 1810.11921},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	pages = {1161--1170},
	file = {Song et al. - 2019 - AutoInt Automatic Feature Interaction Learning vi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UTA8LVQC\\Song et al. - 2019 - AutoInt Automatic Feature Interaction Learning vi.pdf:application/pdf},
}

@article{huangFiBiNETCombiningFeature2019,
	title = {{FiBiNET}: {Combining} {Feature} {Importance} and {Bilinear} feature {Interaction} for {Click}-{Through} {Rate} {Prediction}},
	shorttitle = {{FiBiNET}},
	url = {http://arxiv.org/abs/1905.09433},
	doi = {10.1145/3298689.3347043},
	abstract = {Advertising and feed ranking are essential to many Internet companies such as Facebook and Sina Weibo. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. There are many proposed models in this field such as logistic regression, tree based models, factorization machine based models and deep learning based CTR models. However, many current works calculate the feature interactions in a simple way such as Hadamard product and inner product and they care less about the importance of features. In this paper, a new model named FiBiNET as an abbreviation for Feature Importance and Bilinear feature Interaction NETwork is proposed to dynamically learn the feature importance and fine-grained feature interactions. On the one hand, the FiBiNET can dynamically learn the importance of features via the Squeeze-Excitation network (SENET) mechanism; on the other hand, it is able to effectively learn the feature interactions via bilinear function. We conduct extensive experiments on two realworld datasets and show that our shallow model outperforms other shallow models such as factorization machine(FM) and field-aware factorization machine(FFM). In order to improve performance further, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. The deep FiBiNET consistently outperforms the other state-of-the-art deep models such as DeepFM and extreme deep factorization machine(XdeepFM).},
	language = {en},
	urldate = {2022-02-04},
	journal = {Proceedings of the 13th ACM Conference on Recommender Systems},
	author = {Huang, Tongwen and Zhang, Zhiqi and Zhang, Junlin},
	month = sep,
	year = {2019},
	note = {arXiv: 1905.09433},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, cvr, deepcvr},
	pages = {169--177},
	file = {Huang et al. - 2019 - FiBiNET Combining Feature Importance and Bilinear.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ADRH6WNV\\Huang et al. - 2019 - FiBiNET Combining Feature Importance and Bilinear.pdf:application/pdf},
}

@article{wangRecentAdvancesDeep2020,
	title = {Recent advances in deep learning},
	volume = {11},
	issn = {1868-8071, 1868-808X},
	url = {http://link.springer.com/10.1007/s13042-020-01096-5},
	doi = {10.1007/s13042-020-01096-5},
	language = {en},
	number = {4},
	urldate = {2022-02-04},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Wang, Xizhao and Zhao, Yanxia and Pourpanah, Farhad},
	month = apr,
	year = {2020},
	pages = {747--750},
	file = {Wang et al. - 2020 - Recent advances in deep learning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TFPYFKEF\\Wang et al. - 2020 - Recent advances in deep learning.pdf:application/pdf},
}

@misc{marketsGlobalDigitalAdvertising2021a,
	title = {Global {Digital} {Advertising} and {Marketing} {Market} {Report} 2021: {Market} to {Reach} \$786.2 {Billion} by 2026 - {Digital} {Advertising} {Spending} {Records} {Growth} during {Pandemic}},
	shorttitle = {Global {Digital} {Advertising} and {Marketing} {Market} {Report} 2021},
	url = {https://www.globenewswire.com/news-release/2021/11/22/2339103/28124/en/Global-Digital-Advertising-and-Marketing-Market-Report-2021-Market-to-Reach-786-2-Billion-by-2026-Digital-Advertising-Spending-Records-Growth-during-Pandemic.html},
	abstract = {Dublin, Nov.  22, 2021  (GLOBE NEWSWIRE) -- The "Digital Advertising and Marketing - Global Market" report has been added to ResearchAndMarkets.com's...},
	language = {en},
	urldate = {2022-02-13},
	journal = {GlobeNewswire News Room},
	author = {Markets, Research and},
	month = nov,
	year = {2021},
	keywords = {deepcvr},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PW873KUZ\\Global-Digital-Advertising-and-Marketing-Market-Report-2021-Market-to-Reach-786-2-Billion-by-20.html:text/html},
}

@book{jiTimeawareConversionPrediction2018,
	address = {New Jersey},
	series = {East {China} {Normal} {University} scientific reports},
	title = {Time-aware conversion prediction for e-commerce},
	isbn = {978-981-322-470-4 978-981-322-471-1},
	number = {Volume 7},
	publisher = {World Scientific},
	author = {Ji, Wendi and Wang, Xiaoling and Zhou, Aoying},
	year = {2018},
	keywords = {Data processing, Mathematical models, Electronic commerce},
}

@article{wangMultitaskLearningModel2021,
	title = {A {Multitask} {Learning} {Model} with {Multiperspective} {Attention} and {Its} {Application} in {Recommendation}},
	volume = {2021},
	issn = {1687-5273, 1687-5265},
	url = {https://www.hindawi.com/journals/cin/2021/8550270/},
	doi = {10.1155/2021/8550270},
	abstract = {Training models to predict click and order targets at the same time. For better user satisfaction and business effectiveness, multitask learning is one of the most important methods in e-commerce. Some existing researches model user representation based on historical behaviour sequence to capture user interests. It is often the case that user interests may change from their past routines. However, multi-perspective attention has broad horizon, which covers different characteristics of human reasoning, emotions, perception, attention, and memory. In this paper, we attempt to introduce the multi-perspective attention and sequence behaviour into multitask learning. Our proposed method offers better understanding of user interest and decision. To achieve more flexible parameter sharing and maintaining the special feature advantage of each task, we improve the attention mechanism at the view of expert interactive. To the best of our knowledge, we firstly propose the implicit interaction mode, the explicit hard interaction mode, the explicit soft interaction mode, and the data fusion mode in multitask learning. We do experiments on public data and lab medical data. The results show that our model consistently achieves remarkable improvements to the state-of-the-art method.},
	language = {en},
	urldate = {2022-02-13},
	journal = {Computational Intelligence and Neuroscience},
	author = {Wang, Yingshuai and Zhang, Dezheng and Wulamu, Aziguli},
	editor = {Jing, Jin},
	month = oct,
	year = {2021},
	keywords = {multi task learning, ali-ccp},
	pages = {1--13},
	file = {Wang et al_2021_A Multitask Learning Model with Multiperspective Attention and Its Application.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3JQRH89Y\\Wang et al_2021_A Multitask Learning Model with Multiperspective Attention and Its Application.pdf:application/pdf},
}

@article{leeComparisonInterpretationMachine2021,
	title = {A {Comparison} and {Interpretation} of {Machine} {Learning} {Algorithm} for the {Prediction} of {Online} {Purchase} {Conversion}},
	volume = {16},
	issn = {0718-1876},
	url = {https://www.mdpi.com/0718-1876/16/5/83},
	doi = {10.3390/jtaer16050083},
	abstract = {Machine learning technology is recently being applied to various fields. However, in the field of online consumer conversion, research is limited despite the high possibility of machine learning application due to the availability of big data. In this context, we investigate the following three research questions. First, what is the suitable machine learning model for predicting online consumer behavior? Second, what is the good data sampling method for predicting online con-sumer behavior? Third, can we interpret machine learning’s online consumer behavior prediction results? We analyze 374,749 online consumer behavior data from Google Merchandise Store, an online shopping mall, and explore research questions. As a result of the empirical analysis, the performance of the ensemble model eXtreme Gradient Boosting model is most suitable for pre-dicting purchase conversion of online consumers, and oversampling is the best method to mitigate data imbalance bias. In addition, by applying explainable artificial intelligence methods to the context of retargeting advertisements, we investigate which consumers are effective in retargeting advertisements. This study theoretically contributes to the marketing and machine learning lit-erature by exploring and answering the problems that arise when applying machine learning models to predicting online consumer conversion. It also contributes to the online advertising literature by exploring consumer characteristics that are effective for retargeting advertisements.},
	language = {en},
	number = {5},
	urldate = {2022-02-14},
	journal = {Journal of Theoretical and Applied Electronic Commerce Research},
	author = {Lee, Jungwon and Jung, Okkyung and Lee, Yunhye and Kim, Ohsung and Park, Cheol},
	month = may,
	year = {2021},
	pages = {1472--1491},
	file = {Lee et al_2021_A Comparison and Interpretation of Machine Learning Algorithm for the.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7LI7NJ64\\Lee et al_2021_A Comparison and Interpretation of Machine Learning Algorithm for the.pdf:application/pdf},
}

@article{requenaShopperIntentPrediction2020,
	title = {Shopper intent prediction from clickstream e-commerce data with minimal browsing information},
	volume = {10},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-73622-y},
	doi = {10.1038/s41598-020-73622-y},
	abstract = {Abstract
            
              We address the problem of user intent prediction from clickstream data of an e-commerce website via two conceptually different approaches: a hand-crafted feature-based classification and a deep learning-based classification. In both approaches, we deliberately coarse-grain a new clickstream proprietary dataset to produce symbolic trajectories with minimal information. Then, we tackle the problem of trajectory classification of arbitrary length and ultimately, early prediction of limited-length trajectories, both for balanced and unbalanced datasets. Our analysis shows that
              k
              -gram statistics with visibility graph motifs produce fast and accurate classifications, highlighting that purchase prediction is reliable even for extremely short observation windows. In the deep learning case, we benchmarked previous state-of-the-art (SOTA) models on the new dataset, and improved classification accuracy over SOTA performances with our proposed LSTM architecture. We conclude with an in-depth error analysis and a careful evaluation of the pros and cons of the two approaches when applied to realistic industry use cases.},
	language = {en},
	number = {1},
	urldate = {2022-02-14},
	journal = {Scientific Reports},
	author = {Requena, Borja and Cassani, Giovanni and Tagliabue, Jacopo and Greco, Ciro and Lacasa, Lucas},
	month = dec,
	year = {2020},
	pages = {16983},
	file = {Requena et al_2020_Shopper intent prediction from clickstream e-commerce data with minimal.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FP2NFNGP\\Requena et al_2020_Shopper intent prediction from clickstream e-commerce data with minimal.pdf:application/pdf},
}

@misc{AnacondaStateData,
	title = {Anaconda {\textbar} {State} of {Data} {Science} 2020},
	url = {https://www.anaconda.com/state-of-data-science-2020},
	abstract = {Learn how practitioners and leaders can ensure data science achieves its full potential to improve society, consumer experience, and business performance.},
	language = {en},
	urldate = {2022-03-12},
	journal = {Anaconda},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S9C3TS5G\\state-of-data-science-2020.html:text/html},
}

@misc{GoogleAdsBenchmarks,
	title = {Google {Ads} {Benchmarks} for {YOUR} {Industry} [{Updated}!]},
	url = {https://www.wordstream.com/blog/ws/2016/02/29/google-adwords-industry-benchmarks},
	abstract = {We all want to create unicorn ads that have the highest CTRs and the best conversion rates. But a good metric for one industry isn’t necessarily good for another. What numbers should you be looking to beat in your industry? We dug into our data to find out!},
	language = {en-US},
	urldate = {2022-03-13},
	journal = {WordStream},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZGNHIL34\\google-adwords-industry-benchmarks.html:text/html},
}

@article{hasaninSeverelyImbalancedBig2019,
	title = {Severely imbalanced {Big} {Data} challenges: investigating data sampling approaches},
	volume = {6},
	issn = {2196-1115},
	shorttitle = {Severely imbalanced {Big} {Data} challenges},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0274-4},
	doi = {10.1186/s40537-019-0274-4},
	abstract = {Abstract
            
              Severe class imbalance between majority and minority classes in Big Data can bias the predictive performance of
              Machine Learning
              algorithms toward the majority (negative) class. Where the minority (positive) class holds greater value than the majority (negative) class and the occurrence of false negatives incurs a greater penalty than false positives, the bias may lead to adverse consequences. Our paper incorporates two case studies, each utilizing three learners, six sampling approaches, two performance metrics, and five sampled distribution ratios, to uniquely investigate the effect of severe class imbalance on Big Data analytics. The learners (
              Gradient-Boosted Trees, Logistic Regression, Random Forest
              ) were implemented within the Apache Spark framework. The first case study is based on a Medicare fraud detection dataset. The second case study, unlike the first, includes training data from one source (SlowlorisBig Dataset) and test data from a separate source (POST dataset). Results from the Medicare case study are not conclusive regarding the best sampling approach using
              Area Under the Receiver Operating Characteristic Curve
              and
              Geometric Mean
              performance metrics. However, it should be noted that the
              Random Undersampling
              approach performs adequately in the first case study. For the SlowlorisBig case study, Random Undersampling convincingly outperforms the other five sampling approaches (
              Random Oversampling, Synthetic Minority Over-sampling TEchnique, SMOTE-borderline1 , SMOTE-borderline2 , ADAptive SYNthetic
              ) when measuring performance with
              Area Under the Receiver Operating Characteristic Curve
              and
              Geometric Mean
              metrics. Based on its classification performance in both case studies,
              Random Undersampling
              is the best choice as it results in models with a significantly smaller number of samples, thus reducing computational burden and training time.},
	language = {en},
	number = {1},
	urldate = {2022-03-13},
	journal = {Journal of Big Data},
	author = {Hasanin, Tawfiq and Khoshgoftaar, Taghi M. and Leevy, Joffrey L. and Bauder, Richard A.},
	month = dec,
	year = {2019},
	pages = {107},
	file = {Hasanin et al_2019_Severely imbalanced Big Data challenges.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UQFXVP4W\\Hasanin et al_2019_Severely imbalanced Big Data challenges.pdf:application/pdf},
}

@inproceedings{zhou2019deep,
	title = {Deep interest evolution network for click-through rate prediction},
	volume = {33},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	author = {Zhou, Guorui and Mou, Na and Fan, Ying and Pi, Qi and Bian, Weijie and Zhou, Chang and Zhu, Xiaoqiang and Gai, Kun},
	year = {2019},
	pages = {5941--5948},
}

@inproceedings{pi2019practice,
	title = {Practice on long sequential user behavior modeling for click-through rate prediction},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} international conference on knowledge discovery \& data mining},
	author = {Pi, Qi and Bian, Weijie and Zhou, Guorui and Zhu, Xiaoqiang and Gai, Kun},
	year = {2019},
	pages = {2671--2679},
}

@article{qi2020search,
	title = {Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction},
	journal = {arXiv preprint arXiv:2006.05639},
	author = {Qi, Pi and Zhu, Xiaoqiang and Zhou, Guorui and Zhang, Yujing and Wang, Zhe and Ren, Lejian and Fan, Ying and Gai, Kun},
	year = {2020},
}

@misc{enwiki:1072814170,
	title = {Taobao — {Wikipedia}, the free encyclopedia},
	url = {https://en.wikipedia.org/w/index.php?title=Taobao&oldid=1072814170},
	author = {{Wikipedia contributors}},
	year = {2022},
	keywords = {deepcvr},
}

@article{chawlaSMOTESyntheticMinority2002,
	title = {{SMOTE}: {Synthetic} {Minority} {Over}-sampling {Technique}},
	volume = {16},
	issn = {1076-9757},
	shorttitle = {{SMOTE}},
	url = {https://www.jair.org/index.php/jair/article/view/10302},
	doi = {10.1613/jair.953},
	abstract = {An approach to the construction of classiﬁers from imbalanced datasets is described. A dataset is imbalanced if the classiﬁcation categories are not approximately equally represented. Often real-world data sets are predominately composed of “normal” examples with only a small percentage of “abnormal” or “interesting” examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classiﬁer to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classiﬁer performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classiﬁer performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classiﬁer. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
	language = {en},
	urldate = {2022-03-17},
	journal = {Journal of Artificial Intelligence Research},
	author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
	month = jun,
	year = {2002},
	keywords = {deepcvr},
	pages = {321--357},
	file = {Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MDUEMD3J\\Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf:application/pdf},
}

@article{ceritliPtypecatInferringType2021,
	title = {ptype-cat: {Inferring} the {Type} and {Values} of {Categorical} {Variables}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {ptype-cat},
	url = {https://arxiv.org/abs/2111.11956},
	doi = {10.48550/ARXIV.2111.11956},
	abstract = {Type inference is the task of identifying the type of values in a data column and has been studied extensively in the literature. Most existing type inference methods support data types such as Boolean, date, float, integer and string. However, these methods do not consider non-Boolean categorical variables, where there are more than two possible values encoded by integers or strings. Therefore, such columns are annotated either as integer or string rather than categorical, and need to be transformed into categorical manually by the user. In this paper, we propose a probabilistic type inference method that can identify the general categorical data type (including non-Boolean variables). Additionally, we identify the possible values of each categorical variable by adapting the existing type inference method ptype. Combining these methods, we present ptype-cat which achieves better results than existing applicable solutions.},
	urldate = {2022-03-18},
	author = {Ceritli, Taha and Williams, Christopher K. I.},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Databases (cs.DB), FOS: Computer and information sciences, Machine Learning (cs.LG), deepcvr},
}

@book{diehrDatabaseManagement1989,
	address = {Glenview, Ill},
	title = {Database management},
	isbn = {978-0-673-18820-5},
	publisher = {Scott, Foresman},
	author = {Diehr, George},
	year = {1989},
	keywords = {Database design, Database management, deepCVR},
}

@incollection{cirqueiraCustomerPurchaseBehavior2020,
	address = {Cham},
	title = {Customer {Purchase} {Behavior} {Prediction} in {E}-commerce: {A} {Conceptual} {Framework} and {Research} {Agenda}},
	volume = {11948},
	isbn = {978-3-030-48860-4 978-3-030-48861-1},
	shorttitle = {Customer {Purchase} {Behavior} {Prediction} in {E}-commerce},
	url = {http://link.springer.com/10.1007/978-3-030-48861-1_8},
	language = {en},
	urldate = {2022-04-02},
	booktitle = {New {Frontiers} in {Mining} {Complex} {Patterns}},
	publisher = {Springer International Publishing},
	author = {Cirqueira, Douglas and Hofer, Markus and Nedbal, Dietmar and Helfert, Markus and Bezbradica, Marija},
	editor = {Ceci, Michelangelo and Loglisci, Corrado and Manco, Giuseppe and Masciari, Elio and Ras, Zbigniew},
	year = {2020},
	doi = {10.1007/978-3-030-48861-1_8},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {119--136},
	file = {Cirqueira et al_2020_Customer Purchase Behavior Prediction in E-commerce.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TED7UUPY\\Cirqueira et al_2020_Customer Purchase Behavior Prediction in E-commerce.pdf:application/pdf},
}

@article{persons/Codd71a,
	title = {Further normalization of the data base relational model.},
	volume = {RJ909},
	journal = {Research Report / RJ / IBM / San Jose, California},
	author = {Codd, E. F.},
	month = aug,
	year = {1971},
	note = {tex.added-at: 2020-02-21T00:00:00.000+0100
tex.biburl: https://www.bibsonomy.org/bibtex/2fd32c70c89523301f453113992f3b36e/dblp
tex.interhash: 17b705578f1ee3f1066dedfbd6c5c174
tex.intrahash: fd32c70c89523301f453113992f3b36e
tex.timestamp: 2020-02-22T11:37:43.000+0100},
	keywords = {deepcvr, dblp},
}

@article{coddRelationalModelData1970,
	title = {A relational model of data for large shared data banks},
	volume = {13},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/362384.362685},
	doi = {10.1145/362384.362685},
	abstract = {Future users of large data banks must be protected from having to know how the data is organized in the machine (the internal representation). A prompting service which supplies such information is not a satisfactory solution. Activities of users at terminals and most application programs should remain unaffected when the internal representation of data is changed and even when some aspects of the external representation are changed. Changes in data representation will often be needed as a result of changes in query, update, and report traffic and natural growth in the types of stored information.
            
              Existing noninferential, formatted data systems provide users with tree-structured files or slightly more general network models of the data. In Section 1, inadequacies of these models are discussed. A model based on
              n
              -ary relations, a normal form for data base relations, and the concept of a universal data sublanguage are introduced. In Section 2, certain operations on relations (other than logical inference) are discussed and applied to the problems of redundancy and consistency in the user's model.},
	language = {en},
	number = {6},
	urldate = {2022-04-03},
	journal = {Communications of the ACM},
	author = {Codd, E. F.},
	month = jun,
	year = {1970},
	keywords = {deepcvr},
	pages = {377--387},
}

@book{dateIntroductionDatabaseSystems2004,
	address = {Boston},
	edition = {8th ed},
	title = {An introduction to database systems},
	isbn = {978-0-321-19784-9},
	publisher = {Pearson/Addison Wesley},
	author = {Date, C. J.},
	year = {2004},
	keywords = {deepcvr, Database management},
}

@inproceedings{songAutomatedNeuralInteraction2020,
	address = {Virtual Event CA USA},
	title = {Towards {Automated} {Neural} {Interaction} {Discovery} for {Click}-{Through} {Rate} {Prediction}},
	isbn = {978-1-4503-7998-4},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403137},
	doi = {10.1145/3394486.3403137},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Song, Qingquan and Cheng, Dehua and Zhou, Hanning and Yang, Jiyan and Tian, Yuandong and Hu, Xia},
	month = aug,
	year = {2020},
	pages = {945--955},
	file = {Song et al_2020_Towards Automated Neural Interaction Discovery for Click-Through Rate Prediction.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\K59A3FF5\\Song et al_2020_Towards Automated Neural Interaction Discovery for Click-Through Rate Prediction.pdf:application/pdf;Song et al. - 2020 - Towards Automated Neural Interaction Discovery for.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\284Z8U6W\\Song et al. - 2020 - Towards Automated Neural Interaction Discovery for.pdf:application/pdf},
}

@article{mengGeneralMethodAutomatic2021,
	title = {A {General} {Method} {For} {Automatic} {Discovery} of {Powerful} {Interactions} {In} {Click}-{Through} {Rate} {Prediction}},
	url = {http://arxiv.org/abs/2105.10484},
	doi = {10.1145/3404835.3462842},
	abstract = {Modeling powerful interactions is a critical challenge in Click-through rate (CTR) prediction, which is one of the most typical machine learning tasks in personalized advertising and recommender systems. Although developing hand-crafted interactions is effective for a small number of datasets, it generally requires laborious and tedious architecture engineering for extensive scenarios. In recent years, several neural architecture search (NAS) methods have been proposed for designing interactions automatically. However, existing methods only explore limited types and connections of operators for interaction generation, leading to low generalization ability. To address these problems, we propose a more general automated method for building powerful interactions named AutoPI. The main contributions of this paper are as follows: AutoPI adopts a more general search space in which the computational graph is generalized from existing network connections, and the interactive operators in the edges of the graph are extracted from representative hand-crafted works. It allows searching for various powerful feature interactions to produce higher AUC and lower Logloss in a wide variety of applications. Besides, AutoPI utilizes a gradient-based search strategy for exploration with a significantly low computational cost. Experimentally, we evaluate AutoPI on a diverse suite of benchmark datasets, demonstrating the generalizability and efficiency of AutoPI over hand-crafted architectures and state-of-the-art NAS algorithms.},
	urldate = {2022-04-04},
	journal = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	author = {Meng, Ze and Zhang, Jinnian and Li, Yumeng and Li, Jiancheng and Zhu, Tanchao and Sun, Lifeng},
	month = jul,
	year = {2021},
	note = {arXiv: 2105.10484},
	keywords = {Computer Science - Information Retrieval},
	pages = {1298--1307},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P7X8UQNE\\2105.html:text/html;Meng et al_2021_A General Method For Automatic Discovery of Powerful Interactions In.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QSI2Q5R6\\Meng et al_2021_A General Method For Automatic Discovery of Powerful Interactions In.pdf:application/pdf},
}

@inproceedings{khawarAutoFeatureSearchingFeature2020,
	address = {Virtual Event Ireland},
	title = {{AutoFeature}: {Searching} for {Feature} {Interactions} and {Their} {Architectures} for {Click}-through {Rate} {Prediction}},
	isbn = {978-1-4503-6859-9},
	shorttitle = {{AutoFeature}},
	url = {https://dl.acm.org/doi/10.1145/3340531.3411912},
	doi = {10.1145/3340531.3411912},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {ACM},
	author = {Khawar, Farhan and Hang, Xu and Tang, Ruiming and Liu, Bin and Li, Zhenguo and He, Xiuqiang},
	month = oct,
	year = {2020},
	pages = {625--634},
	file = {Khawar et al. - 2020 - AutoFeature Searching for Feature Interactions an.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RE7P5BLS\\Khawar et al. - 2020 - AutoFeature Searching for Feature Interactions an.pdf:application/pdf},
}

@article{xueAutomatedSearchSpace2022,
	title = {Automated search space and search strategy selection for {AutoML}},
	volume = {124},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320321006506},
	doi = {10.1016/j.patcog.2021.108474},
	language = {en},
	urldate = {2022-04-04},
	journal = {Pattern Recognition},
	author = {Xue, Chao and Hu, Mengting and Huang, Xueqi and Li, Chun-Guang},
	month = apr,
	year = {2022},
	pages = {108474},
	file = {Xue et al. - 2022 - Automated search space and search strategy selecti.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EUMIC95M\\Xue et al. - 2022 - Automated search space and search strategy selecti.pdf:application/pdf},
}

@article{baymurzinaReviewNeuralArchitecture2022,
	title = {A review of neural architecture search},
	volume = {474},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221018439},
	doi = {10.1016/j.neucom.2021.12.014},
	language = {en},
	urldate = {2022-04-04},
	journal = {Neurocomputing},
	author = {Baymurzina, Dilyara and Golikov, Eugene and Burtsev, Mikhail},
	month = feb,
	year = {2022},
	pages = {82--93},
	file = {Baymurzina et al. - 2022 - A review of neural architecture search.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Q6T2JKQW\\Baymurzina et al. - 2022 - A review of neural architecture search.pdf:application/pdf},
}

@article{olouladeGraphNeuralArchitecture2022,
	title = {Graph neural architecture search: {A} survey},
	volume = {27},
	issn = {1007-0214},
	shorttitle = {Graph neural architecture search},
	url = {https://ieeexplore.ieee.org/document/9645440/},
	doi = {10.26599/TST.2021.9010057},
	number = {4},
	urldate = {2022-04-04},
	journal = {Tsinghua Science and Technology},
	author = {Oloulade, Babatounde Moctard and Gao, Jianliang and Chen, Jiamin and Lyu, Tengfei and Al-Sabri, Raeed},
	month = aug,
	year = {2022},
	pages = {692--708},
	file = {Oloulade et al_2022_Graph neural architecture search.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RJ4GSGRH\\Oloulade et al_2022_Graph neural architecture search.pdf:application/pdf},
}

@article{wangAutomatedGraphMachine2022,
	title = {Automated {Graph} {Machine} {Learning}: {Approaches}, {Libraries} and {Directions}},
	shorttitle = {Automated {Graph} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2201.01288},
	abstract = {Graph machine learning has been extensively studied in both academic and industry. However, as the literature on graph learning booms with a vast number of emerging methods and techniques, it becomes increasingly difficult to manually design the optimal machine learning algorithm for different graph-related tasks. To tackle the challenge, automated graph machine learning, which aims at discovering the best hyper-parameter and neural architecture configuration for different graph tasks/data without manual design, is gaining an increasing number of attentions from the research community. In this paper, we extensively discuss automated graph machine approaches, covering hyper-parameter optimization (HPO) and neural architecture search (NAS) for graph machine learning. We briefly overview existing libraries designed for either graph machine learning or automated machine learning respectively, and further in depth introduce AutoGL, our dedicated and the world's first open-source library for automated graph machine learning. Last but not least, we share our insights on future research directions for automated graph machine learning. This paper is the first systematic and comprehensive discussion of approaches, libraries as well as directions for automated graph machine learning.},
	urldate = {2022-04-04},
	journal = {arXiv:2201.01288 [cs]},
	author = {Wang, Xin and Zhang, Ziwei and Zhu, Wenwu},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.01288},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\X6LSCVGV\\2201.html:text/html;Wang et al_2022_Automated Graph Machine Learning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7D59K8VB\\Wang et al_2022_Automated Graph Machine Learning.pdf:application/pdf},
}

@article{guoDifferentiableNeuralArchitecture2022,
	title = {Differentiable neural architecture learning for efficient neural networks},
	volume = {126},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320321006245},
	doi = {10.1016/j.patcog.2021.108448},
	language = {en},
	urldate = {2022-04-04},
	journal = {Pattern Recognition},
	author = {Guo, Qingbei and Wu, Xiao-Jun and Kittler, Josef and Feng, Zhiquan},
	month = jun,
	year = {2022},
	pages = {108448},
}

@article{rhoNASVADNeuralArchitecture2022,
	title = {{NAS}-{VAD}: {Neural} {Architecture} {Search} for {Voice} {Activity} {Detection}},
	shorttitle = {{NAS}-{VAD}},
	url = {http://arxiv.org/abs/2201.09032},
	abstract = {Various neural network-based approaches have been proposed for more robust and accurate voice activity detection (VAD). Manual design of such neural architectures is an error-prone and time-consuming process, which prompted the development of neural architecture search (NAS) that automatically design and optimize network architectures. While NAS has been successfully applied to improve performance in a variety of tasks, it has not yet been exploited in the VAD domain. In this paper, we present the first work that utilizes NAS approaches on the VAD task. To effectively search architectures for the VAD task, we propose a modified macro structure and a new search space with a much broader range of operations that includes attention operations. The results show that the network structures found by the propose NAS framework outperform previous manually designed state-of-the-art VAD models in various noise-added and real-world-recorded datasets. We also show that the architectures searched on a particular dataset achieve improved generalization performance on unseen audio datasets. Our code and models are available at https://github.com/daniel03c1/NAS\_VAD.},
	urldate = {2022-04-04},
	journal = {arXiv:2201.09032 [cs, eess]},
	author = {Rho, Daniel and Park, Jinhyeok and Ko, Jong Hwan},
	month = mar,
	year = {2022},
	note = {arXiv: 2201.09032},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P6QZL35S\\2201.html:text/html;Rho et al_2022_NAS-VAD.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IPIR5G52\\Rho et al_2022_NAS-VAD.pdf:application/pdf},
}

@article{hassantabarCURIOUSEfficientNeural2022,
	title = {{CURIOUS}: {Efficient} {Neural} {Architecture} {Search} {Based} on a {Performance} {Predictor} and {Evolutionary} {Search}},
	issn = {0278-0070, 1937-4151},
	shorttitle = {{CURIOUS}},
	url = {https://ieeexplore.ieee.org/document/9698855/},
	doi = {10.1109/TCAD.2022.3148202},
	urldate = {2022-04-04},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Hassantabar, Shayan and Dai, Xiaoliang and Jha, Niraj K.},
	year = {2022},
	pages = {1--1},
}

@article{weiMOODNASEfficientNeural2022,
	title = {{MOO}-{DNAS}: {Efficient} {Neural} {Network} {Design} via {Differentiable} {Architecture} {Search} {Based} on {Multi}-{Objective} {Optimization}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {{MOO}-{DNAS}},
	url = {https://ieeexplore.ieee.org/document/9698215/},
	doi = {10.1109/ACCESS.2022.3148323},
	urldate = {2022-04-04},
	journal = {IEEE Access},
	author = {Wei, Hui and Lee, Feifei and Hu, Chunyan and Chen, Qiu},
	year = {2022},
	pages = {14195--14207},
	file = {Wei et al_2022_MOO-DNAS.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HGWS5765\\Wei et al_2022_MOO-DNAS.pdf:application/pdf},
}

@misc{saidiAdvancedTechnologiesHumanity2022,
	title = {Advanced {Technologies} for {Humanity}: {Proceedings} of {International} {Conference} on {Advanced} {Technologies} for {Humanity} ({ICATH}'2021)},
	shorttitle = {Advanced {Technologies} for {Humanity}},
	language = {English},
	author = {Saidi, Rajaa and El Bhiri, Brahim and Maleh, Yassine and Mosallam, Ayman and Essaaidi, Mohammed},
	year = {2022},
	note = {ISBN: 9783030941888
OCLC: 1298745367},
}

@article{mehtaNASBenchSuiteNASEvaluation2022,
	title = {{NAS}-{Bench}-{Suite}: {NAS} {Evaluation} is ({Now}) {Surprisingly} {Easy}},
	shorttitle = {{NAS}-{Bench}-{Suite}},
	url = {http://arxiv.org/abs/2201.13396},
	abstract = {The release of tabular benchmarks, such as NAS-Bench-101 and NAS-Bench-201, has significantly lowered the computational overhead for conducting scientific research in neural architecture search (NAS). Although they have been widely adopted and used to tune real-world NAS algorithms, these benchmarks are limited to small search spaces and focus solely on image classification. Recently, several new NAS benchmarks have been introduced that cover significantly larger search spaces over a wide range of tasks, including object detection, speech recognition, and natural language processing. However, substantial differences among these NAS benchmarks have so far prevented their widespread adoption, limiting researchers to using just a few benchmarks. In this work, we present an in-depth analysis of popular NAS algorithms and performance prediction methods across 25 different combinations of search spaces and datasets, finding that many conclusions drawn from a few NAS benchmarks do not generalize to other benchmarks. To help remedy this problem, we introduce NAS-Bench-Suite, a comprehensive and extensible collection of NAS benchmarks, accessible through a unified interface, created with the aim to facilitate reproducible, generalizable, and rapid NAS research. Our code is available at https://github.com/automl/naslib.},
	urldate = {2022-04-04},
	journal = {arXiv:2201.13396 [cs, stat]},
	author = {Mehta, Yash and White, Colin and Zela, Arber and Krishnakumar, Arjun and Zabergja, Guri and Moradian, Shakiba and Safari, Mahmoud and Yu, Kaicheng and Hutter, Frank},
	month = feb,
	year = {2022},
	note = {arXiv: 2201.13396},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VIAFFMHA\\2201.html:text/html;Mehta et al_2022_NAS-Bench-Suite.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8375FXRL\\Mehta et al_2022_NAS-Bench-Suite.pdf:application/pdf},
}

@article{speckhardNeuralArchitectureSearch2022,
	title = {Neural {Architecture} {Search} for {Energy} {Efficient} {Always}-on {Audio} {Models}},
	url = {http://arxiv.org/abs/2202.05397},
	abstract = {Mobile and edge computing devices for always-on audio classification require energy-efficient neural network architectures. We present a neural architecture search (NAS) that optimizes accuracy, energy efficiency and memory usage. The search is run on Vizier, a black-box optimization service. We present a search strategy that uses both Bayesian and regularized evolutionary search with particle swarms, and employs early-stopping to reduce the computational burden. The search returns architectures for a sound-event classification dataset based upon AudioSet with similar accuracy to MobileNetV1/V2 implementations but with an order of magnitude less energy per inference and a much smaller memory footprint.},
	urldate = {2022-04-04},
	journal = {arXiv:2202.05397 [cs, eess]},
	author = {Speckhard, Daniel T. and Misiunas, Karolis and Perel, Sagi and Zhu, Tenghui and Carlile, Simon and Slaney, Malcolm},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.05397},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WBPYFNE6\\2202.html:text/html;Speckhard et al_2022_Neural Architecture Search for Energy Efficient Always-on Audio Models.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E3QINQ7L\\Speckhard et al_2022_Neural Architecture Search for Energy Efficient Always-on Audio Models.pdf:application/pdf},
}

@article{zhengMIGONASFastGeneralizable2021,
	title = {{MIGO}-{NAS}: {Towards} {Fast} and {Generalizable} {Neural} {Architecture} {Search}},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {{MIGO}-{NAS}},
	url = {https://ieeexplore.ieee.org/document/9377468/},
	doi = {10.1109/TPAMI.2021.3065138},
	number = {9},
	urldate = {2022-04-04},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zheng, Xiawu and Ji, Rongrong and Chen, Yuhang and Wang, Qiang and Zhang, Baochang and Chen, Jie and Ye, Qixiang and Huang, Feiyue and Tian, Yonghong},
	month = sep,
	year = {2021},
	pages = {2936--2952},
}

@article{vaccaroEmpiricalReviewAutomated2021,
	title = {An {Empirical} {Review} of {Automated} {Machine} {Learning}},
	volume = {10},
	issn = {2073-431X},
	url = {https://www.mdpi.com/2073-431X/10/1/11},
	doi = {10.3390/computers10010011},
	abstract = {In recent years, Automated Machine Learning (AutoML) has become increasingly important in Computer Science due to the valuable potential it offers. This is testified by the high number of works published in the academic field and the significant efforts made in the industrial sector. However, some problems still need to be resolved. In this paper, we review some Machine Learning (ML) models and methods proposed in the literature to analyze their strengths and weaknesses. Then, we propose their use—alone or in combination with other approaches—to provide possible valid AutoML solutions. We analyze those solutions from a theoretical point of view and evaluate them empirically on three Atari games from the Arcade Learning Environment. Our goal is to identify what, we believe, could be some promising ways to create truly effective AutoML frameworks, therefore able to replace the human expert as much as possible, thereby making easier the process of applying ML approaches to typical problems of specific domains. We hope that the findings of our study will provide useful insights for future research work in AutoML.},
	language = {en},
	number = {1},
	urldate = {2022-04-04},
	journal = {Computers},
	author = {Vaccaro, Lorenzo and Sansonetti, Giuseppe and Micarelli, Alessandro},
	month = jan,
	year = {2021},
	pages = {11},
	file = {Vaccaro et al_2021_An Empirical Review of Automated Machine Learning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TWCHRJIT\\Vaccaro et al_2021_An Empirical Review of Automated Machine Learning.pdf:application/pdf},
}

@article{benmezianeComprehensiveSurveyHardwareAware2021,
	title = {A {Comprehensive} {Survey} on {Hardware}-{Aware} {Neural} {Architecture} {Search}},
	url = {http://arxiv.org/abs/2101.09336},
	abstract = {Neural Architecture Search (NAS) methods have been growing in popularity. These techniques have been fundamental to automate and speed up the time consuming and error-prone process of synthesizing novel Deep Learning (DL) architectures. NAS has been extensively studied in the past few years. Arguably their most significant impact has been in image classification and object detection tasks where the state of the art results have been obtained. Despite the significant success achieved to date, applying NAS to real-world problems still poses significant challenges and is not widely practical. In general, the synthesized Convolution Neural Network (CNN) architectures are too complex to be deployed in resource-limited platforms, such as IoT, mobile, and embedded systems. One solution growing in popularity is to use multi-objective optimization algorithms in the NAS search strategy by taking into account execution latency, energy consumption, memory footprint, etc. This kind of NAS, called hardware-aware NAS (HW-NAS), makes searching the most efficient architecture more complicated and opens several questions. In this survey, we provide a detailed review of existing HW-NAS research and categorize them according to four key dimensions: the search space, the search strategy, the acceleration technique, and the hardware cost estimation strategies. We further discuss the challenges and limitations of existing approaches and potential future directions. This is the first survey paper focusing on hardware-aware NAS. We hope it serves as a valuable reference for the various techniques and algorithms discussed and paves the road for future research towards hardware-aware NAS.},
	urldate = {2022-04-04},
	journal = {arXiv:2101.09336 [cs]},
	author = {Benmeziane, Hadjer and Maghraoui, Kaoutar El and Ouarnoughi, Hamza and Niar, Smail and Wistuba, Martin and Wang, Naigang},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.09336},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Complexity},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DIQHW8X5\\2101.html:text/html;Benmeziane et al_2021_A Comprehensive Survey on Hardware-Aware Neural Architecture Search.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4NKZUVWY\\Benmeziane et al_2021_A Comprehensive Survey on Hardware-Aware Neural Architecture Search.pdf:application/pdf},
}

@article{heAutoMLSurveyStateoftheart2021,
	title = {{AutoML}: {A} survey of the state-of-the-art},
	volume = {212},
	issn = {09507051},
	shorttitle = {{AutoML}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705120307516},
	doi = {10.1016/j.knosys.2020.106622},
	language = {en},
	urldate = {2022-04-04},
	journal = {Knowledge-Based Systems},
	author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
	month = jan,
	year = {2021},
	pages = {106622},
	file = {He et al_2021_AutoML.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DSPYRSQ7\\He et al_2021_AutoML.pdf:application/pdf;He et al. - 2021 - AutoML A survey of the state-of-the-art.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JM7GF2UV\\He et al. - 2021 - AutoML A survey of the state-of-the-art.pdf:application/pdf},
}

@article{liHWNASBenchHardwareAwareNeural2021,
	title = {{HW}-{NAS}-{Bench}:{Hardware}-{Aware} {Neural} {Architecture} {Search} {Benchmark}},
	shorttitle = {{HW}-{NAS}-{Bench}},
	url = {http://arxiv.org/abs/2103.10584},
	abstract = {HardWare-aware Neural Architecture Search (HW-NAS) has recently gained tremendous attention by automating the design of DNNs deployed in more resource-constrained daily life devices. Despite its promising performance, developing optimal HW-NAS solutions can be prohibitively challenging as it requires cross-disciplinary knowledge in the algorithm, micro-architecture, and device-specific compilation. First, to determine the hardware-cost to be incorporated into the NAS process, existing works mostly adopt either pre-collected hardware-cost look-up tables or device-specific hardware-cost models. Both of them limit the development of HW-NAS innovations and impose a barrier-to-entry to non-hardware experts. Second, similar to generic NAS, it can be notoriously difficult to benchmark HW-NAS algorithms due to their significant required computational resources and the differences in adopted search spaces, hyperparameters, and hardware devices. To this end, we develop HW-NAS-Bench, the first public dataset for HW-NAS research which aims to democratize HW-NAS research to non-hardware experts and make HW-NAS research more reproducible and accessible. To design HW-NAS-Bench, we carefully collected the measured/estimated hardware performance of all the networks in the search spaces of both NAS-Bench-201 and FBNet, on six hardware devices that fall into three categories (i.e., commercial edge devices, FPGA, and ASIC). Furthermore, we provide a comprehensive analysis of the collected measurements in HW-NAS-Bench to provide insights for HW-NAS research. Finally, we demonstrate exemplary user cases to (1) show that HW-NAS-Bench allows non-hardware experts to perform HW-NAS by simply querying it and (2) verify that dedicated device-specific HW-NAS can indeed lead to optimal accuracy-cost trade-offs. The codes and all collected data are available at https://github.com/RICE-EIC/HW-NAS-Bench.},
	urldate = {2022-04-04},
	journal = {arXiv:2103.10584 [cs]},
	author = {Li, Chaojian and Yu, Zhongzhi and Fu, Yonggan and Zhang, Yongan and Zhao, Yang and You, Haoran and Yu, Qixuan and Wang, Yue and Lin, Yingyan},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.10584},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TJJUQ45X\\2103.html:text/html;Li et al_2021_HW-NAS-Bench.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N5YPVZLT\\Li et al_2021_HW-NAS-Bench.pdf:application/pdf},
}

@inproceedings{mehrotra2021nasbenchasr,
	title = {\{{NAS}\}-{Bench}-\{{ASR}\}: {Reproducible} neural architecture search for speech recognition},
	url = {https://openreview.net/forum?id=CU0APx9LMaL},
	booktitle = {International conference on learning representations},
	author = {Mehrotra, Abhinav and Ramos, Alberto Gil C. P. and Bhattacharya, Sourav and Dudziak, Łukasz and Vipperla, Ravichander and Chau, Thomas and Abdelfattah, Mohamed S and Ishtiaq, Samin and Lane, Nicholas Donald},
	year = {2021},
	file = {Mehrotra et al_2021_ NAS -Bench- ASR .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\INDMMMP7\\Mehrotra et al_2021_ NAS -Bench- ASR .pdf:application/pdf},
}

@article{aiGPNASNeuralNetwork2021,
	title = {{GPNAS}: {A} {Neural} {Network} {Architecture} {Search} {Framework} {Based} on {Graphical} {Predictor}},
	shorttitle = {{GPNAS}},
	url = {http://arxiv.org/abs/2103.11820},
	abstract = {In practice, the problems encountered in Neural Architecture Search (NAS) training are not simple problems, but often a series of difficult combinations (wrong compensation estimation, curse of dimension, overfitting, high complexity, etc.). In this paper, we propose a framework to decouple network structure from operator search space, and use two BOHBs to search alternatively. Considering that activation function and initialization are also important parts of neural network, the generalization ability of the model will be affected. We introduce an activation function and an initialization method domain, and add them into the operator search space to form a generalized search space, so as to improve the generalization ability of the child model. We then trained a GCN-based predictor using feedback from the child model. This can not only improve the search efficiency, but also solve the problem of dimension curse. Next, unlike other NAS studies, we used predictors to analyze the stability of different network structures. Finally, we applied our framework to neural structure search and achieved significant improvements on multiple datasets.},
	urldate = {2022-04-04},
	journal = {arXiv:2103.11820 [cs]},
	author = {Ai, Dige and Zhang, Hong},
	month = jul,
	year = {2021},
	note = {arXiv: 2103.11820},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Ai_Zhang_2021_GPNAS.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9Z32I7QZ\\Ai_Zhang_2021_GPNAS.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KDU2J4QK\\2103.html:text/html},
}

@article{krishnaDifferentiableNASFramework2021,
	title = {Differentiable {NAS} {Framework} and {Application} to {Ads} {CTR} {Prediction}},
	url = {http://arxiv.org/abs/2110.14812},
	abstract = {Neural architecture search (NAS) methods aim to automatically find the optimal deep neural network (DNN) architecture as measured by a given objective function, typically some combination of task accuracy and inference efficiency. For many areas, such as computer vision and natural language processing, this is a critical, yet still time consuming process. New NAS methods have recently made progress in improving the efficiency of this process. We implement an extensible and modular framework for Differentiable Neural Architecture Search (DNAS) to help solve this problem. We include an overview of the major components of our codebase and how they interact, as well as a section on implementing extensions to it (including a sample), in order to help users adopt our framework for their applications across different categories of deep learning models. To assess the capabilities of our methodology and implementation, we apply DNAS to the problem of ads click-through rate (CTR) prediction, arguably the highest-value and most worked on AI problem at hyperscalers today. We develop and tailor novel search spaces to a Deep Learning Recommendation Model (DLRM) backbone for CTR prediction, and report state-of-the-art results on the Criteo Kaggle CTR prediction dataset.},
	urldate = {2022-04-05},
	journal = {arXiv:2110.14812 [cs]},
	author = {Krishna, Ravi and Kalaiah, Aravind and Wu, Bichen and Naumov, Maxim and Mudigere, Dheevatsa and Smelyanskiy, Misha and Keutzer, Kurt},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.14812},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NIKS7ZIW\\2110.html:text/html;Krishna et al_2021_Differentiable NAS Framework and Application to Ads CTR Prediction.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4FXHK8VC\\Krishna et al_2021_Differentiable NAS Framework and Application to Ads CTR Prediction.pdf:application/pdf},
}

@inproceedings{yuanSurveyRealTime2014,
	address = {Qingdao, China},
	title = {A survey on real time bidding advertising},
	isbn = {978-1-4799-6058-3},
	url = {http://ieeexplore.ieee.org/document/6960761/},
	doi = {10.1109/SOLI.2014.6960761},
	urldate = {2022-04-06},
	booktitle = {Proceedings of 2014 {IEEE} {International} {Conference} on {Service} {Operations} and {Logistics}, and {Informatics}},
	publisher = {IEEE},
	author = {Yuan, Yong and Wang, Feiyue and Li, Juanjuan and Qin, Rui},
	month = oct,
	year = {2014},
	pages = {418--423},
}

@misc{WhatDemandSidePlatform2015,
	title = {What {Is} a {Demand}-{Side} {Platform} ({DSP}) and {How} {Does} {It} {Work}?},
	url = {https://clearcode.cc/blog/demand-side-platform/},
	abstract = {Demand-side platforms (DSPs) are an integral part of the online advertising ecosystem and play a key role in real-time bidding (RTB) auctions.},
	language = {en-US},
	urldate = {2022-04-06},
	journal = {Clearcode {\textbar} Custom AdTech and MarTech Development},
	month = feb,
	year = {2015},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EDLZC8QG\\demand-side-platform.html:text/html},
}

@misc{WhatSupplySidePlatform2018,
	title = {What is a {Supply}-{Side} {Platform} ({SSP}) and {How} {Does} {It} {Work}? {\textbar} {Clearcode}},
	shorttitle = {What is a {Supply}-{Side} {Platform} ({SSP}) and {How} {Does} {It} {Work}?},
	url = {https://clearcode.cc/blog/what-is-supply-side-platform/},
	abstract = {Supply-side platforms provide a programmatic way to deal with the ever-expanding array of AdTech platforms and variables in the programmatic ecosystem.},
	language = {en-US},
	urldate = {2022-04-06},
	journal = {Clearcode {\textbar} Custom AdTech and MarTech Development},
	month = oct,
	year = {2018},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5FYGV6CN\\what-is-supply-side-platform.html:text/html},
}

@misc{NativeAdvertising2022,
	title = {Native advertising},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Native_advertising&oldid=1081123284},
	abstract = {Native advertising, also called sponsored content, is a type of advertising that matches the form and function of the platform upon which it appears. In many cases it functions like an advertorial, and manifests as a video, article or editorial. The word native refers to this coherence of the content with the other media that appear on the platform.
These ads reduce a consumers' ad recognition by blending the ad into the native content of the platform, even if it is labeled as "sponsored" or "branded" content. Readers may have difficulty immediately identifying them as advertisements due to their ambiguous nature, especially when deceptive labels such as "From around the web" are used.Product placement (embedded marketing) is a precursor to native advertising. The former places the product within the content, whereas in native marketing, which is legally permissible in the US to the extent that there is sufficient disclosure, the product and content are merged.},
	language = {en},
	urldate = {2022-04-06},
	journal = {Wikipedia},
	month = apr,
	year = {2022},
	note = {Page Version ID: 1081123284},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7UBZKX3A\\Native_advertising.html:text/html},
}

@misc{DigitalDisplayAdvertising2022,
	title = {Digital display advertising},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Digital_display_advertising&oldid=1075045922},
	abstract = {Digital display advertising is graphic advertising on Internet websites, apps or social media through banners or other advertising formats made of text, images, video, and audio. The main purpose of display advertising is to deliver general advertisements and brand messages to site visitors. A display ad is usually interactive (i.e. clickable), which allows brands and advertisers to engage deeper with the users. A display ad can also be a companion ad for a non-clickable video ad.
According to eMarketer, Facebook and Twitter were set to take 33 percent of display ad spending market share by 2017. 
Desktop display advertising eclipsed search ad buying in 2014, with mobile ad spending overtaking display in 2015.},
	language = {en},
	urldate = {2022-04-06},
	journal = {Wikipedia},
	month = mar,
	year = {2022},
	note = {Page Version ID: 1075045922},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FGPBF9VT\\Digital_display_advertising.html:text/html},
}

@misc{SearchAdvertising2021,
	title = {Search advertising},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Search_advertising&oldid=1060989581},
	abstract = {In Internet marketing, search advertising is a method of placing online advertisements on web pages that show results from search engine queries. Through the same search-engine advertising services, ads can also be placed on Web pages with other published content.

Search advertisements are targeted to match key search terms (called keywords) entered on search engines. This targeting ability has contributed to the attractiveness of search advertising for advertisers. Consumers will often use a search engine to identify and compare purchasing options immediately before making a purchasing decision. The opportunity to present consumers with advertisements tailored to their immediate buying interests encourages consumers to click on search ads instead of unpaid search results, which are often less relevant. For the online user, Sponsored Search Advertisement offers highly relevant search results which are based on the consumer’s own queries and, thus, they are considered less intrusive than banner advertisements or pop-ups advertising. In addition, Sponsored Search Advertisement reduces online user search costs and increases the accessibility to useful information within a limited time frame. Consequently, Sponsored Search Advertisement has become an important element of online users browsing and information searching experiences on the Web. Search advertising is an alternative to SEO and SEM.},
	language = {en},
	urldate = {2022-04-06},
	journal = {Wikipedia},
	month = dec,
	year = {2021},
	note = {Page Version ID: 1060989581},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TKT2Y49F\\Search_advertising.html:text/html},
}

@article{renComprehensiveSurveyNeural2021,
	title = {A {Comprehensive} {Survey} of {Neural} {Architecture} {Search}: {Challenges} and {Solutions}},
	shorttitle = {A {Comprehensive} {Survey} of {Neural} {Architecture} {Search}},
	url = {http://arxiv.org/abs/2006.02903},
	abstract = {Deep learning has made breakthroughs and substantial in many fields due to its powerful automatic representation capabilities. It has been proven that neural architecture design is crucial to the feature representation of data and the final performance. However, the design of the neural architecture heavily relies on the researchers' prior knowledge and experience. And due to the limitations of human' inherent knowledge, it is difficult for people to jump out of their original thinking paradigm and design an optimal model. Therefore, an intuitive idea would be to reduce human intervention as much as possible and let the algorithm automatically design the neural architecture. Neural Architecture Search (NAS) is just such a revolutionary algorithm, and the related research work is complicated and rich. Therefore, a comprehensive and systematic survey on the NAS is essential. Previously related surveys have begun to classify existing work mainly based on the key components of NAS: search space, search strategy, and evaluation strategy. While this classification method is more intuitive, it is difficult for readers to grasp the challenges and the landmark work involved. Therefore, in this survey, we provide a new perspective: beginning with an overview of the characteristics of the earliest NAS algorithms, summarizing the problems in these early NAS algorithms, and then providing solutions for subsequent related research work. Besides, we conduct a detailed and comprehensive analysis, comparison, and summary of these works. Finally, we provide some possible future research directions.},
	urldate = {2022-04-07},
	journal = {arXiv:2006.02903 [cs, stat]},
	author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Chen, Xiaojiang and Wang, Xin},
	month = mar,
	year = {2021},
	note = {arXiv: 2006.02903},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\76H4PLT5\\2006.html:text/html;Ren et al_2021_A Comprehensive Survey of Neural Architecture Search.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9Y2X2VUF\\Ren et al_2021_A Comprehensive Survey of Neural Architecture Search.pdf:application/pdf},
}

@article{DeepNeuralArchitecture2021,
	title = {Deep {Neural} {Architecture} {Search}: {A} {Survey}},
	volume = {58},
	url = {https://crad.ict.ac.cn/EN/abstract/article_4326.shtml},
	doi = {10.7544/issn1000-1239.2021.20190851},
	number = {1},
	journal = {Journal of Computer Research and Development},
	year = {2021},
	note = {Publisher: Journal of Computer Research and Development},
	keywords = {deep learning, neural architecture search (NAS), perfor-mance evaluation, search space, search strategy},
	pages = {22},
}

@article{geraBridgingGapKnowledge2012,
	title = {Bridging the gap in knowledge transfer between academia and practitioners},
	volume = {26},
	issn = {0951-354X},
	url = {https://www.emerald.com/insight/content/doi/10.1108/09513541211213336/full/html},
	doi = {10.1108/09513541211213336},
	abstract = {Purpose
              The paper intends to identify the causes or gaps in transfer of managerial knowledge between academia and practitioners and to develop a framework that overcomes the gaps through knowledge management, information technology and human resource practices. The paper aims to suggest a strategic approach based on the knowledge transfer cycle.
            
            
              Design/methodology/approach
              The paper presents the development of a conceptual model based on existing research findings and conceptual models in the literature combined with the experience of academicians.
            
            
              Findings
              There has been very little transfer of research knowledge due to the inherent barriers in its creation, diffusion, adoption and utilization by practitioners. By enhancing the industry orientation of academicians and adopting systematic processes of review and dissemination, early adopters (practitioners) can experiment and learn to apply theoretical knowledge, which, when supported by institutional mechanisms, of human resource management, information technology and knowledge management (KM), can minimize or eliminate knowledge transfer gaps, leading to improved competitiveness and performance of the firm.
            
            
              Research limitations/implications
              The framework has been developed from concepts of KM and transfer and learning and needs to be validated empirically.
            
            
              Practical implications
              The framework developed can guide researchers in their approach toward knowledge creation so that their output is adopted by industry and thus has value. Practitioner industries can develop practices based on the framework to enhance their ability to leverage academic knowledge for competitive advantage.
            
            
              Social implications
              The paper would enable the framing of policies by higher education institutions and industry to facilitate more effective and efficient transfer of knowledge between researchers and practitioners, leading to enhanced organizational competitive advantage, which would benefit society.
            
            
              Originality/value
              The paper provides a framework based on the knowledge transfer cycle model for enhancing the effectiveness and efficiency of research knowledge adoption and utilization.},
	language = {en},
	number = {3},
	urldate = {2022-04-07},
	journal = {International Journal of Educational Management},
	author = {Gera, Rajat},
	editor = {Ranjan, Jayanthi},
	month = mar,
	year = {2012},
	pages = {252--273},
}

@article{elskenNeuralArchitectureSearch,
	title = {Neural {Architecture} {Search}: {A} {Survey}},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this ﬁeld of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	language = {en},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	pages = {21},
	file = {Elsken et al. - Neural Architecture Search A Survey.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\893RVC4D\\Elsken et al. - Neural Architecture Search A Survey.pdf:application/pdf},
}

@article{cazasnovesNeuralArchitectureSearch,
	title = {Neural {Architecture} {Search} in operational context: a remote sensing case-study},
	abstract = {Deep learning has become in recent years a cornerstone tool fueling key innovations in the industry, such as autonomous driving. To attain good performances, the neural network architecture used for a given application must be chosen with care. These architectures are often handcrafted and therefore prone to human biases and sub-optimal selection. Neural Architecture Search (NAS) is a framework introduced to mitigate such risks by jointly optimizing the network architectures and its weights. Albeit its novelty, it was applied on complex tasks with signiﬁcant results – e.g. semantic image segmentation. In this technical paper, we aim to evaluate its ability to tackle a challenging operational task: semantic segmentation of objects of interest in satellite imagery. Designing a NAS framework is not trivial and has strong dependencies to hardware constraints. We therefore motivate our NAS approach selection and provide corresponding implementation details. We also present novel ideas to carry out other such use-case studies.},
	language = {en},
	author = {Cazasnoves, Anthony and Ganaye, Pierre-Antoine and Sanchis, Kévin and Ceillier, Tugdual},
	pages = {24},
	file = {Cazasnoves et al. - Neural Architecture Search in operational context.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8CYYZLMD\\Cazasnoves et al. - Neural Architecture Search in operational context.pdf:application/pdf},
}

@article{dowdEmergencyDepartmentUtilization2014,
	title = {Emergency {Department} {Utilization} as a {Measure} of {Physician} {Performance}},
	volume = {29},
	issn = {1062-8606, 1555-824X},
	url = {http://journals.sagepub.com/doi/10.1177/1062860613487196},
	doi = {10.1177/1062860613487196},
	language = {en},
	number = {2},
	urldate = {2022-04-08},
	journal = {American Journal of Medical Quality},
	author = {Dowd, Bryan and Karmarker, Medha and Swenson, Tami and Parashuram, Shriram and Kane, Robert and Coulam, Robert and Jeffery, Molly Moore},
	month = mar,
	year = {2014},
	pages = {135--143},
}

@article{omalleyNewApproachesMeasuring2019,
	title = {New approaches to measuring the comprehensiveness of primary care physicians},
	volume = {54},
	issn = {00179124},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1475-6773.13101},
	doi = {10.1111/1475-6773.13101},
	language = {en},
	number = {2},
	urldate = {2022-04-08},
	journal = {Health Services Research},
	author = {O'Malley, Ann S. and Rich, Eugene C. and Shang, Lisa and Rose, Tyler and Ghosh, Arkadipta and Poznyak, Dmitriy and Peikes, Deborah},
	month = apr,
	year = {2019},
	pages = {356--366},
	file = {O'Malley et al_2019_New approaches to measuring the comprehensiveness of primary care physicians.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\K8IZY9XK\\O'Malley et al_2019_New approaches to measuring the comprehensiveness of primary care physicians.pdf:application/pdf},
}

@book{linesBigDataPredictive2020,
	address = {1 Oliver's Yard, 55 City Road, London EC1Y 1SP United Kingdom},
	title = {Big {Data} and {Predictive} {Modeling} {Using} a {Large} {Administrative} {Dataset} to {Understand} {Population}-{Level} {Changes} in {Emergency} {Department} {Use}},
	isbn = {978-1-5297-4451-4},
	url = {https://methods.sagepub.com/case/big-data-predictive-modeling-large-admin-dataset-changes-ed-use},
	urldate = {2022-04-08},
	publisher = {SAGE Publications Ltd},
	author = {Lines, Lisa},
	year = {2020},
	doi = {10.4135/9781529744514},
}

@article{timminsPathwaysReducedEmergency2020,
	title = {Pathways to reduced emergency department and urgent care center use: {Lessons} from the comprehensive primary care initiative},
	volume = {55},
	issn = {0017-9124, 1475-6773},
	shorttitle = {Pathways to reduced emergency department and urgent care center use},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1475-6773.13579},
	doi = {10.1111/1475-6773.13579},
	language = {en},
	number = {6},
	urldate = {2022-04-08},
	journal = {Health Services Research},
	author = {Timmins, Lori and Peikes, Deborah and McCall, Nancy},
	month = dec,
	year = {2020},
	pages = {1003--1012},
}

@misc{ImplicitBiasRacial,
	title = {Implicit {Bias} and {Racial} {Disparities} in {Health} {Care}},
	url = {https://www.americanbar.org/groups/crsj/publications/human_rights_magazine_home/the-state-of-healthcare-in-the-united-states/racial-disparities-in-health-care/},
	abstract = {Health care providers’ implicit biases may help explain racial disparities in health. We ought to take this possibility seriously, and we should not lose sight of structural causes of poor health care outcomes for racial minorities.},
	language = {en},
	urldate = {2022-04-08},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DGZ8LIGZ\\racial-disparities-in-health-care.html:text/html},
}

@misc{StudySuggestsMedical,
	title = {Study {Suggests} {Medical} {Errors} {Now} {Third} {Leading} {Cause} of {Death} in the {U}.{S}. - 05/03/2016},
	url = {https://www.hopkinsmedicine.org/news/media/releases/study_suggests_medical_errors_now_third_leading_cause_of_death_in_the_us},
	abstract = {Analyzing medical death rate data over an eight-year period, Johns Hopkins patient safety experts have calculated that more than 250,000 deaths per year are due to medical error in the U.S. Their figure, published May 3 in\&nbsp; The BMJ , surpasses the U.S. Centers for Disease Control and Prevention’s (CDC’s) third leading cause of death — respiratory disease, which kills close to 150,000 people per year.},
	language = {en},
	urldate = {2022-04-08},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KT65FWEQ\\study_suggests_medical_errors_now_third_leading_cause_of_death_in_the_us.html:text/html},
}

@misc{WhyGettingMedically2020,
	title = {Why {Getting} {Medically} {Misdiagnosed} {Is} {More} {Common} {Than} {You} {May} {Think}},
	url = {https://www.healthline.com/health-news/many-people-experience-getting-misdiagnosed},
	abstract = {Misdiagnosis is an under-discussed issue that touches all aspects of healthcare and kills more than 40,000 people annually.},
	language = {en},
	urldate = {2022-04-08},
	journal = {Healthline},
	month = feb,
	year = {2020},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SD2BCYBC\\many-people-experience-getting-misdiagnosed.html:text/html},
}

@incollection{gastilJurySystemCornerstone2018,
	edition = {1},
	title = {The {Jury} {System} as a {Cornerstone} of {Deliberative} {Democracy}},
	isbn = {978-1-108-28947-4 978-1-108-41820-1},
	url = {https://www.cambridge.org/core/product/identifier/9781108289474%23CN-bp-17/type/book_part},
	urldate = {2022-04-10},
	booktitle = {The {Cambridge} {Handbook} of {Deliberative} {Constitutionalism}},
	publisher = {Cambridge University Press},
	author = {Gastil, John and Hale, Dennis},
	editor = {Levy, Ron and Kong, Hoi and Orr, Graeme and King, Jeff},
	month = apr,
	year = {2018},
	doi = {10.1017/9781108289474.018},
	pages = {233--245},
}

@article{Hans2014DeliberativeDA,
	title = {Deliberative democracy and the american civil jury},
	journal = {LSN: Courts (Topic)},
	author = {Hans, Valerie P. and Gastil, John and Feller, Traci Margaret},
	year = {2014},
}

@book{tocquevilleDemocratieAmeriquePar1835,
	title = {De la démocratie en {Amérique}. {T}.2 / par {Alexis} de {Tocqueville},...},
	copyright = {domaine public},
	url = {https://gallica.bnf.fr/ark:/12148/btv1b86265903},
	abstract = {De la démocratie en Amérique. T.2 / par Alexis de Tocqueville,... -- 1835-1840 -- livre},
	language = {EN},
	urldate = {2022-04-10},
	author = {Tocqueville, Alexis de (1805-1859) Auteur du texte},
	year = {1835},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BULK4AM2\\f9.image.r=De la démocratie en Amérique.html:text/html},
}

@article{burnsHistoryTheoryAmerican1995,
	title = {The {History} and {Theory} of the {American} {Jury}},
	volume = {83},
	issn = {00081221},
	url = {https://www.jstor.org/stable/3480874?origin=crossref},
	doi = {10.2307/3480874},
	number = {6},
	urldate = {2022-04-10},
	journal = {California Law Review},
	author = {Burns, Robert P. and Abramson, Jeffrey},
	month = dec,
	year = {1995},
	pages = {1477},
	file = {Burns_Abramson_1995_The History and Theory of the American Jury.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8XKAEK8B\\Burns_Abramson_1995_The History and Theory of the American Jury.pdf:application/pdf},
}

@article{garelyBenefitRiskAssessmentTolterodine2004,
	title = {Benefit-{Risk} {Assessment} of {Tolterodine} in the {Treatment} of {Overactive} {Bladder} in {Adults}:},
	volume = {27},
	issn = {0114-5916},
	shorttitle = {Benefit-{Risk} {Assessment} of {Tolterodine} in the {Treatment} of {Overactive} {Bladder} in {Adults}},
	url = {http://link.springer.com/10.2165/00002018-200427130-00005},
	doi = {10.2165/00002018-200427130-00005},
	language = {en},
	number = {13},
	urldate = {2022-04-11},
	journal = {Drug Safety},
	author = {Garely, Alan D and Burrows, Lara},
	year = {2004},
	keywords = {oab},
	pages = {1043--1057},
}

@article{zinnerEfficacySafetyTolerability2002,
	title = {Efficacy, {Safety}, and {Tolerability} of {Extended}-{Release} {Once}-{Daily} {Tolterodine} {Treatment} for {Overactive} {Bladder} in {Older} versus {Younger} {Patients}},
	volume = {50},
	issn = {00028614, 15325415},
	url = {http://doi.wiley.com/10.1046/j.1532-5415.2002.50203.x},
	doi = {10.1046/j.1532-5415.2002.50203.x},
	language = {en},
	number = {5},
	urldate = {2022-04-11},
	journal = {Journal of the American Geriatrics Society},
	author = {Zinner, Norman R. and Mattiasson, Anders and Stanton, Stuart L.},
	month = may,
	year = {2002},
	pages = {799--807},
}

@article{staskinTrospiumChlorideHas2010,
	title = {Trospium chloride has no effect on memory testing and is assay undetectable in the central nervous system of older patients with overactive bladder: {Trospium} is undetectable in the older human {CNS}},
	volume = {64},
	issn = {13685031},
	shorttitle = {Trospium chloride has no effect on memory testing and is assay undetectable in the central nervous system of older patients with overactive bladder},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1742-1241.2010.02433.x},
	doi = {10.1111/j.1742-1241.2010.02433.x},
	language = {en},
	number = {9},
	urldate = {2022-04-11},
	journal = {International Journal of Clinical Practice},
	author = {Staskin, D. and Kay, G. and Tannenbaum, C. and Goldman, H. B. and Bhashi, K. and Ling, J. and Oefelein, M. G.},
	month = aug,
	year = {2010},
	pages = {1294--1300},
}

@article{kayPreservingCognitiveFunction2008,
	title = {Preserving cognitive function for patients with overactive bladder: evidence for a differential effect with darifenacin: {Cognitive} function for patients with {OAB}},
	volume = {62},
	issn = {13685031},
	shorttitle = {Preserving cognitive function for patients with overactive bladder},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1742-1241.2008.01849.x},
	doi = {10.1111/j.1742-1241.2008.01849.x},
	language = {en},
	number = {11},
	urldate = {2022-04-11},
	journal = {International Journal of Clinical Practice},
	author = {Kay, G. G. and Ebinger, U.},
	month = aug,
	year = {2008},
	pages = {1792--1800},
	file = {Kay_Ebinger_2008_Preserving cognitive function for patients with overactive bladder.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5I9JTNX5\\Kay_Ebinger_2008_Preserving cognitive function for patients with overactive bladder.pdf:application/pdf},
}

@article{araklitisCognitiveEffectsAnticholinergic2020,
	title = {Cognitive {Effects} of {Anticholinergic} {Load} in {Women} with {Overactive} {Bladder}},
	volume = {Volume 15},
	issn = {1178-1998},
	url = {https://www.dovepress.com/cognitive-effects-of-anticholinergic-load-in-women-with-overactive-bla-peer-reviewed-article-CIA},
	doi = {10.2147/CIA.S252852},
	language = {en},
	urldate = {2022-04-11},
	journal = {Clinical Interventions in Aging},
	author = {Araklitis, George and Robinson, Dudley and Cardozo, Linda},
	month = aug,
	year = {2020},
	pages = {1493--1503},
	file = {Araklitis et al_2020_Cognitive Effects of Anticholinergic Load in Women with Overactive Bladder.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZLIZ66DW\\Araklitis et al_2020_Cognitive Effects of Anticholinergic Load in Women with Overactive Bladder.pdf:application/pdf},
}

@article{zinnerEfficacyTolerabilityDarifenacin2005,
	title = {Efficacy and tolerability of darifenacin, a muscarinic {M3} selective receptor antagonist ({M3} {SRA}), compared with oxybutynin in the treatment of patients with overactive bladder},
	volume = {23},
	issn = {0724-4983, 1433-8726},
	url = {http://link.springer.com/10.1007/s00345-005-0507-3},
	doi = {10.1007/s00345-005-0507-3},
	language = {en},
	number = {4},
	urldate = {2022-04-11},
	journal = {World Journal of Urology},
	author = {Zinner, Norman and Tuttle, John and Marks, Leonard},
	month = sep,
	year = {2005},
	pages = {248--252},
}

@article{kayAntimuscarinicAgentsImplications2005,
	title = {Antimuscarinic agents: {Implications} and concerns in themanagement of overactive bladder in the elderly},
	volume = {27},
	issn = {01492918},
	shorttitle = {Antimuscarinic agents},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S014929180500007X},
	doi = {10.1016/j.clinthera.2005.01.006},
	language = {en},
	number = {1},
	urldate = {2022-04-11},
	journal = {Clinical Therapeutics},
	author = {Kay, Gary G. and Granville, Lisa J.},
	month = jan,
	year = {2005},
	pages = {127--138},
}

@article{manjunathaProspectiveComparativeStudy2015,
	title = {A {Prospective}, {Comparative} {Study} of the {Occurrence} and {Severity} of {Constipation} with {Darifenacin} and {Trospium} in {Overactive} {Bladder}},
	issn = {2249782X},
	url = {http://jcdr.net/article_fulltext.asp?issn=0973-709x&year=2015&volume=9&issue=3&page=FC05&issn=0973-709x&id=5677},
	doi = {10.7860/JCDR/2015/11884.5677},
	urldate = {2022-04-11},
	journal = {JOURNAL OF CLINICAL AND DIAGNOSTIC RESEARCH},
	author = {Manjunatha, Revanna},
	year = {2015},
	file = {Manjunatha_2015_A Prospective, Comparative Study of the Occurrence and Severity of Constipation.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FBQ9NLZ2\\Manjunatha_2015_A Prospective, Comparative Study of the Occurrence and Severity of Constipation.pdf:application/pdf},
}

@misc{DataScienceBootcamp,
	title = {Data {Science} {Bootcamp}: {Become} a {Data} {Scientist}, {Job} {Guaranteed}},
	shorttitle = {Data {Science} {Bootcamp}},
	url = {https://www.springboard.com/courses/data-science-career-track},
	abstract = {Learn data science online with 1:1 mentoring and complete 14 projects to build an impressive portfolio. Get a job or your money back.},
	language = {en},
	urldate = {2022-04-15},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BY6PLZM2\\data-science-career-track.html:text/html},
}

@misc{taylorAdmiralMcRavenMake2014,
	title = {Admiral {McRaven} "{Make} {Your} {Bed}" {Commencement} {Speech} {Transcript}},
	url = {https://www.rev.com/blog/transcripts/admiral-william-mcraven-make-your-bed-commencement-speech-transcript},
	abstract = {Admiral McRaven: (00:00) Thank you very much, thank you. Well, thank you president Powers, Provost Fenves, deans, members of the faculty, family and friends, a},
	language = {en-US},
	urldate = {2022-04-15},
	journal = {Rev},
	author = {Taylor, Ryan},
	month = may,
	year = {2014},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\44E7Q7J8\\admiral-william-mcraven-make-your-bed-commencement-speech-transcript.html:text/html},
}

@misc{SpringboardTuitionPricing,
	title = {Springboard {Tuition}, {Pricing}, and {Payment} {Plan} {Options}},
	url = {https://www.springboard.com/courses/software-engineering-career-track/},
	abstract = {Springboard offers four payment options: upfront, monthly, deferred tuition, and financing - here's how each works for students.},
	language = {en},
	urldate = {2022-04-15},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LN59KN2C\\payment-options.html:text/html},
}

@misc{MeetOurMentors,
	title = {Meet our {Mentors} {\textbar} {Springboard}},
	url = {https://www.springboard.com/mentors},
	abstract = {Springboard's mentors work at some of the world's best companies. Here's what they have to say.},
	language = {en},
	urldate = {2022-04-15},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZYNIHYRN\\mentors.html:text/html},
}

@misc{enwiki:1031231997,
	title = {Cohort (educational group) — {Wikipedia}, the free encyclopedia},
	url = {https://en.wikipedia.org/w/index.php?title=Cohort_(educational_group)&oldid=1031231997},
	author = {{Wikipedia contributors}},
	year = {2021},
}

@misc{SparkPythonBig,
	title = {Spark and {Python} for {Big} {Data} with {PySpark}},
	url = {https://www.udemy.com/course/spark-and-python-for-big-data-with-pyspark/},
	abstract = {Learn how to use Spark with Python, including Spark Streaming, Machine Learning, Spark 2.0 DataFrames and more!},
	language = {en-us},
	urldate = {2022-04-22},
	journal = {Udemy},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EGYKS946\\6681692.html:text/html},
}

@book{bondyPreparingInclusiveTeaching2005,
	address = {Albany},
	title = {Preparing for inclusive teaching: meeting the challenges of teacher education reform},
	isbn = {978-0-7914-6357-4 978-0-7914-6358-1 978-0-7914-8374-9},
	shorttitle = {Preparing for inclusive teaching},
	url = {http://site.ebrary.com/id/10579212},
	language = {English},
	urldate = {2022-04-23},
	publisher = {State University of New York Press},
	author = {Bondy, Elizabeth and Ross, Dorene Doerre},
	year = {2005},
	note = {OCLC: 841724431},
}

@misc{wangLessBetterUnweighted2021,
	title = {Less {Is} {Better}: {Unweighted} {Data} {Subsampling} via {Influence} {Function}},
	shorttitle = {Less {Is} {Better}},
	url = {http://arxiv.org/abs/1912.01321},
	abstract = {In the time of Big Data, training complex models on large-scale data sets is challenging, making it appealing to reduce data volume for saving computation resources by subsampling. Most previous works in subsampling are weighted methods designed to help the performance of subset-model approach the full-set-model, hence the weighted methods have no chance to acquire a subset-model that is better than the full-set-model. However, we question that how can we achieve better model with less data? In this work, we propose a novel Unweighted Influence Data Subsampling (UIDS) method, and prove that the subset-model acquired through our method can outperform the full-set-model. Besides, we show that overly confident on a given test set for sampling is common in Influence-based subsampling methods, which can eventually cause our subset-model's failure in out-of-sample test. To mitigate it, we develop a probabilistic sampling scheme to control the worst-case risk over all distributions close to the empirical distribution. The experiment results demonstrate our methods superiority over existed subsampling methods in diverse tasks, such as text classification, image classification, click-through prediction, etc.},
	urldate = {2022-05-14},
	publisher = {arXiv},
	author = {Wang, Zifeng and Zhu, Hong and Dong, Zhenhua and He, Xiuqiang and Huang, Shao-Lun},
	month = apr,
	year = {2021},
	note = {Number: arXiv:1912.01321
arXiv:1912.01321 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N93WKJZZ\\1912.html:text/html;Wang et al_2021_Less Is Better.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\29Q5JSZJ\\Wang et al_2021_Less Is Better.pdf:application/pdf},
}

@book{dhillonKDD1319th2013,
	title = {{KDD} '13: the 19th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} : {August} 11-14, 2013, {Chicago}, {Illinois}, {USA}.},
	isbn = {978-1-4503-2174-7},
	shorttitle = {{KDD} '13},
	language = {English},
	author = {Dhillon, Inderjit S and {International Conference on Knowledge Discovery \& Data Mining} and International Conference on Knowledge Discovery \& Data Mining, Association for Computing Machinery and {Special Interest Group on Management of Data} and {Association for Computing Machinery} and {Special Interest Group on Knowledge Discovery \& Data Mining}},
	year = {2013},
	note = {OCLC: 1164868226},
}

@inproceedings{yilmazDataMiningTechniques2021,
	address = {Silicon Valley CA USA},
	title = {Data {Mining} {Techniques} in {Direct} {Marketing} on {Imbalanced} {Data} using {Tomek} {Link} {Combined} with {Random} {Under}-sampling},
	isbn = {978-1-4503-8954-9},
	url = {https://dl.acm.org/doi/10.1145/3471287.3471299},
	doi = {10.1145/3471287.3471299},
	language = {en},
	urldate = {2022-05-14},
	booktitle = {2021 the 5th {International} {Conference} on {Information} {System} and {Data} {Mining}},
	publisher = {ACM},
	author = {Yılmaz, Ümit and Gezer, Cengiz and Aydın, Zafer and Güngör, V. Çağrı},
	month = may,
	year = {2021},
	pages = {67--73},
}

@article{atClassificationImbalanceData2016,
	title = {Classification of {Imbalance} {Data} using {Tomek} {Link} ({T}-{Link}) {Combined} with {Random} {Under}-sampling ({RUS}) as a {Data} {Reduction} {Method}},
	volume = {01},
	issn = {22298711},
	url = {https://www.omicsonline.org/open-access/classification-of-imbalance-data-using-tomek-link-tlink-combined-with-random-undersampling-rus-as-a-data-reduction-method-2229-8711-S1111-95226.html},
	doi = {10.4172/2229-8711.S1111},
	number = {S1},
	urldate = {2022-05-14},
	journal = {Global Journal of Technology and Optimization},
	author = {At, Elhassan and M, Aljourf and F, Al-Mohanna and M, Shoukri},
	year = {2016},
	file = {At et al_2016_Classification of Imbalance Data using Tomek Link (T-Link) Combined with Random.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4YF99LK7\\At et al_2016_Classification of Imbalance Data using Tomek Link (T-Link) Combined with Random.pdf:application/pdf},
}

@article{lotschOptimalDistributionpreservingDownsampling2021,
	title = {Optimal distribution-preserving downsampling of large biomedical data sets ({opdisDownsampling})},
	volume = {16},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0255838},
	doi = {10.1371/journal.pone.0255838},
	abstract = {Motivation
              The size of today’s biomedical data sets pushes computer equipment to its limits, even for seemingly standard analysis tasks such as data projection or clustering. Reducing large biomedical data by downsampling is therefore a common early step in data processing, often performed as random uniform class-proportional downsampling. In this report, we hypothesized that this can be optimized to obtain samples that better reflect the entire data set than those obtained using the current standard method.
            
            
              Results
              By repeating the random sampling and comparing the distribution of the drawn sample with the distribution of the original data, it was possible to establish a method for obtaining subsets of data that better reflect the entire data set than taking only the first randomly selected subsample, as is the current standard. Experiments on artificial and real biomedical data sets showed that the reconstruction of the remaining data from the original data set from the downsampled data improved significantly. This was observed with both principal component analysis and autoencoding neural networks. The fidelity was dependent on both the number of cases drawn from the original and the number of samples drawn.
            
            
              Conclusions
              Optimal distribution-preserving class-proportional downsampling yields data subsets that reflect the structure of the entire data better than those obtained with the standard method. By using distributional similarity as the only selection criterion, the proposed method does not in any way affect the results of a later planned analysis.},
	language = {en},
	number = {8},
	urldate = {2022-05-14},
	journal = {PLOS ONE},
	author = {Lötsch, Jörn and Malkusch, Sebastian and Ultsch, Alfred},
	editor = {Mian Qaisar, Saeed},
	month = aug,
	year = {2021},
	pages = {e0255838},
	file = {Lötsch et al_2021_Optimal distribution-preserving downsampling of large biomedical data sets.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D9G5JGGZ\\Lötsch et al_2021_Optimal distribution-preserving downsampling of large biomedical data sets.pdf:application/pdf},
}

@article{xiongClickthroughRatePrediction2019,
	title = {A {Clickthrough} {Rate} {Prediction} {Algorithm} {Based} on {Users}’ {Behaviors}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8918314/},
	doi = {10.1109/ACCESS.2019.2957054},
	urldate = {2022-05-14},
	journal = {IEEE Access},
	author = {Xiong, Xi and Xie, Chuan and Zhao, Rongmei and Li, Yuanyuan and Ju, Shenggen and Jin, Ming},
	year = {2019},
	pages = {174782--174792},
	file = {Xiong et al_2019_A Clickthrough Rate Prediction Algorithm Based on Users’ Behaviors.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XXZZW7S6\\Xiong et al_2019_A Clickthrough Rate Prediction Algorithm Based on Users’ Behaviors.pdf:application/pdf},
}

@article{luAdaptiveHybridXdeepFM2021,
	title = {An adaptive hybrid {XdeepFM} based deep {Interest} network model for click-through rate prediction system},
	volume = {7},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-716},
	doi = {10.7717/peerj-cs.716},
	abstract = {Recent advances in communication enable individuals to use phones and computers to access information on the web. E-commerce has seen rapid development, e.g., Alibaba has nearly 12 hundred million customers in China. Click-Through Rate (CTR) forecasting is a primary task in the e-commerce advertisement system. From the traditional Logistic Regression algorithm to the latest popular deep neural network methods that follow a similar embedding and MLP, several algorithms are used to predict CTR. This research proposes a hybrid model combining the Deep Interest Network (DIN) and eXtreme Deep Factorization Machine (xDeepFM) to perform CTR prediction robustly. The cores of DIN and xDeepFM are attention and feature cross, respectively. DIN follows an adaptive local activation unit that incorporates the attention mechanism to adaptively learn user interest from historical behaviors related to specific advertisements. xDeepFM further includes a critical part, a Compressed Interactions Network (CIN), aiming to generate feature interactions at a vectorwise level implicitly. Furthermore, a CIN, plain DNN, and a linear part are combined into one unified model to form xDeepFM. The proposed end-to-end hybrid model is a parallel ensemble of models via multilayer perceptron. CIN and xDeepFM are trained in parallel, and their output is fed into a multilayer perceptron. We used the e-commerce Alibaba dataset with the focal loss as the loss function for experimental evaluation through online complex example mining (OHEM) in the training process. The experimental result indicates that the proposed hybrid model has better performance than other models.},
	language = {en},
	urldate = {2022-05-15},
	journal = {PeerJ Computer Science},
	author = {Lu, Qiao and Li, Silin and Yang, Tuo and Xu, Chenheng},
	month = sep,
	year = {2021},
	pages = {e716},
	file = {Lu et al_2021_An adaptive hybrid XdeepFM based deep Interest network model for click-through.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TR75L8RG\\Lu et al_2021_An adaptive hybrid XdeepFM based deep Interest network model for click-through.pdf:application/pdf},
}

@misc{liuSurveySamplingProfiling2020,
	title = {A {Survey} on {Sampling} and {Profiling} over {Big} {Data} ({Technical} {Report})},
	url = {http://arxiv.org/abs/2005.05079},
	abstract = {Due to the development of internet technology and computer science, data is exploding at an exponential rate. Big data brings us new opportunities and challenges. On the one hand, we can analyze and mine big data to discover hidden information and get more potential value. On the other hand, the 5V characteristic of big data, especially Volume which means large amount of data, brings challenges to storage and processing. For some traditional data mining algorithms, machine learning algorithms and data profiling tasks, it is very difficult to handle such a large amount of data. The large amount of data is highly demanding hardware resources and time consuming. Sampling methods can effectively reduce the amount of data and help speed up data processing. Hence, sampling technology has been widely studied and used in big data context, e.g., methods for determining sample size, combining sampling with big data processing frameworks. Data profiling is the activity that finds metadata of data set and has many use cases, e.g., performing data profiling tasks on relational data, graph data, and time series data for anomaly detection and data repair. However, data profiling is computationally expensive, especially for large data sets. Therefore, this paper focuses on researching sampling and profiling in big data context and investigates the application of sampling in different categories of data profiling tasks. From the experimental results of these studies, the results got from the sampled data are close to or even exceed the results of the full amount of data. Therefore, sampling technology plays an important role in the era of big data, and we also have reason to believe that sampling technology will become an indispensable step in big data processing in the future.},
	urldate = {2022-05-15},
	publisher = {arXiv},
	author = {Liu, Zhicheng and Zhang, Aoqian},
	month = may,
	year = {2020},
	note = {Number: arXiv:2005.05079
arXiv:2005.05079 [cs]},
	keywords = {Computer Science - Databases},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2GE76GGA\\2005.html:text/html;Liu_Zhang_2020_A Survey on Sampling and Profiling over Big Data (Technical Report).pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RZS4E3MZ\\Liu_Zhang_2020_A Survey on Sampling and Profiling over Big Data (Technical Report).pdf:application/pdf},
}

@article{mahmudSurveyDataPartitioning2020,
	title = {A survey of data partitioning and sampling methods to support big data analysis},
	volume = {3},
	issn = {2096-0654},
	url = {https://ieeexplore.ieee.org/document/9007871/},
	doi = {10.26599/BDMA.2019.9020015},
	abstract = {Computer clusters with the shared-nothing architecture are the major computing platforms for big data processing and analysis. In cluster computing, data partitioning and sampling are two fundamental strategies to speed up the computation of big data and increase scalability. In this paper, we present a comprehensive survey of the methods and techniques of data partitioning and sampling with respect to big data processing and analysis. We start with an overview of the mainstream big data frameworks on Hadoop clusters. The basic methods of data partitioning are then discussed including three classical horizontal partitioning schemes: range, hash, and random partitioning. Data partitioning on Hadoop clusters is also discussed with a summary of new strategies for big data partitioning, including the new Random Sample Partition (RSP) distributed model. The classical methods of data sampling are then investigated, including simple random sampling, stratiﬁed sampling, and reservoir sampling. Two common methods of big data sampling on computing clusters are also discussed: record-level sampling and blocklevel sampling. Record-level sampling is not as efﬁcient as block-level sampling on big distributed data. On the other hand, block-level sampling on data blocks generated with the classical data partitioning methods does not necessarily produce good representative samples for approximate computing of big data. In this survey, we also summarize the prevailing strategies and related work on sampling-based approximation on Hadoop clusters. We believe that data partitioning and sampling should be considered together to build approximate cluster computing frameworks that are reliable in both the computational and statistical respects.},
	language = {en},
	number = {2},
	urldate = {2022-05-15},
	journal = {Big Data Mining and Analytics},
	author = {Mahmud, Mohammad Sultan and Huang, Joshua Zhexue and Salloum, Salman and Emara, Tamer Z. and Sadatdiynov, Kuanishbay},
	month = jun,
	year = {2020},
	pages = {85--101},
	file = {Mahmud et al. - 2020 - A survey of data partitioning and sampling methods.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AJAKZKXS\\Mahmud et al. - 2020 - A survey of data partitioning and sampling methods.pdf:application/pdf},
}

@article{yaoReviewOptimalSubsampling,
	title = {A {Review} on {Optimal} {Subsampling} {Methods} for {Massive} {Datasets}},
	abstract = {Subsampling is an eﬀective way to deal with big data problems and many subsampling approaches have been proposed for diﬀerent models, such as leverage sampling for linear regression models and local case control sampling for logistic regression models. In this article, we focus on optimal subsampling methods, which draw samples according to optimal subsampling probabilities formulated by minimizing some function of the asymptotic distribution. The optimal subsampling methods have been investigated to include logistic regression models, softmax regression models, generalized linear models, quantile regression models, and quasi-likelihood estimation. Real data examples are provided to show how optimal subsampling methods are applied.},
	language = {en},
	author = {Yao, Yaqiong and Wang, HaiYing},
	pages = {22},
	file = {Yao and Wang - A Review on Optimal Subsampling Methods for Massiv.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HYVCGG54\\Yao and Wang - A Review on Optimal Subsampling Methods for Massiv.pdf:application/pdf;Yao and Wang - A Review on Optimal Subsampling Methods for Massiv.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MCXI7XXD\\Yao and Wang - A Review on Optimal Subsampling Methods for Massiv.pdf:application/pdf},
}

@article{fithianLocalCasecontrolSampling2014,
	title = {Local case-control sampling: {Efficient} subsampling in imbalanced data sets},
	volume = {42},
	issn = {0090-5364},
	shorttitle = {Local case-control sampling},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-42/issue-5/Local-case-control-sampling--Efficient-subsampling-in-imbalanced-data/10.1214/14-AOS1220.full},
	doi = {10.1214/14-AOS1220},
	language = {en},
	number = {5},
	urldate = {2022-05-15},
	journal = {The Annals of Statistics},
	author = {Fithian, William and Hastie, Trevor},
	month = oct,
	year = {2014},
	file = {Fithian and Hastie - 2014 - Local case-control sampling Efficient subsampling.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RQJI7GJK\\Fithian and Hastie - 2014 - Local case-control sampling Efficient subsampling.pdf:application/pdf},
}

@misc{daiMultigranularityRelabeledUndersampling2022,
	title = {Multi-granularity {Relabeled} {Under}-sampling {Algorithm} for {Imbalanced} {Data}},
	url = {http://arxiv.org/abs/2201.03957},
	abstract = {The imbalanced classiﬁcation problem turns out to be one of the important and challenging problems in data mining and machine learning. The performances of traditional classiﬁers will be severely aﬀected by many data problems, such as class imbalanced problem, class overlap and noise. When the number of one class in the data set is larger than other classes, class imbalanced problem will inevitably occur. Therefore, many researchers are committed to solving the problem of category imbalance and improving the overall classiﬁcation performances of the classiﬁer. The Tomek-Link algorithm was only used to clean data when it was proposed. In recent years, there have been reports of combining Tomek-Link algorithm with sampling technique. The Tomek-Link sampling algorithm can eﬀectively reduce the class overlap on data, remove the majority instances that are diﬃcult to distinguish, and improve the algorithm classiﬁcation accuracy. However, the Tomek-Links under-sampling algorithm only considers the boundary instances that are the nearest neighbors to each other globally and ignores the potential local overlapping instances. When the number of minority instances is small, the undersampling eﬀect is not satisfactory, and the performance improvement of the classiﬁcation model is not obvious. Therefore, on the basis of Tomek-Link, a multi-granularity relabeled under-sampling algorithm (MGRU) is proposed. This algorithm fully considers the local information of the data set in the local granularity subspace, and detects the local potential overlapping instances in the data set. Then, the overlapped majority instances are eliminated according to the global relabeled index value, which eﬀectively expands the detection range of Tomek-Links. The simulation results show that when we select the optimal global relabeled index value for under-sampling, the classiﬁcation accuracy and generalization performance of the proposed under-sampling algorithm are signiﬁcantly better than other baseline algorithms.},
	language = {en},
	urldate = {2022-05-15},
	publisher = {arXiv},
	author = {Dai, Qi and Liu, Jian-wei and Liu, Yang},
	month = jan,
	year = {2022},
	note = {Number: arXiv:2201.03957
arXiv:2201.03957 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Dai et al. - 2022 - Multi-granularity Relabeled Under-sampling Algorit.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IMZASGA3\\Dai et al. - 2022 - Multi-granularity Relabeled Under-sampling Algorit.pdf:application/pdf},
}

@article{DataWarehouseLife,
	title = {The {Data} {Warehouse} {Life} {Cycle} {Toolkit}},
	language = {en},
	pages = {405},
	file = {The Data Warehouse Life Cycle Toolkit.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GDYE5KLB\\The Data Warehouse Life Cycle Toolkit.pdf:application/pdf},
}

@article{zogajDoingMoreLess2021,
	title = {Doing more with less: characterizing dataset downsampling for {AutoML}},
	volume = {14},
	issn = {2150-8097},
	shorttitle = {Doing more with less},
	url = {https://dl.acm.org/doi/10.14778/3476249.3476262},
	doi = {10.14778/3476249.3476262},
	abstract = {Automated machine learning (AutoML) promises to democratize machine learning by automatically generating machine learning pipelines with little to no user intervention. Typically, a search procedure is used to repeatedly generate and validate candidate pipelines, maximizing a predictive performance metric, subject to a limited execution time budget. While this approach to generating candidates works well for small tabular datasets, the same procedure does not directly scale to larger tabular datasets with 100,000s of observations, often producing fewer candidate pipelines and yielding lower performance, given the same execution time budget. We carry out an extensive empirical evaluation of the impact that downsampling – reducing the number of rows in the input tabular dataset – has on the pipelines produced by a genetic-programmingbased AutoML search for classification tasks.},
	language = {en},
	number = {11},
	urldate = {2022-05-15},
	journal = {Proceedings of the VLDB Endowment},
	author = {Zogaj, Fatjon and Cambronero, José Pablo and Rinard, Martin C. and Cito, Jürgen},
	month = jul,
	year = {2021},
	pages = {2059--2072},
	file = {Zogaj et al. - 2021 - Doing more with less characterizing dataset downs.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DQ5YY6KE\\Zogaj et al. - 2021 - Doing more with less characterizing dataset downs.pdf:application/pdf},
}

@article{mohadjerChapter14Sampling,
	title = {Chapter 14: {Sampling} {Design}},
	language = {en},
	author = {Mohadjer, Leyla and Krenzke, Tom and de Kerckhove, Wendy Van and Li, Lin},
	pages = {102},
	file = {Mohadjer et al. - Chapter 14 Sampling Design.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L886R2Q9\\Mohadjer et al. - Chapter 14 Sampling Design.pdf:application/pdf},
}

@article{liuSelectiveSamplingApproach2004,
	title = {A selective sampling approach to active feature selection},
	volume = {159},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370204000980},
	doi = {10.1016/j.artint.2004.05.009},
	abstract = {Feature selection, as a preprocessing step to machine learning, has been very effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. Traditional feature selection methods resort to random sampling in dealing with data sets with a huge number of instances. In this paper, we introduce the concept of active feature selection, and investigate a selective sampling approach to active feature selection in a ﬁlter model setting. We present a formalism of selective sampling based on data variance, and apply it to a widely used feature selection algorithm Relief. Further, we show how it realizes active feature selection and reduces the required number of training instances to achieve time savings without performance deterioration. We design objective evaluation measures of performance, conduct extensive experiments using both synthetic and benchmark data sets, and observe consistent and signiﬁcant improvement. We suggest some further work based on our study and experiments.},
	language = {en},
	number = {1-2},
	urldate = {2022-05-15},
	journal = {Artificial Intelligence},
	author = {Liu, Huan and Motoda, Hiroshi and Yu, Lei},
	month = nov,
	year = {2004},
	pages = {49--74},
	file = {Liu et al. - 2004 - A selective sampling approach to active feature se.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\86G3CGEM\\Liu et al. - 2004 - A selective sampling approach to active feature se.pdf:application/pdf},
}

@article{leviProvablyNearOptimalSamplingBased2007,
	title = {Provably {Near}-{Optimal} {Sampling}-{Based} {Policies} for {Stochastic} {Inventory} {Control} {Models}},
	volume = {32},
	issn = {0364-765X, 1526-5471},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/moor.1070.0272},
	doi = {10.1287/moor.1070.0272},
	abstract = {In this paper, we consider two fundamental inventory models, the single-period newsvendor problem and its multi-period extension, but under the assumption that the explicit demand distributions are not known and that the only information available is a set of independent samples drawn from the true distributions. Under the assumption that the demand distributions are given explicitly, these models are well-studied and relatively straightforward to solve. However, in most real-life scenarios, the true demand distributions are not available or they are too complex to work with. Thus, a sampling-driven algorithmic framework is very attractive, both in practice and in theory. We shall describe how to compute sampling-based policies, that is, policies that are computed based only on observed samples of the demands without any access to, or assumptions on, the true demand distributions. Moreover, we establish bounds on the number of samples required to guarantee that with high probability, the expected cost of the sampling-based policies is arbitrarily close (i.e., with arbitrarily small relative error) compared to the expected cost of the optimal policies which have full access to the demand distributions. The bounds that we develop are general, easy to compute and do not depend at all on the specifc demand distributions.},
	language = {en},
	number = {4},
	urldate = {2022-05-15},
	journal = {Mathematics of Operations Research},
	author = {Levi, Retsef and Roundy, Robin O. and Shmoys, David B.},
	month = nov,
	year = {2007},
	pages = {821--839},
	file = {Levi et al. - 2007 - Provably Near-Optimal Sampling-Based Policies for .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\82PMTESC\\Levi et al. - 2007 - Provably Near-Optimal Sampling-Based Policies for .pdf:application/pdf},
}

@article{ghufranMultiobjectiveOptimalAllocation1970,
	title = {Multiobjective optimal allocation problem with probabilistic non-linear cost constraint},
	volume = {3},
	issn = {2141-2839, 2141-2820},
	url = {https://www.ajol.info/index.php/ijest/article/view/74981},
	doi = {10.4314/ijest.v3i6.11},
	abstract = {This paper considers the optimum compromise allocation in multivariate stratified sampling with non-linear objective function and probabilistic non-linear cost constraint. The probabilistic non-linear cost constraint is converted into equivalent deterministic one by using Chance Constrained programming. A numerical example is presented to illustrate the computational procedure.},
	language = {en},
	number = {6},
	urldate = {2022-05-15},
	journal = {International Journal of Engineering, Science and Technology},
	author = {Ghufran, S and Khowaja, S and Ahsan, Mj},
	month = jan,
	year = {1970},
	pages = {135--145},
	file = {Ghufran et al. - 1970 - Multiobjective optimal allocation problem with pro.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YPEM9KHN\\Ghufran et al. - 1970 - Multiobjective optimal allocation problem with pro.pdf:application/pdf},
}

@article{kuneAnatomyBigData2016,
	title = {The anatomy of big data computing: {Anatomy} of {Big} {Data} {Computing}},
	volume = {46},
	issn = {00380644},
	shorttitle = {The anatomy of big data computing},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/spe.2374},
	doi = {10.1002/spe.2374},
	abstract = {Advances in information technology and its widespread growth in several areas of business, engineering, medical and scientific studies are resulting in information/data explosion. Knowledge discovery and decision making from such rapidly growing voluminous data is a challenging task in terms of data organization and processing, which is an emerging trend known as Big Data Computing; a new paradigm which combines large scale compute, new data intensive techniques and mathematical models to build data analytics. Big Data computing demands a huge storage and computing for data curation and processing that could be delivered from on-premise or clouds infrastructures. This paper discusses the evolution of Big Data computing, differences between traditional data warehousing and Big Data, taxonomy of Big Data computing and underpinning technologies, integrated platform of Big Data and Clouds known as Big Data Clouds, layered architecture and components of Big Data Cloud and finally discusses open technical challenges and future directions.},
	language = {en},
	number = {1},
	urldate = {2022-05-15},
	journal = {Software: Practice and Experience},
	author = {Kune, Raghavendra and Konugurthi, Pramod Kumar and Agarwal, Arun and Chillarige, Raghavendra Rao and Buyya, Rajkumar},
	month = jan,
	year = {2016},
	pages = {79--105},
	file = {Kune et al. - 2016 - The anatomy of big data computing Anatomy of Big .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\A39XEDSI\\Kune et al. - 2016 - The anatomy of big data computing Anatomy of Big .pdf:application/pdf},
}

@article{litzsingerCOSTEFFECTIVEANALYSIS,
	title = {{COST} {EFFECTIVE} {ANALYSIS} {OF} {BIG} {DATA}},
	abstract = {Background ................................................................................................................................................... 9 Literature Review........................................................................................................................................ 12 Progressive Sampling .............................................................................................................................. 12 Data Analytic Costs and Alternatives ...................................................................................................... 12 Cloud Computing Infrastructure ............................................................................................................. 16 Table of Terms ............................................................................................................................................ 18 Statistical Terms...................................................................................................................................... 18 Software, Cloud and Other Terms .......................................................................................................... 19 Amazon Web Services (AWS) Terms....................................................................................................... 20 Design - Advisory Tool for Starting a Big Data Project ................................................................................ 21 Data Wrangling ....................................................................................................................................... 23 Statistical Analysis................................................................................................................................... 26 Data Engineering..................................................................................................................................... 29
Methods - Big Data Analysis ....................................................................................................................... 32
Results......................................................................................................................................................... 38 Discussion.................................................................................................................................................... 44 Cost Justification ..................................................................................................................................... 47
Conclusion................................................................................................................................................... 49 Bibliography ................................................................................................................................................ 51 Data Bibliography........................................................................................................................................ 54 Appendices.................................................................................................................................................. 55 Appendix A: Data Wrangling Cheat sheet [19] ....................................................................................... 55 Appendix B: Big Data Services Vendor List [18] ...................................................................................... 57 Appendix C1: 990 Tax (1st Analysis) ....................................................................................................... 58 Appendix C2: 990 Tax (2nd Analysis)........................................................................................................ 62 Appendix C4: GDP (1st analysis) ............................................................................................................. 66 Appendix C5: GDP (2nd Analysis)............................................................................................................ 70},
	language = {en},
	author = {Litzsinger, Steven J and Dababo, Nehad},
	pages = {86},
	file = {Litzsinger and Dababo - COST EFFECTIVE ANALYSIS OF BIG DATA.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZTBY8KDR\\Litzsinger and Dababo - COST EFFECTIVE ANALYSIS OF BIG DATA.pdf:application/pdf},
}

@book{riouxDataAnalysisPython2022,
	address = {Shelter Island, NY},
	title = {Data analysis with {Python} and {PySpark}},
	isbn = {978-1-61729-720-5},
	language = {en},
	publisher = {Manning Publications Co},
	author = {Rioux, Jonathan},
	year = {2022},
	keywords = {Data mining, Python (Computer program language), Database management},
	file = {Rioux - 2022 - Data analysis with Python and PySpark.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QWE58JHV\\Rioux - 2022 - Data analysis with Python and PySpark.pdf:application/pdf},
}

@article{dreibelbisEnterpriseMasterData,
	title = {Enterprise {Master} {Data} {Management}},
	language = {en},
	author = {Dreibelbis, Allen and Hechler, Eberhard and Oberhofer, Martin and Wolfson, Dan},
	pages = {410},
	file = {Dreibelbis et al. - Enterprise Master Data Management.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4XJ3AWCA\\Dreibelbis et al. - Enterprise Master Data Management.pdf:application/pdf},
}

@article{doanPrinciplesDataIntegration,
	title = {Principles of {Data} {Integration}},
	language = {en},
	author = {Doan, AnHai and Halevy, Alon and Ives, Zachary},
	pages = {585},
	file = {Doan et al. - Principles of Data Integration.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SZ5XN2HF\\Doan et al. - Principles of Data Integration.pdf:application/pdf},
}

@article{chambersSparkDefinitiveGuide,
	title = {Spark: {The} {Definitive} {Guide}},
	language = {en},
	author = {Chambers, Bill and Zaharia, Matei},
	pages = {601},
	file = {Chambers and Zaharia - Spark The Definitive Guide.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FCYKNDBI\\Chambers and Zaharia - Spark The Definitive Guide.pdf:application/pdf},
}

@article{AnalyzingInterpretingLarge,
	title = {Analyzing and {Interpreting}  {Large} {Datasets}},
	language = {en},
	pages = {82},
	file = {Analyzing and Interpreting  Large Datasets.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KTD28EZY\\Analyzing and Interpreting  Large Datasets.pdf:application/pdf},
}

@article{freemanEvaluatingEffectivenessDownsampling2012,
	title = {Evaluating effectiveness of down-sampling for stratified designs and unbalanced prevalence in {Random} {Forest} models of tree species distributions in {Nevada}},
	volume = {233},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380012001147},
	doi = {10.1016/j.ecolmodel.2012.03.007},
	language = {en},
	urldate = {2022-05-15},
	journal = {Ecological Modelling},
	author = {Freeman, Elizabeth A. and Moisen, Gretchen G. and Frescino, Tracey S.},
	month = may,
	year = {2012},
	pages = {1--10},
	file = {Freeman et al. - 2012 - Evaluating effectiveness of down-sampling for stra.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GYZCTG4S\\Freeman et al. - 2012 - Evaluating effectiveness of down-sampling for stra.pdf:application/pdf},
}

@article{blachnikComparisonInstanceSelection2020,
	title = {Comparison of {Instance} {Selection} and {Construction} {Methods} with {Various} {Classifiers}},
	volume = {10},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/11/3933},
	doi = {10.3390/app10113933},
	abstract = {Instance selection and construction methods were originally designed to improve the performance of the k-nearest neighbors classiﬁer by increasing its speed and improving the classiﬁcation accuracy. These goals were achieved by eliminating redundant and noisy samples, thus reducing the size of the training set. In this paper, the performance of instance selection methods is investigated in terms of classiﬁcation accuracy and reduction of training set size. The classiﬁcation accuracy of the following classiﬁers is evaluated: decision trees, random forest, Naive Bayes, linear model, support vector machine and k-nearest neighbors. The obtained results indicate that for the most of the classiﬁers compressing the training set affects prediction performance and only a small group of instance selection methods can be recommended as a general purpose preprocessing step. These are learning vector quantization based algorithms, along with the Drop2 and Drop3. Other methods are less efﬁcient or provide low compression ratio.},
	language = {en},
	number = {11},
	urldate = {2022-05-15},
	journal = {Applied Sciences},
	author = {Blachnik, Marcin and Kordos, Mirosław},
	month = jun,
	year = {2020},
	pages = {3933},
	file = {Blachnik and Kordos - 2020 - Comparison of Instance Selection and Construction .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JK64BFA3\\Blachnik and Kordos - 2020 - Comparison of Instance Selection and Construction .pdf:application/pdf},
}

@article{ratnooComparativeStudyInstance2013,
	title = {A {Comparative} {Study} of {Instance} {Reduction} {Techniques}},
	abstract = {Dealing with very large databases is one of the major challenges in data mining research and development. It does not matter how powerful the computers are or will be in future, data mining algorithms must consider how to manage this ever-growing data that can be too large (for example terabytes of data) to be processed. Therefore we consider instance reduction as an important task in the data preparation phase of knowledge discovery and data mining. Major approaches for instance reduction are supervised instance filters (resample, spread subsample, stratified remove folds) and unsupervised instance filters (remove with values, reservoir sample, remove percentage). This paper provides a comparative study on existing techniques for instance reduction.},
	language = {en},
	author = {Ratnoo, Saroj},
	year = {2013},
	pages = {7},
	file = {Ratnoo - 2013 - A Comparative Study of Instance Reduction Techniqu.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZQBDI5V4\\Ratnoo - 2013 - A Comparative Study of Instance Reduction Techniqu.pdf:application/pdf},
}

@article{jewettTipsSucceedBig,
	title = {7 {Tips} to {Succeed} with {Big} {Data}},
	language = {en},
	author = {Jewett, Dan},
	pages = {11},
	file = {Jewett - 7 Tips to Succeed with Big Data.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6EDSLXWH\\Jewett - 7 Tips to Succeed with Big Data.pdf:application/pdf},
}

@misc{murrayTfDataMachine2021,
	title = {tf.data: {A} {Machine} {Learning} {Data} {Processing} {Framework}},
	shorttitle = {tf.data},
	url = {http://arxiv.org/abs/2101.12127},
	abstract = {Training machine learning models requires feeding input data for models to ingest. Input pipelines for machine learning jobs are often challenging to implement efficiently as they require reading large volumes of data, applying complex transformations, and transferring data to hardware accelerators while overlapping computation and communication to achieve optimal performance. We present tf.data, a framework for building and executing efficient input pipelines for machine learning jobs. The tf.data API provides operators which can be parameterized with user-defined computation, composed, and reused across different machine learning domains. These abstractions allow users to focus on the application logic of data processing, while tf.data’s runtime ensures that pipelines run efficiently. We demonstrate that input pipeline performance is critical to the end-to-end training time of state-of-the-art machine learning models. tf.data delivers the high performance required, while avoiding the need for manual tuning of performance knobs. We show that tf.data features, such as parallelism, caching, static optimizations, and non-deterministic execution are essential for high performance. Finally, we characterize machine learning input pipelines for millions of jobs that ran in Google’s fleet, showing that input data processing is highly diverse and consumes a significant fraction of job resources. Our analysis motivates future research directions, such as sharing computation across jobs and pushing data projection to the storage layer.},
	language = {en},
	urldate = {2022-05-15},
	publisher = {arXiv},
	author = {Murray, Derek G. and Simsa, Jiri and Klimovic, Ana and Indyk, Ihor},
	month = feb,
	year = {2021},
	note = {Number: arXiv:2101.12127
arXiv:2101.12127 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software},
	file = {Murray et al. - 2021 - tf.data A Machine Learning Data Processing Framew.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QFE3ST3B\\Murray et al. - 2021 - tf.data A Machine Learning Data Processing Framew.pdf:application/pdf},
}

@article{provostDataScienceIts2013,
	title = {Data {Science} and its {Relationship} to {Big} {Data} and {Data}-{Driven} {Decision} {Making}},
	volume = {1},
	issn = {2167-6461, 2167-647X},
	url = {http://www.liebertpub.com/doi/10.1089/big.2013.1508},
	doi = {10.1089/big.2013.1508},
	abstract = {Companies have realized they need to hire data scientists, academic institutions are scrambling to put together datascience programs, and publications are touting data science as a hot—even ‘‘sexy’’—career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the deﬁnition of the practitioner’s ﬁeld; this can result in overlooking the fundamentals of the ﬁeld. We believe that trying to deﬁne the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the ﬁeld in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.},
	language = {en},
	number = {1},
	urldate = {2022-05-15},
	journal = {Big Data},
	author = {Provost, Foster and Fawcett, Tom},
	month = mar,
	year = {2013},
	pages = {51--59},
	file = {Provost and Fawcett - 2013 - Data Science and its Relationship to Big Data and .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T8YXMKFD\\Provost and Fawcett - 2013 - Data Science and its Relationship to Big Data and .pdf:application/pdf},
}

@article{urrehmanBigDataReduction2016,
	title = {Big {Data} {Reduction} {Methods}: {A} {Survey}},
	volume = {1},
	issn = {2364-1185, 2364-1541},
	shorttitle = {Big {Data} {Reduction} {Methods}},
	url = {http://link.springer.com/10.1007/s41019-016-0022-0},
	doi = {10.1007/s41019-016-0022-0},
	language = {en},
	number = {4},
	urldate = {2022-05-15},
	journal = {Data Science and Engineering},
	author = {ur Rehman, Muhammad Habib and Liew, Chee Sun and Abbas, Assad and Jayaraman, Prem Prakash and Wah, Teh Ying and Khan, Samee U.},
	month = dec,
	year = {2016},
	pages = {265--284},
	file = {ur Rehman et al_2016_Big Data Reduction Methods.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7SR6GZSU\\ur Rehman et al_2016_Big Data Reduction Methods.pdf:application/pdf},
}

@inproceedings{barellaInfluenceSamplingImbalanced2019,
	address = {Salvador, Brazil},
	title = {The {Influence} of {Sampling} on {Imbalanced} {Data} {Classification}},
	isbn = {978-1-72814-253-1},
	url = {https://ieeexplore.ieee.org/document/8923714/},
	doi = {10.1109/BRACIS.2019.00045},
	urldate = {2022-05-15},
	booktitle = {2019 8th {Brazilian} {Conference} on {Intelligent} {Systems} ({BRACIS})},
	publisher = {IEEE},
	author = {Barella, Victor and Garcia, Luis and de Carvalho, Andre},
	month = oct,
	year = {2019},
	pages = {210--215},
}

@inproceedings{ramosrojasSamplingTechniquesImprove2017,
	address = {Phoenix, AZ},
	title = {Sampling techniques to improve big data exploration},
	isbn = {978-1-5386-0617-9},
	url = {https://ieeexplore.ieee.org/document/8231848/},
	doi = {10.1109/LDAV.2017.8231848},
	abstract = {The success of Big Data relies fundamentally on the ability of a person (the data scientist) to make sense and generate insights from this wealth of data. The process of generating actionable insights, called data exploration, is a difficult and time-consuming task. Data exploration of a big dataset usually requires first generating a small and representative data sample that can be easily plotted and viewed, managed and interpreted to generate insights. However, the literature on the topic hints at data scientists only using random sampling with regular sized datasets and it is unclear what they do with Big Data. In this work, we first show evidence from a survey that random sampling is the only technique commonly used by data scientists to quickly gain insights from a big dataset despite theoretical and empirical evidence from the active learning community that suggests benefits of using other sampling techniques. Second, to evaluate and demonstrate the benefits of other sampling techniques, we conducted an online study with 34 data scientists. These scientists performed a data exploration task to support a classification goal using data samples from more than 2 million records of editing data from Wikipedia articles, generated using different sampling techniques. The study results demonstrate that sampling techniques other than random sampling can generate insights that help to focus on different characteristics of the data, without compromising quality in a data exploration.},
	language = {en},
	urldate = {2022-05-15},
	booktitle = {2017 {IEEE} 7th {Symposium} on {Large} {Data} {Analysis} and {Visualization} ({LDAV})},
	publisher = {IEEE},
	author = {Ramos Rojas, Julian A. and Beth Kery, Mary and Rosenthal, Stephanie and Dey, Anind},
	month = oct,
	year = {2017},
	pages = {26--35},
	file = {Ramos Rojas et al. - 2017 - Sampling techniques to improve big data exploratio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZXVMEMWF\\Ramos Rojas et al. - 2017 - Sampling techniques to improve big data exploratio.pdf:application/pdf},
}

@article{abedjanProfilingRelationalData2015,
	title = {Profiling relational data: a survey},
	volume = {24},
	issn = {1066-8888, 0949-877X},
	shorttitle = {Profiling relational data},
	url = {http://link.springer.com/10.1007/s00778-015-0389-y},
	doi = {10.1007/s00778-015-0389-y},
	language = {en},
	number = {4},
	urldate = {2022-05-15},
	journal = {The VLDB Journal},
	author = {Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix},
	month = aug,
	year = {2015},
	pages = {557--581},
	file = {Abedjan et al_2015_Profiling relational data.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C4BCGTDZ\\Abedjan et al_2015_Profiling relational data.pdf:application/pdf},
}

@article{liuSamplingBigData2020,
	title = {Sampling for {Big} {Data} {Profiling}: {A} {Survey}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Sampling for {Big} {Data} {Profiling}},
	url = {https://ieeexplore.ieee.org/document/9068262/},
	doi = {10.1109/ACCESS.2020.2988120},
	urldate = {2022-05-15},
	journal = {IEEE Access},
	author = {Liu, Zhicheng and Zhang, Aoqian},
	year = {2020},
	pages = {72713--72726},
	file = {Liu_Zhang_2020_Sampling for Big Data Profiling.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FKE2T4ZK\\Liu_Zhang_2020_Sampling for Big Data Profiling.pdf:application/pdf},
}

@article{fernandezInsightImbalancedBig2017,
	title = {An insight into imbalanced {Big} {Data} classification: outcomes and challenges},
	volume = {3},
	issn = {2199-4536, 2198-6053},
	shorttitle = {An insight into imbalanced {Big} {Data} classification},
	url = {http://link.springer.com/10.1007/s40747-017-0037-9},
	doi = {10.1007/s40747-017-0037-9},
	language = {en},
	number = {2},
	urldate = {2022-05-15},
	journal = {Complex \& Intelligent Systems},
	author = {Fernández, Alberto and del Río, Sara and Chawla, Nitesh V. and Herrera, Francisco},
	month = jun,
	year = {2017},
	pages = {105--120},
	file = {Fernández et al_2017_An insight into imbalanced Big Data classification.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LJVGEHJT\\Fernández et al_2017_An insight into imbalanced Big Data classification.pdf:application/pdf},
}

@article{kimSamplingTechniquesBig2019,
	title = {Sampling {Techniques} for {Big} {Data} {Analysis}},
	volume = {87},
	issn = {0306-7734, 1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/insr.12290},
	doi = {10.1111/insr.12290},
	language = {en},
	number = {S1},
	urldate = {2022-05-15},
	journal = {International Statistical Review},
	author = {Kim, Jae Kwang and Wang, Zhonglei},
	month = may,
	year = {2019},
	pages = {S177--S191},
	file = {Kim_Wang_2019_Sampling Techniques for Big Data Analysis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Q2CHNHYI\\Kim_Wang_2019_Sampling Techniques for Big Data Analysis.pdf:application/pdf},
}

@misc{SupportVectorMachines,
	title = {1.4. {Support} {Vector} {Machines}},
	url = {https://scikit-learn/stable/modules/svm.html},
	abstract = {Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The advantages of support vector machines are: Effective in high ...},
	language = {en},
	urldate = {2022-05-19},
	journal = {scikit-learn},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YJW5RKLI\\svm.html:text/html},
}

@inproceedings{boriahSimilarityMeasuresCategorical2008,
	title = {Similarity {Measures} for {Categorical} {Data}: {A} {Comparative} {Evaluation}},
	isbn = {978-0-89871-654-2 978-1-61197-278-8},
	shorttitle = {Similarity {Measures} for {Categorical} {Data}},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972788.22},
	doi = {10.1137/1.9781611972788.22},
	language = {en},
	urldate = {2022-05-19},
	booktitle = {Proceedings of the 2008 {SIAM} {International} {Conference} on {Data} {Mining}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Boriah, Shyam and Chandola, Varun and Kumar, Vipin},
	month = apr,
	year = {2008},
	pages = {243--254},
}

@article{kroenkePHQ9ValidityBrief2001,
	title = {The {PHQ}-9: {Validity} of a brief depression severity measure},
	volume = {16},
	issn = {0884-8734, 1525-1497},
	shorttitle = {The {PHQ}-9},
	url = {http://link.springer.com/10.1046/j.1525-1497.2001.016009606.x},
	doi = {10.1046/j.1525-1497.2001.016009606.x},
	language = {en},
	number = {9},
	urldate = {2022-06-01},
	journal = {Journal of General Internal Medicine},
	author = {Kroenke, Kurt and Spitzer, Robert L. and Williams, Janet B. W.},
	month = sep,
	year = {2001},
	pages = {606--613},
}

@article{spitzerUtilityNewProcedure1994,
	title = {Utility of a {New} {Procedure} for {Diagnosing} {Mental} {Disorders} in {Primary} {Care}: {The} {PRIME}-{MD} 1000 {Study}},
	volume = {272},
	issn = {0098-7484},
	shorttitle = {Utility of a {New} {Procedure} for {Diagnosing} {Mental} {Disorders} in {Primary} {Care}},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.1994.03520220043029},
	doi = {10.1001/jama.1994.03520220043029},
	language = {en},
	number = {22},
	urldate = {2022-06-01},
	journal = {JAMA},
	author = {Spitzer, Robert L.},
	month = dec,
	year = {1994},
	pages = {1749},
}

@misc{CognitiveTherapyDepression2017,
	title = {Cognitive {Therapy} for {Depression} {\textbar} {Society} of {Clinical} {Psychology}},
	url = {https://div12.org/treatment/cognitive-therapy-for-depression/},
	language = {en-US},
	urldate = {2022-06-01},
	month = mar,
	year = {2017},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GW6AUCNL\\cognitive-therapy-for-depression.html:text/html},
}

@article{chamblessDefiningEmpiricallySupported1998,
	title = {Defining empirically supported therapies.},
	volume = {66},
	issn = {1939-2117, 0022-006X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-006X.66.1.7},
	doi = {10.1037/0022-006X.66.1.7},
	language = {en},
	number = {1},
	urldate = {2022-06-01},
	journal = {Journal of Consulting and Clinical Psychology},
	author = {Chambless, Dianne L. and Hollon, Steven D.},
	year = {1998},
	pages = {7--18},
	file = {Chambless_Hollon_1998_Defining empirically supported therapies.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\99D3IY4V\\Chambless_Hollon_1998_Defining empirically supported therapies.pdf:application/pdf},
}

@article{tolinEmpiricallySupportedTreatment2015,
	title = {Empirically {Supported} {Treatment}: {Recommendations} for a {New} {Model}},
	volume = {22},
	issn = {09695893},
	shorttitle = {Empirically {Supported} {Treatment}},
	url = {http://doi.wiley.com/10.1111/cpsp.12122},
	doi = {10.1111/cpsp.12122},
	language = {en},
	number = {4},
	urldate = {2022-06-01},
	journal = {Clinical Psychology: Science and Practice},
	author = {Tolin, David F. and McKay, Dean and Forman, Evan M. and Klonsky, E. David and Thombs, Brett D.},
	month = dec,
	year = {2015},
	pages = {317--338},
}

@article{korelitzResponsesDrugTherapy1975,
	title = {Responses to drug therapy in ulcerative colitis. {Evaluation} by rectal biopsy and histopathological changes},
	volume = {64},
	issn = {0002-9270},
	abstract = {To evaluate responses to medical therapy in ulcerative colitis, rectal biopsies of patients with active untreated disease, individuals with positive and negative sigmoidoscopic findings treated with salicylazosulfapyridine, prednisone and 6-mercaptopurine, alone and in combinations and noncolitis controls were compared histologically. Predominant histological observations were analyzed statistically. There were fewer crypt abscesses but more mucosal edema after all forms of therapy. Quantitative histopathological analysis failed to demonstrate that the response to one drug was significantly different from another.},
	language = {eng},
	number = {5},
	journal = {The American Journal of Gastroenterology},
	author = {Korelitz, B. I. and Sommers, S. C.},
	month = nov,
	year = {1975},
	pmid = {2008},
	keywords = {Humans, Biopsy, Clinical Trials as Topic, Colitis, Ulcerative, Drug Evaluation, Drug Therapy, Combination, Intestinal Mucosa, Mercaptopurine, Prednisone, Rectum, Remission, Spontaneous, Sigmoidoscopy, Sulfasalazine},
	pages = {365--370},
}

@book{rushHandbookPsychiatricMeasures2008,
	address = {Washington, DC},
	edition = {2nd ed},
	title = {Handbook of psychiatric measures},
	isbn = {978-1-58562-218-4},
	publisher = {American Psychiatric Pub},
	editor = {Rush, A. John and First, Michael B. and Blacker, Deborah and American Psychiatric Association},
	year = {2008},
	note = {OCLC: ocm85885343},
	keywords = {diagnosis, Diagnosis, Mental Disorders, Mental illness, Handbooks, Handbooks, manuals, etc, Psychiatric rating scales, Psychiatric Status Rating Scales},
}

@article{weissmanAssessmentSocialAdjustment1981,
	title = {The {Assessment} of {Social} {Adjustment}: {An} {Update}},
	volume = {38},
	issn = {0003-990X},
	shorttitle = {The {Assessment} of {Social} {Adjustment}},
	url = {http://archpsyc.jamanetwork.com/article.aspx?doi=10.1001/archpsyc.1981.01780360066006},
	doi = {10.1001/archpsyc.1981.01780360066006},
	language = {en},
	number = {11},
	urldate = {2022-06-01},
	journal = {Archives of General Psychiatry},
	author = {Weissman, Myrna M.},
	month = nov,
	year = {1981},
	pages = {1250},
}

@article{endicottQualityLifeEnjoyment1993,
	title = {Quality of {Life} {Enjoyment} and {Satisfaction} {Questionnaire}: a new measure},
	volume = {29},
	issn = {0048-5764},
	shorttitle = {Quality of {Life} {Enjoyment} and {Satisfaction} {Questionnaire}},
	abstract = {The Quality of Life Enjoyment and Satisfaction Questionnaire (Q-LES-Q) is a self-report measure designed to enable investigators to easily obtain sensitive measures of the degree of enjoyment and satisfaction experienced by subjects in various areas of daily functioning. The summary scores were found to be reliable and valid measures of these dimensions in a group of depressed outpatients. The Q-LES-Q measures were related to, but not redundant with, measures of overall severity of illness or severity of depression within this sample. These findings suggest that the Q-LES-Q measures may be sensitive to important differences among depressed patients that are not detected by the measures usually employed.},
	language = {eng},
	number = {2},
	journal = {Psychopharmacology Bulletin},
	author = {Endicott, J. and Nee, J. and Harrison, W. and Blumenthal, R.},
	year = {1993},
	pmid = {8290681},
	keywords = {Humans, Depressive Disorder, Adolescent, Adult, Female, Male, Middle Aged, Quality of Life, Surveys and Questionnaires},
	pages = {321--326},
}

@misc{monica36ItemShortForm,
	title = {36-{Item} {Short} {Form} {Survey} {Instrument} ({SF}-36)},
	url = {https://www.rand.org/health-care/surveys_tools/mos/36-item-short-form/survey-instrument.html},
	abstract = {RAND developed the 36-Item Short Form Health Survey (SF-36) as part of the Medical Outcomes Study (MSO), a multi-year, multi-site study to explain variations in patient outcomes.},
	language = {en},
	urldate = {2022-06-01},
	author = {Monica, 1776 Main Street Santa and California 90401-3208},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3PLWFPGJ\\survey-instrument.html:text/html},
}

@misc{monica36ItemShortForma,
	title = {36-{Item} {Short} {Form} {Survey} from the {RAND} {Medical} {Outcomes} {Study}},
	url = {https://www.rand.org/health-care/surveys_tools/mos/36-item-short-form.html},
	abstract = {The 36-Item Short Form Health Survey (SF-36) is a set of generic, coherent, and easily administered quality-of-life measures.},
	language = {en},
	urldate = {2022-06-01},
	author = {Monica, 1776 Main Street Santa and California 90401-3208},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZPHJMY4W\\36-item-short-form.html:text/html},
}

@incollection{michalosSASSR2014,
	address = {Dordrecht},
	title = {{SAS}-{SR}},
	isbn = {978-94-007-0752-8 978-94-007-0753-5},
	url = {http://link.springer.com/10.1007/978-94-007-0753-5_103620},
	language = {en},
	urldate = {2022-06-01},
	booktitle = {Encyclopedia of {Quality} of {Life} and {Well}-{Being} {Research}},
	publisher = {Springer Netherlands},
	editor = {Michalos, Alex C.},
	year = {2014},
	doi = {10.1007/978-94-007-0753-5_103620},
	pages = {5653--5653},
}

@article{loboSingleCaseDesignAnalysis2017,
	title = {Single-{Case} {Design}, {Analysis}, and {Quality} {Assessment} for {Intervention} {Research}},
	volume = {41},
	issn = {1557-0576},
	url = {https://journals.lww.com/01253086-201707000-00007},
	doi = {10.1097/NPT.0000000000000187},
	language = {en},
	number = {3},
	urldate = {2022-06-01},
	journal = {Journal of Neurologic Physical Therapy},
	author = {Lobo, Michele A. and Moeyaert, Mariola and Baraldi Cunha, Andrea and Babik, Iryna},
	month = jul,
	year = {2017},
	pages = {187--197},
	file = {Lobo et al_2017_Single-Case Design, Analysis, and Quality Assessment for Intervention Research.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DGPVIERS\\Lobo et al_2017_Single-Case Design, Analysis, and Quality Assessment for Intervention Research.pdf:application/pdf},
}

@article{carpenterCognitiveBehavioralTherapy2018,
	title = {Cognitive behavioral therapy for anxiety and related disorders: {A} meta‐analysis of randomized placebo‐controlled trials},
	volume = {35},
	issn = {1091-4269, 1520-6394},
	shorttitle = {Cognitive behavioral therapy for anxiety and related disorders},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/da.22728},
	doi = {10.1002/da.22728},
	language = {en},
	number = {6},
	urldate = {2022-06-01},
	journal = {Depression and Anxiety},
	author = {Carpenter, Joseph K. and Andrews, Leigh A. and Witcraft, Sara M. and Powers, Mark B. and Smits, Jasper A. J. and Hofmann, Stefan G.},
	month = jun,
	year = {2018},
	pages = {502--514},
}

@article{angelakisEffectivenessCognitiveBehavioural2022,
	title = {Effectiveness of cognitive–behavioural therapies of varying complexity in reducing depression in adults: systematic review and network meta-analysis},
	issn = {0007-1250, 1472-1465},
	shorttitle = {Effectiveness of cognitive–behavioural therapies of varying complexity in reducing depression in adults},
	url = {https://www.cambridge.org/core/product/identifier/S0007125022000356/type/journal_article},
	doi = {10.1192/bjp.2022.35},
	abstract = {Background
              Cognitive–behavioural therapy (CBT) is frequently used as an umbrella term to include a variety of psychological interventions. It remains unclear whether more complex CBT contributes to greater depression reduction.
            
            
              Aims
              To (a) compare the effectiveness of core, complex and ultra-complex CBT against other psychological intervention, medication, treatment-as-usual and no treatment in reducing depression at post-treatment and in the long term and (b) explore important factors that could moderate the effectiveness of these interventions.
            
            
              Method
              MEDLINE, PsycInfo, Embase, Web of Science and the Cochrane Register of Controlled Trials were searched to November 2021. Only randomised controlled trials were eligible for the subsequent network meta-analysis.
            
            
              Results
              We included 107 studies based on 15 248 participants. Core (s.m.d. = −1.14, 95\% credible interval (CrI) −1.72 to −0.55 [m.d. = −8.44]), complex (s.m.d. = −1.24, 95\% CrI −1.85 to −0.64 [m.d. = −9.18]) and ultra-complex CBT (s.m.d. = −1.45, 95\% CrI −1.88 to −1.02 [m.d. = −10.73]) were all significant in reducing depression up to 6 months from treatment onset. The significant benefits of the ultra-complex (s.m.d. = −1.09, 95\% CrI −1.61 to −0.56 [m.d. = −8.07]) and complex CBT (s.m.d. = −0.73, 95\% CrI −1.36 to −0.11 [m.d. = −5.40]) extended beyond 6 months. Ultra-complex CBT was most effective in individuals presenting comorbid mental health problems and when delivered by non-mental health specialists. Ultra-complex and complex CBT were more effective for people younger than 59 years.
            
            
              Conclusions
              For people without comorbid conditions healthcare and policy organisations should invest in core CBT. For people {\textless}59 years of age with comorbid conditions investments should focus on ultra-complex and complex CBT delivered without the help of mental health professionals.},
	language = {en},
	urldate = {2022-06-01},
	journal = {The British Journal of Psychiatry},
	author = {Angelakis, Ioannis and Huggett, Charlotte and Gooding, Patricia and Panagioti, Maria and Hodkinson, Alexander},
	month = mar,
	year = {2022},
	pages = {1--9},
	file = {Angelakis et al_2022_Effectiveness of cognitive–behavioural therapies of varying complexity in.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EYI3DXL3\\Angelakis et al_2022_Effectiveness of cognitive–behavioural therapies of varying complexity in.pdf:application/pdf},
}

@article{mchughCognitiveBehavioralTherapy2010,
	title = {Cognitive {Behavioral} {Therapy} for {Substance} {Use} {Disorders}},
	volume = {33},
	issn = {0193953X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0193953X10000547},
	doi = {10.1016/j.psc.2010.04.012},
	language = {en},
	number = {3},
	urldate = {2022-06-01},
	journal = {Psychiatric Clinics of North America},
	author = {McHugh, R. Kathryn and Hearon, Bridget A. and Otto, Michael W.},
	month = sep,
	year = {2010},
	pages = {511--525},
	file = {McHugh et al_2010_Cognitive Behavioral Therapy for Substance Use Disorders.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V9GT494L\\McHugh et al_2010_Cognitive Behavioral Therapy for Substance Use Disorders.pdf:application/pdf},
}

@article{murphyCognitiveBehavioralTherapy2010,
	title = {Cognitive {Behavioral} {Therapy} for {Eating} {Disorders}},
	volume = {33},
	issn = {0193953X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0193953X10000468},
	doi = {10.1016/j.psc.2010.04.004},
	language = {en},
	number = {3},
	urldate = {2022-06-01},
	journal = {Psychiatric Clinics of North America},
	author = {Murphy, Rebecca and Straebler, Suzanne and Cooper, Zafra and Fairburn, Christopher G.},
	month = sep,
	year = {2010},
	pages = {611--627},
	file = {Murphy et al_2010_Cognitive Behavioral Therapy for Eating Disorders.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G4HCCXCW\\Murphy et al_2010_Cognitive Behavioral Therapy for Eating Disorders.pdf:application/pdf},
}

@book{baucomCognitiveBehavioralMaritalTherapy2013,
	edition = {0},
	title = {Cognitive-{Behavioral} {Marital} {Therapy}},
	isbn = {978-1-134-84994-9},
	url = {https://www.taylorfrancis.com/books/9781134849949},
	language = {en},
	urldate = {2022-06-01},
	publisher = {Routledge},
	author = {Baucom, Donald H. and Epstein, Norman},
	month = may,
	year = {2013},
	doi = {10.4324/9780203776599},
}

@article{davidWhyCognitiveBehavioral2018,
	title = {Why {Cognitive} {Behavioral} {Therapy} {Is} the {Current} {Gold} {Standard} of {Psychotherapy}},
	volume = {9},
	issn = {1664-0640},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyt.2018.00004/full},
	doi = {10.3389/fpsyt.2018.00004},
	urldate = {2022-06-01},
	journal = {Frontiers in Psychiatry},
	author = {David, Daniel and Cristea, Ioana and Hofmann, Stefan G.},
	month = jan,
	year = {2018},
	pages = {4},
	file = {David et al_2018_Why Cognitive Behavioral Therapy Is the Current Gold Standard of Psychotherapy.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XD3AHY9C\\David et al_2018_Why Cognitive Behavioral Therapy Is the Current Gold Standard of Psychotherapy.pdf:application/pdf},
}

@article{dobsonTheoryPracticeGapCognitive2013,
	title = {The {Theory}-{Practice} {Gap} in {Cognitive} {Behavioral} {Therapy}: {Reflections} and a {Modest} {Proposal} to {Bridge} the {Gap}},
	volume = {44},
	issn = {00057894},
	shorttitle = {The {Theory}-{Practice} {Gap} in {Cognitive} {Behavioral} {Therapy}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0005789413000154},
	doi = {10.1016/j.beth.2013.03.002},
	language = {en},
	number = {4},
	urldate = {2022-06-02},
	journal = {Behavior Therapy},
	author = {Dobson, Keith and Beshai, Shadi},
	month = dec,
	year = {2013},
	pages = {559--567},
}

@article{jensenPsychosocialTreatmentDepression1994,
	title = {Psychosocial {Treatment} of {Depression} in {Women}: {Nine} {Single}-{Subject} {Evaluations}},
	volume = {4},
	issn = {1049-7315, 1552-7581},
	shorttitle = {Psychosocial {Treatment} of {Depression} in {Women}},
	url = {http://journals.sagepub.com/doi/10.1177/104973159400400301},
	doi = {10.1177/104973159400400301},
	abstract = {Social workers are increasingly likely to deal with clients presenting symptoms of major depression. Because of risk of suicide, increased health care costs, plus the lack of available resources and new methodologies, short-term approaches should be studied. The National Institute of Mental Health Collaborative Study on Depression determined that two of the most promising short-term approaches developed for the treatment of depression have been Cognitive- Behavioral Therapy and Interpersonal Psychotherapy. This current study explores the effective ness of integrating these two approaches in a new, short-term social work intervention model for treating major depression. A single-subject, nonconcurrent, multiple-baseline, across-individuals design was used to test the effectiveness of this new integrated treatment modality through use of the Beck Depression Inventory and the Social Adjustment Scale. The findings revealed that this new integrated modality was effective in decreasing the level of depression.},
	language = {en},
	number = {3},
	urldate = {2022-06-02},
	journal = {Research on Social Work Practice},
	author = {Jensen, Carla},
	month = jul,
	year = {1994},
	pages = {267--282},
}

@article{zettleTreatmentManualsSingleSubject2020,
	title = {Treatment {Manuals}, {Single}-{Subject} {Designs}, and {Evidence}-{Based} {Practice}: {A} {Clinical} {Behavior} {Analytic} {Perspective}},
	volume = {70},
	issn = {0033-2933, 2163-3452},
	shorttitle = {Treatment {Manuals}, {Single}-{Subject} {Designs}, and {Evidence}-{Based} {Practice}},
	url = {https://link.springer.com/10.1007/s40732-020-00394-2},
	doi = {10.1007/s40732-020-00394-2},
	language = {en},
	number = {4},
	urldate = {2022-06-02},
	journal = {The Psychological Record},
	author = {Zettle, Robert D.},
	month = dec,
	year = {2020},
	pages = {649--658},
}

@book{americanpsychiatricassociationDiagnosticStatisticalManual1998,
	address = {Washington, DC},
	edition = {4. ed., 7. print},
	title = {Diagnostic and statistical manual of mental disorders: {DSM}-{IV} ; includes {ICD}-9-{CM} codes effective 1. {Oct}. 96},
	isbn = {978-0-89042-061-4 978-0-89042-062-1},
	shorttitle = {Diagnostic and statistical manual of mental disorders},
	language = {eng},
	editor = {American Psychiatric Association},
	year = {1998},
}

@article{beerEndogenousPsychosesConceptual1996,
	title = {The endogenous psychoses: a conceptual history},
	volume = {7},
	issn = {0957-154X, 1740-2360},
	shorttitle = {The endogenous psychoses},
	url = {http://journals.sagepub.com/doi/10.1177/0957154X9600702501},
	doi = {10.1177/0957154X9600702501},
	language = {en},
	number = {25},
	urldate = {2022-06-04},
	journal = {History of Psychiatry},
	author = {Beer, M. Dominic},
	month = mar,
	year = {1996},
	pages = {001--29},
}

@misc{InternationalClassificationDiseases,
	title = {International {Classification} of {Diseases} ({ICD})},
	url = {https://www.who.int/standards/classifications/classification-of-diseases},
	abstract = {International Classification of Diseases (ICD) Revision},
	language = {en},
	urldate = {2022-06-04},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BGUCCBAU\\classification-of-diseases.html:text/html},
}

@article{lyePredictorsRecurrenceMajor2020,
	title = {Predictors of recurrence of major depressive disorder},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0230363},
	doi = {10.1371/journal.pone.0230363},
	language = {en},
	number = {3},
	urldate = {2022-06-04},
	journal = {PLOS ONE},
	author = {Lye, Munn-Sann and Tey, Yin-Yee and Tor, Yin-Sim and Shahabudin, Aisya Farhana and Ibrahim, Normala and Ling, King-Hwa and Stanslas, Johnson and Loh, Su-Peng and Rosli, Rozita and Lokman, Khairul Aiman and Badamasi, Ibrahim Mohammed and Faris-Aldoghachi, Asraa and Abdul Razak, Nurul Asyikin},
	editor = {Li, Zezhi},
	month = mar,
	year = {2020},
	pages = {e0230363},
	file = {Lye et al_2020_Predictors of recurrence of major depressive disorder.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GS6S22K2\\Lye et al_2020_Predictors of recurrence of major depressive disorder.pdf:application/pdf},
}

@article{nunezAugmentationStrategiesTreatment2022,
	title = {Augmentation strategies for treatment resistant major depression: {A} systematic review and network meta-analysis},
	volume = {302},
	issn = {01650327},
	shorttitle = {Augmentation strategies for treatment resistant major depression},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165032721014518},
	doi = {10.1016/j.jad.2021.12.134},
	language = {en},
	urldate = {2022-06-04},
	journal = {Journal of Affective Disorders},
	author = {Nuñez, Nicolas A and Joseph, Boney and Pahwa, Mehak and Kumar, Rakesh and Resendez, Manuel Gardea and Prokop, Larry J and Veldic, Marin and Seshadri, Ashok and Biernacka, Joanna M and Frye, Mark A and Wang, Zhen and Singh, Balwinder},
	month = apr,
	year = {2022},
	pages = {385--400},
}

@article{brigittaPathophysiologyDepressionMechanisms2002,
	title = {Pathophysiology of depression and mechanisms of treatment},
	volume = {4},
	issn = {1958-5969},
	url = {https://www.tandfonline.com/doi/full/10.31887/DCNS.2002.4.1/bbondy},
	doi = {10.31887/DCNS.2002.4.1/bbondy},
	language = {en},
	number = {1},
	urldate = {2022-06-05},
	journal = {Dialogues in Clinical Neuroscience},
	author = {Brigitta, Bondy},
	month = mar,
	year = {2002},
	pages = {7--20},
}

@article{jacobMajorDepressionRevisiting2009,
	title = {Major depression: revisiting the concept and diagnosis},
	volume = {15},
	issn = {1355-5146, 1472-1481},
	shorttitle = {Major depression},
	url = {https://www.cambridge.org/core/product/identifier/S1355514600005824/type/journal_article},
	doi = {10.1192/apt.bp.108.005827},
	abstract = {Summary
            The classification of depression has been debated for decades. The introduction of operational criteria and the category of major depression were significant advances in the 1970s. However, the validity of the major depression category is controversial. The article highlights the limitations of using severity criteria and cross-sectional evaluation to diagnose depression. It recommends the classic typologies (melancholia, dysthymia and adjustment disorder) for clinical presentations of depression, highlighting the need to use longitudinal clinical patterns and context for diagnosis. Major depression owes its success to its loose definition, to the subordinate status of adjustment disorders and dysthymia and to the mechanistic application of the diagnostic hierarchy and criteria. There is a need to focus more on the context of depression (stress, coping and support) and to reduce the medicalisation of distress.},
	language = {en},
	number = {4},
	urldate = {2022-06-05},
	journal = {Advances in Psychiatric Treatment},
	author = {Jacob, K. S.},
	month = jul,
	year = {2009},
	pages = {279--285},
	file = {Jacob_2009_Major depression.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\52KKYANU\\Jacob_2009_Major depression.pdf:application/pdf},
}

@article{paykelBasicConceptsDepression2008,
	title = {Basic concepts of depression},
	volume = {10},
	issn = {1958-5969},
	url = {https://www.tandfonline.com/doi/full/10.31887/DCNS.2008.10.3/espaykel},
	doi = {10.31887/DCNS.2008.10.3/espaykel},
	language = {en},
	number = {3},
	urldate = {2022-06-05},
	journal = {Dialogues in Clinical Neuroscience},
	author = {Paykel, Eugene S.},
	month = sep,
	year = {2008},
	pages = {279--289},
}

@incollection{leonhardEtiologyEndogenousPsychoses1999,
	address = {Vienna},
	title = {Etiology of {Endogenous} {Psychoses}},
	isbn = {978-3-7091-7308-4 978-3-7091-6371-9},
	url = {http://link.springer.com/10.1007/978-3-7091-6371-9_9},
	language = {en},
	urldate = {2022-06-05},
	booktitle = {Classification of {Endogenous} {Psychoses} and their {Differentiated} {Etiology}},
	publisher = {Springer Vienna},
	author = {Leonhard, Karl and Beckmann, Helmut},
	editor = {Beckmann, Helmut},
	collaborator = {Leonhard, Karl},
	year = {1999},
	doi = {10.1007/978-3-7091-6371-9_9},
	pages = {278--329},
}

@article{PracticeGuidelineTreatment2000,
	title = {Practice guideline for the treatment of patients with major depressive disorder (revision). {American} {Psychiatric} {Association}},
	volume = {157},
	issn = {0002-953X},
	language = {eng},
	number = {4 Suppl},
	journal = {The American Journal of Psychiatry},
	month = apr,
	year = {2000},
	pmid = {10767867},
	keywords = {Humans, Depressive Disorder, Antidepressive Agents, Combined Modality Therapy, Decision Trees, Drug Administration Schedule, Electroconvulsive Therapy, Patient Compliance, Phototherapy, Psychotherapy, Secondary Prevention, Treatment Outcome},
	pages = {1--45},
}

@misc{WeeklyReflectionEd,
	title = {Weekly {Reflection} {\textbar} {Ed} {Blunderfield}},
	url = {https://www.edblunderfield.com/resources/weekly-reflection},
	abstract = {Ed Blunderfield is a certified Integral Coach™ who helps people overcome stress and live happier, more fulfilling lives.},
	urldate = {2022-06-05},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BIAVTYPA\\weekly-reflection.html:text/html},
}

@misc{20QuestionsYou2008,
	title = {20 {Questions} {You} {Should} {Ask} {Yourself} {Every} {Sunday}},
	url = {https://www.marcandangel.com/2008/07/24/20-questions-you-should-ask-yourself-every-sunday/},
	abstract = {At the cusp of new beginnings many of us take time to reflect on our lives by looking back over the past and ahead into the future.  We ponder the successes, failures and standout events that are s…},
	language = {en-US},
	urldate = {2022-06-05},
	journal = {Marc and Angel Hack Life},
	month = jul,
	year = {2008},
}

@misc{FridayReflectionWhat2017,
	title = {Friday {Reflection}: {What} {Have} {You} {Done} {This} {Week}?},
	shorttitle = {Friday {Reflection}},
	url = {https://www.workplaceless.com/blog/friday-reflection},
	abstract = {Happy Friday! Before you dance away from your work for the weekend, take some time to reflect on what you accomplished. Building the habit of reflecting on the week allows you to acknowledge your victories and the things you have learned. It will also prepare you to start next week off on the right foot, … Friday Reflection: What Have You Done This Week? Read More »},
	language = {en-US},
	urldate = {2022-06-05},
	journal = {Workplaceless},
	month = oct,
	year = {2017},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QDEKIZHS\\friday-reflection.html:text/html},
}

@techreport{distefanoMakingExperienceCount2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Making {Experience} {Count}: {The} {Role} of {Reflection} in {Individual} {Learning}},
	shorttitle = {Making {Experience} {Count}},
	url = {https://papers.ssrn.com/abstract=2414478},
	abstract = {In this paper, we build on research on the microfoundations of strategy and learning processes to study the individual underpinnings of organizational learning. We argue that once an individual has accumulated a certain amount of experience with a task, the benefit of accumulating additional experience is inferior to the benefit of deliberately articulating and codifying the experience accumulated in the past. We explain the superior performance outcomes associated with such deliberate learning efforts using both a cognitive (improved task understanding) and an emotional (increased self-efficacy) mechanism. We study the proposed framework by means of a mixed-method experimental design that combines the reach and relevance of a field experiment with the precision of two laboratory experiments. Our results support the proposed theoretical framework and bear important implications from both a theoretical and practical viewpoint.},
	language = {en},
	number = {2414478},
	urldate = {2022-06-05},
	institution = {Social Science Research Network},
	author = {Di Stefano, Giada and Gino, Francesca and Pisano, Gary P. and Staats, Bradley R.},
	month = jun,
	year = {2016},
	doi = {10.2139/ssrn.2414478},
	keywords = {learning, causal ambiguity, codification, field experiment, knowledge, self-efficacy},
	file = {Di Stefano et al_2016_Making Experience Count.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RH77RVST\\Di Stefano et al_2016_Making Experience Count.pdf:application/pdf;Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U7SKZ2CQ\\papers.html:text/html},
}

@misc{SelfReflectionExercisesStart,
	title = {5 {Self}-{Reflection} {Exercises} to {Start} {Your} {Year} {Off} {Right}},
	url = {https://www.idealist.org/en/careers/5-self-reflection-exercises},
	abstract = {There’s no better way to reflect than to embark on some silent and patient self-examination. As we enter 2020, here’s a guide to help you reflect.},
	language = {en},
	urldate = {2022-06-05},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7QLC8C79\\5-self-reflection-exercises.html:text/html},
}

@misc{WhyYouShould,
	title = {Why {You} {Should} {Make} {Time} for {Self}-{Reflection} ({Even} {If} {You} {Hate} {Doing} {It})},
	url = {https://hbr.org/2017/03/why-you-should-make-time-for-self-reflection-even-if-you-hate-doing-it?utm_medium=referral&utm_source=idealist},
	urldate = {2022-06-05},
	file = {Why You Should Make Time for Self-Reflection (Even If You Hate Doing It):C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E6BSZKYY\\why-you-should-make-time-for-self-reflection-even-if-you-hate-doing-it.html:text/html},
}

@misc{ClinicalPracticeGuideline,
	title = {Clinical {Practice} {Guideline} for the {Treatment} of {Depression} {Across} {Three} {Age} {Cohorts}},
	url = {https://www.apa.org/depression-guideline},
	abstract = {The guideline recommends interventions for the treatment of depression in children and adolescents, adults, and older adults. Recommendations are based on a systematic review of the scientific evidence, a weighing of the benefits and harms of interventions, consideration of what is known about patient values and preferences, and consideration of the applicability of the evidence across demographic groups and settings.},
	language = {en},
	urldate = {2022-06-05},
	journal = {https://www.apa.org},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LE2PXXDJ\\depression-guideline.html:text/html},
}

@misc{UnderstandingCBT,
	title = {Understanding {CBT}},
	url = {https://beckinstitute.org/about/intro-to-cbt/},
	abstract = {Cognitive Behavior Therapy is a structured form of psychotherapy found to be highly effective in treating many different mental health conditions.},
	language = {en-US},
	urldate = {2022-06-05},
	journal = {Beck Institute},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YF9WLYR4\\intro-to-cbt.html:text/html},
}

@article{spanembergBiologicalDifferencesMelancholic2014,
	title = {Biological differences between melancholic and nonmelancholic depression subtyped by the {CORE} measure},
	issn = {1178-2021},
	url = {http://www.dovepress.com/biological-differences-between-melancholic-and-nonmelancholic-depressi-peer-reviewed-article-NDT},
	doi = {10.2147/NDT.S66504},
	language = {en},
	urldate = {2022-06-06},
	journal = {Neuropsychiatric Disease and Treatment},
	author = {Spanemberg, Lucas and Caldieraro, Marco and Arrua Vares, Edgar and Wollenhaupt de Aguiar, Bianca and Yuri Kawamoto, Sheila and Parker, Gordon and Pio Fleck, Marcelo and Kauer-SantAnna, Marcia and Galvao, Emily},
	month = aug,
	year = {2014},
	pages = {1523},
	file = {Spanemberg et al_2014_Biological differences between melancholic and nonmelancholic depression.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\56E8L79X\\Spanemberg et al_2014_Biological differences between melancholic and nonmelancholic depression.pdf:application/pdf},
}

@article{parkerDefiningMelancholiaCore2017,
	title = {Defining melancholia: {A} core mood disorder},
	volume = {19},
	issn = {13985647},
	shorttitle = {Defining melancholia},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/bdi.12501},
	doi = {10.1111/bdi.12501},
	language = {en},
	number = {3},
	urldate = {2022-06-06},
	journal = {Bipolar Disorders},
	author = {Parker, Gordon and Bassett, Darryl and Outhred, Tim and Morris, Grace and Hamilton, Amber and Das, Pritha and Baune, Bernhard T and Berk, Michael and Boyce, Philip and Lyndon, Bill and Mulder, Roger and Singh, Ajeet B and Malhi, Gin S},
	month = may,
	year = {2017},
	pages = {235--237},
	file = {Parker et al_2017_Defining melancholia.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3N2KGZU9\\Parker et al_2017_Defining melancholia.pdf:application/pdf},
}

@article{parkerIssuesDSM5Whither2010,
	title = {Issues for {DSM}-5: {Whither} {Melancholia}? {The} {Case} for {Its} {Classification} as a {Distinct} {Mood} {Disorder}},
	volume = {167},
	issn = {0002-953X, 1535-7228},
	shorttitle = {Issues for {DSM}-5},
	url = {http://psychiatryonline.org/doi/abs/10.1176/appi.ajp.2010.09101525},
	doi = {10.1176/appi.ajp.2010.09101525},
	language = {en},
	number = {7},
	urldate = {2022-06-06},
	journal = {American Journal of Psychiatry},
	author = {Parker, Gordon and Fink, Max and Shorter, Edward and Taylor, Michael Alan and Akiskal, Hagop and Berrios, German and Bolwig, Tom and Brown, Walter A. and Carroll, Bernard and Healy, David and Klein, Donald F. and Koukopoulos, Athanasios and Michels, Robert and Paris, Joel and Rubin, Robert T. and Spitzer, Robert and Swartz, Conrad},
	month = jul,
	year = {2010},
	pages = {745--747},
	file = {Parker et al_2010_Issues for DSM-5.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NXRXQSR9\\Parker et al_2010_Issues for DSM-5.pdf:application/pdf},
}

@article{kendellClassificationDepressionsReview1976,
	title = {The {Classification} of {Depressions}: {A} {Review} of {Contemporary} {Confusion}},
	volume = {129},
	issn = {0007-1250, 1472-1465},
	shorttitle = {The {Classification} of {Depressions}},
	url = {https://www.cambridge.org/core/product/identifier/S0007125000093351/type/journal_article},
	doi = {10.1192/bjp.129.1.15},
	abstract = {During the last fifty years, and particularly the last twenty, innumerable different classifications of depressive illness have been proposed, and several disputes are smouldering away between the protagonists of rival schools. Much of this is too well known to need, or bear, repetition. But the conflicting claims and proposals are now so numerous, and methodological problems loom so large, that those who are not intimately involved have increasing difficulty in understanding what is going on; while those who are involved are mostly too intent on developing and promoting their own particular schema to review the overall situation in any broader context. This article is an attempt to describe the main elements in a confusing situation, to outline the important problems, and to indicate the areas of agreement that are beginning to emerge, without advocating any particular solution.},
	language = {en},
	number = {1},
	urldate = {2022-06-06},
	journal = {British Journal of Psychiatry},
	author = {Kendell, R. E.},
	month = jul,
	year = {1976},
	pages = {15--28},
}

@article{farmerClassificationDepressionsContemporary1989,
	title = {The {Classification} of the {Depressions}: {Contemporary} {Confusion} {Revisited}},
	volume = {155},
	issn = {0007-1250, 1472-1465},
	shorttitle = {The {Classification} of the {Depressions}},
	url = {https://www.cambridge.org/core/product/identifier/S0007125000016378/type/journal_article},
	doi = {10.1192/bjp.155.4.437},
	abstract = {It is 13 years since Kendell (1976) reviewed the ‘contemporary confusion’ surrounding the classification of depression. Reconsideration of this issue is now timely, especially in light of the development of the new classifications of affective disorder included in DSM–III (American Psychiatric Association, 1980), the revised version, DSM–III–R (American Psychiatric Association, 1987), and the forthcoming ICD–10 (World Health Organization, 1988). Recent activities in neurobiological, genetic and social research also bear importantly on our concepts of the aetiology of depression.},
	language = {en},
	number = {4},
	urldate = {2022-06-06},
	journal = {British Journal of Psychiatry},
	author = {Farmer, Anne and McGuffin, Peter},
	month = oct,
	year = {1989},
	pages = {437--443},
}

@article{kleinEndogenomorphicDepressionConceptual1974,
	title = {Endogenomorphic {Depression}: {A} {Conceptual} and {Terminological} {Revision}},
	volume = {31},
	issn = {0003-990X},
	shorttitle = {Endogenomorphic {Depression}},
	url = {http://archpsyc.jamanetwork.com/article.aspx?doi=10.1001/archpsyc.1974.01760160005001},
	doi = {10.1001/archpsyc.1974.01760160005001},
	language = {en},
	number = {4},
	urldate = {2022-06-06},
	journal = {Archives of General Psychiatry},
	author = {Klein, Donald F.},
	month = oct,
	year = {1974},
	pages = {447},
}

@book{parkerMelancholiaDisorderMovement1996,
	edition = {1},
	title = {Melancholia: {A} {Disorder} of {Movement} and {Mood}: {A} {Phenomenological} and {Neurobiological} {Review}},
	isbn = {978-0-521-47275-3 978-0-511-75902-4},
	shorttitle = {Melancholia},
	url = {https://www.cambridge.org/core/product/identifier/9780511759024/type/book},
	urldate = {2022-06-06},
	publisher = {Cambridge University Press},
	editor = {Parker, Gordon and Hadzi-Pavlovic, Dusan},
	month = mar,
	year = {1996},
	doi = {10.1017/CBO9780511759024},
}

@article{showrakiReactiveDepressionLost2019,
	title = {Reactive {Depression}: {Lost} in {Translation}!},
	volume = {207},
	issn = {1539-736X, 0022-3018},
	shorttitle = {Reactive {Depression}},
	url = {https://journals.lww.com/10.1097/NMD.0000000000000989},
	doi = {10.1097/NMD.0000000000000989},
	language = {en},
	number = {9},
	urldate = {2022-06-06},
	journal = {Journal of Nervous \& Mental Disease},
	author = {Showraki, Mostafa},
	month = sep,
	year = {2019},
	pages = {755--759},
}

@article{lawrencet.parkPromisingStrategiesTreatmentResistant2022,
	series = {Vol 39, {Issue} 5},
	title = {Promising {Strategies} for {Treatment}-{Resistant} {Depression}},
	volume = {28},
	url = {https://www.psychiatrictimes.com/view/atypical-depression-21st-century-diagnostic-and-treatment-issues},
	abstract = {Identification of atypical features is important in the treatment of depression for both treatment selection and prognosis, especially when initial measures prove ineffective. The concept of atypical depression has evolved over many years, and now it appears timely for a further revision.},
	language = {en},
	number = {1},
	urldate = {2022-06-06},
	journal = {Psychiatric Times},
	author = {Lawrence T. Park, M. D. and Sarah H. Lisanby, M. D. and Carlos A. Zarate, Jr},
	month = may,
	year = {2022},
	note = {Publisher: MJH Life Sciences},
	file = {Lawrence T. Park et al_2022_Promising Strategies for Treatment-Resistant Depression.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VZSGA87S\\Lawrence T. Park et al_2022_Promising Strategies for Treatment-Resistant Depression.pdf:application/pdf;Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7KQ2EGDI\\atypical-depression-21st-century-diagnostic-and-treatment-issues.html:text/html},
}

@article{lojkoAtypicalDepressionCurrent2017,
	title = {Atypical depression: current perspectives},
	volume = {Volume 13},
	issn = {1178-2021},
	shorttitle = {Atypical depression},
	url = {https://www.dovepress.com/atypical-depression-current-perspectives-peer-reviewed-article-NDT},
	doi = {10.2147/NDT.S147317},
	language = {en},
	urldate = {2022-06-06},
	journal = {Neuropsychiatric Disease and Treatment},
	author = {Łojko, Dorota and Rybakowski, Janusz},
	month = sep,
	year = {2017},
	pages = {2447--2456},
	file = {Łojko_Rybakowski_2017_Atypical depression.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9RZY32KD\\Łojko_Rybakowski_2017_Atypical depression.pdf:application/pdf},
}

@article{liebowitzAntidepressantSpecificityAtypical1988,
	title = {Antidepressant {Specificity} in {Atypical} {Depression}},
	volume = {45},
	issn = {0003-990X},
	url = {http://archpsyc.jamanetwork.com/article.aspx?doi=10.1001/archpsyc.1988.01800260037004},
	doi = {10.1001/archpsyc.1988.01800260037004},
	language = {en},
	number = {2},
	urldate = {2022-06-06},
	journal = {Archives of General Psychiatry},
	author = {Liebowitz, Michael R.},
	month = feb,
	year = {1988},
	pages = {129},
}

@article{marioa.cristanchoAtypicalDepression21st2012,
	series = {Psychiatric {Times} {Vol} 28 {No} 1},
	title = {Atypical {Depression} in the 21st {Century}: {Diagnostic} and {Treatment} {Issues}},
	volume = {28},
	shorttitle = {Atypical {Depression} in the 21st {Century}},
	url = {https://www.psychiatrictimes.com/view/atypical-depression-21st-century-diagnostic-and-treatment-issues},
	abstract = {Identification of atypical features is important in the treatment of depression for both treatment selection and prognosis, especially when initial measures prove ineffective. The concept of atypical depression has evolved over many years, and now it appears timely for a further revision.},
	language = {en},
	number = {1},
	urldate = {2022-06-06},
	journal = {Psychiatric Times},
	author = {Mario A. Cristancho, M. D. and John P. O’reardon, M. D. and Michael E. Thase, M. D.},
	month = nov,
	year = {2012},
	note = {Publisher: MJH Life Sciences},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ILMAH3M2\\atypical-depression-21st-century-diagnostic-and-treatment-issues.html:text/html},
}

@article{bayesComparisonGuidelinesTreatment2018,
	title = {Comparison of guidelines for the treatment of unipolar depression: a focus on pharmacotherapy and neurostimulation},
	volume = {137},
	issn = {0001690X},
	shorttitle = {Comparison of guidelines for the treatment of unipolar depression},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/acps.12878},
	doi = {10.1111/acps.12878},
	language = {en},
	number = {6},
	urldate = {2022-06-06},
	journal = {Acta Psychiatrica Scandinavica},
	author = {Bayes, A. J. and Parker, G. B.},
	month = jun,
	year = {2018},
	pages = {459--471},
}

@article{fournierDifferentialChangeSpecific2013,
	title = {Differential change in specific depressive symptoms during antidepressant medication or cognitive therapy},
	volume = {51},
	issn = {00057967},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0005796713000582},
	doi = {10.1016/j.brat.2013.03.010},
	language = {en},
	number = {7},
	urldate = {2022-06-06},
	journal = {Behaviour Research and Therapy},
	author = {Fournier, Jay C. and DeRubeis, Robert J. and Hollon, Steven D. and Gallop, Robert and Shelton, Richard C. and Amsterdam, Jay D.},
	month = jul,
	year = {2013},
	pages = {392--398},
	file = {Fournier et al_2013_Differential change in specific depressive symptoms during antidepressant.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZR4NLKNW\\Fournier et al_2013_Differential change in specific depressive symptoms during antidepressant.pdf:application/pdf},
}

@article{PMID:1592844,
	title = {A pilot sequential study of cognitive therapy and pharmacotherapy of atypical depression},
	volume = {53},
	issn = {0160-6689},
	url = {http://europepmc.org/abstract/MED/1592844},
	abstract = {\&lt;h4\&gt;Background\&lt;/h4\&gt;Parallel comparison studies of cognitive therapy and antidepressant medication have suggested that both treatments are effective. However, we cannot determine from these studies whether cognitive therapy and antidepressant medication are effective for the same populations of depressives. A sequential study in which nonresponders to the first treatment are then treated with the second can address this issue.\&lt;h4\&gt;Method\&lt;/h4\&gt;Twenty-seven patients meeting DSM-III criteria for major depression or dysthymic disorder and Columbia criteria for atypical depression received cognitive therapy followed by antidepressant medication for cognitive therapy nonresponders. A response rate with the second treatment equal to that expected with placebo would suggest both treatments target the same depressive population.\&lt;h4\&gt;Results\&lt;/h4\&gt;Of the 25 completers of the study, 14 (56\%) were judged responders to cognitive therapy alone. Sixty-nine percent (9/13) of the responders maintained their benefits for 6 months or more. Seven of the 11 cognitive therapy nonresponders (63\%) responded to antidepressant medication. These results were compared with those of a concurrent double-blind medication study; both its sample and ours were drawn from the same population at the same time: cognitive therapy and antidepressant medication response rates were higher than expected with placebo (28\%).\&lt;h4\&gt;Conclusion\&lt;/h4\&gt;The results suggest that (1) cognitive therapy and antidepressant medication are effective treatments for differing populations of depressed patients, as the antidepressant medication response of cognitive therapy nonresponders was greater than expected with placebo, and (2) cognitive therapy has a lasting effect.},
	number = {5},
	journal = {The Journal of clinical psychiatry},
	author = {Mercier, MA and Stewart, JW and Quitkin, FM},
	month = may,
	year = {1992},
	pages = {166--170},
}

@article{doi:10.3109/10401230600614496,
	title = {Clinical and demographic factors associated with {DSM}-{IV} melancholic depression},
	volume = {18},
	url = {https://www.tandfonline.com/doi/abs/10.3109/10401230600614496},
	doi = {10.3109/10401230600614496},
	abstract = {Background. The purpose of this paper is to use demographic and clinical data from a large diverse group of outpatients diagnosed with non-psychotic major depression to investigate the validity of the DSM-IV concept of melancholic depression.Methods. Baseline clinical and demographic data were collected on 1500 outpatients (1456 of whom melancholia could be determined) with non-psychotic major depressive disorder (MDD) participating in the Sequenced Treatment Alternatives to Relieve Depression (STAR*D) study. Depressive symptom severity was assessed by clinical telephone interview using the 17-item Hamilton Rating Scale for Depression (HRS-D17) and the 30-item Inventory of Depressive Symptomatology (IDS-C30). The types and degrees of concurrent psychiatric symptoms were measured using a self report, the Psychiatric Diagnostic Screening Questionnaire (PDSQ), by recording the number of items relevant to each diagnostic category endorsed by study participants.Results. Adjusting for severity of depression (as measured by the total HRS-D17 scores), no differences were found in the rate of melancholic depression by race, marital status, education, employment status, family history of depression, primary care versus specialty care, monthly income, and degree of psychiatric and medical co-morbidity. Melancholic depression was significantly more likely in men than women. Melancholic depression after adjustment for severity was associated with a slightly younger age at study entry, as well as with greater illness severity, and slightly shorter duration of current episode. Hispanic ethnicity was associated with lower melancholic depression rates at the .06 level of significance.Conclusions. Among outpatients with MDD, melancholic features were less likely in Hispanic patients, but more likely in slightly younger patients and in men. Melancholic features were also related to a slightly shorter current episode. These findings are consistent with the notion that external socio-demographic factors do not play an important role in the pathophysiology of melancholic depression.},
	number = {2},
	journal = {Annals of Clinical Psychiatry},
	author = {Khan, Ahsan Y. and Carrithers, Joe and Preskorn, Sheldon H. and Lear, Rex and Wisniewski, Stephen R. and Rush, A. John and Stegman, Diane and Kelley, Colleen and Kreiner, Karen and Nierenberg, Andrew A. and Fava, Maurizio},
	year = {2006},
	note = {Publisher: Taylor \& Francis
tex.eprint: https://www.tandfonline.com/doi/pdf/10.3109/10401230600614496},
	pages = {91--98},
}

@article{leventhalEmpiricalStatusMelancholia2005,
	title = {The empirical status of melancholia: {Implications} for psychology},
	volume = {25},
	issn = {02727358},
	shorttitle = {The empirical status of melancholia},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0272735804001205},
	doi = {10.1016/j.cpr.2004.09.001},
	language = {en},
	number = {1},
	urldate = {2022-06-06},
	journal = {Clinical Psychology Review},
	author = {Leventhal, A and Rehm, L},
	month = jan,
	year = {2005},
	pages = {25--44},
}

@article{giliClinicalPatternsTreatment2012,
	title = {Clinical patterns and treatment outcome in patients with melancholic, atypical and non-melancholic depressions},
	volume = {7},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0048200},
	abstract = {OBJECTIVE: To assess sociodemographic, clinical and treatment factors as well as depression outcome in a large representative clinical sample of psychiatric depressive outpatients and to determine if melancholic and atypical depression can be differentiated from residual non-melancholic depressive conditions.
SUBJECTS/MATERIALS AND METHOD: A prospective, naturalistic, multicentre, nationwide epidemiological study of 1455 depressive outpatients was undertaken. Severity of depressive symptoms was assessed by the Hamilton Depression Rating Scale (HDRS) and the Self Rated Inventory of Depressive Symptomatology (IDS-SR(30)). IDS-SR(30) defines melancholic and atypical depression according to DSM-IV criteria. Assessments were carried out after 6-8 weeks of antidepressant treatment and after 14-20 weeks of continuation treatment.
RESULTS: Melancholic patients (16.2\%) were more severely depressed, had more depressive episodes and shorter episode duration than atypical (24.7\%) and non-melancholic patients. Atypical depressive patients showed higher rates of co-morbid anxiety disorders and substance abuse. Melancholic patients showed lower rates of remission.
CONCLUSION: Our study supports a different clinical pattern and treatment outcome for melancholic and atypical depression subtypes.},
	language = {eng},
	number = {10},
	journal = {PloS One},
	author = {Gili, Margalida and Roca, Miquel and Armengol, Silvia and Asensio, David and Garcia-Campayo, Javier and Parker, Gordon},
	year = {2012},
	pmid = {23110213},
	pmcid = {PMC3482206},
	keywords = {Humans, Depressive Disorder, Depressive Disorder, Major, Adult, Female, Male, Middle Aged, Antidepressive Agents, Anxiety Disorders, Depression, Diagnostic and Statistical Manual of Mental Disorders, Prospective Studies},
	pages = {e48200},
	file = {Gili et al_2012_Clinical patterns and treatment outcome in patients with melancholic, atypical.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8XQ724XB\\Gili et al_2012_Clinical patterns and treatment outcome in patients with melancholic, atypical.pdf:application/pdf},
}

@article{parkerInchingBethlehemMapping2010,
	title = {Inching toward {Bethlehem}: mapping melancholia},
	volume = {123},
	issn = {1573-2517},
	shorttitle = {Inching toward {Bethlehem}},
	doi = {10.1016/j.jad.2009.10.001},
	abstract = {BACKGROUND: As melancholia has resisted symptom-based definition, this report considers possible explanations and options for moving forward. Clinician-assigned melancholic and non-melancholic groups were initially compared to refine a candidate set of differentiating symptoms alone for examination against a set of non-clinical validators. Analyses then examined the capacity of both the refined symptom and validator sets to discriminate the assigned melancholic and non-melancholic subjects.
METHODS: Subjects completed measures assessing symptoms and correlates (putative validators) of diagnostic sub-type, and were assessed independently by two psychiatrists.
RESULTS: Analyses identified 14 severity-based symptoms as discriminating clinically-diagnosed groups - with melancholic subjects differing significantly from non-melancholic subjects across a number of validators. Such symptom-based discrimination was superior to DSM-IV and Newcastle Index assignment in a study sub-set. While the refined symptom set had an overall accurate classificatory rate of 68\%, use of the combined sets of refined symptoms and validators improved classification to 80\%.
CONCLUSIONS: Melancholia definition is improved by the use of correlates in addition to depressive symptoms, suggesting that melancholia may be mapped more precisely by use of multiple co-ordinates or data sources.},
	language = {eng},
	number = {1-3},
	journal = {Journal of Affective Disorders},
	author = {Parker, Gordon and Fletcher, Kathryn and Barrett, Melissa and Synnott, Howe and Breakspear, Michael and Rees, Anne-Marie and Hadzi-Pavlovic, Dusan},
	month = jun,
	year = {2010},
	pmid = {19896203},
	keywords = {Humans, Reproducibility of Results, Depressive Disorder, Diagnosis, Differential, Adult, Female, Male, Middle Aged, Diagnostic and Statistical Manual of Mental Disorders, Comorbidity, Personality Assessment, Psychometrics},
	pages = {291--298},
}

@article{monzonMelancholicNonmelancholicDepression2010,
	title = {Melancholic versus non-melancholic depression: differences on cognitive function. {A} longitudinal study protocol},
	volume = {10},
	issn = {1471-244X},
	shorttitle = {Melancholic versus non-melancholic depression},
	doi = {10.1186/1471-244X-10-48},
	abstract = {BACKGROUND: Cognitive dysfunction is common among depressed patients. However, the pattern and magnitude of impairment during episodes of major depressive disorder (MDD) through to clinical remission remains unclear. Heterogeneity of depressive patients and the lack of longitudinal studies may account for contradictory results in previous research.
METHODS/DESIGN: This longitudinal study will analyze cognitive differences between CORE-defined melancholic depressed patients (n = 60) and non-melancholic depressed patients (n = 60). A comprehensive clinical and cognitive assessment will be performed at admission and after 6 months. Cognitive dysfunction in both groups will be longitudinally compared, and the persistence of cognitive impairment after clinical remission will be determined.
DISCUSSION: The study of neuropsychological dysfunction and the cognitive changes through the different phases of depression arise a wide variety of difficulties. Several confounding variables must be controlled to determine if the presence of depression could be considered the only factor accounting for group differences.},
	language = {eng},
	journal = {BMC psychiatry},
	author = {Monzón, Saray and Gili, Margalida and Vives, Margalida and Serrano, Maria Jesus and Bauza, Natalia and Molina, Rosa and García-Toro, Mauro and Salvà, Joan and Llobera, Joan and Roca, Miquel},
	month = jun,
	year = {2010},
	pmid = {20565743},
	pmcid = {PMC2896936},
	keywords = {Humans, Depressive Disorder, Depressive Disorder, Major, Diagnosis, Differential, Psychiatric Status Rating Scales, Adolescent, Adult, Female, Male, Middle Aged, Comorbidity, Clinical Protocols, Cognition Disorders, Cohort Studies, Diagnostic Errors, Longitudinal Studies, Neuropsychological Tests, Severity of Illness Index},
	pages = {48},
	file = {Monzón et al_2010_Melancholic versus non-melancholic depression.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MFEIQXG3\\Monzón et al_2010_Melancholic versus non-melancholic depression.pdf:application/pdf},
}

@misc{SignsMajorDepression2020,
	title = {Signs of {Major} {Depression} {Subtypes}: {Melancholic} {Features}},
	shorttitle = {Signs of {Major} {Depression} {Subtypes}},
	url = {https://psychcentral.com/pro/new-therapist/2020/07/signs-of-major-depression-subtypes-melancholic-features},
	abstract = {Melancholia, a genetic subtype, is the most sinister form of Major Depression, always requiring more than psychotherapy.},
	language = {en},
	urldate = {2022-06-06},
	journal = {Psych Central},
	month = jul,
	year = {2020},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9LAYDWKT\\signs-of-major-depression-subtypes-melancholic-features.html:text/html},
}

@misc{parkerBackBlackWhy,
	title = {Back to black: why melancholia must be understood as distinct from depression},
	shorttitle = {Back to black},
	url = {http://theconversation.com/back-to-black-why-melancholia-must-be-understood-as-distinct-from-depression-38025},
	abstract = {Melancholia has a strong genetic contribution, so it’s largely biologically underpinned rather than caused by social factors (stressors) or psychological factors, such as personality style.},
	language = {en},
	urldate = {2022-06-06},
	journal = {The Conversation},
	author = {Parker, Gordon},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WPE8ICTJ\\back-to-black-why-melancholia-must-be-understood-as-distinct-from-depression-38025.html:text/html},
}

@article{peselowMelancholicEndogenousDepression1992,
	title = {Melancholic/endogenous depression and response to somatic treatment and placebo},
	volume = {149},
	issn = {0002-953X},
	doi = {10.1176/ajp.149.10.1324},
	abstract = {OBJECTIVE: The authors' goals were to examine the effects of somatic treatment and placebo in patients with and without endogenous/melancholic depression.
METHOD: Before entry into one of four trials of antidepressant drugs versus placebo, 231 patients were assessed as to whether they met Research Diagnostic Criteria for definite endogenous depression and/or DSM-III criteria for major depressive episode with melancholia. These patients were prospectively assessed for subsequent response to antidepressant treatment or placebo. Previous studies of the effect of endogenous/melancholic depression on treatment response were also reviewed.
RESULTS: Of the 76 patients with DSM-III melancholia given active medication, 41 (54\%) had a complete or partial response, but only 10 (23\%) of the 44 patients with melancholia given placebo had a complete or partial response. Of the 76 depressed patients without melancholia given active medication, 46 (61\%) had a complete or partial response, and 15 (43\%) of the 35 depressed patients without melancholia given placebo had a complete or partial response. Moderately depressed patients with DSM-III melancholia had a significantly better response to active medication than did severely depressed patients with melancholia and showed the greatest difference between response to active medication and response to placebo. The results of the review of previous studies of the effect of endogenous/melancholic depression on treatment response were mixed.
CONCLUSIONS: Depressed patients with melancholia were not particularly different from depressed patients without melancholia in their responses to antidepressant medication but did differ from patients without melancholia in their responses to active medication versus placebo, particularly if their depression was moderate and not severe. This suggests that patients with DSM-III melancholia may be unresponsive to nonsomatic treatments.},
	language = {eng},
	number = {10},
	journal = {The American Journal of Psychiatry},
	author = {Peselow, E. D. and Sanfilipo, M. P. and Difiglia, C. and Fieve, R. R.},
	month = oct,
	year = {1992},
	pmid = {1388334},
	keywords = {Humans, Depressive Disorder, Psychiatric Status Rating Scales, Antidepressive Agents, Treatment Outcome, Prospective Studies, Double-Blind Method, Fluoxetine, Imipramine, Oximes, Paroxetine, Piperidines, Placebos},
	pages = {1324--1334},
}

@article{cuijpersEffectsPsychotherapiesDepression2021,
	title = {The effects of psychotherapies for depression on response, remission, reliable change, and deterioration: {A} meta‐analysis},
	volume = {144},
	issn = {0001-690X, 1600-0447},
	shorttitle = {The effects of psychotherapies for depression on response, remission, reliable change, and deterioration},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/acps.13335},
	doi = {10.1111/acps.13335},
	language = {en},
	number = {3},
	urldate = {2022-06-06},
	journal = {Acta Psychiatrica Scandinavica},
	author = {Cuijpers, Pim and Karyotaki, Eirini and Ciharova, Marketa and Miguel, Clara and Noma, Hisashi and Furukawa, Toshi A.},
	month = sep,
	year = {2021},
	pages = {288--299},
	file = {Cuijpers et al_2021_The effects of psychotherapies for depression on response, remission, reliable.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5EPBR6J3\\Cuijpers et al_2021_The effects of psychotherapies for depression on response, remission, reliable.pdf:application/pdf},
}

@incollection{janssonConclusionMelancholiaDepression2021,
	address = {Cham},
	title = {Conclusion: {Melancholia}, {Depression}, and the {Politics} of {Classification}},
	isbn = {978-3-030-54801-8 978-3-030-54802-5},
	shorttitle = {Conclusion},
	url = {http://link.springer.com/10.1007/978-3-030-54802-5_7},
	abstract = {Abstract
            The conclusion briefly highlights some alternative models of melancholia at the turn of the twentieth century, before turning the focus to the decline of the melancholia diagnosis and the rise of clinical depression as the new dominant mood disorder in diagnostic literature. The Conclusion considers how the developments outlined in the previous chapters have been foundational not only for the modern psychiatric concepts ‘mood disorder’ and ‘clinical depression’, but also more broadly for classification and clinical practice in twentieth- and twenty-first-century psychiatry. Finally, the book turns the spotlight to the politics of psychiatric classification, and asks what is at work, and at stake, when psychiatry tries to label, classify, and diagnose psychological distress.},
	language = {en},
	urldate = {2022-06-06},
	booktitle = {From {Melancholia} to {Depression}},
	publisher = {Springer International Publishing},
	author = {Jansson, Åsa},
	collaborator = {Jansson, Åsa},
	year = {2021},
	doi = {10.1007/978-3-030-54802-5_7},
	pages = {209--228},
	file = {Jansson_2021_Conclusion.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WBGHQJ9Q\\Jansson_2021_Conclusion.pdf:application/pdf},
}

@article{ettmanPersistentDepressiveSymptoms2022,
	title = {Persistent depressive symptoms during {COVID}-19: a national, population-representative, longitudinal study of {U}.{S}. adults},
	volume = {5},
	issn = {2667-193X},
	shorttitle = {Persistent depressive symptoms during {COVID}-19},
	url = {https://www.thelancet.com/journals/lanam/article/PIIS2667-193X(21)00087-9/fulltext},
	doi = {10.1016/j.lana.2021.100091},
	language = {English},
	urldate = {2022-06-09},
	journal = {The Lancet Regional Health – Americas},
	author = {Ettman, Catherine K. and Cohen, Gregory H. and Abdalla, Salma M. and Sampson, Laura and Trinquart, Ludovic and Castrucci, Brian C. and Bork, Rachel H. and Clark, Melissa A. and Wilson, Ira and Vivier, Patrick M. and Galea, Sandro},
	month = jan,
	year = {2022},
	pmid = {34635882},
	note = {Publisher: Elsevier},
	keywords = {Depression, Economic inequities, Low-income, Mental Health, Stressors, Wealth},
	file = {Ettman et al_2022_Persistent depressive symptoms during COVID-19.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7CYM9KLB\\Ettman et al_2022_Persistent depressive symptoms during COVID-19.pdf:application/pdf;Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VAKY3QKV\\fulltext.html:text/html},
}

@article{chirikovUndergraduateGraduateStudents2020,
	title = {Undergraduate and {Graduate} {Students}’ {Mental} {Health} {During} the {COVID}-19 {Pandemic}},
	url = {https://escholarship.org/uc/item/80k5d5hw},
	abstract = {The COVID-19 pandemic has looming negative impacts on mental health of undergraduate and graduate students at research universities, according to the Student Experience in the Research University (SERU) Consortium survey of 30,725 undergraduate students and 15,346 graduate and professional students conducted in May-July 2020 at nine public research universities. Based on PHQ-2 and GAD-2 screening tools, 35\% of undergraduates and 32\% of graduate and professional students screened positive for major depressive disorder, while 39\% of undergraduate and graduate and professional students screened positive for generalized anxiety disorder. Major depressive disorder and generalized anxiety disorder rates are more pronounced among low-income students; students of color; women and non-binary students; transgender students; gay or lesbian, bisexual, queer, questioning, asexual, and pansexual students; and, students who are caregivers. The prevalence of major depressive disorder and generalized anxiety disorder is higher among the undergraduate and graduate students who did not adapt well to remote instruction.Furthermore, the pandemic has led to increases in students’ mental health disorders compared to previous years. In fact, the prevalence of major depressive disorder among graduate and professional students is two times higher in 2020 compared to 2019 and the prevalence of generalized anxiety disorder is 1.5 times higher than in 2019.},
	language = {en},
	urldate = {2022-06-09},
	author = {Chirikov, Igor and Soria, Krista M. and Horgos, Bonnie and Jones-White, Daniel},
	month = aug,
	year = {2020},
	file = {Chirikov et al_2020_Undergraduate and Graduate Students’ Mental Health During the COVID-19 Pandemic.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\A4FPU6G3\\Chirikov et al_2020_Undergraduate and Graduate Students’ Mental Health During the COVID-19 Pandemic.pdf:application/pdf;Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9FA2AFXV\\80k5d5hw.html:text/html},
}

@article{queirazzaNeuralCorrelatesWeighted2019,
	title = {Neural correlates of weighted reward prediction error during reinforcement learning classify response to cognitive behavioral therapy in depression},
	volume = {5},
	issn = {2375-2548},
	url = {https://www.science.org/doi/10.1126/sciadv.aav4962},
	doi = {10.1126/sciadv.aav4962},
	abstract = {fMRI activity encoding acquisition and processing of feedback enables discrimination of response to self-help CBT in depression.
          , 
            While cognitive behavioral therapy (CBT) is an effective treatment for major depressive disorder, only up to 45\% of depressed patients will respond to it. At present, there is no clinically viable neuroimaging predictor of CBT response. Notably, the lack of a mechanistic understanding of treatment response has hindered identification of predictive biomarkers. To obtain mechanistically meaningful fMRI predictors of CBT response, we capitalize on pretreatment neural activity encoding a weighted reward prediction error (RPE), which is implicated in the acquisition and processing of feedback information during probabilistic learning. Using a conventional mass-univariate fMRI analysis, we demonstrate that, at the group level, responders exhibit greater pretreatment neural activity encoding a weighted RPE in the right striatum and right amygdala. Crucially, using multivariate methods, we show that this activity offers significant out-of-sample classification of treatment response. Our findings support the feasibility and validity of neurocomputational approaches to treatment prediction in psychiatry.},
	language = {en},
	number = {7},
	urldate = {2022-06-10},
	journal = {Science Advances},
	author = {Queirazza, Filippo and Fouragnan, Elsa and Steele, J. Douglas and Cavanagh, Jonathan and Philiastides, Marios G.},
	month = jul,
	year = {2019},
	pages = {eaav4962},
	file = {Queirazza et al_2019_Neural correlates of weighted reward prediction error during reinforcement.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YKCRGQTA\\Queirazza et al_2019_Neural correlates of weighted reward prediction error during reinforcement.pdf:application/pdf},
}

@article{sotskyPatientPredictorsResponse2006,
	title = {Patient {Predictors} of {Response} to {Psychotherapy} and {Pharmacotherapy}: {Findings} in the {NIMH} {Treatment} of {Depression} {Collaborative} {Research} {Program}},
	volume = {4},
	issn = {1541-4094, 1541-4108},
	shorttitle = {Patient {Predictors} of {Response} to {Psychotherapy} and {Pharmacotherapy}},
	url = {http://psychiatryonline.org/doi/abs/10.1176/foc.4.2.278},
	doi = {10.1176/foc.4.2.278},
	language = {en},
	number = {2},
	urldate = {2022-06-10},
	journal = {FOCUS},
	author = {Sotsky, Stuart M. and Glass, David R. and Shea, M. Tracie and Pilkonis, Paul A. and Collins, F. and Elkin, Irene and Watkins, John T. and Imber, Stanley D. and Leber, William R. and Moyer, Janet and Oliveri, Mary Ellen},
	month = apr,
	year = {2006},
	pages = {278--290},
}

@misc{TreatingMelancholicDepression,
	title = {treating melancholic depression - {Google} {Search}},
	url = {https://www.google.com/search?q=treating+melancholic+depression&rlz=1C1RXQR_enUS958US958&ei=-r2jYsSOIbTNwbkPo6GI0Ac&oq=treating+meleancholic+depre&gs_lcp=Cgdnd3Mtd2l6EAMYADIECAAQDTIFCAAQhgM6BQgAEJECOgsIABCABBCxAxCDAToRCC4QgAQQsQMQgwEQxwEQowI6EQguEIAEELEDEIMBEMcBENEDOg4ILhCABBCxAxCDARDUAjoLCC4QgAQQxwEQ0QM6CggAELEDEIMBEEM6BAgAEEM6BwguENQCEEM6CAguEIAEELEDOgsILhCABBDHARCvAToOCC4QgAQQxwEQrwEQ1AI6BQguEIAEOggIABCABBCxAzoFCAAQgAQ6BwgAELEDEEM6DQgAELEDEJECEEYQ-QE6DQgAEIAEELEDEEYQ-QE6BQgAELEDOgoIABCABBBGEPkBOggIABAeEBYQCjoGCAAQHhAWOgYIABAeEA1KBAhBGABKBAhGGABQAFicImDqJ2gAcAB4AYABiQOIAb4hkgEIMC4yNC4yLjGYAQCgAQHAAQE&sclient=gws-wiz},
	urldate = {2022-06-10},
	file = {treating melancholic depression - Google Search:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F278XIZA\\search.html:text/html},
}

@book{wareUserManualSF36v20000,
	address = {London},
	edition = {2nd ed.},
	title = {User's manual for the {SF}-36v2 health survey},
	isbn = {1-891810-16-2},
	language = {English},
	publisher = {Quality Metric},
	author = {Ware, John E.},
	year = {0000},
	keywords = {Data Collection -- instrumentation., Health Status., Health Surveys -- standards., Outcome Assessment, Health Care.},
}

@article{graceDysregulationDopamineSystem2016,
	title = {Dysregulation of the dopamine system in the pathophysiology of schizophrenia and depression},
	volume = {17},
	issn = {1471-003X, 1471-0048},
	url = {http://www.nature.com/articles/nrn.2016.57},
	doi = {10.1038/nrn.2016.57},
	language = {en},
	number = {8},
	urldate = {2022-06-11},
	journal = {Nature Reviews Neuroscience},
	author = {Grace, Anthony A.},
	month = aug,
	year = {2016},
	pages = {524--532},
}

@article{taylorRestoringMelancholiaClassification2008,
	title = {Restoring melancholia in the classification of mood disorders},
	volume = {105},
	issn = {01650327},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165032707002157},
	doi = {10.1016/j.jad.2007.05.023},
	language = {en},
	number = {1-3},
	urldate = {2022-06-13},
	journal = {Journal of Affective Disorders},
	author = {Taylor, Michael Alan and Fink, Max},
	month = jan,
	year = {2008},
	pages = {1--14},
}

@article{tondoMelancholicNonmelancholicMajor2020,
	title = {Melancholic versus {Nonmelancholic} {Major} {Depression} {Compared}},
	volume = {266},
	issn = {01650327},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165032719330228},
	doi = {10.1016/j.jad.2020.01.139},
	language = {en},
	urldate = {2022-06-13},
	journal = {Journal of Affective Disorders},
	author = {Tondo, L. and Vázquez, G.H. and Baldessarini, R.J.},
	month = apr,
	year = {2020},
	pages = {760--765},
}

@book{taylorMelancholiaDiagnosisPathophysiology2006,
	address = {Cambridge; New York},
	title = {Melancholia: the diagnosis, pathophysiology, and treatment of depressive illness},
	isbn = {9780511219061 9780521841511 9780511221057 9780511219740 9780511220739 9781280480324 9781107163119 9786610480326 9780511317071 9780511544330 9780521131247},
	shorttitle = {Melancholia},
	url = {https://doi.org/10.1017/CBO9780511544330},
	abstract = {This book provides a comprehensive review of melancholia as a severe disorder of mood, associated with suicide, psychosis and catatonia. The syndrome is defined with a clear diagnosis, prognosis and range of management strategies. It challenges accepted doctrines and describes melancholia as a treatable and preventable mental illness.},
	language = {English},
	urldate = {2022-06-14},
	publisher = {Cambridge University Press},
	author = {Taylor, Michael Alan and Fink, Max},
	year = {2006},
	note = {OCLC: 173610113},
}

@article{gelenbergPracticeGuidelineTreatment2010,
	title = {Practice {Guideline} for the {Treatment} of {Patients} with {Major} {Depressive} {Disorder}},
	language = {en},
	author = {Gelenberg, Alan J and Freeman, Marlene P and Markowitz, John C and Rosenbaum, Jerrold F and Thase, Michael E and Trivedi, Madhukar H and Rhoads, Richard S Van and Reus, Victor I and DePaulo, J Raymond and Fawcett, Jan A and Schneck, Christopher D and Silbersweig, David A},
	year = {2010},
	pages = {152},
	file = {Gelenberg et al. - 2010 - WORK GROUP ON MAJOR DEPRESSIVE DISORDER.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KBYWLXA4\\Gelenberg et al. - 2010 - WORK GROUP ON MAJOR DEPRESSIVE DISORDER.pdf:application/pdf},
}

@article{undurragaRandomizedPlaceboControlledTrials2012,
	title = {Randomized, {Placebo}-{Controlled} {Trials} of {Antidepressants} for {Acute} {Major} {Depression}: {Thirty}-{Year} {Meta}-{Analytic} {Review}},
	volume = {37},
	issn = {0893-133X, 1740-634X},
	shorttitle = {Randomized, {Placebo}-{Controlled} {Trials} of {Antidepressants} for {Acute} {Major} {Depression}},
	url = {http://www.nature.com/articles/npp2011306},
	doi = {10.1038/npp.2011.306},
	language = {en},
	number = {4},
	urldate = {2022-06-14},
	journal = {Neuropsychopharmacology},
	author = {Undurraga, Juan and Baldessarini, Ross J},
	month = mar,
	year = {2012},
	pages = {851--864},
	file = {Undurraga_Baldessarini_2012_Randomized, Placebo-Controlled Trials of Antidepressants for Acute Major.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NSZERLZB\\Undurraga_Baldessarini_2012_Randomized, Placebo-Controlled Trials of Antidepressants for Acute Major.pdf:application/pdf},
}

@article{taliazOptimizingPredictionResponse2021,
	title = {Optimizing prediction of response to antidepressant medications using machine learning and integrated genetic, clinical, and demographic data},
	volume = {11},
	issn = {2158-3188},
	url = {http://www.nature.com/articles/s41398-021-01488-3},
	doi = {10.1038/s41398-021-01488-3},
	abstract = {Abstract
            
              Major depressive disorder (MDD) is complex and multifactorial, posing a major challenge of tailoring the optimal medication for each patient. Current practice for MDD treatment mainly relies on trial and error, with an estimated 42–53\% response rates for antidepressant use. Here, we sought to generate an accurate predictor of response to a panel of antidepressants and optimize treatment selection using a data-driven approach analyzing combinations of genetic, clinical, and demographic factors. We analyzed the response patterns of patients to three antidepressant medications in the Sequenced Treatment Alternatives to Relieve Depression (STAR*D) study, and employed state-of-the-art machine learning (ML) tools to generate a predictive algorithm. To validate our results, we assessed the algorithm’s capacity to predict individualized antidepressant responses on a separate set of 530 patients in STAR*D, consisting of 271 patients in a validation set and 259 patients in the final test set. This assessment yielded an average balanced accuracy rate of 72.3\% (SD 8.1) and 70.1\% (SD 6.8) across the different medications in the validation and test set, respectively (
              p
               {\textless} 0.01 for all models). To further validate our design scheme, we obtained data from the Pharmacogenomic Research Network Antidepressant Medication Pharmacogenomic Study (PGRN-AMPS) of patients treated with citalopram, and applied the algorithm’s citalopram model. This external validation yielded highly similar results for STAR*D and PGRN-AMPS test sets, with a balanced accuracy of 60.5\% and 61.3\%, respectively (both
              p
              ’s {\textless} 0.01). These findings support the feasibility of using ML algorithms applied to large datasets with genetic, clinical, and demographic features to improve accuracy in antidepressant prescription.},
	language = {en},
	number = {1},
	urldate = {2022-06-14},
	journal = {Translational Psychiatry},
	author = {Taliaz, Dekel and Spinrad, Amit and Barzilay, Ran and Barnett-Itzhaki, Zohar and Averbuch, Dana and Teltsh, Omri and Schurr, Roy and Darki-Morag, Sne and Lerer, Bernard},
	month = dec,
	year = {2021},
	pages = {381},
	file = {Taliaz et al_2021_Optimizing prediction of response to antidepressant medications using machine.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PG53ZK5W\\Taliaz et al_2021_Optimizing prediction of response to antidepressant medications using machine.pdf:application/pdf},
}

@article{mendlewiczAchievingRemissionTreatment2008,
	title = {Towards achieving remission in the treatment of depression},
	volume = {10},
	issn = {1294-8322},
	abstract = {The burden of depressive illness constitutes a major public health issue. Despite real progress and better tolerance of new antidepressant medications, a significant number of depressed patients still suffer from rather severe residual depressive symptoms.This relative lack of efficacy clearly interferes with their psychosocial functioning and their quality of life. In addition, it is now well-recognized that the failure to reach full clinical remission after antidepressant treatment involves a high risk of relapse or recurrence in patients suffering from major depression, This paper reviews the concept of remission across different definitions, and the potential risk factors associated with the failure to reach clinical remission. The identification of specific residual symptoms in nonremitted patients is also of great importance, in order to assess the predictive value of those symptoms in relation to relapse and recurrence. Some methodological issues are also discussed, as well as various therapeutic strategies aimed at relieving residual depressive symptoms. Clinical remission remains a gold standard and a primary objective of modern antidepressant therapy.},
	language = {eng},
	number = {4},
	journal = {Dialogues in Clinical Neuroscience},
	author = {Mendlewicz, Julien},
	year = {2008},
	pmid = {19170394},
	pmcid = {PMC3181889},
	keywords = {Humans, Psychiatric Status Rating Scales, Antidepressive Agents, Depression, Psychiatry, Remission Induction},
	pages = {371--375},
}

@article{furukawaPlaceboResponseRates2016,
	title = {Placebo response rates in antidepressant trials: a systematic review of published and unpublished double-blind randomised controlled studies},
	volume = {3},
	issn = {22150366},
	shorttitle = {Placebo response rates in antidepressant trials},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2215036616303078},
	doi = {10.1016/S2215-0366(16)30307-8},
	language = {en},
	number = {11},
	urldate = {2022-06-14},
	journal = {The Lancet Psychiatry},
	author = {Furukawa, Toshi A and Cipriani, Andrea and Atkinson, Lauren Z and Leucht, Stefan and Ogawa, Yusuke and Takeshima, Nozomi and Hayasaka, Yu and Chaimani, Anna and Salanti, Georgia},
	month = nov,
	year = {2016},
	pages = {1059--1066},
	file = {Furukawa et al_2016_Placebo response rates in antidepressant trials.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZKELCSPA\\Furukawa et al_2016_Placebo response rates in antidepressant trials.pdf:application/pdf},
}

@article{esmaeilzadehConsciousAI2021,
	title = {Conscious {AI}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2105.07879},
	doi = {10.48550/ARXIV.2105.07879},
	abstract = {Recent advances in artificial intelligence (AI) have achieved human-scale speed and accuracy for classification tasks. In turn, these capabilities have made AI a viable replacement for many human activities that at their core involve classification, such as basic mechanical and analytical tasks in low-level service jobs. Current systems do not need to be conscious to recognize patterns and classify them. However, for AI to progress to more complicated tasks requiring intuition and empathy, it must develop capabilities such as metathinking, creativity, and empathy akin to human self-awareness or consciousness. We contend that such a paradigm shift is possible only through a fundamental shift in the state of artificial intelligence toward consciousness, a shift similar to what took place for humans through the process of natural selection and evolution. As such, this paper aims to theoretically explore the requirements for the emergence of consciousness in AI. It also provides a principled understanding of how conscious AI can be detected and how it might be manifested in contrast to the dominant paradigm that seeks to ultimately create machines that are linguistically indistinguishable from humans.},
	urldate = {2022-06-29},
	author = {Esmaeilzadeh, Hadi and Vaezi, Reza},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computers and Society (cs.CY)},
	file = {Conscious AI.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Conscious AI.md:text/plain},
}

@article{agarwalFunctionallyEffectiveConscious2020,
	title = {Functionally {Effective} {Conscious} {AI} {Without} {Suffering}},
	volume = {07},
	issn = {2705-0785, 2705-0793},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S2705078520300030},
	doi = {10.1142/S2705078520300030},
	abstract = {Insofar as consciousness has a functional role in facilitating learning and behavioral control, the builders of autonomous Artificial Intelligence (AI) systems are likely to attempt to incorporate it into their designs. The extensive literature on the ethics of AI is concerned with ensuring that AI systems, and especially autonomous conscious ones, behave ethically. In contrast, our focus here is on the rarely discussed complementary aspect of engineering conscious AI: how to avoid condemning such systems, for whose creation we would be solely responsible, to unavoidable suffering brought about by phenomenal self-consciousness. We outline two complementary approaches to this problem, one motivated by a philosophical analysis of the phenomenal self, and the other by certain computational concepts in reinforcement learning.},
	language = {en},
	number = {01},
	urldate = {2022-06-30},
	journal = {Journal of Artificial Intelligence and Consciousness},
	author = {Agarwal, A. and Edelman, S.},
	month = mar,
	year = {2020},
	pages = {39--50},
	file = {Agarwal_Edelman_2020_Functionally Effective Conscious AI Without Suffering.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S8N6KHDA\\Agarwal_Edelman_2020_Functionally Effective Conscious AI Without Suffering.pdf:application/pdf},
}

@article{boltucConsciousAIEdge2020,
	title = {Conscious {AI} at the {Edge} of {Chaos}},
	volume = {07},
	issn = {2705-0785, 2705-0793},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S2705078520500010},
	doi = {10.1142/S2705078520500010},
	abstract = {The main problem for AI consciousness is to operate within the right kind of AI. We distinguish between the traditional computing (GOFAI), and the computing based on stochastic pattern optimization. The latter will be called here computing at the edge of chaos. Optimization of learning patterns, which is the gist of its success, often happens between the areas of too much repetitive order and those of hard to predict and control stochastic processes. This is to change the focus from the opposition of symbolic versus sub-symbolic computing; symbols can appear at different granularities and the hedge between The Physical Symbol System Hypothesis and neural nets seems no longer the most productive cut to make. Computing at the edge of chaos is promising for AGI, especially for AGI consciousness. The second problem for AI consciousness is to work with the right definitions of consciousness.},
	language = {en},
	number = {01},
	urldate = {2022-06-30},
	journal = {Journal of Artificial Intelligence and Consciousness},
	author = {Bołtuć, Piotr},
	month = mar,
	year = {2020},
	pages = {25--38},
}

@article{wangConsciousControlFlowCCFConscious2022,
	title = {{ConsciousControlFlow}({CCF}): {Conscious} {Artificial} {Intelligence} {Based} on {Needs}},
	volume = {09},
	issn = {2705-0785, 2705-0793},
	shorttitle = {{ConsciousControlFlow}({CCF})},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S2705078521500132},
	doi = {10.1142/S2705078521500132},
	abstract = {The major criteria to distinguish conscious Artificial Intelligence (AI) and non-conscious AI is whether the conscious is from the needs. Based on this criteria, we develop ConsciousControlFlow(CCF) to show the need-based conscious AI. The system is based on the computational model with a short-term memory (STM) and long-term memory (LTM) for consciousness and the hierarchy of needs. To generate AI based on real needs of the agent, we developed several LTMs for special functions such as feeling and sensor. Experiments have demonstrated that the agents in the proposed system behave according to the needs, which coincides with the prediction.},
	language = {en},
	number = {01},
	urldate = {2022-06-30},
	journal = {Journal of Artificial Intelligence and Consciousness},
	author = {Wang, Hongzhi and Chen, Bozhou and Xu, Yueyang and Zhang, Kaixin and Zheng, Shengwen},
	month = mar,
	year = {2022},
	pages = {93--110},
}

@article{schlagelWhyNotArtificial1998,
	title = {Why not artificial consciousness or thought?},
	volume = {9},
	issn = {09246495},
	url = {http://link.springer.com/10.1023/A:1008374714117},
	doi = {10.1023/A:1008374714117},
	number = {1},
	urldate = {2022-06-30},
	journal = {Minds and Machines},
	author = {Schlagel, Richard H.},
	year = {1998},
	pages = {3--28},
}

@article{buttazzoArtificialConsciousnessUtopia2001,
	title = {Artificial consciousness: {Utopia} or real possibility?},
	volume = {34},
	issn = {00189162},
	shorttitle = {Artificial consciousness},
	url = {http://ieeexplore.ieee.org/document/933500/},
	doi = {10.1109/2.933500},
	number = {7},
	urldate = {2022-06-30},
	journal = {Computer},
	author = {Buttazzo, G.},
	month = jul,
	year = {2001},
	pages = {24--30},
}

@book{franklinArtificialMinds1995,
	address = {Cambridge, Mass},
	title = {Artificial minds},
	isbn = {978-0-262-06178-0},
	publisher = {MIT Press},
	author = {Franklin, Stan},
	year = {1995},
	keywords = {Artificial intelligence, Brain, Cognitive science},
}

@article{baarsCONSCIOUSNESSCOMPUTATIONALLIDA2009,
	title = {{CONSCIOUSNESS} {IS} {COMPUTATIONAL}: {THE} {LIDA} {MODEL} {OF} {GLOBAL} {WORKSPACE} {THEORY}},
	volume = {01},
	issn = {1793-8430, 1793-8473},
	shorttitle = {{CONSCIOUSNESS} {IS} {COMPUTATIONAL}},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S1793843009000050},
	doi = {10.1142/S1793843009000050},
	language = {en},
	number = {01},
	urldate = {2022-06-30},
	journal = {International Journal of Machine Consciousness},
	author = {Baars, Bernard J. and Franklin, Stan},
	month = jun,
	year = {2009},
	pages = {23--32},
}

@article{franklinLIDACognitiveModel2016,
	title = {A {LIDA} cognitive model tutorial},
	volume = {16},
	issn = {2212683X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212683X16300196},
	doi = {10.1016/j.bica.2016.04.003},
	language = {en},
	urldate = {2022-06-30},
	journal = {Biologically Inspired Cognitive Architectures},
	author = {Franklin, Stan and Madl, Tamas and Strain, Steve and Faghihi, Usef and Dong, Daqi and Kugele, Sean and Snaider, Javier and Agrawal, Pulin and Chen, Sheng},
	month = apr,
	year = {2016},
	pages = {105--130},
	file = {Franklin et al_2016_A LIDA cognitive model tutorial.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MSH2GYG9\\Franklin et al_2016_A LIDA cognitive model tutorial.pdf:application/pdf},
}

@article{wallachConceptualComputationalModel2010,
	title = {A {Conceptual} and {Computational} {Model} of {Moral} {Decision} {Making} in {Human} and {Artificial} {Agents}},
	volume = {2},
	issn = {17568757},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1756-8765.2010.01095.x},
	doi = {10.1111/j.1756-8765.2010.01095.x},
	language = {en},
	number = {3},
	urldate = {2022-06-30},
	journal = {Topics in Cognitive Science},
	author = {Wallach, Wendell and Franklin, Stan and Allen, Colin},
	month = may,
	year = {2010},
	pages = {454--485},
	file = {Wallach et al_2010_A Conceptual and Computational Model of Moral Decision Making in Human and.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KK57TNR7\\Wallach et al_2010_A Conceptual and Computational Model of Moral Decision Making in Human and.pdf:application/pdf},
}

@book{goertzelAdvancesArtificialGeneral2007,
	address = {Amsterdam ; Washington, DC},
	series = {Frontiers in artificial intelligence and applications},
	title = {Advances in artificial general intelligence: concepts, architectures and algorithms: proceedings of the {AGI} {Workshop} 2006},
	isbn = {978-1-58603-758-1},
	shorttitle = {Advances in artificial general intelligence},
	number = {v. 157},
	publisher = {IOS Press},
	editor = {Goertzel, Ben and Wang, Pei},
	year = {2007},
	note = {Meeting Name: AGI Workshop},
	keywords = {Artificial intelligence, Congresses},
}

@misc{CLARIONCognitiveArchitecture2022,
	title = {{CLARION} (cognitive architecture)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=CLARION_(cognitive_architecture)&oldid=1069647525},
	abstract = {Connectionist Learning with Adaptive Rule Induction On-line (CLARION) is a computational cognitive architecture that has been used to simulate many domains and tasks in cognitive psychology and social psychology, as well as implementing intelligent systems in artificial intelligence applications.  An important feature of CLARION is the distinction between implicit and explicit processes and focusing on capturing the interaction between these two types of processes.  The system was created by the research group led by Ron Sun.},
	language = {en},
	urldate = {2022-06-30},
	journal = {Wikipedia},
	month = feb,
	year = {2022},
	note = {Page Version ID: 1069647525},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\687L3M6U\\CLARION_(cognitive_architecture).html:text/html},
}

@misc{OpenCog2022,
	title = {{OpenCog}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=OpenCog&oldid=1081226752},
	abstract = {OpenCog is a project that aims to build an open source artificial intelligence framework. OpenCog Prime is an architecture for robot and virtual embodied cognition that defines a set of interacting components designed to give rise to human-equivalent artificial general intelligence (AGI) as an emergent phenomenon of the whole system. OpenCog Prime's design is primarily the work of Ben Goertzel while the OpenCog framework is intended as a generic framework for broad-based AGI research. Research utilizing OpenCog has been published in journals and presented at conferences and workshops including the annual Conference on Artificial General Intelligence. OpenCog is released under the terms of the GNU Affero General Public License.
OpenCog is in use by more than 50 companies, including Huawei and Cisco.},
	language = {en},
	urldate = {2022-06-30},
	journal = {Wikipedia},
	month = apr,
	year = {2022},
	note = {Page Version ID: 1081226752},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LJYPJFUZ\\OpenCog.html:text/html},
}

@article{kraussWillWeEver2020,
	title = {Will {We} {Ever} {Have} {Conscious} {Machines}?},
	volume = {14},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2020.556544/full},
	doi = {10.3389/fncom.2020.556544},
	abstract = {The question of whether artificial beings or machines could become self-aware or conscious has been a philosophical question for centuries. The main problem is that self-awareness cannot be observed from an outside perspective and the distinction of being really self-aware or merely a clever imitation cannot be answered without access to knowledge about the mechanism's inner workings. We investigate common machine learning approaches with respect to their potential ability to become self-aware. We realize that many important algorithmic steps toward machines with a core consciousness have already been taken.},
	urldate = {2022-07-01},
	journal = {Frontiers in Computational Neuroscience},
	author = {Krauss, Patrick and Maier, Andreas},
	month = dec,
	year = {2020},
	pages = {556544},
	file = {Krauss_Maier_2020_Will We Ever Have Conscious Machines.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PFE9I444\\Krauss_Maier_2020_Will We Ever Have Conscious Machines.pdf:application/pdf;Will We Ever Have Conscious Machines.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Will We Ever Have Conscious Machines.md:text/plain},
}

@article{tononiNoTitleFound2004,
	title = {[{No} title found]},
	volume = {5},
	issn = {14712202},
	url = {http://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-5-42},
	doi = {10.1186/1471-2202-5-42},
	number = {1},
	urldate = {2022-07-02},
	journal = {BMC Neuroscience},
	author = {Tononi, Giulio},
	year = {2004},
	pages = {42},
	file = {Tononi_2004_[No title found].pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PBFXBR5W\\Tononi_2004_[No title found].pdf:application/pdf},
}

@article{mnihHumanlevelControlDeep2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2022-07-02},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
}

@book{suttonReinforcementLearningIntroduction2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
	file = {Reinforcement learning an introduction.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Reinforcement learning an introduction.md:text/plain},
}

@article{manzottiGoodOldFashionedArtificial2018,
	title = {Good {Old}-{Fashioned} {Artificial} {Consciousness} and the {Intermediate} {Level} {Fallacy}},
	volume = {5},
	issn = {2296-9144},
	url = {http://journal.frontiersin.org/article/10.3389/frobt.2018.00039/full},
	doi = {10.3389/frobt.2018.00039},
	urldate = {2022-07-04},
	journal = {Frontiers in Robotics and AI},
	author = {Manzotti, Riccardo and Chella, Antonio},
	month = apr,
	year = {2018},
	pages = {39},
	file = {Manzotti_Chella_2018_Good Old-Fashioned Artificial Consciousness and the Intermediate Level Fallacy.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H8P5GFYX\\Manzotti_Chella_2018_Good Old-Fashioned Artificial Consciousness and the Intermediate Level Fallacy.pdf:application/pdf},
}

@article{meissnerArtificialIntelligenceConsciousness2020,
	title = {Artificial intelligence: consciousness and conscience},
	volume = {35},
	issn = {0951-5666, 1435-5655},
	shorttitle = {Artificial intelligence},
	url = {http://link.springer.com/10.1007/s00146-019-00880-4},
	doi = {10.1007/s00146-019-00880-4},
	language = {en},
	number = {1},
	urldate = {2022-07-04},
	journal = {AI \& SOCIETY},
	author = {Meissner, Gunter},
	month = mar,
	year = {2020},
	pages = {225--235},
}

@inproceedings{schmidhuberOnlineAlgorithmDynamic1990,
	address = {San Diego, CA, USA},
	title = {An on-line algorithm for dynamic reinforcement learning and planning in reactive environments},
	url = {http://ieeexplore.ieee.org/document/5726682/},
	doi = {10.1109/IJCNN.1990.137723},
	urldate = {2022-07-05},
	booktitle = {1990 {IJCNN} {International} {Joint} {Conference} on {Neural} {Networks}},
	publisher = {IEEE},
	author = {Schmidhuber, J.},
	year = {1990},
	pages = {253--258 vol.2},
	file = {An on-line algorithm for dynamic reinforcement learning and planning in reactive environments.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\An on-line algorithm for dynamic reinforcement learning and planning in reactive environments.md:text/plain},
}

@book{internationalconferenceonsimulationofadaptivebehaviorAnimalsAnimatsProceedings1991,
	address = {Cambridge, Mass.},
	title = {From animals to animats proceedings of the {First} {International} {Conference} on {Simulation} of {Adaptive} {Behavior}},
	isbn = {978-0-262-25667-4},
	url = {http://ieeexplore.ieee.org/servlet/opac?bknumber=6267308},
	language = {English},
	urldate = {2022-07-05},
	publisher = {MIT Press},
	author = {International Conference on Simulation of Adaptive Behavior, Stewart W, Wilson and Meyer, Jean-Arcady},
	year = {1991},
	note = {OCLC: 945455936},
	file = {From animals to animats proceedings of the First International Conference on Simulation of Adaptive Behavior.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\From animals to animats proceedings of the First International Conference on Simulation of Adaptive Behavior.md:text/plain},
}

@misc{schmidhuberLearningThinkAlgorithmic2015,
	title = {On {Learning} to {Think}: {Algorithmic} {Information} {Theory} for {Novel} {Combinations} of {Reinforcement} {Learning} {Controllers} and {Recurrent} {Neural} {World} {Models}},
	shorttitle = {On {Learning} to {Think}},
	url = {http://arxiv.org/abs/1511.09249},
	abstract = {This paper addresses the general problem of reinforcement learning (RL) in partially observable environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initially unknown environment, and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic information theory, we describe RNN-based AIs (RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user, others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the RNNAI learns to actively query its model for abstract reasoning and planning and decision making, essentially "learning to think." The basic ideas of this report can be applied to many other cases where one RNN-like system exploits the algorithmic information content of another. They are taken from a grant proposal submitted in Fall 2014, and also explain concepts such as "mirror neurons." Experimental results will be described in separate papers.},
	urldate = {2022-07-05},
	publisher = {arXiv},
	author = {Schmidhuber, Juergen},
	month = nov,
	year = {2015},
	note = {arXiv:1511.09249 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WSLC2WLG\\1511.html:text/html;On Learning to Think Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\On Learning to Think Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models.md:text/plain;Schmidhuber_2015_On Learning to Think.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KEUWA6F6\\Schmidhuber_2015_On Learning to Think.pdf:application/pdf},
}

@incollection{baarsGlobalWorkspaceTheory2007,
	address = {Malden, MA, USA},
	title = {The {Global} {Workspace} {Theory} of {Consciousness}},
	isbn = {978-0-470-75146-6 978-1-4051-2019-7},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9780470751466.ch19},
	urldate = {2022-07-05},
	booktitle = {The {Blackwell} {Companion} to {Consciousness}},
	publisher = {Blackwell Publishing},
	author = {Baars, Bernard J.},
	editor = {Velmans, Max and Schneider, Susan},
	month = dec,
	year = {2007},
	doi = {10.1002/9780470751466.ch19},
	pages = {236--246},
}

@misc{PatientBillRights,
	title = {Patient's {Bill} of {Rights} and {Responsibilities} {\textbar} {McKinley} {Health} {Center} {\textbar} {University} of {Illinois} at {Urbana}-{Champaign}},
	url = {https://mckinley.illinois.edu/about/patients-bill-rights-and-responsibilities},
	urldate = {2022-07-12},
	file = {Patient's Bill of Rights and Responsibilities | McKinley Health Center | University of Illinois at Urbana-Champaign:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H5NZJK6G\\patients-bill-rights-and-responsibilities.html:text/html},
}

@article{PsychiatricPatientRight1990,
	title = {The psychiatric patient's right to effective treatment: implications of {Osheroff} v. {Chestnut} {Lodge}},
	volume = {147},
	issn = {0002-953X, 1535-7228},
	shorttitle = {The psychiatric patient's right to effective treatment},
	url = {http://psychiatryonline.org/doi/abs/10.1176/ajp.147.4.409},
	doi = {10.1176/ajp.147.4.409},
	language = {en},
	number = {4},
	urldate = {2022-07-12},
	journal = {American Journal of Psychiatry},
	month = apr,
	year = {1990},
	pages = {409--418},
}

@article{OsheroffDebateFinale1991,
	title = {The {Osheroff} debate: finale},
	volume = {148},
	issn = {0002-953X, 1535-7228},
	shorttitle = {The {Osheroff} debate},
	url = {http://psychiatryonline.org/doi/abs/10.1176/ajp.148.3.387},
	doi = {10.1176/ajp.148.3.387},
	language = {en},
	number = {3},
	urldate = {2022-07-12},
	journal = {American Journal of Psychiatry},
	month = mar,
	year = {1991},
	pages = {387--390},
}

@article{eastmanBehavioralFormulationsDepression1976,
	title = {Behavioral formulations of depression},
	volume = {83},
	issn = {0033-295X},
	language = {eng},
	number = {4},
	journal = {Psychological Review},
	author = {Eastman, C.},
	month = jul,
	year = {1976},
	pmid = {959440},
	keywords = {Humans, Depression, Models, Psychological, Reinforcement, Psychology},
	pages = {277--291},
}

@article{libetConceptSocialSkill1973,
	title = {Concept of social skill with special reference to the behavior of depressed persons.},
	volume = {40},
	issn = {1939-2117, 0022-006X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0034530},
	doi = {10.1037/h0034530},
	language = {en},
	number = {2},
	urldate = {2022-07-12},
	journal = {Journal of Consulting and Clinical Psychology},
	author = {Libet, Julian M. and Lewinsohn, Peter M.},
	year = {1973},
	pages = {304--312},
}

@book{beckDepressionCausesTreatment2009,
	address = {Philadelphia},
	edition = {2nd ed},
	title = {Depression: causes and treatment},
	isbn = {978-0-8122-1964-7},
	shorttitle = {Depression},
	publisher = {University of Pennsylvania Press},
	author = {Beck, Aaron T. and Alford, Brad A.},
	year = {2009},
	note = {OCLC: ocn229036125},
	keywords = {Depressive Disorder, Depression, Mental},
}

@book{seligmanHelplessnessDepressionDevelopment1992,
	address = {New York},
	series = {A {Series} of books in psychology},
	title = {Helplessness: on depression, development, and death},
	isbn = {978-0-7167-2328-8},
	shorttitle = {Helplessness},
	publisher = {W. H. Freeman},
	author = {Seligman, Martin E. P.},
	year = {1992},
	keywords = {Depression, Mental, Helplessness (Psychology)},
}

@article{rehmSelfcontrolModelDepression1977,
	title = {A self-control model of depression},
	volume = {8},
	issn = {00057894},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0005789477801500},
	doi = {10.1016/S0005-7894(77)80150-0},
	language = {en},
	number = {5},
	urldate = {2022-07-12},
	journal = {Behavior Therapy},
	author = {Rehm, Lynn P.},
	month = nov,
	year = {1977},
	pages = {787--804},
}

@misc{CS234ReinforcementLearning,
	title = {{CS234}: {Reinforcement} {Learning} {Winter} 2022},
	url = {http://web.stanford.edu/class/cs234/assignments.html},
	urldate = {2022-07-13},
	file = {CS234\: Reinforcement Learning Winter 2022:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DQ3ITSKJ\\assignments.html:text/html},
}

@article{sharmaComprehensiveReportMachine2023,
	title = {A {Comprehensive} {Report} on {Machine} {Learning}-based {Early} {Detection} of {Alzheimer}'s {Disease} using {Multi}-modal {Neuroimaging} {Data}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3492865},
	doi = {10.1145/3492865},
	abstract = {Alzheimer's Disease (AD)
              is a devastating neurodegenerative brain disorder with no cure. An early identification helps patients with AD sustain a normal living. We have outlined
              machine learning (ML)
              methodologies with different schemes of feature extraction to synergize complementary and correlated characteristics of data acquired from multiple modalities of neuroimaging. A variety of feature selection, scaling, and fusion methodologies along with confronted challenges are elaborated for designing an ML-based AD diagnosis system. Additionally, thematic analysis has been provided to compare the ML workflow for possible diagnostic solutions. This comprehensive report adds value to the further advancement of computer-aided early diagnosis system based on multi-modal neuroimaging data from patients with AD.},
	language = {en},
	number = {2},
	urldate = {2022-07-23},
	journal = {ACM Computing Surveys},
	author = {Sharma, Shallu and Mandal, Pravat Kumar},
	month = mar,
	year = {2023},
	keywords = {healthcare},
	pages = {1--44},
}

@article{chenSurveyApplicationsArtificial2022,
	title = {A {Survey} on {Applications} of {Artificial} {Intelligence} in {Fighting} {Against} {COVID}-19},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3465398},
	doi = {10.1145/3465398},
	abstract = {The COVID-19 pandemic caused by the SARS-CoV-2 virus has spread rapidly worldwide, leading to a global outbreak. Most governments, enterprises, and scientific research institutions are participating in the COVID-19 struggle to curb the spread of the pandemic. As a powerful tool against COVID-19, artificial intelligence (AI) technologies are widely used in combating this pandemic. In this survey, we investigate the main scope and contributions of AI in combating COVID-19 from the aspects of disease detection and diagnosis, virology and pathogenesis, drug and vaccine development, and epidemic and transmission prediction. In addition, we summarize the available data and resources that can be used for AI-based COVID-19 research. Finally, the main challenges and potential directions of AI in fighting against COVID-19 are discussed. Currently, AI mainly focuses on medical image inspection, genomics, drug development, and transmission prediction, and thus AI still has great potential in this field. This survey presents medical and AI researchers with a comprehensive view of the existing and potential applications of AI technology in combating COVID-19 with the goal of inspiring researchers to continue to maximize the advantages of AI and big data to fight COVID-19.},
	language = {en},
	number = {8},
	urldate = {2022-07-23},
	journal = {ACM Computing Surveys},
	author = {Chen, Jianguo and Li, Kenli and Zhang, Zhaolei and Li, Keqin and Yu, Philip S.},
	month = nov,
	year = {2022},
	keywords = {healthcare},
	pages = {1--32},
	file = {Chen et al_2022_A Survey on Applications of Artificial Intelligence in Fighting Against COVID-19.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7VXLABLP\\Chen et al_2022_A Survey on Applications of Artificial Intelligence in Fighting Against COVID-19.pdf:application/pdf},
}

@inproceedings{rochmawantiChestXRayImage2021,
	address = {Malang Indonesia},
	title = {Chest {X}-{Ray} {Image} to {Classify} {Lung} diseases in {Different} {Resolution} {Size} using {DenseNet}-121 {Architectures}},
	isbn = {978-1-4503-8407-0},
	url = {https://dl.acm.org/doi/10.1145/3479645.3479667},
	doi = {10.1145/3479645.3479667},
	language = {en},
	urldate = {2022-07-23},
	booktitle = {6th {International} {Conference} on {Sustainable} {Information} {Engineering} and {Technology} 2021},
	publisher = {ACM},
	author = {Rochmawanti, Ovy and Utaminingrum, Fitri},
	month = sep,
	year = {2021},
	keywords = {healthcare},
	pages = {327--331},
	file = {Rochmawanti and Utaminingrum - 2021 - Chest X-Ray Image to Classify Lung diseases in Dif.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\28FNU475\\Rochmawanti and Utaminingrum - 2021 - Chest X-Ray Image to Classify Lung diseases in Dif.pdf:application/pdf},
}

@inproceedings{luNewPulmonaryDisease2021,
	address = {Leicester United Kingdom},
	title = {A new pulmonary disease diagnosis system based on {EfficientNet} and transfer learning: pulmonary disease diagnosis based on {EfficientNet} and {TL}},
	isbn = {978-1-4503-9163-4},
	shorttitle = {A new pulmonary disease diagnosis system based on {EfficientNet} and transfer learning},
	url = {https://dl.acm.org/doi/10.1145/3492323.3495568},
	doi = {10.1145/3492323.3495568},
	language = {en},
	urldate = {2022-07-23},
	booktitle = {Proceedings of the 14th {IEEE}/{ACM} {International} {Conference} on {Utility} and {Cloud} {Computing} {Companion}},
	publisher = {ACM},
	author = {Lu, Siyuan and Zhang, Xin and Zhang, Yu-Dong},
	month = dec,
	year = {2021},
	pages = {1--4},
	file = {Lu et al. - 2021 - A new pulmonary disease diagnosis system based on .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8WKY6UMD\\Lu et al. - 2021 - A new pulmonary disease diagnosis system based on .pdf:application/pdf},
}

@article{pangDeepLearningAnomaly2022,
	title = {Deep {Learning} for {Anomaly} {Detection}: {A} {Review}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Deep {Learning} for {Anomaly} {Detection}},
	url = {https://dl.acm.org/doi/10.1145/3439950},
	doi = {10.1145/3439950},
	abstract = {Anomaly detection, a.k.a. outlier detection or novelty detection, has been a lasting yet active research area in various research communities for several decades. There are still some unique problem complexities and challenges that require advanced approaches. In recent years, deep learning enabled anomaly detection, i.e.,
              deep anomaly detection
              , has emerged as a critical direction. This article surveys the research of deep anomaly detection with a comprehensive taxonomy, covering advancements in 3 high-level categories and 11 fine-grained categories of the methods. We review their key intuitions, objective functions, underlying assumptions, advantages, and disadvantages and discuss how they address the aforementioned challenges. We further discuss a set of possible future opportunities and new perspectives on addressing the challenges.},
	language = {en},
	number = {2},
	urldate = {2022-07-23},
	journal = {ACM Computing Surveys},
	author = {Pang, Guansong and Shen, Chunhua and Cao, Longbing and Hengel, Anton Van Den},
	month = mar,
	year = {2022},
	pages = {1--38},
	file = {Pang et al_2022_Deep Learning for Anomaly Detection.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PM8W6QJ9\\Pang et al_2022_Deep Learning for Anomaly Detection.pdf:application/pdf;Pang et al. - 2022 - Deep Learning for Anomaly Detection A Review.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YDCMISQV\\Pang et al. - 2022 - Deep Learning for Anomaly Detection A Review.pdf:application/pdf},
}

@inproceedings{essafImprovedConvolutionalNeural2020,
	address = {Sanya China},
	title = {Improved {Convolutional} {Neural} {Network} for {Lung} {Cancer} {Detection}},
	isbn = {978-1-4503-7771-3},
	url = {https://dl.acm.org/doi/10.1145/3398329.3398337},
	doi = {10.1145/3398329.3398337},
	language = {en},
	urldate = {2022-07-23},
	booktitle = {Proceedings of the 2020 {International} {Conference} on {Computing}, {Networks} and {Internet} of {Things}},
	publisher = {ACM},
	author = {Essaf, Firdaous and Li, Yujian and Sakho, Seybou and Gadosey, Pius Kwao},
	month = apr,
	year = {2020},
	pages = {48--54},
	file = {Essaf et al. - 2020 - Improved Convolutional Neural Network for Lung Can.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6IALDKLF\\Essaf et al. - 2020 - Improved Convolutional Neural Network for Lung Can.pdf:application/pdf;Improved Convolutional Neural Network for Lung Cancer Detection.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Improved Convolutional Neural Network for Lung Cancer Detection.md:text/plain},
}

@article{causeySpatialPyramidPooling2020,
	title = {Spatial {Pyramid} {Pooling} with {3D} {Convolution} {Improves} {Lung} {Cancer} {Detection}},
	issn = {1545-5963, 1557-9964, 2374-0043},
	url = {https://ieeexplore.ieee.org/document/9209148/},
	doi = {10.1109/TCBB.2020.3027744},
	urldate = {2022-07-23},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Causey, Jason and Li, Keyu and Chen, Xianghao and Dong, Wei and Walker, Karl and Qualls, Jake and Stubblefield, Jonathan and Moore, Jason H. and Guan, Yuanfang and Huang, Xiuzhen},
	year = {2020},
	pages = {1--1},
	file = {Causey et al. - 2020 - Spatial Pyramid Pooling with 3D Convolution Improv.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H33BTDFQ\\Causey et al. - 2020 - Spatial Pyramid Pooling with 3D Convolution Improv.pdf:application/pdf;Spatial Pyramid Pooling with 3D Convolution Improves Lung Cancer Detection.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Spatial Pyramid Pooling with 3D Convolution Improves Lung Cancer Detection.md:text/plain},
}

@inproceedings{yanqiLungCTImage2020,
	address = {Xiamen China},
	title = {Lung {CT} {Image} {Segmentation} {Based} on {Capsule} {Network}},
	isbn = {978-1-4503-8781-1},
	url = {https://dl.acm.org/doi/10.1145/3443467.3443787},
	doi = {10.1145/3443467.3443787},
	language = {en},
	urldate = {2022-07-23},
	booktitle = {Proceedings of the 2020 4th {International} {Conference} on {Electronic} {Information} {Technology} and {Computer} {Engineering}},
	publisher = {ACM},
	author = {Yanqi, Shen and Aiping, Jiang and Xiaojing, Li and Guotao, Wang},
	month = nov,
	year = {2020},
	pages = {386--390},
	file = {Lung CT Image Segmentation Based on Capsule Network.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Lung CT Image Segmentation Based on Capsule Network.md:text/plain;Yanqi et al. - 2020 - Lung CT Image Segmentation Based on Capsule Networ.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\73I9BIP8\\Yanqi et al. - 2020 - Lung CT Image Segmentation Based on Capsule Networ.pdf:application/pdf},
}

@inproceedings{liuSegmentationModelLung2022,
	address = {Guangzhou China},
	title = {A {Segmentation} {Model} of {Lung} {Parenchyma} in {Chest} {CT} {Based} on {ResUnet}},
	isbn = {978-1-4503-9570-0},
	url = {https://dl.acm.org/doi/10.1145/3529836.3529917},
	doi = {10.1145/3529836.3529917},
	language = {en},
	urldate = {2022-07-23},
	booktitle = {2022 14th {International} {Conference} on {Machine} {Learning} and {Computing} ({ICMLC})},
	publisher = {ACM},
	author = {Liu, Bing and Ye, Chengxu and Yang, Ping and Miao, Zhikun and Liu, Rui and Chen, Ying},
	month = feb,
	year = {2022},
	pages = {429--434},
	file = {A Segmentation Model of Lung Parenchyma in Chest CT Based on ResUnet.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\A Segmentation Model of Lung Parenchyma in Chest CT Based on ResUnet.md:text/plain;A Segmentation Model of Lung Parenchyma in Chest CT Based on ResUnet.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\A Segmentation Model of Lung Parenchyma in Chest CT Based on ResUnet.md:text/plain;Liu et al. - 2022 - A Segmentation Model of Lung Parenchyma in Chest C.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QH4SKYNW\\Liu et al. - 2022 - A Segmentation Model of Lung Parenchyma in Chest C.pdf:application/pdf},
}

@inproceedings{essafReviewDeepLearning2019,
	address = {Sanya China},
	title = {Review on {Deep} {Learning} {Methods} {Used} for {Computer}-aided {Lung} {Cancer} {Detection} and {Diagnosis}},
	isbn = {978-1-4503-7261-9},
	url = {http://dl.acm.org/doi/10.1145/3377713.3377732},
	doi = {10.1145/3377713.3377732},
	language = {en},
	urldate = {2022-07-24},
	booktitle = {Proceedings of the 2019 2nd {International} {Conference} on {Algorithms}, {Computing} and {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Essaf, Firdaous and Li, Yujian and Sakho, Seybou and Kiki, Mesmin J. Mbyamm},
	month = dec,
	year = {2019},
	pages = {104--111},
	file = {Essaf et al. - 2019 - Review on Deep Learning Methods Used for Computer-.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4Y33SACB\\Essaf et al. - 2019 - Review on Deep Learning Methods Used for Computer-.pdf:application/pdf},
}

@inproceedings{liNewMultiscaleDilated2021,
	address = {Beijing China},
	title = {A new {Multi}-scale {Dilated} deep {ResNet} model for {Classification} of {Lung} {Nodules} in {CT} images},
	isbn = {978-1-4503-8519-0},
	url = {https://dl.acm.org/doi/10.1145/3507971.3507988},
	doi = {10.1145/3507971.3507988},
	language = {en},
	urldate = {2022-07-24},
	booktitle = {2021 the 7th {International} {Conference} on {Communication} and {Information} {Processing} ({ICCIP})},
	publisher = {ACM},
	author = {Li, Fenglian and Sherazi, Syed Nisar Yousaf and Zhang, Yan and Wu, Zelin},
	month = dec,
	year = {2021},
	pages = {89--95},
	file = {A new Multi-scale Dilated deep ResNet model for Classification of Lung Nodules in CT images.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\A new Multi-scale Dilated deep ResNet model for Classification of Lung Nodules in CT images.md:text/plain;Li et al. - 2021 - A new Multi-scale Dilated deep ResNet model for Cl.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6MS6GG2D\\Li et al. - 2021 - A new Multi-scale Dilated deep ResNet model for Cl.pdf:application/pdf},
}

@inproceedings{xiaoDetectionPulmonaryNodules2019,
	address = {Beijing China},
	title = {Detection of {Pulmonary} {Nodules} based on {Reception} and {Faster} {R}-{CNN}},
	isbn = {978-1-4503-7657-0},
	url = {https://dl.acm.org/doi/10.1145/3373509.3373587},
	doi = {10.1145/3373509.3373587},
	language = {en},
	urldate = {2022-07-24},
	booktitle = {Proceedings of the 2019 8th {International} {Conference} on {Computing} and {Pattern} {Recognition}},
	publisher = {ACM},
	author = {Xiao, Zhitao and Liu, Bowen and Geng, Lei and Wu, Jun and Liu, Yanbei},
	month = oct,
	year = {2019},
	pages = {160--166},
	file = {Detection of Pulmonary Nodules based on Reception and Faster R-CNN.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Detection of Pulmonary Nodules based on Reception and Faster R-CNN.md:text/plain;Xiao et al. - 2019 - Detection of Pulmonary Nodules based on Reception .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T3UD4R8V\\Xiao et al. - 2019 - Detection of Pulmonary Nodules based on Reception .pdf:application/pdf},
}

@inproceedings{linUsing3DConvolutional2020,
	address = {Shanghai China},
	title = {Using {3D} {Convolutional} {Networks} with {Shortcut} {Connections} for {Improved} {Lung} {Nodules} {Classification}},
	isbn = {978-1-4503-7722-5},
	url = {https://dl.acm.org/doi/10.1145/3404512.3404525},
	doi = {10.1145/3404512.3404525},
	language = {en},
	urldate = {2022-07-24},
	booktitle = {Proceedings of the 2020 2nd {International} {Conference} on {Big} {Data} {Engineering}},
	publisher = {ACM},
	author = {Lin, Zhifeng and Zheng, Jun and Hu, Wenxin},
	month = may,
	year = {2020},
	pages = {42--49},
	file = {Lin et al. - 2020 - Using 3D Convolutional Networks with Shortcut Conn.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QK6UYRZY\\Lin et al. - 2020 - Using 3D Convolutional Networks with Shortcut Conn.pdf:application/pdf;Using 3D Convolutional Networks with Shortcut Connections for Improved Lung Nodules Classification.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Using 3D Convolutional Networks with Shortcut Connections for Improved Lung Nodules Classification.md:text/plain},
}

@inproceedings{yuAutomaticPulmonaryOrgan2022,
	address = {Harbin China},
	title = {Automatic pulmonary organ and small nodule segmentation in {CT} scans: basing on k-means and {DU}-{Net}++},
	isbn = {978-1-4503-9575-5},
	shorttitle = {Automatic pulmonary organ and small nodule segmentation in {CT} scans},
	url = {https://dl.acm.org/doi/10.1145/3523286.3524503},
	doi = {10.1145/3523286.3524503},
	language = {en},
	urldate = {2022-07-24},
	booktitle = {2022 2nd {International} {Conference} on {Bioinformatics} and {Intelligent} {Computing}},
	publisher = {ACM},
	author = {Yu, Hui and Wang, Qingsong and Wang, Guangpu and Sun, Jinglai and Zheng, Jie and Wang, Shuo},
	month = jan,
	year = {2022},
	pages = {6--10},
	file = {Automatic pulmonary organ and small nodule segmentation in CT scans basing on k-means and DU-Net++.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automatic pulmonary organ and small nodule segmentation in CT scans basing on k-means and DU-Net++.md:text/plain;Yu et al. - 2022 - Automatic pulmonary organ and small nodule segment.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YRSM84GS\\Yu et al. - 2022 - Automatic pulmonary organ and small nodule segment.pdf:application/pdf},
}

@inproceedings{sunClassificationLungNodules2020,
	address = {Sanya China},
	title = {Classification of {Lung} {Nodules} {Based} on {GAN} and {3D} {CNN}},
	isbn = {978-1-4503-7772-0},
	url = {https://dl.acm.org/doi/10.1145/3424978.3425094},
	doi = {10.1145/3424978.3425094},
	language = {en},
	urldate = {2022-07-24},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Computer} {Science} and {Application} {Engineering}},
	publisher = {ACM},
	author = {Sun, Bin and Liu, Fengyin and Zhou, Yusun and Jin, Shaolei and Li, Qiang and Jin, Xinyu},
	month = oct,
	year = {2020},
	pages = {1--5},
	file = {Classification of Lung Nodules Based on GAN and 3D CNN.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Classification of Lung Nodules Based on GAN and 3D CNN.md:text/plain;Sun et al. - 2020 - Classification of Lung Nodules Based on GAN and 3D.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NU6LC2F8\\Sun et al. - 2020 - Classification of Lung Nodules Based on GAN and 3D.pdf:application/pdf},
}

@inproceedings{chenEndtoendFrameworkPulmonary2020,
	address = {Shenyang China},
	title = {An end-to-end framework for pulmonary nodule detection and false positive reduction from {CT} {Images}},
	isbn = {978-1-4503-8968-6},
	url = {https://dl.acm.org/doi/10.1145/3451421.3451455},
	doi = {10.1145/3451421.3451455},
	language = {en},
	urldate = {2022-07-24},
	booktitle = {The {Fourth} {International} {Symposium} on {Image} {Computing} and {Digital} {Medicine}},
	publisher = {ACM},
	author = {Chen, Yafang and Cao, Peng and Dou, Lili and Yang, Jinzhu},
	month = dec,
	year = {2020},
	pages = {156--162},
	file = {An end-to-end framework for pulmonary nodule detection and false positive reduction from CT Images.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\An end-to-end framework for pulmonary nodule detection and false positive reduction from CT Images.md:text/plain;Chen et al. - 2020 - An end-to-end framework for pulmonary nodule detec.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AVVWGMGP\\Chen et al. - 2020 - An end-to-end framework for pulmonary nodule detec.pdf:application/pdf},
}

@inproceedings{xuSegmentationLungParenchyma2020,
	address = {Shenyang China},
	title = {Segmentation of {Lung} {Parenchyma} {Based} on {FPN}++{Mask} {R}-{CNN} {Model}},
	isbn = {978-1-4503-8968-6},
	url = {https://dl.acm.org/doi/10.1145/3451421.3451461},
	doi = {10.1145/3451421.3451461},
	language = {en},
	urldate = {2022-07-24},
	booktitle = {The {Fourth} {International} {Symposium} on {Image} {Computing} and {Digital} {Medicine}},
	publisher = {ACM},
	author = {Xu, Xingfei and Li, Gen and Li, Xiaoshuo and Liu, Pan and Tan, Wenjun},
	month = dec,
	year = {2020},
	pages = {187--191},
	file = {Segmentation of Lung Parenchyma Based on FPN++Mask R-CNN Model.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Segmentation of Lung Parenchyma Based on FPN++Mask R-CNN Model.md:text/plain;Xu et al. - 2020 - Segmentation of Lung Parenchyma Based on FPN++Mask.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZSKK2VUE\\Xu et al. - 2020 - Segmentation of Lung Parenchyma Based on FPN++Mask.pdf:application/pdf},
}

@article{ronnebergerUNetConvolutionalNetworks2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {U-{Net}},
	url = {https://arxiv.org/abs/1505.04597},
	doi = {10.48550/ARXIV.1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2022-07-24},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year = {2015},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Computer Vision and Pattern Recognition (cs.CV)},
	file = {U-Net Convolutional Networks for Biomedical Image Segmentation.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\U-Net Convolutional Networks for Biomedical Image Segmentation.md:text/plain},
}

@article{pehrsonAutomaticPulmonaryNodule2019,
	title = {Automatic {Pulmonary} {Nodule} {Detection} {Applying} {Deep} {Learning} or {Machine} {Learning} {Algorithms} to the {LIDC}-{IDRI} {Database}: {A} {Systematic} {Review}},
	volume = {9},
	issn = {2075-4418},
	shorttitle = {Automatic {Pulmonary} {Nodule} {Detection} {Applying} {Deep} {Learning} or {Machine} {Learning} {Algorithms} to the {LIDC}-{IDRI} {Database}},
	url = {https://www.mdpi.com/2075-4418/9/1/29},
	doi = {10.3390/diagnostics9010029},
	abstract = {The aim of this study was to provide an overview of the literature available on machine learning (ML) algorithms applied to the Lung Image Database Consortium Image Collection (LIDC-IDRI) database as a tool for the optimization of detecting lung nodules in thoracic CT scans. This systematic review was compiled according to Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Only original research articles concerning algorithms applied to the LIDC-IDRI database were included. The initial search yielded 1972 publications after removing duplicates, and 41 of these articles were included in this study. The articles were divided into two subcategories describing their overall architecture. The majority of feature-based algorithms achieved an accuracy {\textgreater}90\% compared to the deep learning (DL) algorithms that achieved an accuracy in the range of 82.2\%–97.6\%. In conclusion, ML and DL algorithms are able to detect lung nodules with a high level of accuracy, sensitivity, and specificity using ML, when applied to an annotated archive of CT scans of the lung. However, there is no consensus on the method applied to determine the efficiency of ML algorithms.},
	language = {en},
	number = {1},
	urldate = {2022-07-25},
	journal = {Diagnostics},
	author = {Pehrson, Lea and Nielsen, Michael and Ammitzbøl Lauridsen, Carsten},
	month = mar,
	year = {2019},
	pages = {29},
	file = {Pehrson et al_2019_Automatic Pulmonary Nodule Detection Applying Deep Learning or Machine Learning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E2TQ295Q\\Pehrson et al_2019_Automatic Pulmonary Nodule Detection Applying Deep Learning or Machine Learning.pdf:application/pdf},
}

@article{nasrullahAutomatedLungNodule2019,
	title = {Automated {Lung} {Nodule} {Detection} and {Classification} {Using} {Deep} {Learning} {Combined} with {Multiple} {Strategies}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/17/3722},
	doi = {10.3390/s19173722},
	abstract = {Lung cancer is one of the major causes of cancer-related deaths due to its aggressive nature and delayed detections at advanced stages. Early detection of lung cancer is very important for the survival of an individual, and is a significant challenging problem. Generally, chest radiographs (X-ray) and computed tomography (CT) scans are used initially for the diagnosis of the malignant nodules; however, the possible existence of benign nodules leads to erroneous decisions. At early stages, the benign and the malignant nodules show very close resemblance to each other. In this paper, a novel deep learning-based model with multiple strategies is proposed for the precise diagnosis of the malignant nodules. Due to the recent achievements of deep convolutional neural networks (CNN) in image analysis, we have used two deep three-dimensional (3D) customized mixed link network (CMixNet) architectures for lung nodule detection and classification, respectively. Nodule detections were performed through faster R-CNN on efficiently-learned features from CMixNet and U-Net like encoder–decoder architecture. Classification of the nodules was performed through a gradient boosting machine (GBM) on the learned features from the designed 3D CMixNet structure. To reduce false positives and misdiagnosis results due to different types of errors, the final decision was performed in connection with physiological symptoms and clinical biomarkers. With the advent of the internet of things (IoT) and electro-medical technology, wireless body area networks (WBANs) provide continuous monitoring of patients, which helps in diagnosis of chronic diseases—especially metastatic cancers. The deep learning model for nodules’ detection and classification, combined with clinical factors, helps in the reduction of misdiagnosis and false positive (FP) results in early-stage lung cancer diagnosis. The proposed system was evaluated on LIDC-IDRI datasets in the form of sensitivity (94\%) and specificity (91\%), and better results were obatined compared to the existing methods.},
	language = {en},
	number = {17},
	urldate = {2022-07-25},
	journal = {Sensors},
	author = {Nasrullah, Nasrullah and Sang, Jun and Alam, Mohammad S. and Mateen, Muhammad and Cai, Bin and Hu, Haibo},
	month = aug,
	year = {2019},
	pages = {3722},
	file = {Nasrullah et al_2019_Automated Lung Nodule Detection and Classification Using Deep Learning Combined.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UBHY9WAY\\Nasrullah et al_2019_Automated Lung Nodule Detection and Classification Using Deep Learning Combined.pdf:application/pdf},
}

@article{yuDesignLungNodules2021,
	title = {Design of lung nodules segmentation and recognition algorithm based on deep learning},
	volume = {22},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04234-0},
	doi = {10.1186/s12859-021-04234-0},
	abstract = {Abstract
            
              Background
              Accurate segmentation and recognition algorithm of lung nodules has great important value of reference for early diagnosis of lung cancer. An algorithm is proposed for 3D CT sequence images in this paper based on 3D Res U-Net segmentation network and 3D ResNet50 classification network. The common convolutional layers in encoding and decoding paths of U-Net are replaced by residual units while the loss function is changed to Dice loss after using cross entropy loss to accelerate network convergence. Since the lung nodules are small and rich in 3D information, the ResNet50 is improved by replacing the 2D convolutional layers with 3D convolutional layers and reducing the sizes of some convolution kernels, 3D ResNet50 network is obtained for the diagnosis of benign and malignant lung nodules.
            
            
              Results
              3D Res U-Net was trained and tested on 1044 CT subcases in the LIDC-IDRI database. The segmentation result shows that the Dice coefficient of 3D Res U-Net is above 0.8 for the segmentation of lung nodules larger than 10 mm in diameter. 3D ResNet50 was trained and tested on 2960 lung nodules in the LIDC-IDRI database. The classification result shows that the diagnostic accuracy of 3D ResNet50 is 87.3\% and AUC is 0.907.
            
            
              Conclusion
              The 3D Res U-Net module improves segmentation performance significantly with the comparison of 3D U-Net model based on residual learning mechanism. 3D Res U-Net can identify small nodules more effectively and improve its segmentation accuracy for large nodules. Compared with the original network, the classification performance of 3D ResNet50 is significantly improved, especially for small benign nodules.},
	language = {en},
	number = {S5},
	urldate = {2022-07-25},
	journal = {BMC Bioinformatics},
	author = {Yu, Hui and Li, Jinqiu and Zhang, Lixin and Cao, Yuzhen and Yu, Xuyao and Sun, Jinglai},
	month = nov,
	year = {2021},
	pages = {314},
	file = {Yu et al_2021_Design of lung nodules segmentation and recognition algorithm based on deep.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QYWM44YU\\Yu et al_2021_Design of lung nodules segmentation and recognition algorithm based on deep.pdf:application/pdf},
}

@inproceedings{jhaResUNetAdvancedArchitecture2019,
	address = {San Diego, CA, USA},
	title = {{ResUNet}++: {An} {Advanced} {Architecture} for {Medical} {Image} {Segmentation}},
	isbn = {978-1-72815-606-4},
	shorttitle = {{ResUNet}++},
	url = {https://ieeexplore.ieee.org/document/8959021/},
	doi = {10.1109/ISM46123.2019.00049},
	urldate = {2022-07-26},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Multimedia} ({ISM})},
	publisher = {IEEE},
	author = {Jha, Debesh and Smedsrud, Pia H. and Riegler, Michael A. and Johansen, Dag and Lange, Thomas De and Halvorsen, Pal and D. Johansen, Havard},
	month = dec,
	year = {2019},
	pages = {225--2255},
	file = {Jha et al_2019_ResUNet++.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MFJKXDUS\\Jha et al_2019_ResUNet++.pdf:application/pdf;Jha et al. - 2019 - ResUNet++ An Advanced Architecture for Medical Im.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N6NR3YU9\\Jha et al. - 2019 - ResUNet++ An Advanced Architecture for Medical Im.pdf:application/pdf},
}

@article{hancockLungNoduleMalignancy2016,
	title = {Lung nodule malignancy classification using only radiologist-quantified image features as inputs to statistical learning algorithms: probing the {Lung} {Image} {Database} {Consortium} dataset with two statistical learning methods},
	volume = {3},
	issn = {2329-4302},
	shorttitle = {Lung nodule malignancy classification using only radiologist-quantified image features as inputs to statistical learning algorithms},
	url = {http://medicalimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JMI.3.4.044504},
	doi = {10.1117/1.JMI.3.4.044504},
	language = {en},
	number = {4},
	urldate = {2022-07-26},
	journal = {Journal of Medical Imaging},
	author = {Hancock, Matthew C. and Magnan, Jerry F.},
	month = dec,
	year = {2016},
	pages = {044504},
	file = {Hancock_Magnan_2016_Lung nodule malignancy classification using only radiologist-quantified image.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P4EBWJ78\\Hancock_Magnan_2016_Lung nodule malignancy classification using only radiologist-quantified image.pdf:application/pdf},
}

@article{armatoLungImageDatabase2011,
	title = {The {Lung} {Image} {Database} {Consortium} ({LIDC}) and {Image} {Database} {Resource} {Initiative} ({IDRI}): {A} {Completed} {Reference} {Database} of {Lung} {Nodules} on {CT} {Scans}: {The} {LIDC}/{IDRI} thoracic {CT} database of lung nodules},
	volume = {38},
	issn = {00942405},
	shorttitle = {The {Lung} {Image} {Database} {Consortium} ({LIDC}) and {Image} {Database} {Resource} {Initiative} ({IDRI})},
	url = {http://doi.wiley.com/10.1118/1.3528204},
	doi = {10.1118/1.3528204},
	language = {en},
	number = {2},
	urldate = {2022-07-26},
	journal = {Medical Physics},
	author = {Armato, Samuel G. and McLennan, Geoffrey and Bidaut, Luc and McNitt-Gray, Michael F. and Meyer, Charles R. and Reeves, Anthony P. and Zhao, Binsheng and Aberle, Denise R. and Henschke, Claudia I. and Hoffman, Eric A. and Kazerooni, Ella A. and MacMahon, Heber and van Beek, Edwin J. R. and Yankelevitz, David and Biancardi, Alberto M. and Bland, Peyton H. and Brown, Matthew S. and Engelmann, Roger M. and Laderach, Gary E. and Max, Daniel and Pais, Richard C. and Qing, David P.-Y. and Roberts, Rachael Y. and Smith, Amanda R. and Starkey, Adam and Batra, Poonam and Caligiuri, Philip and Farooqi, Ali and Gladish, Gregory W. and Jude, C. Matilda and Munden, Reginald F. and Petkovska, Iva and Quint, Leslie E. and Schwartz, Lawrence H. and Sundaram, Baskaran and Dodd, Lori E. and Fenimore, Charles and Gur, David and Petrick, Nicholas and Freymann, John and Kirby, Justin and Hughes, Brian and Vande Casteele, Alessi and Gupte, Sangeeta and Sallam, Maha and Heath, Michael D. and Kuhn, Michael H. and Dharaiya, Ekta and Burns, Richard and Fryd, David S. and Salganicoff, Marcos and Anand, Vikram and Shreter, Uri and Vastagh, Stephen and Croft, Barbara Y. and Clarke, Laurence P.},
	month = jan,
	year = {2011},
	pages = {915--931},
	file = {Armato et al_2011_The Lung Image Database Consortium (LIDC) and Image Database Resource.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7I8KANFQ\\Armato et al_2011_The Lung Image Database Consortium (LIDC) and Image Database Resource.pdf:application/pdf},
}

@article{nazirEfficientPreProcessingSegmentation2021,
	title = {Efficient {Pre}-{Processing} and {Segmentation} for {Lung} {Cancer} {Detection} {Using} {Fused} {CT} {Images}},
	volume = {11},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/11/1/34},
	doi = {10.3390/electronics11010034},
	abstract = {Over the last two decades, radiologists have been using multi-view images to detect tumors. Computer Tomography (CT) imaging is considered as one of the reliable imaging techniques. Many medical-image-processing techniques have been developed to diagnoses lung cancer at early or later stages through CT images; however, it is still a big challenge to improve the accuracy and sensitivity of the algorithms. In this paper, we propose an algorithm based on image fusion for lung segmentation to optimize lung cancer diagnosis. The image fusion technique was developed through Laplacian Pyramid (LP) decomposition along with Adaptive Sparse Representation (ASR). The suggested fusion technique fragments medical images into different sizes using the LP. After that, the LP is used to fuse the four decomposed layers. For the evaluation purposes of the proposed technique, the Lungs Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) was used. The results showed that the Dice Similarity Coefficient (DSC) index of our proposed method was 0.9929, which is better than recently published results. Furthermore, the values of other evaluation parameters such as the sensitivity, specificity, and accuracy were 89\%, 98\% and 99\%, respectively, which are also competitive with the recently published results.},
	language = {en},
	number = {1},
	urldate = {2022-07-27},
	journal = {Electronics},
	author = {Nazir, Imran and Haq, Ihsan Ul and Khan, Muhammad Mohsin and Qureshi, Muhammad Bilal and Ullah, Hayat and Butt, Sharjeel},
	month = dec,
	year = {2021},
	pages = {34},
	file = {Nazir et al_2021_Efficient Pre-Processing and Segmentation for Lung Cancer Detection Using Fused.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SSNQA9LG\\Nazir et al_2021_Efficient Pre-Processing and Segmentation for Lung Cancer Detection Using Fused.pdf:application/pdf},
}

@article{jalaliResBCDUNetDeepLearning2021,
	title = {{ResBCDU}-{Net}: {A} {Deep} {Learning} {Framework} for {Lung} {CT} {Image} {Segmentation}},
	volume = {21},
	issn = {1424-8220},
	shorttitle = {{ResBCDU}-{Net}},
	url = {https://www.mdpi.com/1424-8220/21/1/268},
	doi = {10.3390/s21010268},
	abstract = {Lung CT image segmentation is a key process in many applications such as lung cancer detection. It is considered a challenging problem due to existing similar image densities in the pulmonary structures, different types of scanners, and scanning protocols. Most of the current semi-automatic segmentation methods rely on human factors therefore it might suffer from lack of accuracy. Another shortcoming of these methods is their high false-positive rate. In recent years, several approaches, based on a deep learning framework, have been effectively applied in medical image segmentation. Among existing deep neural networks, the U-Net has provided great success in this field. In this paper, we propose a deep neural network architecture to perform an automatic lung CT image segmentation process. In the proposed method, several extensive preprocessing techniques are applied to raw CT images. Then, ground truths corresponding to these images are extracted via some morphological operations and manual reforms. Finally, all the prepared images with the corresponding ground truth are fed into a modified U-Net in which the encoder is replaced with a pre-trained ResNet-34 network (referred to as Res BCDU-Net). In the architecture, we employ BConvLSTM (Bidirectional Convolutional Long Short-term Memory)as an advanced integrator module instead of simple traditional concatenators. This is to merge the extracted feature maps of the corresponding contracting path into the previous expansion of the up-convolutional layer. Finally, a densely connected convolutional layer is utilized for the contracting path. The results of our extensive experiments on lung CT images (LIDC-IDRI database) confirm the effectiveness of the proposed method where a dice coefficient index of 97.31\% is achieved.},
	language = {en},
	number = {1},
	urldate = {2022-07-28},
	journal = {Sensors},
	author = {Jalali, Yeganeh and Fateh, Mansoor and Rezvani, Mohsen and Abolghasemi, Vahid and Anisi, Mohammad Hossein},
	month = jan,
	year = {2021},
	pages = {268},
	file = {Jalali et al_2021_ResBCDU-Net.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CB7XURDY\\Jalali et al_2021_ResBCDU-Net.pdf:application/pdf},
}

@article{yangUncertaintyGuidedLungNodule2021,
	title = {Uncertainty-{Guided} {Lung} {Nodule} {Segmentation} with {Feature}-{Aware} {Attention}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2110.12372},
	doi = {10.48550/ARXIV.2110.12372},
	abstract = {Since radiologists have different training and clinical experiences, they may provide various segmentation annotations for a lung nodule. Conventional studies choose a single annotation as the learning target by default, but they waste valuable information of consensus or disagreements ingrained in the multiple annotations. This paper proposes an Uncertainty-Guided Segmentation Network (UGS-Net), which learns the rich visual features from the regions that may cause segmentation uncertainty and contributes to a better segmentation result. With an Uncertainty-Aware Module, this network can provide a Multi-Confidence Mask (MCM), pointing out regions with different segmentation uncertainty levels. Moreover, this paper introduces a Feature-Aware Attention Module to enhance the learning of the nodule boundary and density differences. Experimental results show that our method can predict the nodule regions with different uncertainty levels and achieve superior performance in LIDC-IDRI dataset.},
	urldate = {2022-07-28},
	author = {Yang, Han and Shen, Lu and Zhang, Mengke and Wang, Qiuli},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {FOS: Computer and information sciences, Computer Vision and Pattern Recognition (cs.CV), FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV)},
}

@article{liaoEvaluateMalignancyPulmonary2017,
	title = {Evaluate the {Malignancy} of {Pulmonary} {Nodules} {Using} the {3D} {Deep} {Leaky} {Noisy}-or {Network}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1711.08324},
	doi = {10.48550/ARXIV.1711.08324},
	abstract = {Automatic diagnosing lung cancer from Computed Tomography (CT) scans involves two steps: detect all suspicious lesions (pulmonary nodules) and evaluate the whole-lung/pulmonary malignancy. Currently, there are many studies about the first step, but few about the second step. Since the existence of nodule does not definitely indicate cancer, and the morphology of nodule has a complicated relationship with cancer, the diagnosis of lung cancer demands careful investigations on every suspicious nodule and integration of information of all nodules. We propose a 3D deep neural network to solve this problem. The model consists of two modules. The first one is a 3D region proposal network for nodule detection, which outputs all suspicious nodules for a subject. The second one selects the top five nodules based on the detection confidence, evaluates their cancer probabilities and combines them with a leaky noisy-or gate to obtain the probability of lung cancer for the subject. The two modules share the same backbone network, a modified U-net. The over-fitting caused by the shortage of training data is alleviated by training the two modules alternately. The proposed model won the first place in the Data Science Bowl 2017 competition. The code has been made publicly available.},
	urldate = {2022-07-28},
	author = {Liao, Fangzhou and Liang, Ming and Li, Zhe and Hu, Xiaolin and Song, Sen},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Computer Vision and Pattern Recognition (cs.CV)},
}

@article{shaffieComputerAssistedImageProcessing2022,
	title = {Computer-{Assisted} {Image} {Processing} {System} for {Early} {Assessment} of {Lung} {Nodule} {Malignancy}},
	volume = {14},
	issn = {2072-6694},
	url = {https://www.mdpi.com/2072-6694/14/5/1117},
	doi = {10.3390/cancers14051117},
	abstract = {Lung cancer is one of the most dreadful cancers, and its detection in the early stage is very important and challenging. This manuscript proposes a new computer-aided diagnosis system for lung cancer diagnosis from chest computed tomography scans. The proposed system extracts two different kinds of features, namely, appearance features and shape features. For the appearance features, a Histogram of oriented gradients, a Multi-view analytical Local Binary Pattern, and a Markov Gibbs Random Field are developed to give a good description of the lung nodule texture, which is one of the main distinguishing characteristics between benign and malignant nodules. For the shape features, Multi-view Peripheral Sum Curvature Scale Space, Spherical Harmonics Expansion, and a group of some fundamental morphological features are implemented to describe the outer contour complexity of the nodules, which is main factor in lung nodule diagnosis. Each feature is fed into a stacked auto-encoder followed by a soft-max classifier to generate the initial malignancy probability. Finally, all these probabilities are combined together and fed to the last network to give the final diagnosis. The system is validated using 727 nodules which are subset from the Lung Image Database Consortium (LIDC) dataset. The system shows very high performance measures and achieves 92.55\%, 91.70\%, and 93.40\% for the accuracy, sensitivity, and specificity, respectively. This high performance shows the ability of the system to distinguish between the malignant and benign nodules precisely.},
	language = {en},
	number = {5},
	urldate = {2022-07-28},
	journal = {Cancers},
	author = {Shaffie, Ahmed and Soliman, Ahmed and Eledkawy, Amr and van Berkel, Victor and El-Baz, Ayman},
	month = feb,
	year = {2022},
	pages = {1117},
	file = {Shaffie et al_2022_Computer-Assisted Image Processing System for Early Assessment of Lung Nodule.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VQ7PUAZ4\\Shaffie et al_2022_Computer-Assisted Image Processing System for Early Assessment of Lung Nodule.pdf:application/pdf},
}

@article{banuAWEUNetAttentionAwareWeight2021,
	title = {{AWEU}-{Net}: {An} {Attention}-{Aware} {Weight} {Excitation} {U}-{Net} for {Lung} {Nodule} {Segmentation}},
	volume = {11},
	issn = {2076-3417},
	shorttitle = {{AWEU}-{Net}},
	url = {https://www.mdpi.com/2076-3417/11/21/10132},
	doi = {10.3390/app112110132},
	abstract = {Lung cancer is a deadly cancer that causes millions of deaths every year around the world. Accurate lung nodule detection and segmentation in computed tomography (CT) images is a vital step for diagnosing lung cancer early. Most existing systems face several challenges, such as the heterogeneity in CT images and variation in nodule size, shape, and location, which limit their accuracy. In an attempt to handle these challenges, this article proposes a fully automated deep learning framework that consists of lung nodule detection and segmentation models. Our proposed system comprises two cascaded stages: (1) nodule detection based on fine-tuned Faster R-CNN to localize the nodules in CT images, and (2) nodule segmentation based on the U-Net architecture with two effective blocks, namely position attention-aware weight excitation (PAWE) and channel attention-aware weight excitation (CAWE), to enhance the ability to discriminate between nodule and non-nodule feature representations. The experimental results demonstrate that the proposed system yields a Dice score of 89.79\% and 90.35\%, and an intersection over union (IoU) of 82.34\% and 83.21\% on the publicly available LUNA16 and LIDC-IDRI datasets, respectively.},
	language = {en},
	number = {21},
	urldate = {2022-07-28},
	journal = {Applied Sciences},
	author = {Banu, Syeda Furruka and Sarker, Md. Mostafa Kamal and Abdel-Nasser, Mohamed and Puig, Domenec and Raswan, Hatem A.},
	month = oct,
	year = {2021},
	pages = {10132},
	file = {Banu et al_2021_AWEU-Net.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TLYT638M\\Banu et al_2021_AWEU-Net.pdf:application/pdf},
}

@article{causeyHighlyAccurateModel2018,
	title = {Highly accurate model for prediction of lung nodule malignancy with {CT} scans},
	volume = {8},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-018-27569-w},
	doi = {10.1038/s41598-018-27569-w},
	language = {en},
	number = {1},
	urldate = {2022-07-30},
	journal = {Scientific Reports},
	author = {Causey, Jason L. and Zhang, Junyu and Ma, Shiqian and Jiang, Bo and Qualls, Jake A. and Politte, David G. and Prior, Fred and Zhang, Shuzhong and Huang, Xiuzhen},
	month = dec,
	year = {2018},
	pages = {9286},
	file = {Causey et al_2018_Highly accurate model for prediction of lung nodule malignancy with CT scans.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S2DUB5BY\\Causey et al_2018_Highly accurate model for prediction of lung nodule malignancy with CT scans.pdf:application/pdf},
}

@article{bianconiComparativeEvaluationConventional2021,
	title = {Comparative evaluation of conventional and deep learning methods for semi-automated segmentation of pulmonary nodules on {CT}},
	volume = {11},
	issn = {22234292, 22234306},
	url = {https://qims.amegroups.com/article/view/67617/html},
	doi = {10.21037/qims-20-1356},
	number = {7},
	urldate = {2022-07-30},
	journal = {Quantitative Imaging in Medicine and Surgery},
	author = {Bianconi, Francesco and Fravolini, Mario Luca and Pizzoli, Sofia and Palumbo, Isabella and Minestrini, Matteo and Rondini, Maria and Nuvoli, Susanna and Spanu, Angela and Palumbo, Barbara},
	month = jul,
	year = {2021},
	pages = {3286--3305},
	file = {Bianconi et al_2021_Comparative evaluation of conventional and deep learning methods for.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZNHQLPCN\\Bianconi et al_2021_Comparative evaluation of conventional and deep learning methods for.pdf:application/pdf},
}

@article{mehtaLungNoduleClassification2021,
	title = {Lung {Nodule} {Classification} {Using} {Biomarkers}, {Volumetric} {Radiomics}, and {3D} {CNNs}},
	volume = {34},
	issn = {0897-1889, 1618-727X},
	url = {https://link.springer.com/10.1007/s10278-020-00417-y},
	doi = {10.1007/s10278-020-00417-y},
	abstract = {Abstract
            We present a hybrid algorithm to estimate lung nodule malignancy that combines imaging biomarkers from Radiologist’s annotation with image classification of CT scans. Our algorithm employs a 3D Convolutional Neural Network (CNN) as well as a Random Forest in order to combine CT imagery with biomarker annotation and volumetric radiomic features. We analyze and compare the performance of the algorithm using only imagery, only biomarkers, combined imagery + biomarkers, combined imagery + volumetric radiomic features, and finally the combination of imagery + biomarkers + volumetric features in order to classify the suspicion level of nodule malignancy. The National Cancer Institute (NCI) Lung Image Database Consortium (LIDC) IDRI dataset is used to train and evaluate the classification task. We show that the incorporation of semi-supervised learning by means of K-Nearest-Neighbors (KNN) can increase the available training sample size of the LIDC-IDRI, thereby further improving the accuracy of malignancy estimation of most of the models tested although there is no significant improvement with the use of KNN semi-supervised learning if image classification with CNNs and volumetric features is combined with descriptive biomarkers. Unexpectedly, we also show that a model using image biomarkers alone is more accurate than one that combines biomarkers with volumetric radiomics, 3D CNNs, and semi-supervised learning. We discuss the possibility that this result may be influenced by cognitive bias in LIDC-IDRI because malignancy estimates were recorded by the same radiologist panel as biomarkers, as well as future work to incorporate pathology information over a subset of study participants.},
	language = {en},
	number = {3},
	urldate = {2022-07-31},
	journal = {Journal of Digital Imaging},
	author = {Mehta, Kushal and Jain, Arshita and Mangalagiri, Jayalakshmi and Menon, Sumeet and Nguyen, Phuong and Chapman, David R.},
	month = jun,
	year = {2021},
	pages = {647--666},
	file = {Mehta et al_2021_Lung Nodule Classification Using Biomarkers, Volumetric Radiomics, and 3D CNNs.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6E46YEII\\Mehta et al_2021_Lung Nodule Classification Using Biomarkers, Volumetric Radiomics, and 3D CNNs.pdf:application/pdf},
}

@inproceedings{agarwalLungCancerDetection2021,
	address = {Coimbatre, India},
	title = {Lung {Cancer} {Detection} and {Classification} {Based} on {Alexnet} {CNN}},
	isbn = {978-1-66543-587-1},
	url = {https://ieeexplore.ieee.org/document/9489033/},
	doi = {10.1109/ICCES51350.2021.9489033},
	urldate = {2022-07-31},
	booktitle = {2021 6th {International} {Conference} on {Communication} and {Electronics} {Systems} ({ICCES})},
	publisher = {IEEE},
	author = {Agarwal, Aman and Patni, Kritik and D, Rajeswari},
	month = jul,
	year = {2021},
	pages = {1390--1397},
	file = {Agarwal et al. - 2021 - Lung Cancer Detection and Classification Based on .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\68W4GE68\\Agarwal et al. - 2021 - Lung Cancer Detection and Classification Based on .pdf:application/pdf},
}

@inproceedings{abdulAutomaticLungCancer2020,
	address = {Liverpool, United Kingdom},
	title = {An {Automatic} {Lung} {Cancer} {Detection} and {Classification} ({ALCDC}) {System} {Using} {Convolutional} {Neural} {Network}},
	isbn = {978-1-66542-238-3},
	url = {https://ieeexplore.ieee.org/document/9450778/},
	doi = {10.1109/DeSE51703.2020.9450778},
	urldate = {2022-07-31},
	booktitle = {2020 13th {International} {Conference} on {Developments} in {eSystems} {Engineering} ({DeSE})},
	publisher = {IEEE},
	author = {Abdul, Wadood},
	month = dec,
	year = {2020},
	pages = {443--446},
	file = {Abdul - 2020 - An Automatic Lung Cancer Detection and Classificat.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6LD8YZDF\\Abdul - 2020 - An Automatic Lung Cancer Detection and Classificat.pdf:application/pdf},
}

@article{monkamEnsembleLearningMultipleView2019,
	title = {Ensemble {Learning} of {Multiple}-{View} {3D}-{CNNs} {Model} for {Micro}-{Nodules} {Identification} in {CT} {Images}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8586874/},
	doi = {10.1109/ACCESS.2018.2889350},
	abstract = {Numerous automatic systems of pulmonary nodules detection have been proposed, but very few of them have been consecrated to micro-nodules (diameter {\textless} 3 mm) even though they are regarded as the earliest manifestations of lung cancer. Moreover, most available systems present high false positive rate resulting from their incapability of discriminating between micro-nodules and non-nodules. Thus, this paper proposes a system to differentiate between the micro-nodules and non-nodules in computed tomography (CT) images by an ensemble learning of multiple-view 3-D convolutional neural networks (3D-CNNs). A total of 34 494 volumetric image samples, including 13 179 micro-nodules and 21 315 non-nodules, are acquired from the 1010 CT scans of the LIDC/IDRI database. The pulmonary nodule candidates are cropped with ﬁve different sizes, including 20 × 20 × 3, 16 × 16 × 3, 12 × 12 × 3, 8 × 8 × 3, and 4 × 4 × 3. Then, ﬁve distinct 3D-CNN models are built and implemented on one size of the nodule candidates. An extreme learning machine (ELM) network is utilized to integrate the ﬁve 3D-CNN outputs, yielding the ﬁnal classiﬁcation results. The performance of the proposed system is assessed in terms of accuracy, area under the curve (AUC), F-score, and sensitivity. It is found that the proposed system yields an accuracy, AUC, F-score, and sensitivity of 97.35\%, 0.98, 96.42\%, and 96.57\%, respectively. These performances are highly superior to those of 2D-CNNs, single 3D-CNN model, as well as those by the state-of-the-art methods implemented on the same dataset. For the ensemble method, ELM achieves better performance than the majority voting, averaging, AND operator, and autoencoder. The results demonstrate that developing an automatic system for discriminating between micro-nodules and non-nodules in CT images is feasible, which extends lung cancer studies to micro-nodules. The combination of multiple-view 3D-CNNs and ensemble learning contribute to excellent identiﬁcation performance, and this strategy may help develop other reliable clinical-decision support systems.},
	language = {en},
	urldate = {2022-07-31},
	journal = {IEEE Access},
	author = {Monkam, Patrice and Qi, Shouliang and Xu, Mingjie and Li, Haoming and Han, Fangfang and Teng, Yueyang and Qian, Wei},
	year = {2019},
	pages = {5564--5576},
	file = {Monkam et al. - 2019 - Ensemble Learning of Multiple-View 3D-CNNs Model f.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FNVXJE3T\\Monkam et al. - 2019 - Ensemble Learning of Multiple-View 3D-CNNs Model f.pdf:application/pdf},
}

@inproceedings{ghouriRadiomicFeaturesExtraction2020,
	address = {Bahawalpur, Pakistan},
	title = {Radiomic {Features} {Extraction} {Based} on {Genetic} {Algorithm}},
	isbn = {978-1-72819-893-4},
	url = {https://ieeexplore.ieee.org/document/9318183/},
	doi = {10.1109/INMIC50486.2020.9318183},
	abstract = {Radiomics is a non-invasive technique that enhances imaging analysis through extracting an enormous amount of molecular biological features. These features are then used to customize personal medical treatment and precision medicine. The proposed method utilized a publicly available dataset of the Lung Image Database Consortium image collection (LIDC-IDRI) for experimentation and validation. The Convolutional Neural Network (CNN) with Support Vector Machine (SVM) is used to extract and classify deep radiomic features. Two optimization techniques (GA and NSGA-II) are applied on these features to overcome the imbalanced and nonsorted data problem. The CNN is used with SVM (no optimization) showed the accuracy of 80\% and 82\% for radiomic and radiomic+deep features. The CNN and SVM used with GA showed an accuracy of 82.5\% and 83\% and used with NSGA-II showed an accuracy of 82\% and 83.5\% for radiomic and radiomic+deep features. Thus, the results showed that the inclusion of deep features increased overall performance by a small amount of GA optimization which led to better results.},
	language = {en},
	urldate = {2022-07-31},
	booktitle = {2020 {IEEE} 23rd {International} {Multitopic} {Conference} ({INMIC})},
	publisher = {IEEE},
	author = {Ghouri, Muhammad Hamza and Khan, Khan Bahadar},
	month = nov,
	year = {2020},
	pages = {1--6},
	file = {Ghouri and Khan - 2020 - Radiomic Features Extraction Based on Genetic Algo.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6BILSIEG\\Ghouri and Khan - 2020 - Radiomic Features Extraction Based on Genetic Algo.pdf:application/pdf},
}

@inproceedings{yangRelationalLearningMultiple2020,
	address = {Iowa City, IA, USA},
	title = {Relational {Learning} {Between} {Multiple} {Pulmonary} {Nodules} via {Deep} {Set} {Attention} {Transformers}},
	isbn = {978-1-5386-9330-8},
	url = {https://ieeexplore.ieee.org/document/9098722/},
	doi = {10.1109/ISBI45749.2020.9098722},
	abstract = {Diagnosis and treatment of multiple pulmonary nodules are clinically important but challenging. Prior studies on nodule characterization use solitary-nodule approaches on multiple nodular patients, which ignores the relations between nodules. In this study, we propose a multiple instance learning (MIL) approach and empirically prove the beneﬁt to learn the relations between multiple nodules. By treating the multiple nodules from a same patient as a whole, critical relational information between solitary-nodule voxels is extracted. To our knowledge, it is the ﬁrst study to learn the relations between multiple pulmonary nodules. Inspired by recent advances in natural language processing (NLP) domain, we introduce a self-attention transformer equipped with 3D CNN, named NoduleSAT, to replace typical pooling-based aggregation in multiple instance learning. Extensive experiments on lung nodule false positive reduction on LUNA16 database, and malignancy classiﬁcation on LIDC-IDRI database, validate the effectiveness of the proposed method.},
	language = {en},
	urldate = {2022-07-31},
	booktitle = {2020 {IEEE} 17th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	publisher = {IEEE},
	author = {Yang, Jiancheng and Deng, Haoran and Huang, Xiaoyang and Ni, Bingbing and Xu, Yi},
	month = apr,
	year = {2020},
	pages = {1875--1878},
	file = {Yang et al. - 2020 - Relational Learning Between Multiple Pulmonary Nod.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RYMPAQAH\\Yang et al. - 2020 - Relational Learning Between Multiple Pulmonary Nod.pdf:application/pdf},
}

@inproceedings{zhuDeepLungDeep3D2018,
	address = {Lake Tahoe, NV},
	title = {{DeepLung}: {Deep} {3D} {Dual} {Path} {Nets} for {Automated} {Pulmonary} {Nodule} {Detection} and {Classification}},
	isbn = {978-1-5386-4886-5},
	shorttitle = {{DeepLung}},
	url = {https://ieeexplore.ieee.org/document/8354183/},
	doi = {10.1109/WACV.2018.00079},
	language = {en},
	urldate = {2022-07-31},
	booktitle = {2018 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Zhu, Wentao and Liu, Chaochun and Fan, Wei and Xie, Xiaohui},
	month = mar,
	year = {2018},
	pages = {673--681},
	file = {Zhu et al. - 2018 - DeepLung Deep 3D Dual Path Nets for Automated Pul.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T5B9NRV8\\Zhu et al. - 2018 - DeepLung Deep 3D Dual Path Nets for Automated Pul.pdf:application/pdf},
}

@article{zhangEnsembleLearnersMultiple2019,
	title = {Ensemble {Learners} of {Multiple} {Deep} {CNNs} for {Pulmonary} {Nodules} {Classification} {Using} {CT} {Images}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8790699/},
	doi = {10.1109/ACCESS.2019.2933670},
	abstract = {Various deep convolutional neural networks (CNNs) have been used to distinguish between benign and malignant pulmonary nodules using CT images. However, single learner usually presents unsatisﬁed performance due to limited hypothesis space, or falling into local minima, or wrong selection of hypothesis space. To tackle these issues, we propose to build ensemble learners through fusing multiple deep CNN learners for pulmonary nodules classiﬁcation. CT image patches of 743 nodules are extracted from LIDC-IDRI database and utilized. First, eight deep CNN learners with different architectures are trained and evaluated by 10-fold cross-validation. Each nodule has eight predictions from the eight primary learners. Second, we fuse these eight predictions by the strategies of majority voting (VOT), averaging (AVE), or machine learning. Speciﬁcally, different machine learning algorithms including K-NearestNeighbor (KNN), Support Vector Machines (SVM), Naive Bayes (NB), Decision Trees (DT), Multi-layer Perceptron (MLP), Random Forests (RF), Gradient Boosting Regression Trees (GBRT) and Adaptive Boosting (AdaBoost) are implemented. Moreover, the correlation coefﬁcients between the predictions of 10 ensemble learners are calculated, and the hierarchical clustering dendrogram is drawn. It is found that the ensemble learners achieve higher prediction accuracy (84.0\% vs 81.7\%) than single CNN learner. The overlap ratio among the 10 ensemble learners is much higher than that of the 8 primary learners (62.9\% vs 33.2\%). In addition, it is shown that ensemble learners are roughly divided into three categories: the ﬁrst (SVM, MLP, GBRT and RF) achieves the best performance; the second (VOT and AVE) is better than the third (AdaBoost, DT, NB and KNN). VOT and AVE yield higher recall than the machine learning algorithms. These results indicate that ensemble learners based on multiple CNN learners can achieve better performances for pulmonary nodules classiﬁcation using CT images and that preferred fusion strategies include SVM, MLP, GBRT and RF.},
	language = {en},
	urldate = {2022-07-31},
	journal = {IEEE Access},
	author = {Zhang, Baihua and Qi, Shouliang and Monkam, Patrice and Li, Chen and Yang, Fan and Yao, Yu-Dong and Qian, Wei},
	year = {2019},
	pages = {110358--110371},
	file = {Zhang et al. - 2019 - Ensemble Learners of Multiple Deep CNNs for Pulmon.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6QD8M6A7\\Zhang et al. - 2019 - Ensemble Learners of Multiple Deep CNNs for Pulmon.pdf:application/pdf},
}

@inproceedings{ramirezLocalizingPulmonaryLesions2019,
	address = {Timisoara, Romania},
	title = {Localizing {Pulmonary} {Lesions} {Using} {Fuzzy} {Deep} {Learning}},
	isbn = {978-1-72815-724-5},
	url = {https://ieeexplore.ieee.org/document/9049881/},
	doi = {10.1109/SYNASC49474.2019.00048},
	abstract = {The usage of medical images is part of the clinical daily in several healthcare centers around the world. Particularly, Computer Tomography (CT) images are an important key in the early detection of suspicious lung lesions. The CT image exploration allows the detection of lung lesions before any invasive procedure (e.g. bronchoscopy, biopsy). The effective localization of lesions is performed using different image processing and computer vision techniques. Lately, the usage of deep learning models into medical imaging from detection to prediction shown that is a powerful tool for Computeraided software. In this paper, we present an approach to localize pulmonary lung lesion using fuzzy deep learning. Our approach uses a simple convolutional neural network based using the LIDC-IDRI dataset. Each image is divided into patches associated a probability vector (fuzzy) according their belonging to anatomical structures on a CT. We showcase our approach as part of a full CAD system to exploration, planning, guiding and detection of pulmonary lesions.},
	language = {en},
	urldate = {2022-07-31},
	booktitle = {2019 21st {International} {Symposium} on {Symbolic} and {Numeric} {Algorithms} for {Scientific} {Computing} ({SYNASC})},
	publisher = {IEEE},
	author = {Ramirez, Esmitt and Sanchez, Carles and Gil, Debora},
	month = sep,
	year = {2019},
	pages = {290--294},
	file = {Ramirez et al. - 2019 - Localizing Pulmonary Lesions Using Fuzzy Deep Lear.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8VGYPG7X\\Ramirez et al. - 2019 - Localizing Pulmonary Lesions Using Fuzzy Deep Lear.pdf:application/pdf},
}

@inproceedings{abdelrahmanPulmonaryNoduleMalignancy2019,
	address = {Dallas, TX, USA},
	title = {Pulmonary {Nodule} and {Malignancy} {Classification} {Employing} {Triplanar} {Views} and {Convolutional} {Neural} {Network}},
	isbn = {978-1-72812-788-0},
	url = {https://ieeexplore.ieee.org/document/8885090/},
	doi = {10.1109/MWSCAS.2019.8885090},
	abstract = {Appearance of pulmonary lesions in a Computed Tomography (CT) scan of lung has the potential to become cancerous. Therefore, classiﬁcation of lesion at early stage is a powerful tool to increase the chance of survival of a patient. In this paper a novel system for lesion classiﬁcation as either nodule or non-nodule followed by malignancy classiﬁcation (benign or malignant) is presented. Three dimensional (3D) lung tissue is constructed from the two dimensional (2D) slices of the patient CT scan. Lung is segmented by a thresholding based method, and lesions are extracted with triplanar views, axial, coronal, and sagittal. For classiﬁcation, two Convolutional Neural Networks (CNNs) are cascaded, where the ﬁrst network for lesion classiﬁcation and the second for malignancy classiﬁcation with concatenated triplaner views as an input in both stages. Experimental results performed on 1010 patients from Lung Image Database Consortium (LIDC) reveal that, the proposed approach achieved the best performance in lesion and malignancy classiﬁcation with an accuracy of 82.58\% and 82.62\% respectively compared with existing methods.},
	language = {en},
	urldate = {2022-07-31},
	booktitle = {2019 {IEEE} 62nd {International} {Midwest} {Symposium} on {Circuits} and {Systems} ({MWSCAS})},
	publisher = {IEEE},
	author = {Abdelrahman, Shimaa A. and Abdelwahab, Moataz M. and Sayed, Mohammed S.},
	month = aug,
	year = {2019},
	pages = {594--597},
	file = {Abdelrahman et al. - 2019 - Pulmonary Nodule and Malignancy Classification Emp.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H2QEXNLS\\Abdelrahman et al. - 2019 - Pulmonary Nodule and Malignancy Classification Emp.pdf:application/pdf},
}

@article{yePulmonaryNoduleDetection2020,
	title = {Pulmonary {Nodule} {Detection} {Using} {V}-{Net} and {High}-{Level} {Descriptor} {Based} {SVM} {Classifier}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9204700/},
	doi = {10.1109/ACCESS.2020.3026168},
	abstract = {Early detection of the pulmonary nodule is critical to increase the ﬁve-year survival rate of lung cancer. Many computer-aided diagnosis (CAD) systems have been proposed for nodule detection to assist radiologists in diagnosis. Along this direction, this paper proposes a novel automated pulmonary nodule detection model using the modiﬁed V-Nets and a high-level descriptor based support vector machine (SVM) classiﬁer. The former is for nodule candidate detection and the latter is for false positive (FP) reduction. A hard mining scheme for retraining is devised to improve the FP reduction performance. The proposed SVM classiﬁer, which employs more critical features of CT images, performs superior in FP reduction than other SVM based classiﬁers and CNN classiﬁers. Experimental results using the LIDC-IDRI dataset are presented to demonstrate the effectiveness of the proposed CAD model.},
	language = {en},
	urldate = {2022-07-31},
	journal = {IEEE Access},
	author = {Ye, Yuyun and Tian, Miao and Liu, Qiyu and Tai, Heng-Ming},
	year = {2020},
	pages = {176033--176041},
	file = {Ye et al. - 2020 - Pulmonary Nodule Detection Using V-Net and High-Le.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z4TJMGTP\\Ye et al. - 2020 - Pulmonary Nodule Detection Using V-Net and High-Le.pdf:application/pdf},
}

@article{huangSelfSupervisedTransferLearning2022,
	title = {Self-{Supervised} {Transfer} {Learning} {Based} on {Domain} {Adaptation} for {Benign}-{Malignant} {Lung} {Nodule} {Classification} on {Thoracic} {CT}},
	issn = {2168-2194, 2168-2208},
	url = {https://ieeexplore.ieee.org/document/9767702/},
	doi = {10.1109/JBHI.2022.3171851},
	abstract = {The spatial heterogeneity is an important indicator of the malignancy of lung nodules in lung cancer diagnosis. Compared with 2D nodule CT images, the 3D volumes with entire nodule objects hold richer discriminative information. However, for deep learning methods driven by massive data, effectively capturing the 3D discriminative features of nodules in limited labeled samples is a challenging task. Different from previous models that proposed transfer learning models in a 2D pattern or learning from scratch 3D models, we develop a self-supervised transfer learning based on domain adaptation (SSTL-DA) 3D CNN framework for benign-malignant lung nodule classiﬁcation. At ﬁrst, a data pre-processing strategy termed adaptive slice selection (ASS) is developed to eliminate the redundant noise of the input samples with lung nodules. Then, the self-supervised learning network is constructed to learn robust image representations from CT images. Finally, a transfer learning method based on domain adaptation is designed to obtain discriminant features for classiﬁcation. The proposed SSTL-DA method has been assessed on the LIDC-IDRI benchmark dataset, and it obtains an accuracy of 91.07\% and an AUC of 95.84\%. These results demonstrate that the SSTL-DA model achieves quite a competitive classiﬁcation performance compared with some state-of-the-art approaches.},
	language = {en},
	urldate = {2022-07-31},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Huang, Hong and Wu, Ruoyu and Li, Yuan and Chao, Peng},
	year = {2022},
	pages = {1--1},
	file = {Huang et al. - 2022 - Self-Supervised Transfer Learning Based on Domain .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ULCKGNK3\\Huang et al. - 2022 - Self-Supervised Transfer Learning Based on Domain .pdf:application/pdf},
}

@inproceedings{sethiTransferLearningDeep2020,
	address = {RUPNAGAR, India},
	title = {Transfer {Learning} by {Deep} {Tuning} of {Pre}-trained {Networks} for {Pulmonary} {Nodule} {Detection}},
	isbn = {978-1-72818-524-8},
	url = {https://ieeexplore.ieee.org/document/9342686/},
	doi = {10.1109/ICIIS51140.2020.9342686},
	urldate = {2022-07-31},
	booktitle = {2020 {IEEE} 15th {International} {Conference} on {Industrial} and {Information} {Systems} ({ICIIS})},
	publisher = {IEEE},
	author = {Sethi, Dhaarna and Arora, Kriti and Susan, Seba},
	month = nov,
	year = {2020},
	pages = {168--173},
	file = {Sethi et al. - 2020 - Transfer Learning by Deep Tuning of Pre-trained Ne.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\486BEKNS\\Sethi et al. - 2020 - Transfer Learning by Deep Tuning of Pre-trained Ne.pdf:application/pdf},
}

@article{sahuLightweightMultiSectionCNN2019,
	title = {A {Lightweight} {Multi}-{Section} {CNN} for {Lung} {Nodule} {Classification} and {Malignancy} {Estimation}},
	volume = {23},
	issn = {2168-2194, 2168-2208},
	url = {https://ieeexplore.ieee.org/document/8525322/},
	doi = {10.1109/JBHI.2018.2879834},
	abstract = {The size and shape of a nodule are the essential indicators of malignancy in lung cancer diagnosis. However, effectively capturing the nodule’s structural information from CT scans in a computer-aided system is a challenging task. Unlike previous models that proposed computationally intensive deep ensemble models or three-dimensional CNN models, we propose a lightweight, multiple view sampling based multi-section CNN architecture. The model obtains a nodule’s cross sections from multiple view angles and encodes the nodule’s volumetric information into a compact representation by aggregating information from its different cross sections via a view pooling layer. The compact feature is subsequently used for the task of nodule classiﬁcation. The method does not require the nodule’s spatial annotation and works directly on the cross sections generated from volume enclosing the nodule. We evaluated the proposed method on lung image database consortium (LIDC) and image database resource initiative (IDRI) dataset. It achieved the state-of-the-art performance with a mean 93.18\% classiﬁcation accuracy. The architecture could also be used to select the representative cross sections determining the nodule’s malignancy that facilitates in the interpretation of results. Because of being lightweight, the model could be ported to mobile devices, which brings the power of artiﬁcial intelligence (AI) driven application directly into the practitioner’s hand.},
	language = {en},
	number = {3},
	urldate = {2022-07-31},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Sahu, Pranjal and Yu, Dantong and Dasari, Mallesham and Hou, Fei and Qin, Hong},
	month = may,
	year = {2019},
	pages = {960--968},
	file = {Sahu et al. - 2019 - A Lightweight Multi-Section CNN for Lung Nodule Cl.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I9TMPDLB\\Sahu et al. - 2019 - A Lightweight Multi-Section CNN for Lung Nodule Cl.pdf:application/pdf},
}

@article{masoodAutomatedDecisionSupport2020,
	title = {Automated {Decision} {Support} {System} for {Lung} {Cancer} {Detection} and {Classification} via {Enhanced} {RFCN} {With} {Multilayer} {Fusion} {RPN}},
	volume = {16},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/9006938/},
	doi = {10.1109/TII.2020.2972918},
	abstract = {Detection of lung cancer at early stages is critical, in most of the cases radiologists read computed tomography (CT) images to prescribe follow-up treatment. The conventional method for detecting nodule presence in CT images is tedious. In this article, we propose an enhanced multidimensional region-based fully convolutional network (mRFCN) based automated decision support system for lung nodule detection and classiﬁcation. The mRFCN is used as an image classiﬁer backbone for feature extraction along with the novel multilayer fusion region proposal network (mLRPN) with position-sensitive score maps being explored. We applied a median intensity projection to leverage three-dimensional information from CT scans and introduced deconvolutional layer to adopt proposed mLRPN in our architecture to automatically select the potential region of interest. Our system has been trained and evaluated using LIDC dataset, and the experimental results showed promising detection performance in comparison to the state-of-the-art nodule detection/classiﬁcation methods, achieving a sensitivity of 98.1\% and classiﬁcation accuracy of 97.91\%.},
	language = {en},
	number = {12},
	urldate = {2022-07-31},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Masood, Anum and Sheng, Bin and Yang, Po and Li, Ping and Li, Huating and Kim, Jinman and Feng, David Dagan},
	month = dec,
	year = {2020},
	pages = {7791--7801},
	file = {Masood et al. - 2020 - Automated Decision Support System for Lung Cancer .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZBHM87KB\\Masood et al. - 2020 - Automated Decision Support System for Lung Cancer .pdf:application/pdf},
}

@inproceedings{bhatConvolutionalNeuralNetwork2020,
	address = {Coimbatore},
	title = {Convolutional {Neural} {Network} approach for the {Classification} and {Recognition} of {Lung} {Nodules}},
	isbn = {978-1-72816-387-1},
	url = {https://ieeexplore.ieee.org/document/9297626/},
	doi = {10.1109/ICECA49313.2020.9297626},
	abstract = {Lung cancer is remaining as a major cause of cancer death, where it makes up almost 25\% of cancer deaths worldwide. As per the report of World Health Organization (WHO), around 18 million people die due to lung cancer every year. Though there are many standard detecting techniques available, most of the patients die in a year of diagnosis. Hence, it is essential to find an alternative approach for the early recognition of lung cancer, which altogether improves the odds of endurance. This paper addresses the classification and recognition task to detect lung tumor in the early stages of Computerized Tomography (CT) scans for lungs. Lung Image Database Consortium and Infectious Disease Research Institute (LIDC/IDRI) lung image dataset is used for this purpose. Convolutional-Neural-Network (CNN) based deep learning (DL) approach is proposed for the recognition and classification from the CT scan images and the results are compared with the classical machine learning algorithms in the literature with respect to standard evaluation metrics. It is additionally observed that, the Deep CNN classifier has performed better than all the other three traditional classification algorithm in the literature and viewed as the viable classifier.},
	language = {en},
	urldate = {2022-07-31},
	booktitle = {2020 4th {International} {Conference} on {Electronics}, {Communication} and {Aerospace} {Technology} ({ICECA})},
	publisher = {IEEE},
	author = {Bhat, Sachin and Shashikala, R and Kumar, Sandesh and Gururaj, K},
	month = nov,
	year = {2020},
	pages = {1310--1314},
	file = {Bhat et al. - 2020 - Convolutional Neural Network approach for the Clas.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YV7ZV97J\\Bhat et al. - 2020 - Convolutional Neural Network approach for the Clas.pdf:application/pdf},
}

@inproceedings{deyDiagnosticClassificationLung2018,
	address = {Washington, DC},
	title = {Diagnostic classification of lung nodules using {3D} neural networks},
	isbn = {978-1-5386-3636-7},
	url = {https://ieeexplore.ieee.org/document/8363687/},
	doi = {10.1109/ISBI.2018.8363687},
	abstract = {Lung cancer is the leading cause of cancer-related death worldwide. Early diagnosis of pulmonary nodules in Computed Tomography (CT) chest scans provides an opportunity for designing effective treatment and making ﬁnancial and care plans. In this paper, we consider the problem of diagnostic classiﬁcation between benign and malignant lung nodules in CT images, which aims to learn a direct mapping from 3D images to class labels. To achieve this goal, four two-pathway Convolutional Neural Networks (CNN) are proposed, including a basic 3D CNN, a novel multi-output network, a 3D DenseNet, and an augmented 3D DenseNet with multi-outputs. These four networks are evaluated on the public LIDC-IDRI dataset and outperform most existing methods. In particular, the 3D multi-output DenseNet (MoDenseNet) achieves the state-of-the-art classiﬁcation accuracy on the task of end-to-end lung nodule diagnosis. In addition, the networks pretrained on the LIDC-IDRI dataset can be further extended to handle smaller datasets using transfer learning. This is demonstrated on our dataset with encouraging prediction accuracy in lung nodule classiﬁcation.},
	language = {en},
	urldate = {2022-07-31},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	publisher = {IEEE},
	author = {Dey, Raunak and Lu, Zhongjie and Hong, Yi},
	month = apr,
	year = {2018},
	pages = {774--778},
	file = {Dey et al. - 2018 - Diagnostic classification of lung nodules using 3D.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PZWLLZJU\\Dey et al. - 2018 - Diagnostic classification of lung nodules using 3D.pdf:application/pdf},
}

@article{aliEfficientLungNodule2020,
	title = {Efficient {Lung} {Nodule} {Classification} {Using} {Transferable} {Texture} {Convolutional} {Neural} {Network}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9204580/},
	doi = {10.1109/ACCESS.2020.3026080},
	abstract = {Lung nodules are vital indicators for the presence of lung cancer. An early detection enhances the survival rate of the patient by starting treatment at the right time. The detection and classiﬁcation of malignancy in Computed Tomography (CT) images is a very time-consuming and difﬁcult task for radiologists which lead the researchers to develop algorithms for Computer-Aided Diagnosis (CAD) systems to mitigate this burden. The performance of CAD systems is continuously improving by using various deep learning techniques for screening of lung cancer. In this paper, we proposed transferable texture Convolutional Neural Networks (CNN) to improve the classiﬁcation performance of pulmonary nodules in CT scans. An Energy Layer (EL) is incorporated in our scheme, which extracts texture features from the convolutional layer. The inclusion of EL reduces the number of learnable parameters of the network, which further reduces the memory requirements and computational complexity. The proposed model has only three convolutional layers and one EL, instead of pooling layer. Overall proposed CNN architecture comprises of nine layers for automatic feature extraction and classiﬁcation of pulmonary nodule candidates as malignant or benign. Furthermore, the pre-trained model of proposed CNN is also used to handle the smaller dataset classiﬁcation problem by using transfer learning. This work has been evaluated on publicly available LIDC-IDRI and the LUNGx Challenge database through different evaluation matrices, such as; the accuracy, speciﬁcity, error rate and AUC. The proposed model is trained by six-fold cross-validation and achieved an accuracy score of 96.69\% ± 0.72\% with only 3.30\% ± 0.72\% error rate. Whereas, the measured AUC and recall is 99.11\%±0.45\% and 97.19\%±0.57\%, respectively. Moreover, we also tested our proposed technique on the MNIST dataset and achieved state-of-the-art results in terms of accuracy and error rate.},
	language = {en},
	urldate = {2022-07-31},
	journal = {IEEE Access},
	author = {Ali, Imdad and Muzammil, Muhammad and Haq, Ihsan Ul and Khaliq, Amir A. and Abdullah, Suheel},
	year = {2020},
	pages = {175859--175870},
	file = {Ali et al. - 2020 - Efficient Lung Nodule Classification Using Transfe.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S2GAHKKQ\\Ali et al. - 2020 - Efficient Lung Nodule Classification Using Transfe.pdf:application/pdf},
}

@inproceedings{fangNovelComputerAidedLung2018,
	address = {Beijing},
	title = {A {Novel} {Computer}-{Aided} {Lung} {Cancer} {Detection} {Method} {Based} on {Transfer} {Learning} from {GoogLeNet} and {Median} {Intensity} {Projections}},
	isbn = {978-1-5386-7437-6},
	url = {https://ieeexplore.ieee.org/document/8542189/},
	doi = {10.1109/CCET.2018.8542189},
	abstract = {In this research, a fast, accurate, and stable system of lung cancer detection based on novel deep learning techniques is proposed. A convolutional neural network (CNN) structure akin to that of GoogLeNet was built using a transfer learning approach. In contrast to previous studies, Median Intensity Projection (MIP) was employed to include multi-view features of three-dimensional computed tomography (CT) scans. The system was evaluated on the LIDC-IDRI public dataset of lung nodule images and 100-fold data augmentation was performed to ensure training efficiency. The trained system produced 81\% accuracy, 84\% sensitivity, and 78\% specificity after 300 epochs, better than other available programs. In addition, a t-based confidence interval for the population mean of the validation accuracies verified that the proposed system would produce consistent results for multiple trials. Subsequently, a controlled variable experiment was performed to elucidate the net effects of two core factors of the system - fine-tuned GoogLeNet and MIPs - on its detection accuracy. Four treatment groups were set by training and testing fine-tuned GoogLeNet and Alexnet on MIPs and common 2D CT scans, respectively. It was noteworthy that MIPs improved the network’s accuracy by 12.3\%, and GoogLeNet outperformed Alexnet by 2\%. Lastly, remote access to the GPU-based system was enabled through a web server, which allows long-distance management of the system and its future transition into a practical tool.},
	language = {en},
	urldate = {2022-07-31},
	booktitle = {2018 {IEEE} {International} {Conference} on {Computer} and {Communication} {Engineering} {Technology} ({CCET})},
	publisher = {IEEE},
	author = {Fang, Tiantian},
	month = aug,
	year = {2018},
	pages = {286--290},
	file = {Fang - 2018 - A Novel Computer-Aided Lung Cancer Detection Metho.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LPHLFPVP\\Fang - 2018 - A Novel Computer-Aided Lung Cancer Detection Metho.pdf:application/pdf},
}

@inproceedings{yanImprovedMaskRCNN2019,
	address = {Qingdao, China},
	title = {Improved {Mask} {R}-{CNN} for {Lung} {Nodule} {Segmentation}},
	isbn = {978-1-72813-918-0},
	url = {https://ieeexplore.ieee.org/document/8964832/},
	doi = {10.1109/ITME.2019.00041},
	abstract = {With more and more people suffer from lung cancer, computer-aided diagnosis plays a more and more important role in lung cancer diagnosis. CNN has achieved state-of-the-art performance in image processing, and Mask RCNN outperforms most other methods on instance segmentation. However, the target is extraordinarily small, and the background is very large in images, which results in a large number of negative examples and most of them are easy negatives. They will contribute a large part of the loss value in smooth loss function. The class imbalance problem leads to inefficient training, which makes model degenerated. In this paper, we propose a method based on Mask R-CNN to segment lung nodules. Due to the non-uniformity of CT values, we use the Laplacian operator to do feature dimensionality reduction for filtering out part of the noise. In our model, the novel function Focal Loss is used to suppress well-classified examples. The model is tested on LIDC-IDRI dataset and the results showed that the average precision of lung nodules reaches 78\%. Compared with the smooth loss function in Mask R-CNN it improves by 7\%.},
	language = {en},
	urldate = {2022-07-31},
	booktitle = {2019 10th {International} {Conference} on {Information} {Technology} in {Medicine} and {Education} ({ITME})},
	publisher = {IEEE},
	author = {Yan, Huanlan and Lu, Huijuan and Ye, Minchao and Yan, Ke and Xu, Yige and Jin, Qun},
	month = aug,
	year = {2019},
	pages = {137--141},
	file = {Yan et al. - 2019 - Improved Mask R-CNN for Lung Nodule Segmentation.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RSIREJDC\\Yan et al. - 2019 - Improved Mask R-CNN for Lung Nodule Segmentation.pdf:application/pdf},
}

@inproceedings{sreekumarMalignantLungNodule2020,
	address = {Chennai, India},
	title = {Malignant {Lung} {Nodule} {Detection} using {Deep} {Learning}},
	isbn = {978-1-72814-988-2},
	url = {https://ieeexplore.ieee.org/document/9182258/},
	doi = {10.1109/ICCSP48568.2020.9182258},
	abstract = {Lung Carcinoma, commonly known as Lung Cancer is an infectious lung tumour caused by uncontrollable tissue growth in the lungs. Presented here is an approach to detect malignant pulmonary nodules from CT scans using Deep Learning. A preprocessing pipeline was used to mask out the lung regions from the scans. The features were then extracted using a 3D CNN model based on the C3D network architecture. The LIDC-IDRI is the primary dataset used along with a few resources from the LUNA16 grand challenge for the reduction of false-positives. The end product is a model that predicts the coordinates of malignant pulmonary nodules and demarcates the corresponding areas from the CT scans. The final model achieved a sensitivity of 86 percent for detecting malignant Lung Nodules and predicting its malignancy scores.},
	language = {en},
	urldate = {2022-07-31},
	booktitle = {2020 {International} {Conference} on {Communication} and {Signal} {Processing} ({ICCSP})},
	publisher = {IEEE},
	author = {Sreekumar, Amrit and Nair, Karthika Rajan and Sudheer, Sneha and Ganesh Nayar, H and Nair, Jyothisha J},
	month = jul,
	year = {2020},
	pages = {0209--0212},
	file = {Sreekumar et al. - 2020 - Malignant Lung Nodule Detection using Deep Learnin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EB96KP62\\Sreekumar et al. - 2020 - Malignant Lung Nodule Detection using Deep Learnin.pdf:application/pdf},
}

@article{jiangMultipleResolutionResidually2019,
	title = {Multiple {Resolution} {Residually} {Connected} {Feature} {Streams} for {Automatic} {Lung} {Tumor} {Segmentation} {From} {CT} {Images}},
	volume = {38},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8417454/},
	doi = {10.1109/TMI.2018.2857800},
	abstract = {Volumetric lung tumor segmentation and accurate longitudinal tracking of tumor volume changes from computed tomography images are essential for monitoring tumor response to therapy. Hence, we developed two multiple resolution residually connected network (MRRN) formulations called incremental-MRRN and dense-MRRN. Our networks simultaneously combine features across multiple image resolution and feature levels through residual connections to detect and segment the lung tumors. We evaluated our method on a total of 1210 non-small cell (NSCLC) lung tumors and nodules from three data sets consisting of 377 tumors from the open-source Cancer Imaging Archive (TCIA), 304 advanced stage NSCLC treated with anti- PD-1 checkpoint immunotherapy from internal institution MSKCC data set, and 529 lung nodules from the Lung Image Database Consortium (LIDC). The algorithm was trained using 377 tumors from the TCIA data set and validated on the MSKCC and tested on LIDC data sets. The segmentation accuracy compared to expert delineations was evaluated by computing the dice similarity coefﬁcient, Hausdorff distances, sensitivity, and precision metrics. Our best performing incremental-MRRN method produced the highest DSC of 0.74 ± 0.13 for TCIA, 0.75±0.12 for MSKCC, and 0.68±0.23 for the LIDC data sets. There was no signiﬁcant difference in the estimations of volumetric tumor changes computed using the incrementalMRRN method compared with the expert segmentation. In summary, we have developed a multi-scale CNN approach for volumetrically segmenting lung tumors which enables accurate, automated identiﬁcation of and serial measurement of tumor volumes in the lung.},
	language = {en},
	number = {1},
	urldate = {2022-07-31},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Jiang, Jue and Hu, Yu-Chi and Liu, Chia-Ju and Halpenny, Darragh and Hellmann, Matthew D. and Deasy, Joseph O. and Mageras, Gig and Veeraraghavan, Harini},
	month = jan,
	year = {2019},
	pages = {134--144},
	file = {Jiang et al. - 2019 - Multiple Resolution Residually Connected Feature S.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FUZF8W3A\\Jiang et al. - 2019 - Multiple Resolution Residually Connected Feature S.pdf:application/pdf},
}

@inproceedings{ramanjaneyuluDetectionClassificationLung2022,
	address = {Chennai, India},
	title = {Detection and {Classification} of {Lung} {Cancer} {Using} {VGG}-16},
	isbn = {978-1-66548-385-8},
	url = {https://ieeexplore.ieee.org/document/9783556/},
	doi = {10.1109/ICESIC53714.2022.9783556},
	abstract = {One of the main sources of fatalities in the world is lung cancer. Lung cancer is responsible for 7.6 million deaths worldwide each year, as per the statistics of World Health Organization (WHO). It is only possible to treat lung cancer when it is identified at an early stage. Lung cancer may be diagnosed using a variety of technologies, including isotope, MRI, CT, as well as X-ray. CT scan images are not easy to understand, but using CNN with Image Segmentation is a straightforward approach to detect Lung cancer. CNN (Convolutional Neural Network) is a deep structured technique that has been extensively used to investigate the potential to extract and visualize hidden texture information from image datasets. The objective of the examination is to consequently extricate self-learned components with a start to finish learning CNN and contrast the discoveries with the presentation of standard best in class and customary PC helped indicative frameworks.},
	language = {en},
	urldate = {2022-07-31},
	booktitle = {2022 {International} {Conference} on {Electronic} {Systems} and {Intelligent} {Computing} ({ICESIC})},
	publisher = {IEEE},
	author = {Ramanjaneyulu, K. and Kumar, K. Hemanth and Snehith, K. and Jyothirmai, G. and Krishna, K. Venkata},
	month = apr,
	year = {2022},
	pages = {69--72},
	file = {Ramanjaneyulu et al. - 2022 - Detection and Classification of Lung Cancer Using .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8AYD2RJ7\\Ramanjaneyulu et al. - 2022 - Detection and Classification of Lung Cancer Using .pdf:application/pdf},
}

@article{yuReproducibleMachineLearning2020,
	title = {Reproducible {Machine} {Learning} {Methods} for {Lung} {Cancer} {Detection} {Using} {Computed} {Tomography} {Images}: {Algorithm} {Development} and {Validation}},
	volume = {22},
	issn = {1438-8871},
	shorttitle = {Reproducible {Machine} {Learning} {Methods} for {Lung} {Cancer} {Detection} {Using} {Computed} {Tomography} {Images}},
	url = {https://www.jmir.org/2020/8/e16709},
	doi = {10.2196/16709},
	abstract = {Background
              Chest computed tomography (CT) is crucial for the detection of lung cancer, and many automated CT evaluation methods have been proposed. Due to the divergent software dependencies of the reported approaches, the developed methods are rarely compared or reproduced.
            
            
              Objective
              The goal of the research was to generate reproducible machine learning modules for lung cancer detection and compare the approaches and performances of the award-winning algorithms developed in the Kaggle Data Science Bowl.
            
            
              Methods
              We obtained the source codes of all award-winning solutions of the Kaggle Data Science Bowl Challenge, where participants developed automated CT evaluation methods to detect lung cancer (training set n=1397, public test set n=198, final test set n=506). The performance of the algorithms was evaluated by the log-loss function, and the Spearman correlation coefficient of the performance in the public and final test sets was computed.
            
            
              Results
              Most solutions implemented distinct image preprocessing, segmentation, and classification modules. Variants of U-Net, VGGNet, and residual net were commonly used in nodule segmentation, and transfer learning was used in most of the classification algorithms. Substantial performance variations in the public and final test sets were observed (Spearman correlation coefficient = .39 among the top 10 teams). To ensure the reproducibility of results, we generated a Docker container for each of the top solutions.
            
            
              Conclusions
              We compared the award-winning algorithms for lung cancer detection and generated reproducible Docker images for the top solutions. Although convolutional neural networks achieved decent accuracy, there is plenty of room for improvement regarding model generalizability.},
	language = {en},
	number = {8},
	urldate = {2022-08-01},
	journal = {Journal of Medical Internet Research},
	author = {Yu, Kun-Hsing and Lee, Tsung-Lu Michael and Yen, Ming-Hsuan and Kou, S C and Rosen, Bruce and Chiang, Jung-Hsien and Kohane, Isaac S},
	month = aug,
	year = {2020},
	pages = {e16709},
	file = {Yu et al_2020_Reproducible Machine Learning Methods for Lung Cancer Detection Using Computed.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JUCAIJHA\\Yu et al_2020_Reproducible Machine Learning Methods for Lung Cancer Detection Using Computed.pdf:application/pdf;Yu et al. - 2020 - Reproducible Machine Learning Methods for Lung Can.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EGPFJ6GK\\Yu et al. - 2020 - Reproducible Machine Learning Methods for Lung Can.pdf:application/pdf},
}

@misc{dumoulinGuideConvolutionArithmetic2018,
	title = {A guide to convolution arithmetic for deep learning},
	url = {http://arxiv.org/abs/1603.07285},
	abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
	language = {en},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Dumoulin, Vincent and Visin, Francesco},
	month = jan,
	year = {2018},
	note = {arXiv:1603.07285 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Dumoulin and Visin - 2018 - A guide to convolution arithmetic for deep learnin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EE5ZXIVV\\Dumoulin and Visin - 2018 - A guide to convolution arithmetic for deep learnin.pdf:application/pdf},
}

@techreport{zhuDeepLung3DDeep2017,
	type = {preprint},
	title = {{DeepLung}: {3D} {Deep} {Convolutional} {Nets} for {Automated} {Pulmonary} {Nodule} {Detection} and {Classification}},
	shorttitle = {{DeepLung}},
	url = {http://biorxiv.org/lookup/doi/10.1101/189928},
	abstract = {In this work, we present a fully automated lung CT cancer diagnosis system, DeepLung. DeepLung contains two parts, nodule detection and classiﬁcation. Considering the 3D nature of lung CT data, two 3D networks are designed for the nodule detection and classiﬁcation respectively. Speciﬁcally, a 3D Faster R-CNN is designed for nodule detection with a U-net-like encoder-decoder structure to effectively learn nodule features. For nodule classiﬁcation, gradient boosting machine (GBM) with 3D dual path network (DPN) features is proposed. The nodule classiﬁcation subnetwork is validated on a public dataset from LIDC-IDRI, on which it achieves better performance than state-of-the-art approaches, and surpasses the average performance of four experienced doctors. For the DeepLung system, candidate nodules are detected ﬁrst by the nodule detection subnetwork, and nodule diagnosis is conducted by the classiﬁcation subnetwork. Extensive experimental results demonstrate the DeepLung is comparable to the experienced doctors both for the nodule-level and patient-level diagnosis on the LIDC-IDRI dataset.},
	language = {en},
	urldate = {2022-08-01},
	institution = {Bioinformatics},
	author = {Zhu, Wentao and Liu, Chaochun and Fan, Wei and Xie, Xiaohui},
	month = sep,
	year = {2017},
	doi = {10.1101/189928},
	file = {Zhu et al. - 2017 - DeepLung 3D Deep Convolutional Nets for Automated.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RUU7CSEF\\Zhu et al. - 2017 - DeepLung 3D Deep Convolutional Nets for Automated.pdf:application/pdf},
}

@article{olafenwaBasicsImageClassification,
	title = {Basics of {Image} {Classification} with {PyTorch}},
	language = {en},
	author = {Olafenwa, John},
	pages = {19},
	file = {Olafenwa - Basics of Image Classification with PyTorch.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8E6KMG5V\\Olafenwa - Basics of Image Classification with PyTorch.pdf:application/pdf},
}

@article{jungClassificationLungNodules2018,
	title = {Classification of lung nodules in {CT} scans using three-dimensional deep convolutional neural networks with a checkpoint ensemble method},
	volume = {18},
	issn = {1471-2342},
	url = {https://bmcmedimaging.biomedcentral.com/articles/10.1186/s12880-018-0286-0},
	doi = {10.1186/s12880-018-0286-0},
	abstract = {Background: Accurately detecting and examining lung nodules early is key in diagnosing lung cancers and thus one of the best ways to prevent lung cancer deaths. Radiologists spend countless hours detecting small spherical-shaped nodules in computed tomography (CT) images. In addition, even after detecting nodule candidates, a considerable amount of effort and time is required for them to determine whether they are real nodules. The aim of this paper is to introduce a high performance nodule classification method that uses three dimensional deep convolutional neural networks (DCNNs) and an ensemble method to distinguish nodules between non-nodules.
Methods: In this paper, we use a three dimensional deep convolutional neural network (3D DCNN) with shortcut connections and a 3D DCNN with dense connections for lung nodule classification. The shortcut connections and dense connections successfully alleviate the gradient vanishing problem by allowing the gradient to pass quickly and directly. Connections help deep structured networks to obtain general as well as distinctive features of lung nodules. Moreover, we increased the dimension of DCNNs from two to three to capture 3D features. Compared with shallow 3D CNNs used in previous studies, deep 3D CNNs more effectively capture the features of spherical-shaped nodules. In addition, we use an alternative ensemble method called the checkpoint ensemble method to boost performance.
Results: The performance of our nodule classification method is compared with that of the state-of-the-art methods which were used in the LUng Nodule Analysis 2016 Challenge. Our method achieves higher competition performance metric (CPM) scores than the state-of-the-art methods using deep learning. In the experimental setup ESB-ALL, the 3D DCNN with shortcut connections and the 3D DCNN with dense connections using the checkpoint ensemble method achieved the highest CPM score of 0.910.
Conclusion: The result demonstrates that our method of using a 3D DCNN with shortcut connections, a 3D DCNN with dense connections, and the checkpoint ensemble method is effective for capturing 3D features of nodules and distinguishing nodules between non-nodules.},
	language = {en},
	number = {1},
	urldate = {2022-08-01},
	journal = {BMC Medical Imaging},
	author = {Jung, Hwejin and Kim, Bumsoo and Lee, Inyeop and Lee, Junhyun and Kang, Jaewoo},
	month = dec,
	year = {2018},
	pages = {48},
	file = {Jung et al. - 2018 - Classification of lung nodules in CT scans using t.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S9U26S4R\\Jung et al. - 2018 - Classification of lung nodules in CT scans using t.pdf:application/pdf},
}

@book{luDeepLearningConvolutional2019,
	address = {Cham},
	series = {Advances in {Computer} {Vision} and {Pattern} {Recognition}},
	title = {Deep {Learning} and {Convolutional} {Neural} {Networks} for {Medical} {Imaging} and {Clinical} {Informatics}},
	isbn = {978-3-030-13968-1 978-3-030-13969-8},
	url = {http://link.springer.com/10.1007/978-3-030-13969-8},
	language = {en},
	urldate = {2022-08-01},
	publisher = {Springer International Publishing},
	editor = {Lu, Le and Wang, Xiaosong and Carneiro, Gustavo and Yang, Lin},
	year = {2019},
	doi = {10.1007/978-3-030-13969-8},
	file = {Lu et al. - 2019 - Deep Learning and Convolutional Neural Networks fo.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\S9HIEFFH\\Lu et al. - 2019 - Deep Learning and Convolutional Neural Networks fo.pdf:application/pdf},
}

@article{rosebrockDeepLearningComputer,
	title = {Deep {Learning} for {Computer} {Vision} with {Python}. 2 - {Practitioner} {Bundle}},
	language = {en},
	author = {Rosebrock, Adrian},
	pages = {210},
	file = {Rosebrock - Deep Learning for Computer Vision with Python. 2 -.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WMSI76S9\\Rosebrock - Deep Learning for Computer Vision with Python. 2 -.pdf:application/pdf},
}

@article{shuDeepLearningImage,
	title = {Deep learning for image classification on very small datasets using transfer learning},
	language = {en},
	author = {Shu, Mengying},
	pages = {40},
	file = {Shu - Deep learning for image classification on very sma.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JCC4N4Y4\\Shu - Deep learning for image classification on very sma.pdf:application/pdf},
}

@article{riquelmeDeepLearningLung2020,
	title = {Deep {Learning} for {Lung} {Cancer} {Nodules} {Detection} and {Classification} in {CT} {Scans}},
	volume = {1},
	issn = {2673-2688},
	url = {https://www.mdpi.com/2673-2688/1/1/3},
	doi = {10.3390/ai1010003},
	abstract = {Detecting malignant lung nodules from computed tomography (CT) scans is a hard and time-consuming task for radiologists. To alleviate this burden, computer-aided diagnosis (CAD) systems have been proposed. In recent years, deep learning approaches have shown impressive results outperforming classical methods in various ﬁelds. Nowadays, researchers are trying different deep learning techniques to increase the performance of CAD systems in lung cancer screening with computed tomography. In this work, we review recent state-of-the-art deep learning algorithms and architectures proposed as CAD systems for lung cancer detection. They are divided into two categories—(1) Nodule detection systems, which from the original CT scan detect candidate nodules; and (2) False positive reduction systems, which from a set of given candidate nodules classify them into benign or malignant tumors. The main characteristics of the different techniques are presented, and their performance is analyzed. The CT lung datasets available for research are also introduced. Comparison between the different techniques is presented and discussed.},
	language = {en},
	number = {1},
	urldate = {2022-08-01},
	journal = {AI},
	author = {Riquelme, Diego and Akhloufi, Moulay},
	month = jan,
	year = {2020},
	pages = {28--67},
	file = {Riquelme and Akhloufi - 2020 - Deep Learning for Lung Cancer Nodules Detection an.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XT6Z49KU\\Riquelme and Akhloufi - 2020 - Deep Learning for Lung Cancer Nodules Detection an.pdf:application/pdf},
}

@article{shimazakiDeepLearningbasedAlgorithm2022,
	title = {Deep learning-based algorithm for lung cancer detection on chest radiographs using the segmentation method},
	volume = {12},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-04667-w},
	doi = {10.1038/s41598-021-04667-w},
	abstract = {Abstract
            We developed and validated a deep learning (DL)-based model using the segmentation method and assessed its ability to detect lung cancer on chest radiographs. Chest radiographs for use as a training dataset and a test dataset were collected separately from January 2006 to June 2018 at our hospital. The training dataset was used to train and validate the DL-based model with five-fold cross-validation. The model sensitivity and mean false positive indications per image (mFPI) were assessed with the independent test dataset. The training dataset included 629 radiographs with 652 nodules/masses and the test dataset included 151 radiographs with 159 nodules/masses. The DL-based model had a sensitivity of 0.73 with 0.13 mFPI in the test dataset. Sensitivity was lower in lung cancers that overlapped with blind spots such as pulmonary apices, pulmonary hila, chest wall, heart, and sub-diaphragmatic space (0.50–0.64) compared with those in non-overlapped locations (0.87). The dice coefficient for the 159 malignant lesions was on average 0.52. The DL-based model was able to detect lung cancers on chest radiographs, with low mFPI.},
	language = {en},
	number = {1},
	urldate = {2022-08-01},
	journal = {Scientific Reports},
	author = {Shimazaki, Akitoshi and Ueda, Daiju and Choppin, Antoine and Yamamoto, Akira and Honjo, Takashi and Shimahara, Yuki and Miki, Yukio},
	month = dec,
	year = {2022},
	pages = {727},
	file = {Shimazaki et al. - 2022 - Deep learning-based algorithm for lung cancer dete.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IMFGA94A\\Shimazaki et al. - 2022 - Deep learning-based algorithm for lung cancer dete.pdf:application/pdf},
}

@misc{heDeepResidualLearning2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	language = {en},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G4DRHZHM\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@misc{minaeeDeepCOVIDPredictingCOVID192020,
	title = {Deep-{COVID}: {Predicting} {COVID}-19 {From} {Chest} {X}-{Ray} {Images} {Using} {Deep} {Transfer} {Learning}},
	shorttitle = {Deep-{COVID}},
	url = {http://arxiv.org/abs/2004.09363},
	abstract = {The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in ﬁghting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detecting this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed speciﬁc abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their chest radiography images. We ﬁrst prepare a dataset of 5,000 Chest X-rays from the publicly available datasets. Images exhibiting COVID-19 disease presence were identiﬁed by board-certiﬁed radiologist. Transfer learning on a subset of 2,000 radiograms was used to train four popular convolutional neural networks, including ResNet18, ResNet50, SqueezeNet, and DenseNet-121, to identify COVID-19 disease in the analyzed chest X-ray images. We evaluated these models on the remaining 3,000 images, and most of these networks achieved a sensitivity rate of 98\% (± 3\%), while having a speciﬁcity rate of around 90\%. Besides sensitivity and speciﬁcity rates, we also present the receiver operating characteristic (ROC) curve, precision-recall curve, average prediction, and confusion matrix of each model. We also used a technique to generate heatmaps of lung regions potentially infected by COVID-19 and show that the generated heatmaps contain most of the infected areas annotated by our board certiﬁed radiologist. While the achieved performance is very encouraging, further analysis is required on a larger set of COVID-19 images, to have a more reliable estimation of accuracy rates. The dataset, model implementations (in PyTorch), and evaluations, are all made publicly available for research community at https://github.com/shervinmin/DeepCovid.git c 2020 Elsevier B. V. All rights reserved.},
	language = {en},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Minaee, Shervin and Kafieh, Rahele and Sonka, Milan and Yazdani, Shakib and Soufi, Ghazaleh Jamalipour},
	month = jul,
	year = {2020},
	note = {arXiv:2004.09363 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Minaee et al. - 2020 - Deep-COVID Predicting COVID-19 From Chest X-Ray I.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DIHWMM83\\Minaee et al. - 2020 - Deep-COVID Predicting COVID-19 From Chest X-Ray I.pdf:application/pdf},
}

@article{pittsDeepTumourDeepLearning,
	title = {{DeepTumour}: {Deep} {Learning} for the detection of lung tumours in {Computed} {Tomography} scans.},
	language = {en},
	author = {Pitts, Joseph David},
	pages = {69},
	file = {Pitts - DeepTumour Deep Learning for the detection of lun.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TAT7W5TB\\Pitts - DeepTumour Deep Learning for the detection of lun.pdf:application/pdf},
}

@misc{huangDenselyConnectedConvolutional2018,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	language = {en},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = jan,
	year = {2018},
	note = {arXiv:1608.06993 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Huang et al. - 2018 - Densely Connected Convolutional Networks.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E89ANDRP\\Huang et al. - 2018 - Densely Connected Convolutional Networks.pdf:application/pdf},
}

@article{antinDetectingPneumoniaChest,
	title = {Detecting {Pneumonia} in {Chest} {X}-{Rays} with {Supervised} {Learning}},
	language = {en},
	author = {Antin, Benjamin and Kravitz, Joshua and Martayan, Emil},
	pages = {5},
	file = {Antin et al. - Detecting Pneumonia in Chest X-Rays with Supervise.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V3NH4WQ6\\Antin et al. - Detecting Pneumonia in Chest X-Rays with Supervise.pdf:application/pdf},
}

@inproceedings{phankokkruadEnsembleTransferLearning2021,
	address = {Shanghai China},
	title = {Ensemble {Transfer} {Learning} for {Lung} {Cancer} {Detection}},
	isbn = {978-1-4503-9024-8},
	url = {https://dl.acm.org/doi/10.1145/3478905.3478995},
	doi = {10.1145/3478905.3478995},
	abstract = {Lung cancer is the most leading cause of death. One of the significant screening problems is the difficulty in diagnosing it at an early stage. Consequently, this is a better way to study the ensemble of the transfer learning model to improve their accuracy and performance for lung cancer detection. This study has proposed the three CNN models for detecting lung cancer using VGG16, ResNet50V2, and DenseNet201 architecture based on transfer learning. The proposed models enhance to classify lung cancer into five different classes. The three transfer learning of CNN architectures were used to train, test, and validate based on the image dataset. The results reveal that the proposed models have performed the classification task for detecting lung cancer. The models achieved an accuracy level of VGG16, ResNet50V2, and DenseNet201 were 62\%, 90\%, and 89\%, respectively. Finally, the ensemble of the three proposed CNN models is created and validated. The final proposed ensemble model achieved 91\% validation accuracy that performed better than the other existing models.},
	language = {en},
	urldate = {2022-08-01},
	booktitle = {2021 4th {International} {Conference} on {Data} {Science} and {Information} {Technology}},
	publisher = {ACM},
	author = {Phankokkruad, Manop},
	month = jul,
	year = {2021},
	pages = {438--442},
	file = {Phankokkruad - 2021 - Ensemble Transfer Learning for Lung Cancer Detecti.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3YNE9CNI\\Phankokkruad - 2021 - Ensemble Transfer Learning for Lung Cancer Detecti.pdf:application/pdf},
}

@article{liaoEvaluateMalignancyPulmonary2019,
	title = {Evaluate the {Malignancy} of {Pulmonary} {Nodules} {Using} the {3D} {Deep} {Leaky} {Noisy}-or {Network}},
	volume = {30},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/1711.08324},
	doi = {10.1109/TNNLS.2019.2892409},
	abstract = {Automatic diagnosing lung cancer from Computed Tomography (CT) scans involves two steps: detect all suspicious lesions (pulmonary nodules) and evaluate the wholelung/pulmonary malignancy. Currently, there are many studies about the ﬁrst step, but few about the second step. Since the existence of nodule does not deﬁnitely indicate cancer, and the morphology of nodule has a complicated relationship with cancer, the diagnosis of lung cancer demands careful investigations on every suspicious nodule and integration of information of all nodules. We propose a 3D deep neural network to solve this problem. The model consists of two modules. The ﬁrst one is a 3D region proposal network for nodule detection, which outputs all suspicious nodules for a subject. The second one selects the top ﬁve nodules based on the detection conﬁdence, evaluates their cancer probabilities and combines them with a leaky noisy-or gate to obtain the probability of lung cancer for the subject. The two modules share the same backbone network, a modiﬁed U-net. The over-ﬁtting caused by the shortage of training data is alleviated by training the two modules alternately. The proposed model won the ﬁrst place in the Data Science Bowl 2017 competition. The code has been made publicly available1.},
	language = {en},
	number = {11},
	urldate = {2022-08-01},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Liao, Fangzhou and Liang, Ming and Li, Zhe and Hu, Xiaolin and Song, Sen},
	month = nov,
	year = {2019},
	note = {arXiv:1711.08324 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {3484--3495},
	file = {Liao et al. - 2019 - Evaluate the Malignancy of Pulmonary Nodules Using.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MCRRYVPG\\Liao et al. - 2019 - Evaluate the Malignancy of Pulmonary Nodules Using.pdf:application/pdf},
}

@article{yuhengImageSegmentationAlgorithms,
	title = {Image {Segmentation} {Algorithms} {Overview}},
	abstract = {The technology of image segmentation is widely used in medical image processing, face recognition pedestrian detection, etc. The current image segmentation techniques include region-based segmentation, edge detection segmentation, segmentation based on clustering, segmentation based on weakly-supervised learning in CNN, etc. This paper analyzes and summarizes these algorithms of image segmentation, and compares the advantages and disadvantages of different algorithms. Finally, we make a prediction of the development trend of image segmentation with the combination of these algorithms.},
	language = {en},
	author = {Yuheng, Song and Hao, Yan},
	pages = {6},
	file = {Yuheng and Hao - Image Segmentation Algorithms Overview.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JL4X5ZWB\\Yuheng and Hao - Image Segmentation Algorithms Overview.pdf:application/pdf},
}

@inproceedings{niuImprovementSpeechEmotion2018,
	address = {Chengdu, China},
	title = {Improvement on {Speech} {Emotion} {Recognition} {Based} on {Deep} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4503-6419-5},
	url = {http://dl.acm.org/citation.cfm?doid=3194452.3194460},
	doi = {10.1145/3194452.3194460},
	abstract = {Speech emotion recognition (SER) is to study the formation and change of speaker’s emotional state from the speech signal perspective, so as to make the interaction between human and computer more intelligent. SER is a challenging task that has encountered the problem of less training data and low prediction accuracy. Here we propose a data processing algorithm based on the imaging principle of the retina and convex lens (DPARIP), to acquire the different sizes of spectrogram and get different training data by changing the distance between the spectrogram and the convex lens. Meanwhile, with the help of deep learning to get the high-level features, we apply the AlexNet on the IEMOCAP database and achieve the average accuracy over 48.8\% on six emotions. The experimental results indicate that our proposed data preprocessing algorithm is effective and more accurate compared to existing emotion recognition algorithms.},
	language = {en},
	urldate = {2022-08-01},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Computing} and {Artificial} {Intelligence} - {ICCAI} 2018},
	publisher = {ACM Press},
	author = {Niu, Yafeng and Zou, Dongsheng and Niu, Yadong and He, Zhongshi and Tan, Hua},
	year = {2018},
	note = {ISBN: 9781450364195},
	keywords = {Deep learning, Speech emotion recognition, stock price prediction, Speech spectrogram},
	pages = {13--18},
	file = {Niu et al. - 2018 - Improvement on Speech Emotion Recognition Based on.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VT3SYAN2\\Niu et al. - 2018 - Improvement on Speech Emotion Recognition Based on.pdf:application/pdf},
}

@article{chenLungCancerDiagnosis2022,
	title = {Lung cancer diagnosis using deep attention‐based multiple instance learning and radiomics},
	volume = {49},
	issn = {0094-2405, 2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/mp.15539},
	doi = {10.1002/mp.15539},
	abstract = {Background: Early diagnosis of lung cancer is a key intervention for the treatment of lung cancer in which computer-aided diagnosis (CAD) can play a crucial role. Most published CAD methods perform lung cancer diagnosis by classifying each lung nodule in isolation. However, this does not reﬂect clinical practice, where clinicians diagnose a patient based on a set of images of nodules,instead of looking at one nodule at a time. Besides, the low interpretability of the output provided by these methods presents an important barrier for their adoption.},
	language = {en},
	number = {5},
	urldate = {2022-08-01},
	journal = {Medical Physics},
	author = {Chen, Junhua and Zeng, Haiyan and Zhang, Chong and Shi, Zhenwei and Dekker, Andre and Wee, Leonard and Bermejo, Inigo},
	month = may,
	year = {2022},
	pages = {3134--3143},
	file = {Chen et al. - 2022 - Lung cancer diagnosis using deep attention‐based m.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\B9QC2ZIM\\Chen et al. - 2022 - Lung cancer diagnosis using deep attention‐based m.pdf:application/pdf},
}

@article{zhangLungNoduleClassification2021,
	title = {Lung {Nodule} {Classification} in {CT} {Images} {Using} {3D} {DenseNet}},
	volume = {1827},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1827/1/012155},
	doi = {10.1088/1742-6596/1827/1/012155},
	abstract = {Lung cancer is the main malignant tumour affecting the health of residents in China. Automatically discriminating benign and malignant pulmonary nodules can facilitate the early detection of lung cancer, which reduces lung cancer mortality. The rising quantity of public available lung CT datasets made it possible to use deep learning approaches for lung nodules malignancy classification. Unlike most of the previous models that focused on 2D convolutional neural nets (CNN), here we explore the use of the DenseNet architecture with 3D filters and pooling kernels. The performance of the proposed nodule classification was evaluated on publicly available LUNA16 dataset, a subset of lung image database consortium and image database resource initiative dataset (LIDC/IDRI). It achieved a 92.4\% classification accuracy. The proposed method provides an independent module with encouraging prediction accuracy that can be easily incorporated with a lung cancer computer-aided diagnosis system.},
	language = {en},
	number = {1},
	urldate = {2022-08-01},
	journal = {Journal of Physics: Conference Series},
	author = {Zhang, Ge and Lin, Lan and Wang, Jingxuan},
	month = mar,
	year = {2021},
	pages = {012155},
	file = {Zhang et al. - 2021 - Lung Nodule Classification in CT Images Using 3D D.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XGQLH2PZ\\Zhang et al. - 2021 - Lung Nodule Classification in CT Images Using 3D D.pdf:application/pdf},
}

@inproceedings{kumarLungNoduleClassification2015,
	address = {Halifax, NS, Canada},
	title = {Lung {Nodule} {Classification} {Using} {Deep} {Features} in {CT} {Images}},
	isbn = {978-1-4799-1986-4},
	url = {http://ieeexplore.ieee.org/document/7158331/},
	doi = {10.1109/CRV.2015.25},
	language = {en},
	urldate = {2022-08-01},
	booktitle = {2015 12th {Conference} on {Computer} and {Robot} {Vision}},
	publisher = {IEEE},
	author = {Kumar, Devinder and Wong, Alexander and Clausi, David A.},
	month = jun,
	year = {2015},
	pages = {133--138},
	file = {Kumar et al. - 2015 - Lung Nodule Classification Using Deep Features in .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I3J373Y9\\Kumar et al. - 2015 - Lung Nodule Classification Using Deep Features in .pdf:application/pdf},
}

@inproceedings{kaurObjectDetectionUsing2021,
	address = {Windhoek Namibia},
	title = {Object {Detection} using {Deep} {Learning}: {A} {Review}},
	isbn = {978-1-4503-8763-7},
	shorttitle = {Object {Detection} using {Deep} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3484824.3484889},
	doi = {10.1145/3484824.3484889},
	abstract = {Object detection is one of the most critical and challenging tasks in computer vision. It is the process of finding objects belonging to some predefined categories and determining their location in an image or video. This paper reviews deep learning-based object detection models. The paper discusses some benchmark datasets. The performance evaluation of different detectors on different datasets based on mean Average Precision (mAP) is reviewed. Object detection is used in different fields in different forms. Applications of object detection like pedestrian detection, autonomous driving, face detection, etc., are presented. Finally, the future scope is discussed to work on new techniques for object detection.},
	language = {en},
	urldate = {2022-08-01},
	booktitle = {Proceedings of the {International} {Conference} on {Data} {Science}, {Machine} {Learning} and {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Kaur, Biponjot and Singh, Sarbjeet},
	month = aug,
	year = {2021},
	pages = {328--334},
	file = {Kaur and Singh - 2021 - Object Detection using Deep Learning A Review.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9BX8RZA3\\Kaur and Singh - 2021 - Object Detection using Deep Learning A Review.pdf:application/pdf},
}

@article{koulPracticalDeepLearning,
	title = {Practical {Deep} {Learning} for {Cloud}, {Mobile}, and {Edge}},
	language = {en},
	author = {Koul, Anirudh and Ganju, Siddha and Kasam, Meher},
	pages = {959},
	file = {Koul et al. - Practical Deep Learning for Cloud, Mobile, and Edg.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MB8TLAVH\\Koul et al. - Practical Deep Learning for Cloud, Mobile, and Edg.pdf:application/pdf},
}

@article{gomesPulmonaryNoduleSegmentation,
	title = {Pulmonary {Nodule} {Segmentation} in {Computed} {Tomography} with {Deep} {Learning}},
	language = {en},
	author = {Gomes, João Henriques Oliveira},
	pages = {80},
	file = {Gomes - Pulmonary Nodule Segmentation in Computed Tomograp.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5DQNFGP6\\Gomes - Pulmonary Nodule Segmentation in Computed Tomograp.pdf:application/pdf},
}

@article{xuSDDCNNSmallDataDriven2019,
	title = {{SDD}-{CNN}: {Small} {Data}-{Driven} {Convolution} {Neural} {Networks} for {Subtle} {Roller} {Defect} {Inspection}},
	volume = {9},
	issn = {2076-3417},
	shorttitle = {{SDD}-{CNN}},
	url = {https://www.mdpi.com/2076-3417/9/7/1364},
	doi = {10.3390/app9071364},
	abstract = {Roller bearings are some of the most critical and widely used components in rotating machinery. Appearance defect inspection plays a key role in bearing quality control. However, in real industries, bearing defects are usually extremely subtle and have a low probability of occurrence. This leads to distribution discrepancies between the number of positive and negative samples, which makes intelligent data-driven inspection methods difﬁcult to develop and deploy. This paper presents a small data-driven convolution neural network (SDD-CNN) for roller subtle defect inspection via an ensemble method for small data preprocessing. First, label dilation (LD) is applied to solve the problem of an imbalance in class distribution. Second, a semi-supervised data augmentation (SSDA) method is proposed to extend the dataset in a more efﬁcient and controlled way. In this method, a coarse CNN model is trained to generate ground truth class activation and guide the random cropping of images. Third, four variants of the CNN model, namely, SqueezeNet v1.1, Inception v3, VGG-16, and ResNet-18, are introduced and employed to inspect and classify the surface defects of rollers. Finally, a rich set of experiments and assessments is conducted, indicating that these SDD-CNN models, particularly the SDD-Inception v3 model, perform exceedingly well in the roller defect classiﬁcation task with a top-1 accuracy reaching 99.56\%. In addition, the convergence time and classiﬁcation accuracy for an SDD-CNN model achieve signiﬁcant improvement compared to that for the original CNN. Overall, using an SDD-CNN architecture, this paper provides a clear path toward a higher precision and efﬁciency for roller defect inspection in smart manufacturing.},
	language = {en},
	number = {7},
	urldate = {2022-08-01},
	journal = {Applied Sciences},
	author = {Xu, Xiaohang and Zheng, Hong and Guo, Zhongyuan and Wu, Xiongbin and Zheng, Zhaohui},
	month = mar,
	year = {2019},
	pages = {1364},
	file = {Xu et al. - 2019 - SDD-CNN Small Data-Driven Convolution Neural Netw.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YTYDKV5P\\Xu et al. - 2019 - SDD-CNN Small Data-Driven Convolution Neural Netw.pdf:application/pdf},
}

@misc{iandolaSqueezeNetAlexNetlevelAccuracy2016a,
	title = {{SqueezeNet}: {AlexNet}-level accuracy with 50x fewer parameters and {\textless}0.{5MB} model size},
	shorttitle = {{SqueezeNet}},
	url = {http://arxiv.org/abs/1602.07360},
	abstract = {Recent research on deep convolutional neural networks (CNNs) has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple CNN architectures that achieve that accuracy level. With equivalent accuracy, smaller CNN architectures offer at least three advantages: (1) Smaller CNNs require less communication across servers during distributed training. (2) Smaller CNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller CNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small CNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques, we are able to compress SqueezeNet to less than 0.5MB (510× smaller than AlexNet).},
	language = {en},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
	month = nov,
	year = {2016},
	note = {arXiv:1602.07360 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AMWNGJJ8\\Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer .pdf:application/pdf},
}

@inproceedings{haqueEffectPreTrainingThoracic2021,
	address = {Virtual (GMT+7 Bangkok Time) Thailand},
	title = {The {Effect} of {PreTraining} {Thoracic} {Disease} {Detection} {Systems} on {Large}-{Scale} {Chest} {X}-{Ray} {Domain} {Datasets}},
	isbn = {978-1-4503-8510-7},
	url = {https://dl.acm.org/doi/10.1145/3486713.3486735},
	doi = {10.1145/3486713.3486735},
	abstract = {The COVID-19 pandemic has impacted many countries around the world resulting in the need to develop quick and effective screening methods to ease the burden and overcome the limitations of varying healthcare capacities. Given the nature of the disease, the use of Chest X-ray (CXR) medical imaging has proven to be very useful which has prompted the exploration of computer-aided diagnosis tools to augment and assist radiologists. However, recent reports have deemed many of the proposed methods to be impractical for use in real-life applications due to models with poor generalization capabilities, an issue closely related to the quality of current datasets in the CXR domain. Typically, deep convolutional neural network (CNN) based classification systems utilize transfer learning techniques when data is limited. We suggest first training models on publicly available large-scale and CXR specific datasets, such as CheXpert, and using these pretrained weights when initializing the final model. Compared with a CNN pretrained on the more general ImageNet dataset, pretraining on large-scale domain specific data increased the model’s ability to generalize to unseen data.},
	language = {en},
	urldate = {2022-08-01},
	booktitle = {The 12th {International} {Conference} on {Computational} {Systems}-{Biology} and {Bioinformatics}},
	publisher = {ACM},
	author = {Haque, Shafinul and H. Chan, Jonathan},
	month = oct,
	year = {2021},
	pages = {44--47},
	file = {Haque and H. Chan - 2021 - The Effect of PreTraining Thoracic Disease Detecti.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9BSMQINW\\Haque and H. Chan - 2021 - The Effect of PreTraining Thoracic Disease Detecti.pdf:application/pdf},
}

@article{mcnitt-grayLungImageDatabase2008,
	title = {The {Lung} {Image} {Database} {Consortium} ({LIDC}) {Data} {Collection} {Process} for {Nodule} {Detection} and {Annotation}},
	language = {en},
	author = {McNitt-Gray, Michael F and Iii, Samuel G Armato and Meyer, Charles R and Reeves, Anthony P and McLennan, Geoffrey and Pais, Richie C and Freymann, John and Brown, Matthew S and Bland, Peyton H and Laderach, Gary E and Piker, Chris and Guo, Junfeng and Qing, David P-Y and Yankelevitz, David F and Aberle, Denise R and van, Edwin J R and MacMahon, Heber and Kazerooni, Ella A and Croft, Barbara Y and Clarke, Laurence P},
	year = {2008},
	pages = {19},
	file = {McNitt-Gray et al. - 2008 - The Lung Image Database Consortium (LIDC) Data Col.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KAL28744\\McNitt-Gray et al. - 2008 - The Lung Image Database Consortium (LIDC) Data Col.pdf:application/pdf},
}

@inproceedings{mastouriTransferLearningVs2020,
	address = {Xiamen China},
	title = {Transfer {Learning} {Vs}. {Fine}-{Tuning} in {Bilinear} {CNN} for {Lung} {Nodules} {Classification} on {CT} {Scans}},
	isbn = {978-1-4503-7551-1},
	url = {https://dl.acm.org/doi/10.1145/3430199.3430211},
	doi = {10.1145/3430199.3430211},
	abstract = {Lung cancer is one of the leading causes of death worldwide. Its early detection in its nodular form is extremely effective in improving patient survival rate. Deep learning (DL) and especially Convolutional Neural Network (CNN) have an important development over the past decade and were largely explored in medical imaging analysis. In this paper, a trending DL model composed of two CNN streams, named Bilinear CNN (B-CNN), was proposed for lung nodules classification on CT scans. In the developed B-CNN model, the pre-trained VGG16 architecture was trained as a feature extractor. It is the most important part of the proposed model in which its effectiveness depends stringently on its performances. Aiming to improve these performances, we address this question: what process leads with the performance improvement of the feature extractors? Transfer learning or Finetuning? To answer this question, two B-CNN models were implemented, in which the first one was based on transfer learning process and the second was based on fine-tuning, using VGG16 networks. A set of experiments was conducted and the results have shown the outperformance of the fine-tuned B-CNN model compared to the transfer learning-based model. Moreover, the proposed B-CNN model was demonstrating its efficiency and viability for the classification of lung nodules in terms of accuracy and AUC compared to existing works.},
	language = {en},
	urldate = {2022-08-01},
	booktitle = {Proceedings of the 2020 3rd {International} {Conference} on {Artificial} {Intelligence} and {Pattern} {Recognition}},
	publisher = {ACM},
	author = {Mastouri, Rekka and Khlifa, Nawres and Neji, Henda and Hantous-Zannad, Saoussen},
	month = jun,
	year = {2020},
	pages = {99--103},
	file = {Mastouri et al. - 2020 - Transfer Learning Vs. Fine-Tuning in Bilinear CNN .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TCG44J2X\\Mastouri et al. - 2020 - Transfer Learning Vs. Fine-Tuning in Bilinear CNN .pdf:application/pdf},
}

@misc{usmanVolumetricLungNodule2020,
	title = {Volumetric {Lung} {Nodule} {Segmentation} using {Adaptive} {ROI} with {Multi}-{View} {Residual} {Learning}},
	url = {http://arxiv.org/abs/1912.13335},
	abstract = {Accurate quantiﬁcation of pulmonary nodules can assist the early diagnosis of lung cancer, enhancing patient survival possibilities. A number of nodule segmentation techniques, which rely on a radiologist-provided 3-D volume of interest (VOI) input or the use of a constant region of interest (ROI), are proposed; however, these techniques can only investigate the presence of nodule voxels within the given VOI. Such approaches restrain the solutions to freely investigate the nodule presence outside the given VOI and also include the redundant structures (non-nodule) into VOI, which could lead to inaccurate nodule segmentation. In this work, a novel semi-automated approach for 3-D segmentation of nodule in volumetric computerized tomography (CT) lung scans has been proposed. The technique is segregated into two stages. In the ﬁrst stage, a 2-D ROI containing the nodule is provided as an input to perform a patchwise exploration along the axial axis using a novel adaptive ROI algorithm. This strategy enables the dynamic selection of the ROI in the surrounding slices to investigate the presence of nodules using a Deep Residual U-Net architecture. This stage provides the initial estimation of the nodule utilized to extract the VOI. In the second stage, the extracted VOI is further explored along the coronal and sagittal axes with two networks . All the estimated masks are then fed into a consensus module to produce a ﬁnal volumetric segmentation of the nodule. The proposed approach is rigorously evaluated on LIDC-IDRI dataset, which is the largest publicly available dataset. The results suggest that the proposed approach is more robust and accurate than the previous stateof-the-art techniques.},
	language = {en},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Usman, Muhammad and Lee, Byoung-Dai and Byon, Shi Sub and Kim, Sung Hyun and Byung-ilLee},
	month = feb,
	year = {2020},
	note = {arXiv:1912.13335 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Usman et al. - 2020 - Volumetric Lung Nodule Segmentation using Adaptive.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AWSFBN8E\\Usman et al. - 2020 - Volumetric Lung Nodule Segmentation using Adaptive.pdf:application/pdf},
}

@inproceedings{salimAutomatedEnglishDigital2019,
	address = {Yogyakarta, Indonesia},
	title = {Automated {English} {Digital} {Essay} {Grader} {Using} {Machine} {Learning}},
	isbn = {978-1-72812-665-4},
	url = {https://ieeexplore.ieee.org/document/9226022/},
	doi = {10.1109/TALE48000.2019.9226022},
	abstract = {Automatic essay grader is a program that is designed to grade an essay automatically. Some of automatic essay grader have been made using string kernel, word embedding and reinforced learning method. The objective of this research is to develop an application to help users in grading English digital essays. Grading is done based on 12 score features. Argumentative and narrative essays written by junior high school students are utilized as the dataset. By using XGBoost as the classifier, this research produces an automated essay grader with an average accuracy of 66.87\%. Evaluation is conducted using 5-fold cross validation method.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2019 {IEEE} {International} {Conference} on {Engineering}, {Technology} and {Education} ({TALE})},
	publisher = {IEEE},
	author = {Salim, Yafet and Stevanus, Valdi and Barlian, Edwardo and Sari, Azani Cempaka and Suhartono, Derwin},
	month = dec,
	year = {2019},
	pages = {1--6},
	file = {Salim et al. - 2019 - Automated English Digital Essay Grader Using Machi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UFS3E5PU\\Salim et al. - 2019 - Automated English Digital Essay Grader Using Machi.pdf:application/pdf},
}

@inproceedings{pengComparativeStudyNeural2020,
	address = {Shenyang, China},
	title = {A {Comparative} {Study} of {Neural} {Network} for {Text} {Classification}},
	isbn = {978-1-72818-117-2},
	url = {https://ieeexplore.ieee.org/document/9339702/},
	doi = {10.1109/TOCS50858.2020.9339702},
	abstract = {This With the popularity of artificial intelligence in recent years, Natural Language Processing (NLP) technology has also become the focus of research. NLP technology's unique machine translation and text sentiment analysis functions can prevent people from experiencing poor language communication when travelling abroad and help artificial intelligence understand people's language better. This article has made corresponding practice and analysis for the critical requirement of "text classification" in NLP. In the experiment, we used the Internet Movie Database (IMDB) film review forum as the dataset. Recurrent Neural Network (RNN) and the corresponding variants of RNN (Long Short Term Memory (LSTM)) are analyzed and compared from the theoretical aspect. Moreover, we introduced a bidirectional mechanism to optimize RNN and reduce the influence of parameter changes on model training by comparing specific neural network structures. We found the benefits of LSTM in text classification applications compared with RNN and simple neural networks by comparing experiments. Besides, we explored the role of the bidirectional mechanism for RNN. Finally, we create a two-way LSTM model for text classification model and obtain the model training results indicating less overfitting and less loss than other structures.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} {Conference} on {Telecommunications}, {Optics} and {Computer} {Science} ({TOCS})},
	publisher = {IEEE},
	author = {Peng, Xi},
	month = dec,
	year = {2020},
	pages = {214--218},
	file = {Peng - 2020 - A Comparative Study of Neural Network for Text Cla.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8KE8I7HP\\Peng - 2020 - A Comparative Study of Neural Network for Text Cla.pdf:application/pdf},
}

@inproceedings{chistolComparativeStudyParametric2020,
	address = {Suceava, Romania},
	title = {A {Comparative} {Study} of {Parametric} {Versus} {Non}-{Parametric} {Text} {Classification} {Algorithms}},
	isbn = {978-1-72816-870-8},
	url = {https://ieeexplore.ieee.org/document/9108968/},
	doi = {10.1109/DAS49615.2020.9108968},
	abstract = {Evolution of modern technologies allowed to store the text in various digital formats such as e-mails, e-documents, libraries, etc. The amount of text data that is produced daily is increasing dramatically. Discovering useful patterns in text that can be represented in unstructured, semi-structured or structured format is a difficult task that requires a good understanding of machine learning algorithms. Finding a suitable algorithm for text mining tasks such as classification, clustering or natural language processing is a demanding situation that tests researchers' abilities. This paper provides an overview of the text mining process also, presents a comparison of the performance and limitations of two predictive models generated using the parametric Naïve Bayes algorithm and nonparametric Deep Learning neural network. RapidMiner data science software platform has been used for models’ implementations and e-mail classification.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {International} {Conference} on {Development} and {Application} {Systems} ({DAS})},
	publisher = {IEEE},
	author = {Chistol, Mihaela},
	month = may,
	year = {2020},
	pages = {208--213},
	file = {Chistol - 2020 - A Comparative Study of Parametric Versus Non-Param.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\R663Q9HX\\Chistol - 2020 - A Comparative Study of Parametric Versus Non-Param.pdf:application/pdf},
}

@inproceedings{piComparativeStudyThree2020,
	address = {London United Kingdom},
	title = {A {Comparative} {Study} on {Three} {Multi}-{Label} {Classification} {Tools}},
	isbn = {978-1-4503-7546-7},
	url = {https://dl.acm.org/doi/10.1145/3416028.3416042},
	doi = {10.1145/3416028.3416042},
	abstract = {Many science, technology and innovation (STI) resources are attached with several different labels, such as IPC and CPC for patents, and PACS (Physics and Astronomy Classification Scheme) numbers for scientific publications. This problem is well known as the multi-label classification. Though there are a number of approaches and open-source tools for this task in the literature that work well on benchmark datasets, real-world is more complex in terms of both the number and hierarchy of labels. This work aims to compare comprehensively the performance of three state-of-the-art tools, Dependency LDA, Scikit-Multilearn and Neural Classifier on Scigraph of academic resource data. It is found that Neural Classifier works better on an unbalanced distribution dataset with more complex hierarchical structure and a larger number of label scale in terms of Micro F1, Micro F1 and Hamming Loss than the other two tools. On the basis of our comparisons, several directions are suggested in the near future.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 2020 3rd {International} {Conference} on {Information} {Management} and {Management} {Science}},
	publisher = {ACM},
	author = {Pi, Sainan and An, Xin and Xu, Shuo and Li, Jinghong},
	month = aug,
	year = {2020},
	pages = {8--12},
	file = {Pi et al. - 2020 - A Comparative Study on Three Multi-Label Classific.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QTXC6U7T\\Pi et al. - 2020 - A Comparative Study on Three Multi-Label Classific.pdf:application/pdf},
}

@inproceedings{wangComparativeStudyWord2020,
	address = {Seoul Republic of Korea},
	title = {A {Comparative} {Study} on {Word} {Embeddings} in {Deep} {Learning} for {Text} {Classification}},
	isbn = {978-1-4503-7760-7},
	url = {https://dl.acm.org/doi/10.1145/3443279.3443304},
	doi = {10.1145/3443279.3443304},
	abstract = {Word embeddings act as an important component of deep models for providing input features in downstream language tasks, such as sequence labelling and text classification. In the last decade, a substantial number of word embedding methods have been proposed for this purpose, mainly falling into the categories of classic and context-based word embeddings. In this paper, we conduct controlled experiments to systematically examine both classic and contextualised word embeddings for the purposes of text classification. To encode a sequence from word representations, we apply two encoders, namely CNN and BiLSTM, in the downstream network architecture. To study the impact of word embeddings on different datasets, we select four benchmarking classification datasets with varying average sample length, comprising both single-label and multi-label classification tasks. The evaluation results with confidence intervals indicate that CNN as the downstream encoder outperforms BiLSTM in most situations, especially for document context-insensitive datasets. This study recommends choosing CNN over BiLSTM for document classification datasets where the context in sequence is not as indicative of class membership as sentence datasets. For word embeddings, concatenation of multiple classic embeddings or increasing their size does not lead to a statistically significant difference in performance despite a slight improvement in some cases. For context-based embeddings, we studied both ELMo and BERT. The results show that BERT overall outperforms ELMo, especially for long document datasets. Compared with classic embeddings, both achieve an improved performance for short datasets while the improvement is not observed in longer datasets.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Natural} {Language} {Processing} and {Information} {Retrieval}},
	publisher = {ACM},
	author = {Wang, Congcong and Nulty, Paul and Lillis, David},
	month = dec,
	year = {2020},
	pages = {37--46},
	file = {Wang et al. - 2020 - A Comparative Study on Word Embeddings in Deep Lea.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FTZQJ2BC\\Wang et al. - 2020 - A Comparative Study on Word Embeddings in Deep Lea.pdf:application/pdf},
}

@inproceedings{koksalComparativeTextClassification2022,
	address = {Alanya, Turkey},
	title = {A {Comparative} {Text} {Classification} {Study} with {Deep} {Learning}-{Based} {Algorithms}},
	isbn = {978-1-66546-754-4},
	url = {https://ieeexplore.ieee.org/document/9772587/},
	doi = {10.1109/ICEEE55327.2022.9772587},
	abstract = {As a well-known Natural Language Processing (NLP) task, text classification can be defined as the process of categorizing documents depending on their content. In this process, selecting classification algorithms and tuning classification parameters are crucial for efficient classification. In recent years, many deep learning algorithms have been used successfully in text classification tasks. This paper performed a comparative study utilizing and optimizing several deep learningbased algorithms. We have implemented deep neural networks (DNN), convolutional neural networks (CNN), long shortest-term memory (LSTM), and gated recurrent units (GRU). In addition, we performed extensive experiments by tuning hyperparameters to improve classification accuracy. In addition, we implemented word embeddings techniques to acquire feature vectors of text data. Then we compared our word embeddings results with traditional TF-IDF vectorization results of DNN and CNN. In our experiments, we used an open-source Turkish News benchmarking dataset to compare our results with previous studies in the literature. Our experimental results revealed significant improvements in classification performance using word embeddings with deep learning-based algorithms and tuning hyperparameters. Furthermore, our work outperformed previous results on the selected dataset.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 9th {International} {Conference} on {Electrical} and {Electronics} {Engineering} ({ICEEE})},
	publisher = {IEEE},
	author = {Koksal, Omer and Akgul, Ozlem},
	month = mar,
	year = {2022},
	pages = {387--391},
	file = {Koksal and Akgul - 2022 - A Comparative Text Classification Study with Deep .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TPTBSSES\\Koksal and Akgul - 2022 - A Comparative Text Classification Study with Deep .pdf:application/pdf},
}

@inproceedings{wangDataEfficientMethodOneShot2022,
	address = {Beijing, China},
	title = {A {Data}-{Efficient} {Method} for {One}-{Shot} {Text} {Classification}},
	isbn = {978-1-66549-663-6},
	url = {https://ieeexplore.ieee.org/document/9807798/},
	doi = {10.1109/CCAI55564.2022.9807798},
	abstract = {In this paper, we propose BiGBERT (Binary Grouping BERT), a data-efficient training method for one-shot text classification. With the idea of One-vs-Rest method, we designed an extensible output layer for BERT, which can increase the usability of the training data. To evaluate our approach, we conducted extensive experiments on four celebrated text classification datasets, and reform these datasets into one-shot training scenario, which is approximately equal to the situation of our commercial datasets. The experiment result shows our approach achieves 54.9\% in 5AbstractsGroup dataset, 40.2\% in 20NewsGroup dataset, 57.0\% in IMDB dataset, and 33.6\% in TREC dataset. Overall, compare to the baseline BERT, our proposed method achieves 2.3\% {\textasciitilde} 28.6\% improved in accuracy. This result shows BiGBERT is stable and have significantly improved on one-shot text classification.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 {IEEE} 2nd {International} {Conference} on {Computer} {Communication} and {Artificial} {Intelligence} ({CCAI})},
	publisher = {IEEE},
	author = {Wang, Hsin-Yang and Liu, Mu and Yamashita, Katsushi and Okamoto, Yasuhiro and Yamada, Satoshi},
	month = may,
	year = {2022},
	pages = {76--80},
	file = {Wang et al. - 2022 - A Data-Efficient Method for One-Shot Text Classifi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GKJTERRN\\Wang et al. - 2022 - A Data-Efficient Method for One-Shot Text Classifi.pdf:application/pdf},
}

@article{dongFusionModelBasedLabel2020,
	title = {A {Fusion} {Model}-{Based} {Label} {Embedding} and {Self}-{Interaction} {Attention} for {Text} {Classification}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8908707/},
	doi = {10.1109/ACCESS.2019.2954985},
	abstract = {Text classiﬁcation is a pivotal task in NLP (Natural Language Processing), which has received widespread attention recently. Most of the existing methods leverage the power of deep learning to improve the performance of models. However, these models ignore the interaction information between all the sentences in a text when generating the current text representation, which results in a partial semantics loss. Labels play a central role in text classiﬁcation. And the attention learned from text-label in the joint space of labels and words is not leveraged, leaving enough room for further improvement. In this paper, we propose a text classiﬁcation method based on Self-Interaction attention mechanism and label embedding. Firstly, our method introduce BERT (Bidirectional Encoder Representation from Transformers) to extract text features. Then Self-Interaction attention mechanism is employed to obtain text representations containing more comprehensive semantics. Moreover, we focus on the embedding of labels and words in the joint space to achieve the dual-label embedding, which further leverages the attention learned from text-label. Finally, the texts are classiﬁed by the classiﬁer according to the weighted labels representations. The experimental results show that our method outperforms other state-of-the-art methods in terms of classiﬁcation accuracy.},
	language = {en},
	urldate = {2022-08-03},
	journal = {IEEE Access},
	author = {Dong, Yanru and Liu, Peiyu and Zhu, Zhenfang and Wang, Qicai and Zhang, Qiuyue},
	year = {2020},
	pages = {30548--30559},
	file = {Dong et al. - 2020 - A Fusion Model-Based Label Embedding and Self-Inte.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F4YBVV86\\Dong et al. - 2020 - A Fusion Model-Based Label Embedding and Self-Inte.pdf:application/pdf},
}

@inproceedings{sahaHeuristicApproachText2021,
	address = {Greater Noida, India},
	title = {A {Heuristic} {Approach} for {Text} {Classification} with {Ontology}: {A} {Review}},
	isbn = {978-1-72818-529-3},
	shorttitle = {A {Heuristic} {Approach} for {Text} {Classification} with {Ontology}},
	url = {https://ieeexplore.ieee.org/document/9397061/},
	doi = {10.1109/ICCCIS51004.2021.9397061},
	abstract = {In day to day life, mass information is readily available. So, the motive and main concern of automatic text classifiers is to segregate the relevant documents and irrelevant documents. The automated text classifier organizes the clustered data automatically. In this paper, a method for automatically classifying text has been implemented, by pre-processing the data in hand and selecting the necessary features and then classifying the data with ontology. Here, two techniques for pre-processing have been used that are stop words removal and stemming. In this paper, when the words are stemmed, it’s necessary to remove the noise from the data, which has been implemented here using feature selection technique known as univariate selection and using tools named as PyDotPlus and Graphviz, followed by ontology-based classification of documents which can be implemented in future.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {International} {Conference} on {Computing}, {Communication}, and {Intelligent} {Systems} ({ICCCIS})},
	publisher = {IEEE},
	author = {Saha, Kamalika and Rathee, Preeti and Malik, Rijul Singh and Malik, Sanjay Kumar},
	month = feb,
	year = {2021},
	pages = {137--142},
	file = {Saha et al. - 2021 - A Heuristic Approach for Text Classification with .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8KFJJKWF\\Saha et al. - 2021 - A Heuristic Approach for Text Classification with .pdf:application/pdf},
}

@inproceedings{zhangHierarchicalFineTuningBased2020,
	address = {Chengdu, China},
	title = {A {Hierarchical} {Fine}-{Tuning} {Based} {Approach} for {Multi}-label {Text} {Classification}},
	isbn = {978-1-72816-024-5},
	url = {https://ieeexplore.ieee.org/document/9095668/},
	doi = {10.1109/ICCCBDA49378.2020.9095668},
	abstract = {Hierarchical Text classification has recently become increasingly challenging with the growing number of classification labels. In this paper, we propose a hierarchical fine-tuning based approach for hierarchical text classification. We use the ordered neurons LSTM (ONLSTM) model by combining the embedding of text and parent category for hierarchical text classification with a large number of categories, which makes full use of the connection between the upper-level and lower-level labels. Extensive experiments show that our model outperforms the state-of-the-art hierarchical model at a lower computation cost.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} 5th {International} {Conference} on {Cloud} {Computing} and {Big} {Data} {Analytics} ({ICCCBDA})},
	publisher = {IEEE},
	author = {Zhang, Qiang and Chai, Bo and Song, Bochuan and Zhao, Jingpeng},
	month = apr,
	year = {2020},
	pages = {51--54},
	file = {Zhang et al. - 2020 - A Hierarchical Fine-Tuning Based Approach for Mult.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BZ8IAGZ7\\Zhang et al. - 2020 - A Hierarchical Fine-Tuning Based Approach for Mult.pdf:application/pdf},
}

@inproceedings{rajabiMultichannelBiLSTMCNNModel2020,
	address = {San Diego, CA, USA},
	title = {A {Multi}-channel {BiLSTM}-{CNN} {Model} for {Multilabel} {Emotion} {Classification} of {Informal} {Text}},
	isbn = {978-1-72816-332-1},
	url = {https://ieeexplore.ieee.org/document/9031463/},
	doi = {10.1109/ICSC.2020.00060},
	abstract = {State-of-the-art research on emotion classiﬁcation from text primarily focuses on binary or ternary classiﬁcation. Yet, humans express a variety of emotions. Here we approach the classiﬁcation of emotions from short, informal text as a multi-label problem, employing popular psychology models of basic and advanced human emotions. We account for imbalanced datasets differing in annotation schema, psychological models considered, and number of annotated emotions. We show that a multi-channel, multi-ﬁlter CNN-BiLSTM outperforms existing models, achieving 85.1\% accuracy on the multi-label SemEval18EC dataset.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} 14th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	publisher = {IEEE},
	author = {Rajabi, Zahra and Shehu, Amarda and Uzuner, Ozlem},
	month = feb,
	year = {2020},
	pages = {303--306},
	file = {Rajabi et al. - 2020 - A Multi-channel BiLSTM-CNN Model for Multilabel Em.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XT8FSZXE\\Rajabi et al. - 2020 - A Multi-channel BiLSTM-CNN Model for Multilabel Em.pdf:application/pdf},
}

@inproceedings{luoMultifeatureFusionMethod2022,
	address = {Shanghai China},
	title = {A {Multi}-feature {Fusion} {Method} with {Attention} {Mechanism} for {Long} {Text} {Classification}},
	isbn = {978-1-4503-9547-2},
	url = {https://dl.acm.org/doi/10.1145/3523089.3523093},
	doi = {10.1145/3523089.3523093},
	abstract = {As for the situation that the text content is long and contains much information irrelevant to the subject, which affects the performance of text classification. This paper proposes a multi-feature fusion method with attention mechanism for long text classification. Long text can be regarded as a hierarchical structure of sentences composed of words and paragraphs composed of sentences. Firstly, sentences are encoded and attention mechanism is introduced to aggregate into sentence level representation according to the different contributions of words. Then, based on the contribution of sentence level, aggregate the representation of growing text level. In sentence coding, based on the global target vector, convolutional neural network is used to extract the local features of words and average representation features of words, so as to further enhance the semantic representation of text. Finally, the important information features of long text content are fused and classified in the linear layer. The experimental results on manually processed THUCNews data show that the model has excellent classification performance in long text data with hierarchical structure, and the classification accuracy can reach 0.952.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 {The} 6th {International} {Conference} on {Compute} and {Data} {Analysis}},
	publisher = {ACM},
	author = {Luo, Tianyue and Liu, Yuqi and Li, Tianning},
	month = feb,
	year = {2022},
	pages = {18--23},
	file = {Luo et al. - 2022 - A Multi-feature Fusion Method with Attention Mecha.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\R87ZDJT6\\Luo et al. - 2022 - A Multi-feature Fusion Method with Attention Mecha.pdf:application/pdf},
}

@inproceedings{albqmiMultipleSVMsClassifier2020,
	address = {Melbourne, Australia},
	title = {A {Multiple} {SVMs} {Classifier} in {Three}-{Way} {Decisions} {Framework} for {Text} {Classification}},
	isbn = {978-1-66541-924-6},
	url = {https://ieeexplore.ieee.org/document/9457828/},
	doi = {10.1109/WIIAT50758.2020.00133},
	abstract = {Text classification has been recognized as an essential technique for handling and organizing text data. However, the classic text classifiers cannot clearly describe the difference between relevant and irrelevant information because of uncertainty. The underlying reason is that it is very hard for most text classifiers to explicitly describe the large uncertain boundary between two classes. A three-way decisions based framework is an interesting methodology for dealing with uncertainties in binary classification. However, it is not easy to effectively integrate the framework with a popular classifier (e.g. SVM). By integrating the distinct aspects of three-way decisions theory and the capacities of a support vector machine (SVM), a Multiple-SVMs classifier is proposed in this paper to address this issue. The proposed approach starts from the strategy of partitioning the training set into three regions, namely, positive, negative and boundary regions, to ensure the certainty of extracted knowledge for describing relevant information. Based on these three regions, an innovative and effective probabilistic feature-weighting approach has been proposed to accurately weight the representative terms. The model then organizes training samples to design a MultipleSVMs classifier which is capable of predicting the classes of each document. Experimental results on the standard datasets RCV1 and R21578 show that the proposed method significantly outperforms other state-of-the-art methods in different popular measures.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE}/{WIC}/{ACM} {International} {Joint} {Conference} on {Web} {Intelligence} and {Intelligent} {Agent} {Technology} ({WI}-{IAT})},
	publisher = {IEEE},
	author = {Albqmi, Aisha Rashed and Li, Yuefeng and Xu, Yue},
	month = dec,
	year = {2020},
	pages = {869--876},
	file = {Albqmi et al. - 2020 - A Multiple SVMs Classifier in Three-Way Decisions .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QA3I9ZB5\\Albqmi et al. - 2020 - A Multiple SVMs Classifier in Three-Way Decisions .pdf:application/pdf},
}

@inproceedings{shenNovelDeepLearningBasedModel2020,
	address = {Xiamen China},
	title = {A {Novel} {Deep}-{Learning}-{Based} {Model} for {Medical} {Text} {Classification}},
	isbn = {978-1-4503-8783-5},
	url = {https://dl.acm.org/doi/10.1145/3436369.3436469},
	doi = {10.1145/3436369.3436469},
	abstract = {In recent years, with development of the Internet hospitals and natural language processing technology, intelligent medical guidance based on machine learning has been gained increasing attentions. Medical text classification is indispensable for intelligent medical guidance. In this paper, we propose a novel deep-learning-based model named CNN-MHA-BLSTM for medical text classification. The model combines the characteristics of CNN, Multi-Head Attention and Bidirectional LSTM to capture local potential features, contextual information and contribution of each feature to the classification. We conduct numerical simulations using real dataset to verify the validation of the model. The results show that the proposed model achieves a performance with accuracy of 91.99\% and F1-score of 92.03\%, which outperforms some typical methods for text classification.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 2020 9th {International} {Conference} on {Computing} and {Pattern} {Recognition}},
	publisher = {ACM},
	author = {Shen, Zhengfei and Zhang, Shaohua},
	month = oct,
	year = {2020},
	pages = {267--273},
	file = {Shen and Zhang - 2020 - A Novel Deep-Learning-Based Model for Medical Text.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TKHYPJKC\\Shen and Zhang - 2020 - A Novel Deep-Learning-Based Model for Medical Text.pdf:application/pdf},
}

@inproceedings{husnainNovelPreprocessingTechnique2021,
	address = {Islamabad, Pakistan},
	title = {A {Novel} {Preprocessing} {Technique} for {Toxic} {Comment} {Classification}},
	isbn = {978-1-66543-293-1},
	url = {https://ieeexplore.ieee.org/document/9445252/},
	doi = {10.1109/ICAI52203.2021.9445252},
	abstract = {The threat of online abuse and harassment is increasing day by day in the cyber community. To tackle this problem, many platforms have devised policies. But these policies require prior identification of the content that is inappropriate and offensive. Furthermore, the data contains various aspects of negativity, for example, a particular piece of comment can express, disgust, disbelief, and threat at the same time. It points that even the negativity/toxicity exhibited in a comment can have various facets. Hence, the challenge is to identify what exactly is exhibited in comments so that respective policies can be formulated and applied to penalize the offender. This study makes use of two approaches to identify these underlying toxicities in the comments. The first approach is to train separate classifiers against each facet of the toxicity in comments. The second approach deals with the problem as a multi-label classification problem. Different machine learning approaches including logistic regression, Naive Bayes, and decision tree classification are employed to carry out this study. The dataset is taken from Kaggle and 10-fold cross-validation is used to report the robustness of the model. The study uses a novel preprocessing scheme that transforms the multi-label classification problem into the multi-class classification problem. The preprocessing strategy has shown a significant improvement in the accuracies when employed for simple classification models encouraging its use for other sophisticated models as well. Experimental results show that in both the binary classification and the multi-classification, logistic regression turns out to be a better performer. This indicates the potential use of the preprocessing for the neural classification models.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {International} {Conference} on {Artificial} {Intelligence} ({ICAI})},
	publisher = {IEEE},
	author = {Husnain, Muhammad and Khalid, Adnan and Shafi, Numan},
	month = apr,
	year = {2021},
	pages = {22--27},
	file = {Husnain et al. - 2021 - A Novel Preprocessing Technique for Toxic Comment .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4XJUMFI4\\Husnain et al. - 2021 - A Novel Preprocessing Technique for Toxic Comment .pdf:application/pdf},
}

@inproceedings{jeonPeerLearningMethod2021,
	address = {Jeju Island, Korea (South)},
	title = {A {Peer} {Learning} {Method} for {Building} {Robust} {Text} {Classification} {Models}},
	isbn = {978-1-72818-924-6},
	url = {https://ieeexplore.ieee.org/document/9373139/},
	doi = {10.1109/BigComp51126.2021.00069},
	abstract = {Classiﬁcation is an essential task in many practical problems. A machine learning based classiﬁcation model is built to minimize the error between actual labels and predicted labels generated by the model. When the model depends on only actual labels during the training, it can generate monotonous distributional predictions. In order to make a robust model, it needs to use other sources of information in addition to the original labels. To address this issue, we propose a peer learning method that enables the target model to reference multiple peer models and that can control the impact of peers on the target model during the training phase. The experiment results indicate that the proposed method is promising.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} and {Smart} {Computing} ({BigComp})},
	publisher = {IEEE},
	author = {Jeon, Hyun-Kyu and Cheong, Yun-Gyung},
	month = jan,
	year = {2021},
	pages = {321--324},
	file = {Jeon and Cheong - 2021 - A Peer Learning Method for Building Robust Text Cl.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CUZUCUH2\\Jeon and Cheong - 2021 - A Peer Learning Method for Building Robust Text Cl.pdf:application/pdf},
}

@inproceedings{bhavaniReviewStateArt2021,
	address = {Erode, India},
	title = {A {Review} of {State} {Art} of {Text} {Classification} {Algorithms}},
	isbn = {978-1-66540-360-3},
	url = {https://ieeexplore.ieee.org/document/9418262/},
	doi = {10.1109/ICCMC51019.2021.9418262},
	abstract = {In the recent years, the categorization of text documents into predefined classifications has perceived a growing interest due to the growing of documents in digital form and needs to organize them. Text categorization is one of the extensively used for natural language processing (NLP) applications have achieved using machine learning algorithms. Text classification is a challenging researcher to find the best suitable structure and technique. Classification process done using manual and automatic classification. This research paper covers the preprocessing, feature extraction, different algorithms and techniques for text classification and finally evaluates the performance metrics for assessment.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 5th {International} {Conference} on {Computing} {Methodologies} and {Communication} ({ICCMC})},
	publisher = {IEEE},
	author = {Bhavani, A. and Santhosh Kumar, B.},
	month = apr,
	year = {2021},
	pages = {1484--1490},
	file = {Bhavani and Santhosh Kumar - 2021 - A Review of State Art of Text Classification Algor.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GIZXZZ78\\Bhavani and Santhosh Kumar - 2021 - A Review of State Art of Text Classification Algor.pdf:application/pdf},
}

@inproceedings{gbSemanticApproachComputing2022,
	address = {Trichy, India},
	title = {A {Semantic} {Approach} for {Computing} {Speech} {Emotion} {Text} {Classification} {Using} {Machine} {Learning} {Algorithms}},
	isbn = {978-1-66543-647-2},
	url = {https://ieeexplore.ieee.org/document/9768465/},
	doi = {10.1109/ICEEICT53079.2022.9768465},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 {First} {International} {Conference} on {Electrical}, {Electronics}, {Information} and {Communication} {Technologies} ({ICEEICT})},
	publisher = {IEEE},
	author = {Gb, Shushma and Jacob, I Jeena},
	month = feb,
	year = {2022},
	pages = {1--5},
	file = {Gb and Jacob - 2022 - A Semantic Approach for Computing Speech Emotion T.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZMIV4JMS\\Gb and Jacob - 2022 - A Semantic Approach for Computing Speech Emotion T.pdf:application/pdf},
}

@inproceedings{yangShortTextSentiment2021,
	address = {Kunming, China},
	title = {A short text sentiment classification method based on feature expansion and bidirectional neural network},
	isbn = {978-1-66542-561-2},
	url = {https://ieeexplore.ieee.org/document/9516543/},
	doi = {10.1109/BDACS53596.2021.00050},
	urldate = {2022-08-03},
	booktitle = {2021 {International} {Conference} on {Big} {Data} {Analysis} and {Computer} {Science} ({BDACS})},
	publisher = {IEEE},
	author = {Yang, Changhui and Zheng, Wenguang and Xiao, Yingyuan and Dong, Chen},
	month = jun,
	year = {2021},
	pages = {195--198},
	file = {Yang et al. - 2021 - A short text sentiment classification method based.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EVN53CDG\\Yang et al. - 2021 - A short text sentiment classification method based.pdf:application/pdf},
}

@inproceedings{yangSurveyTextClassification2020,
	address = {Shanghai China},
	title = {A survey of text classification models},
	isbn = {978-1-4503-8830-6},
	url = {https://dl.acm.org/doi/10.1145/3438872.3439101},
	doi = {10.1145/3438872.3439101},
	abstract = {With the rapid development of artificial intelligence, text classification method based on deep learning model has surpassed traditional machine learning method in various aspects. This paper introduces dozens of deep learning models for text classification according to the different network structures of the models. In addition, this paper briefly introduces the evaluation indicators and application scenarios of text classification, summarizes and forecasts the current challenges and future development trend of text classification.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 2020 2nd {International} {Conference} on {Robotics}, {Intelligent} {Control} and {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Yang, JinXiong and Bai, Liang and Guo, Yanming},
	month = oct,
	year = {2020},
	pages = {327--334},
	file = {Yang et al. - 2020 - A survey of text classification models.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H2TQHUEV\\Yang et al. - 2020 - A survey of text classification models.pdf:application/pdf},
}

@article{liSurveyTextClassification2022,
	title = {A {Survey} on {Text} {Classification}: {From} {Traditional} to {Deep} {Learning}},
	volume = {13},
	issn = {2157-6904, 2157-6912},
	shorttitle = {A {Survey} on {Text} {Classification}},
	url = {https://dl.acm.org/doi/10.1145/3495162},
	doi = {10.1145/3495162},
	abstract = {Text classification is the most fundamental and essential task in natural language processing. The last decade has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches from 1961 to 2021, focusing on models from traditional models to deep learning. We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area.},
	language = {en},
	number = {2},
	urldate = {2022-08-03},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Li, Qian and Peng, Hao and Li, Jianxin and Xia, Congying and Yang, Renyu and Sun, Lichao and Yu, Philip S. and He, Lifang},
	month = apr,
	year = {2022},
	pages = {1--41},
	file = {Li et al. - 2022 - A Survey on Text Classification From Traditional .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BTLSF8LM\\Li et al. - 2022 - A Survey on Text Classification From Traditional .pdf:application/pdf},
}

@inproceedings{liuTextClassificationMethod2021,
	address = {Nanchang, China},
	title = {A {Text} {Classification} {Method} {Based} on {Graph} {Attention} {Networks}},
	isbn = {978-1-66540-099-2},
	url = {https://ieeexplore.ieee.org/document/9694328/},
	doi = {10.1109/ICITBE54178.2021.00017},
	abstract = {With the rapid generation and dissemination of information data in modern society, intelligent processing of text classiﬁcation is becoming more and more important. The Sequential and Graph-based deep learning models are often used in Natural Language Processing (NLP). The Sequential model usually uses Recurrent Neural Network (RNN), Convolutional Neural Network (CNN) and Bidirectional Encoder Representations from Transformers (BERT) The model performs natural language processing. The graph-based depth model uses the Co-occurrence relationship between texts to learn the characteristics of texts and texts for classiﬁcation. In this paper, we use RNN to preliminarily calculate the features in the text as the node of the graph, construct a graph with the help of the modiﬁcation relationship between texts, and then use the graph model to obtain the ﬁnal text features used to predict the text category. The experiment was compared with a variety of methods through a variety of data sets, and the results showed that the method in this paper achieved better results on the text data set used for emotion classiﬁcation, and the accuracy rate reached 82.03\%.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {International} {Conference} on {Information} {Technology} and {Biomedical} {Engineering} ({ICITBE})},
	publisher = {IEEE},
	author = {Liu, Yong and Gou, Xiangnan},
	month = dec,
	year = {2021},
	pages = {35--39},
	file = {Liu and Gou - 2021 - A Text Classification Method Based on Graph Attent.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2MB2R3PE\\Liu and Gou - 2021 - A Text Classification Method Based on Graph Attent.pdf:application/pdf},
}

@inproceedings{dongTextClassificationModel2022,
	address = {Tianjin China},
	title = {A {Text} {Classification} {Model} {Based} on {GCN} and {BiGRU} {Fusion}},
	isbn = {978-1-4503-9611-0},
	url = {https://dl.acm.org/doi/10.1145/3532213.3532260},
	doi = {10.1145/3532213.3532260},
	abstract = {A text classification model with the fusion of graph convolutional neural network (GCN) and bi-directional gated recurrent unit (BiGRU) is designed to address the lack of ability of simple neural networks to capture the contextual semantics of text, extract spatial feature information of text and nonlinear complex semantic relations. First, the text is preprocessed and text vectorization is performed by Word2Vec; then, the graph convolutional neural network and bi-directional gated recurrent unit are fused to form a hybrid model so that it can extract complex semantic relations and spatial feature information of the text; finally, the classification is performed by a softmax classifier. Experiments are conducted on a publicly available dataset, and the results demonstrate that the model can effectively improve the performance of text classification.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Computing} and {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Dong, Yonghao and Yang, Zhenmin and Cao, Hui},
	month = mar,
	year = {2022},
	pages = {318--322},
	file = {Dong et al. - 2022 - A Text Classification Model Based on GCN and BiGRU.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2ZS28MFJ\\Dong et al. - 2022 - A Text Classification Model Based on GCN and BiGRU.pdf:application/pdf},
}

@article{pTwostageTextFeature2021,
	title = {A {Two}-stage {Text} {Feature} {Selection} {Algorithm} for {Improving} {Text} {Classification}},
	volume = {20},
	issn = {2375-4699, 2375-4702},
	url = {https://dl.acm.org/doi/10.1145/3425781},
	doi = {10.1145/3425781},
	abstract = {As the number of digital text documents increases on a daily basis, the classification of text is becoming a challenging task. Each text document consists of a large number of words (or features) that drive down the efficiency of a classification algorithm. This article presents an optimized feature selection algorithm designed to reduce a large number of features to improve the accuracy of the text classification algorithm. The proposed algorithm uses noun-based filtering, a word ranking that enhances the performance of the text classification algorithm. Experiments are carried out on three benchmark datasets, and the results show that the proposed classification algorithm has achieved the maximum accuracy when compared to the existing algorithms. The proposed algorithm is compared to Term Frequency-Inverse Document Frequency, Balanced Accuracy Measure, GINI Index, Information Gain, and Chi-Square. The experimental results clearly show the strength of the proposed algorithm.},
	language = {en},
	number = {3},
	urldate = {2022-08-03},
	journal = {ACM Transactions on Asian and Low-Resource Language Information Processing},
	author = {P, Ashokkumar and G, Siva Shankar and Srivastava, Gautam and Maddikunta, Praveen Kumar Reddy and Gadekallu, Thippa Reddy},
	month = may,
	year = {2021},
	pages = {1--19},
	file = {P et al. - 2021 - A Two-stage Text Feature Selection Algorithm for I.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PGPX3TQS\\P et al. - 2021 - A Two-stage Text Feature Selection Algorithm for I.pdf:application/pdf},
}

@inproceedings{muACLRoBERTaCNNTextClassification2021,
	address = {Guiyang, China},
	title = {{ACL}-{RoBERTa}-{CNN} {Text} {Classification} {Model} {Combined} with {Contrastive} {Learning}},
	isbn = {978-1-66543-957-2},
	url = {https://ieeexplore.ieee.org/document/9626229/},
	doi = {10.1109/BDEE52938.2021.00041},
	abstract = {Aiming at the problem of sparse Chinese text features and mixing of long and short texts, which lead to the difﬁculty of extracting word vector features and the single convolution kernel of traditional neural network and redundant parameters, the ACL-RoBERTa-CNN text classiﬁcation model uses a contrast learning method to learn a uniformly distributed vector representation in order to achieve the effect of regular expression of space. Input the same sentence into dropout twice as “positive pairs”, replacing the traditional data enhancement method. Use the contrast learned RoBERTa pre-training model to train the word vector, send the word vector to the CNN layer, use different size convolution kernels to capture the information of different length words in each piece of data, ﬁnally combine with Softmax the classiﬁer classiﬁes the extracted features. The experimental results on two public data sets show that ACL-RoBERTa-CNN classiﬁcation performance is better than TextCNN, TextRNN, LSTM-ATT, RoBERTa-LSTM, RoBERTaCNN and other deep learning text classiﬁcation models.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {International} {Conference} on {Big} {Data} {Engineering} and {Education} ({BDEE})},
	publisher = {IEEE},
	author = {Mu, Zhibo and Zheng, Shuang and Wang, Quanmin},
	month = aug,
	year = {2021},
	pages = {193--197},
	file = {Mu et al. - 2021 - ACL-RoBERTa-CNN Text Classification Model Combined.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2XBHF6Q2\\Mu et al. - 2021 - ACL-RoBERTa-CNN Text Classification Model Combined.pdf:application/pdf},
}

@inproceedings{ozdilAdTextClassification2021,
	address = {Ankara, Turkey},
	title = {Ad {Text} {Classification} with {Bidirectional} {Encoder} {Representations}},
	isbn = {978-1-66542-908-5},
	url = {https://ieeexplore.ieee.org/document/9558966/},
	doi = {10.1109/UBMK52708.2021.9558966},
	language = {tr},
	urldate = {2022-08-03},
	booktitle = {2021 6th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	publisher = {IEEE},
	author = {Ozdil, Umut and Arslan, Busra and Tasar, Davut Emre and Polat, Gokce and Ozan, Sukru},
	month = sep,
	year = {2021},
	pages = {169--173},
	file = {Ozdil et al. - 2021 - Ad Text Classification with Bidirectional Encoder .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MHCQRB9Z\\Ozdil et al. - 2021 - Ad Text Classification with Bidirectional Encoder .pdf:application/pdf},
}

@inproceedings{liAdversarialConvolutionalNeural2020,
	address = {Xiamen China},
	title = {Adversarial {Convolutional} {Neural} {Network} for {Text} {Classification}},
	isbn = {978-1-4503-8781-1},
	url = {https://dl.acm.org/doi/10.1145/3443467.3443837},
	doi = {10.1145/3443467.3443837},
	abstract = {Text classification is a fundamental task of natural language processing. The convolutional neural network (CNN) has been employed popularly and achieved excellent results on text classification. Nevertheless, the training parameters of CNN is prone to overfitting in the training process, which limits the performance of the convolutional neural network. Adversarial training is an effective regularization method to restrain overfitting to make the model robust against the worst perturbation. In this article, adversarial training is proposed in the convolutional neural network for text classification. In our model, the perturbation is applied to the word embedding layer, not the original input. We implement training and verification on five benchmark datasets. Our experiments indicate that the classifier with adversarial training performs better resisting small perturbation and effectively controls the overfitting. We also analyzed the influence of norm constraint parameters on the classifier and make the model find the appropriate parameters.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 2020 4th {International} {Conference} on {Electronic} {Information} {Technology} and {Computer} {Engineering}},
	publisher = {ACM},
	author = {Li, Lianjie and Zhu, Zi and Du, Dongyu and Ren, Shuxia and Zheng, Yao and Chang, Guangsheng},
	month = nov,
	year = {2020},
	pages = {692--696},
	file = {Li et al. - 2020 - Adversarial Convolutional Neural Network for Text .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L7D9YSXR\\Li et al. - 2020 - Adversarial Convolutional Neural Network for Text .pdf:application/pdf},
}

@inproceedings{wangEnsembleClassificationAlgorithm2020,
	address = {Dalian, China},
	title = {An {Ensemble} {Classification} {Algorithm} for {Text} {Data} {Stream} based on {Feature} {Selection} and {Topic} {Model}},
	isbn = {978-1-72817-005-3},
	url = {https://ieeexplore.ieee.org/document/9181903/},
	doi = {10.1109/ICAICA50127.2020.9181903},
	abstract = {How to mine valuable information that users are interested in from a continuous text data stream, text data stream classification has received widespread attention as a core technology to solve the problem. This paper proposes a text data stream ensemble classification algorithm that combines feature selection and topic model. Firstly, the mutual information feature selection method is used to remove features that are not related to classification. Secondly, the LDA topic model is used to establish the document-topic distribution. Finally, the pre-processed text data stream is classified by an ensemble classification model. The experimental results show that the proposed text data stream ensemble classification algorithm can improve the classification performance of text data stream.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} {International} {Conference} on {Artificial} {Intelligence} and {Computer} {Applications} ({ICAICA})},
	publisher = {IEEE},
	author = {Wang, Zhongxin and Liu, Jianqiao and Sun, Gang and Zhao, Jia and Ding, Zhengqi and Guan, Xiaowen},
	month = jun,
	year = {2020},
	pages = {1377--1380},
	file = {Wang et al. - 2020 - An Ensemble Classification Algorithm for Text Data.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8IQ25CYB\\Wang et al. - 2020 - An Ensemble Classification Algorithm for Text Data.pdf:application/pdf},
}

@inproceedings{tzimourtasExplorationTextClassification2021,
	address = {Volos Greece},
	title = {An exploration on text classification using machine learning techniques},
	isbn = {978-1-4503-9555-7},
	url = {https://dl.acm.org/doi/10.1145/3503823.3503869},
	doi = {10.1145/3503823.3503869},
	abstract = {This paper provides an exploration of the applicability and effectiveness of Machine Learning models on text classification. In particular traditional machine learning such as Support Vector Machines, Naïve Bayes and Random Forests are scrutinized. Apart from a comparison of their performance on text classification for real-world corpora, the paper also describes and highlights the significance of other steps involved in the text classification workflow, such as preprocessing, lemmatization and use of n-grams in the discussed datasets.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {25th {Pan}-{Hellenic} {Conference} on {Informatics}},
	publisher = {ACM},
	author = {Tzimourtas, Athanasios and Bakalakos, Spyros and Tselenti, Panagiota and Voulodimos, Athanasios},
	month = nov,
	year = {2021},
	pages = {247--249},
	file = {Tzimourtas et al. - 2021 - An exploration on text classification using machin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QUELP3KM\\Tzimourtas et al. - 2021 - An exploration on text classification using machin.pdf:application/pdf},
}

@inproceedings{yanImprovedTextClassification2020,
	address = {Xiamen China},
	title = {An improved text classification method based on convolutional neural networks},
	isbn = {978-1-4503-8805-4},
	url = {https://dl.acm.org/doi/10.1145/3437802.3437833},
	doi = {10.1145/3437802.3437833},
	abstract = {To improve the classification accuracy of complaint work order text data, a deep learning-based classification method is designed. The word vector of this paper uses word2vec. Although word2vec represents the semantic richness of the words, it ignores the semantic information of the local words of the sentence. The word vector using a combination of n-gram and word2vec is both semantically rich and takes into account the local word order. In terms of the classification model, a combination of attention and CNN is used to consider both global and local features. After several sets of comparative experiments, the proposed algorithm for text classification on a company’s complaint text effectively improves the accuracy rate. The accuracy rate is better than other algorithms reaching more than 90\%.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {International} {Conference} on {Control}, {Robotics} and {Intelligent} {System}},
	publisher = {ACM},
	author = {Yan, Yan and Li, Wenya and Chen, Guanhua and Liu, Wei},
	month = oct,
	year = {2020},
	pages = {185--190},
	file = {Yan et al. - 2020 - An improved text classification method based on co.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\J4A7SEVG\\Yan et al. - 2020 - An improved text classification method based on co.pdf:application/pdf},
}

@inproceedings{zhangClinicalShortText2022,
	address = {Xi'an, China},
	title = {Clinical short text classification method based on {ALBERT} and {GAT}},
	isbn = {978-1-66547-857-1},
	url = {https://ieeexplore.ieee.org/document/9778426/},
	doi = {10.1109/ICSP54964.2022.9778426},
	abstract = {As one of the key tasks in natural language processing, the accuracy of short text classification will directly affect the performance of subsequent downstream tasks. Most of the existing short text classification algorithms usually do not understand the deep semantic information related to the medical field and lack the feature extraction of medical proprietary vocabulary. Traditional models often extract features from context information, which is limited by the lack of access to global and deep semantic information. This paper presents a short text classification algorithm for Chinese clinical medicine combining ALBERT pre-training model and graph attention network. The ALBERT pre-training model is used to represent the feature vectors of the short text of the problem, and then the graphical structure data is generated from the feature vectors. GAT is used to assign different weights to each node in a uniform neighborhood. Finally, a map-level semantic representation for category prediction is generated. The experimental results show that the accuracy of the model is 83.27\% on Chinese clinical problem datasets, which effectively improves the model performance.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 7th {International} {Conference} on {Intelligent} {Computing} and {Signal} {Processing} ({ICSP})},
	publisher = {IEEE},
	author = {Zhang, Ziyue and Jin, Li},
	month = apr,
	year = {2022},
	pages = {401--404},
	file = {Zhang and Jin - 2022 - Clinical short text classification method based on.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P3Q6A825\\Zhang and Jin - 2022 - Clinical short text classification method based on.pdf:application/pdf},
}

@inproceedings{miaoCompareMachineLearning2021,
	address = {Xiamen China},
	title = {Compare {Machine} {Learning} {Models} in {Text} {Classification} {Using} {Steam} {User} {Reviews}},
	isbn = {978-1-4503-8521-3},
	url = {https://dl.acm.org/doi/10.1145/3507473.3507480},
	doi = {10.1145/3507473.3507480},
	abstract = {Text Classification and Sentiment Analysis of game reviews are viewed as important parts in not only academic fields but also in game studies. In this paper, with more than 400 thousand game reviews on Steam platform, we preprocess the data using different libraries (sklearn, nltk, and spaCy) and use them as inputs to build three sentiment classification models based on different algorithms (Naive Bayes, SVM, and Random Forest). In contrast to previous studies that only focus on different sentiment analysis models, our paper also highlights the use of different APIs to preprocess the data and their corresponding model performance. The results show that no matter which API we choose, Random Forest models always perform the best. However, in terms of training time, Naive Bayes is the fastest. This work can be used to apply grid search for researchers to automatically find the optimum API before conducting sentiment analysis in the future.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 3rd {International} {Conference} on {Software} {Engineering} and {Development} ({ICSED})},
	publisher = {ACM},
	author = {Miao, Youchen and Jin, Zeyu and Zhang, Yumeng and Chen, Yuchen and Lai, Junren},
	month = nov,
	year = {2021},
	pages = {40--45},
	file = {Miao et al. - 2021 - Compare Machine Learning Models in Text Classifica.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\77HLPKES\\Miao et al. - 2021 - Compare Machine Learning Models in Text Classifica.pdf:application/pdf},
}

@inproceedings{solovyevaComparisonDifferentMachine2022,
	address = {Saint Petersburg, Russian Federation},
	title = {Comparison of {Different} {Machine} {Learning} {Approaches} to {Text} {Classification}},
	isbn = {978-1-66540-993-3},
	url = {https://ieeexplore.ieee.org/document/9755806/},
	doi = {10.1109/ElConRus54750.2022.9755806},
	abstract = {Naive Bayes, logistic regression, linear support vector machine, and deep neural networks are estimated and compared from the point of fulfilling text classification. The naive Bayes approach can be used in real-time with big datasets, but it assumes that each feature makes an independent and equal contribution to the outcome. Logistic regression is a linear model that is easy to implement in classification problems but has poor performance on nonlinear data. The support vector machine algorithm is a linear algorithm that performs well in higher dimensions, but it requires significant time to process. The main advantage of the deep neural network model over the other techniques is that it reduces the need for extensive feature extraction and selection and the distributed representation of words as features input into the network.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 {Conference} of {Russian} {Young} {Researchers} in {Electrical} and {Electronic} {Engineering} ({ElConRus})},
	publisher = {IEEE},
	author = {Solovyeva, Elena B. and Abdullah, Ali},
	month = jan,
	year = {2022},
	pages = {1427--1430},
	file = {Solovyeva and Abdullah - 2022 - Comparison of Different Machine Learning Approache.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3NY79NX3\\Solovyeva and Abdullah - 2022 - Comparison of Different Machine Learning Approache.pdf:application/pdf},
}

@inproceedings{xunCorrelationNetworksExtreme2020,
	address = {Virtual Event CA USA},
	title = {Correlation {Networks} for {Extreme} {Multi}-label {Text} {Classification}},
	isbn = {978-1-4503-7998-4},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403151},
	doi = {10.1145/3394486.3403151},
	abstract = {This paper develops the Correlation Networks (CorNet) architecture for the extreme multi-label text classification (XMTC) task, where the objective is to tag an input text sequence with the most relevant subset of labels from an extremely large label set. XMTC can be found in many real-world applications, such as document tagging and product annotation. Recently, deep learning models have achieved outstanding performances in XMTC tasks. However, these deep XMTC models ignore the useful correlation information among different labels. CorNet addresses this limitation by adding an extra CorNet module at the prediction layer of a deep model, which is able to learn label correlations, enhance raw label predictions with correlation knowledge and output augmented label predictions. We show that CorNet can be easily integrated with deep XMTC models and generalize effectively across different datasets. We further demonstrate that CorNet can bring significant improvements over the existing deep XMTC models in terms of both performance and convergence rate. The models and datasets are available at https://github.com/XunGuangxu/CorNet.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Xun, Guangxu and Jha, Kishlay and Sun, Jianhui and Zhang, Aidong},
	month = aug,
	year = {2020},
	pages = {1074--1082},
	file = {Xun et al. - 2020 - Correlation Networks for Extreme Multi-label Text .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VTWWCXTY\\Xun et al. - 2020 - Correlation Networks for Extreme Multi-label Text .pdf:application/pdf},
}

@inproceedings{liangCTGControllableText2021,
	address = {Beijing, China},
	title = {{CTG}:{A} {Controllable} {Text} {Generation} {Method} based on the {Joint} {Work} of {Language} {Model} and {Text} {Classifier}},
	isbn = {978-1-66540-352-8},
	shorttitle = {{CTG}},
	url = {https://ieeexplore.ieee.org/document/9445873/},
	doi = {10.1109/CISCE52179.2021.9445873},
	urldate = {2022-08-03},
	booktitle = {2021 {International} {Conference} on {Communications}, {Information} {System} and {Computer} {Engineering} ({CISCE})},
	publisher = {IEEE},
	author = {Liang, Xuyuan and Tian, Lihua and Li, Chen and Mandi, Zhang},
	month = may,
	year = {2021},
	pages = {817--820},
	file = {Liang et al. - 2021 - CTGA Controllable Text Generation Method based on.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HG3XBL56\\Liang et al. - 2021 - CTGA Controllable Text Generation Method based on.pdf:application/pdf},
}

@inproceedings{liuDeepActiveLearning2021,
	address = {Virtual Event Queensland Australia},
	title = {Deep {Active} {Learning} for {Text} {Classification} with {Diverse} {Interpretations}},
	isbn = {978-1-4503-8446-9},
	url = {https://dl.acm.org/doi/10.1145/3459637.3482080},
	doi = {10.1145/3459637.3482080},
	abstract = {Recently, Deep Neural Networks (DNNs) have made remarkable progress for text classification, which, however, still require a large number of labeled data. To train high-performing models with the minimal annotation cost, active learning is proposed to select and label the most informative samples, yet it is still challenging to measure informativeness of samples used in DNNs. In this paper, inspired by piece-wise linear interpretability of DNNs, we propose a novel Active Learning with DivErse iNterpretations (ALDEN) approach. With local interpretations in DNNs, ALDEN identifies linearly separable regions of samples. Then, it selects samples according to their diversity of local interpretations and queries their labels. To tackle the text classification problem, we choose the word with the most diverse interpretations to represent the whole sentence. Extensive experiments demonstrate that ALDEN consistently outperforms several state-of-the-art deep active learning methods.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {ACM},
	author = {Liu, Qiang and Zhu, Yanqiao and Liu, Zhaocheng and Zhang, Yufeng and Wu, Shu},
	month = oct,
	year = {2021},
	pages = {3263--3267},
	file = {Liu et al. - 2021 - Deep Active Learning for Text Classification with .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BMFRCPII\\Liu et al. - 2021 - Deep Active Learning for Text Classification with .pdf:application/pdf},
}

@inproceedings{sovranoDeepLearningBased2020,
	address = {Athens Greece},
	title = {Deep learning based multi-label text classification of {UNGA} resolutions},
	isbn = {978-1-4503-7674-7},
	url = {https://dl.acm.org/doi/10.1145/3428502.3428604},
	doi = {10.1145/3428502.3428604},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Theory} and {Practice} of {Electronic} {Governance}},
	publisher = {ACM},
	author = {Sovrano, Francesco and Palmirani, Monica and Vitali, Fabio},
	month = sep,
	year = {2020},
	pages = {686--695},
	file = {Sovrano et al. - 2020 - Deep learning based multi-label text classificatio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\W9IZRCJY\\Sovrano et al. - 2020 - Deep learning based multi-label text classificatio.pdf:application/pdf},
}

@article{zhangDeepLearningBased2020,
	title = {Deep {Learning} {Based} {Robust} {Text} {Classification} {Method} via {Virtual} {Adversarial} {Training}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9040544/},
	doi = {10.1109/ACCESS.2020.2981616},
	abstract = {The existing methods of generating adversarial texts usually change the original meanings of texts signiﬁcantly and even generate the unreadable texts. These less readable adversarial texts can misclassify the machine classiﬁer successfully, but they cannot deceive the human observers very well. In this paper, we propose a novel method that generates readable adversarial texts with some perturbations that can also confuse human observers successfully. Based on the continuous bag-of-words (CBOW) model, the proposed method looks for the appropriate perturbations to generate the adversarial texts through controlling the perturbation direction vectors. Meanwhile, we apply adversarial training to regularize the classiﬁcation model and extend it to semi-supervised tasks with virtual adversarial training. Experiments are conducted to show that the generated adversaries are interpretable and confused to humans and the virtual adversarial training effectively improves the robustness of the model.},
	language = {en},
	urldate = {2022-08-03},
	journal = {IEEE Access},
	author = {Zhang, Wei and Chen, Qian and Chen, Yunfang},
	year = {2020},
	pages = {61174--61182},
	file = {Zhang et al. - 2020 - Deep Learning Based Robust Text Classification Met.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z7CJS9MG\\Zhang et al. - 2020 - Deep Learning Based Robust Text Classification Met.pdf:application/pdf},
}

@inproceedings{liuDeepLearningExtreme2017,
	address = {Shinjuku Tokyo Japan},
	title = {Deep {Learning} for {Extreme} {Multi}-label {Text} {Classification}},
	isbn = {978-1-4503-5022-8},
	url = {https://dl.acm.org/doi/10.1145/3077136.3080834},
	doi = {10.1145/3077136.3080834},
	abstract = {Extreme multi-label text classi cation (XMTC) refers to the problem of assigning to each document its most relevant subset of class labels from an extremely large label collection, where the number of labels could reach hundreds of thousands or millions. e huge label space raises research challenges such as data sparsity and scalability. Signi cant progress has been made in recent years by the development of new machine learning methods, such as tree induction with large-margin partitions of the instance spaces and label-vector embedding in the target space. However, deep learning has not been explored for XMTC, despite its big successes in other related areas. is paper presents the rst a empt at applying deep learning to XMTC, with a family of new Convolutional Neural Network (CNN) models which are tailored for multi-label classi cation in particular. With a comparative evaluation of 7 state-of-the-art methods on 6 benchmark datasets where the number of labels is up to 670,000, we show that the proposed CNN approach successfully scaled to the largest datasets, and consistently produced the best or the second best results on all the datasets. On the Wikipedia dataset with over 2 million documents and 500,000 labels in particular, it outperformed the second best method by 11.7\% ∼ 15.3\% in precision@K and by 11.5\% ∼ 11.7\% in NDCG@K for K = 1,3,5.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 40th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Liu, Jingzhou and Chang, Wei-Cheng and Wu, Yuexin and Yang, Yiming},
	month = aug,
	year = {2017},
	pages = {115--124},
	file = {Liu et al. - 2017 - Deep Learning for Extreme Multi-label Text Classif.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EHQETRJ4\\Liu et al. - 2017 - Deep Learning for Extreme Multi-label Text Classif.pdf:application/pdf},
}

@inproceedings{erciyesDeepLearningMethods2021,
	address = {Ankara, Turkey},
	title = {Deep {Learning} {Methods} with {Pre}-{Trained} {Word} {Embeddings} and {Pre}-{Trained} {Transformers} for {Extreme} {Multi}-{Label} {Text} {Classification}},
	isbn = {978-1-66542-908-5},
	url = {https://ieeexplore.ieee.org/document/9558977/},
	doi = {10.1109/UBMK52708.2021.9558977},
	urldate = {2022-08-03},
	booktitle = {2021 6th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	publisher = {IEEE},
	author = {Erciyes, Necdet Eren and Gorur, Abdul Kadir},
	month = sep,
	year = {2021},
	pages = {50--55},
	file = {Erciyes and Gorur - 2021 - Deep Learning Methods with Pre-Trained Word Embedd.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GXR7ILCD\\Erciyes and Gorur - 2021 - Deep Learning Methods with Pre-Trained Word Embedd.pdf:application/pdf},
}

@article{minaeeDeepLearningBased2022,
	title = {Deep {Learning}--based {Text} {Classification}: {A} {Comprehensive} {Review}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Deep {Learning}--based {Text} {Classification}},
	url = {https://dl.acm.org/doi/10.1145/3439726},
	doi = {10.1145/3439726},
	abstract = {Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.},
	language = {en},
	number = {3},
	urldate = {2022-08-03},
	journal = {ACM Computing Surveys},
	author = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
	month = apr,
	year = {2022},
	pages = {1--40},
	file = {Minaee et al. - 2022 - Deep Learning--based Text Classification A Compre.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\J8MY99IA\\Minaee et al. - 2022 - Deep Learning--based Text Classification A Compre.pdf:application/pdf},
}

@inproceedings{triapitsynDesigningClassifierUnstructured2020,
	address = {Vienna, Austria},
	title = {Designing of a {Classifier} for the {Unstructured} {Text} {Formalization} {Model} {Based} on {Word} {Embedding}},
	isbn = {978-1-66540-448-8},
	url = {https://ieeexplore.ieee.org/document/9261546/},
	doi = {10.1109/EMCTECH49634.2020.9261546},
	abstract = {The active use of artificial intelligence technologies has a direct positive impact on the development of society in various areas of human life. The article describes the developed model of processing and formalization of textual unstructured information in the form of a continuous flow of text information taken from the news feed of news agencies. A method for preprocessing text to reduce the execution time of the algorithm and save CPU resources is given. A method for representing words as a real vector is formed using various algorithms for training artificial neural networks and their properties. A model of the first stage of the text information processing system as a subsystem for classifying the subject of a news article text based on a vector representation of words, including a description of the word vectorization algorithm, an example of the type of word structure with a corresponding numeric vector, and a metric that determines the proximity of vectors to each other in space. The results of the experiment are obtained and a method for setting a decision criterion for the implemented classifier is proposed. The area of use of the proposed classifier is the sphere of information security. The results of the experiment can be indicators of the suitability of using the classifier as a definition of the subject of a news article.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {International} {Conference} on {Engineering} {Management} of {Communication} and {Technology} ({EMCTECH})},
	publisher = {IEEE},
	author = {Triapitsyn, A. D. and Larin, A.I.},
	month = oct,
	year = {2020},
	pages = {1--5},
	file = {Triapitsyn and Larin - 2020 - Designing of a Classifier for the Unstructured Tex.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HX7IDHYE\\Triapitsyn and Larin - 2020 - Designing of a Classifier for the Unstructured Tex.pdf:application/pdf},
}

@inproceedings{miriEvaluationMultiLabel2022,
	address = {Bam, Iran, Islamic Republic of},
	title = {Evaluation multi label feature selection for text classification using weighted borda count approach},
	isbn = {978-1-66547-872-4},
	url = {https://ieeexplore.ieee.org/document/9756467/},
	doi = {10.1109/CFIS54774.2022.9756467},
	abstract = {Due to the existence of text data, multi-label (ML) text classification is an essential task in machine learning. Feature selection is an essential and effective preprocess to enhance the learning process. Choosing a Multi-Label Feature Selection (MLFS) algorithm is the most basic, critical, and sensitive choice in ML classification operations. If this choice is based on a criterion, it cannot be attributed to always being sound. Choosing the best algorithm must be evaluated using several different criteria to be examined from different aspects. In this article, we turn the issue into an election and use the Weighted Borda Count method for voting. We do the voting in three stages continuously so that a subset of different features does the voting. In the second stage, voting of different methods is done with six criteria, and each criterion selects the methods in order of priority from the beginning to the end. Voting steps 1 and 2 are performed on eighteen text datasets used. Finally, in the final voting stage, the methods are evaluated and voted on by different text datasets. The final result of the voting in the third stage shows the desired MLFS methods based on their performance from beginning to end. According to the experiments performed and the results obtained, it can be seen that the selection of the algorithm based on several different criteria and considering the overall performance of the algorithm will be better than the selection based on one criterion.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 9th {Iranian} {Joint} {Congress} on {Fuzzy} and {Intelligent} {Systems} ({CFIS})},
	publisher = {IEEE},
	author = {Miri, Mohsen and Dowlatshahi, Mohammad Bagher and Hashemi, Amin},
	month = mar,
	year = {2022},
	pages = {1--6},
	file = {Miri et al. - 2022 - Evaluation multi label feature selection for text .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BIRUXWHK\\Miri et al. - 2022 - Evaluation multi label feature selection for text .pdf:application/pdf},
}

@inproceedings{zubiagaExploitingClassLabels2020,
	address = {Virtual Event Ireland},
	title = {Exploiting {Class} {Labels} to {Boost} {Performance} on {Embedding}-based {Text} {Classification}},
	isbn = {978-1-4503-6859-9},
	url = {https://dl.acm.org/doi/10.1145/3340531.3417444},
	doi = {10.1145/3340531.3417444},
	abstract = {Text classification is one of the most frequent tasks for processing textual data, facilitating among others research from large-scale datasets. Embeddings of different kinds have recently become the de facto standard as features used for text classification. These embeddings have the capacity to capture meanings of words inferred from occurrences in large external collections. While they are built out of external collections, they are unaware of the distributional characteristics of words in the classification dataset at hand, including most importantly the distribution of words across classes in training data. To make the most of these embeddings as features and to boost the performance of classifiers using them, we introduce a weighting scheme, Term Frequency-Category Ratio (TF-CR), which can weight high-frequency, category-exclusive words higher when computing word embeddings. Our experiments on eight datasets show the effectiveness of TF-CR, leading to improved performance scores over the well-known weighting schemes TF-IDF and KLD as well as over the absence of a weighting scheme in most cases.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {ACM},
	author = {Zubiaga, Arkaitz},
	month = oct,
	year = {2020},
	pages = {3357--3360},
	file = {Zubiaga - 2020 - Exploiting Class Labels to Boost Performance on Em.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XLV9Y2IY\\Zubiaga - 2020 - Exploiting Class Labels to Boost Performance on Em.pdf:application/pdf},
}

@inproceedings{guanFewShotTextClassification2021,
	address = {Xia men China},
	title = {Few-{Shot} {Text} {Classification} with {External} {Knowledge} {Expansion}},
	isbn = {978-1-4503-8863-4},
	url = {https://dl.acm.org/doi/10.1145/3461353.3461389},
	doi = {10.1145/3461353.3461389},
	abstract = {The performance of most current models for text classification drops dramatically when annotated data is scarce. In such challenging scenarios, the existing models for few-shot text classification are not accurate or robust enough due to limited capture of semantic knowledge. In this paper, we propose a method of few-shot text classification based on external knowledge expansion and two strategies of expansion to supervise richer information during training and prediction, by leveraging WordNet and pre-trained model BERT. We split texts into sentences, develop techniques to select terms to semantically expand sentences based on knowledge and measure the text instance representation after knowledge expansion. In this way, we find the method is capable of improving the performance on the task of few-shot text classification. We evaluate our method on two English text classification datasets - IMDB and ASRS across a range of training set sizes. Experiment results show that by knowledge expansion, our method is robust and yields better or comparable performance to the state-of-the-art methods on both datasets, which achieves 2.7\% relative improvement compared with previous method on the ASRS test set with the training set size of 380.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 the 5th {International} {Conference} on {Innovation} in {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Guan, Jian and Xu, Rui and Ya, Jing and Tang, Qiu and Xue, Jidong and Zhang, Ni},
	month = mar,
	year = {2021},
	pages = {184--189},
	file = {Guan et al. - 2021 - Few-Shot Text Classification with External Knowled.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9B5RW4XV\\Guan et al. - 2021 - Few-Shot Text Classification with External Knowled.pdf:application/pdf},
}

@inproceedings{yangGraphConvolutionWord2022,
	address = {Haikou China},
	title = {Graph {Convolution} {Word} {Embedding} and {Attention} for {Text} {Classification}∗},
	isbn = {978-1-4503-8747-7},
	url = {https://dl.acm.org/doi/10.1145/3523150.3523175},
	doi = {10.1145/3523150.3523175},
	abstract = {Text classification is an important and classic task of natural language processing. Deep neural networks are becoming more and more popular in text classification due to their expressive power and low requirements for feature engineering. However, the more flexible graph convolutional neural network is rarely used for this task. This paper proposes a text classification model (GENET) based on graph convolution word embedding and attention mechanism. The model can better combine the semantic and lexical information of the text with the discontinuous global word co-occurrence information and long-distance semantic information in the corpus. It breaks the shortcomings of traditional neural networks that have limited structure and can only learn local information. Experimental results show that our proposed text classification method out performs other models on multiple datasets. At the same time, it is proved through experiments that the word global information obtained by GCN is an important supplement to the word embedding representation.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 {The} 6th {International} {Conference} on {Machine} {Learning} and {Soft} {Computing}},
	publisher = {ACM},
	author = {Yang, Yi and Cui, Qihui and Ji, Lijun and Cheng, Zhuoran},
	month = jan,
	year = {2022},
	pages = {160--166},
	file = {Yang et al. - 2022 - Graph Convolution Word Embedding and Attention for.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SVDJKY9J\\Yang et al. - 2022 - Graph Convolution Word Embedding and Attention for.pdf:application/pdf},
}

@article{esuliICSTotalFreedom2022,
	title = {{ICS}: {Total} {Freedom} in {Manual} {Text} {Classification} {Supported} by {Unobtrusive} {Machine} {Learning}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {{ICS}},
	url = {https://ieeexplore.ieee.org/document/9798802/},
	doi = {10.1109/ACCESS.2022.3184009},
	abstract = {We present the Interactive Classiﬁcation System (ICS), a web-based application that supports the activity of manual text classiﬁcation. The application uses machine learning to continuously ﬁt automatic classiﬁcation models that are in turn used to actively support its users with classiﬁcation suggestions. The key requirement we have established for the development of ICS is to give its users total freedom of action: they can at any time modify any classiﬁcation schema and any label assignment, possibly reusing any relevant information from previous activities. We investigate how this requirement challenges the typical scenarios faced in machine learning research, which instead give no active role to humans or place them into very constrained roles, e.g., on-demand labeling in active learning processes, and always assume some degree of batch processing of data. We satisfy the ‘‘total freedom’’ requirement by designing an unobtrusive machine learning model, i.e., the machine learning component of ICS acts as an unobtrusive observer of the users, that never interrupts them, continuously adapts and updates its models in response to their actions, and it is always available to perform automatic classiﬁcations. Our efﬁcient implementation of the unobtrusive machine learning model combines various machine learning methods and technologies, such as hash-based feature mapping, random indexing, online learning, active learning, and asynchronous processing.},
	language = {en},
	urldate = {2022-08-03},
	journal = {IEEE Access},
	author = {Esuli, Andrea},
	year = {2022},
	pages = {64741--64760},
	file = {Esuli - 2022 - ICS Total Freedom in Manual Text Classification S.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\696BX52W\\Esuli - 2022 - ICS Total Freedom in Manual Text Classification S.pdf:application/pdf},
}

@inproceedings{zhangImprovedDeepLearning2021,
	address = {Xi'an, China},
	title = {Improved deep learning model text classification},
	isbn = {978-1-66540-413-6},
	url = {https://ieeexplore.ieee.org/document/9408716/},
	doi = {10.1109/ICSP51882.2021.9408716},
	abstract = {Deep learning technology develops rapidly. Convolutional Neural Network (CNN), as a key technology in deep learning, has been favored and concerned by many scholars and widely applied in information retrieval, classification, data management, mining and other fields. In order to fully obtain the local features and key words of text, a TCNN-DAM model is proposed based on the study of TCNN model, which aims at maximizing the representation of text features, improving the text classification effect, and promoting the model to better classify in sogou news corpus. Tests show that the improved model has outstanding classification effects, which can effectively improve the accuracy, precision, recall and F1 value of classification.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 6th {International} {Conference} on {Intelligent} {Computing} and {Signal} {Processing} ({ICSP})},
	publisher = {IEEE},
	author = {Zhang, Di and Dong, Mingxing},
	month = apr,
	year = {2021},
	pages = {217--220},
	file = {Zhang and Dong - 2021 - Improved deep learning model text classification.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7K7QXH3X\\Zhang and Dong - 2021 - Improved deep learning model text classification.pdf:application/pdf},
}

@inproceedings{zhangImprovingTextClassification2021,
	address = {Xiamen, China},
	title = {Improving {Text} {Classification} {Using} {Knowledge} in {Labels}},
	isbn = {978-1-66541-513-2},
	url = {https://ieeexplore.ieee.org/document/9403092/},
	doi = {10.1109/ICBDA51983.2021.9403092},
	abstract = {Various algorithms and models have been proposed to address text classification tasks; however, they rarely consider incorporating the additional knowledge hidden in class labels. We argue that hidden information in class labels leads to better classification accuracy. In this study, instead of encoding the labels into numerical values, we incorporated the knowledge in the labels into the original model without changing the model architecture. We combined the output of an original classification model with the relatedness calculated based on the embeddings of a sequence and a keyword set. A keyword set is a word set to represent knowledge in the labels. Usually, it is generated from the classes while it could also be customized by the users. The experimental results show that our proposed method achieved statistically significant improvements in text classification tasks. The source code and experimental details of this study can be found on Github1.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {IEEE} 6th {International} {Conference} on {Big} {Data} {Analytics} ({ICBDA})},
	publisher = {IEEE},
	author = {Zhang, Cheng and Yamana, Hayato},
	month = mar,
	year = {2021},
	pages = {193--197},
	file = {Zhang and Yamana - 2021 - Improving Text Classification Using Knowledge in L.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FXP5SSFL\\Zhang and Yamana - 2021 - Improving Text Classification Using Knowledge in L.pdf:application/pdf},
}

@inproceedings{chelyshevInformationSystemAutomatic2022,
	address = {Moscow, Russian Federation},
	title = {Information {System} for {Automatic} {News} {Text} {Classification}},
	isbn = {978-1-66540-577-5},
	url = {https://ieeexplore.ieee.org/document/9782937/},
	doi = {10.1109/Inforino53888.2022.9782937},
	abstract = {In this paper, the design and development of an information system for the classification of Russian-language news texts using machine learning algorithms are considered. The information system under consideration consists of the automatic classification system and the website. The text data has been preprocessed. A number of experiments were conducted to train classifiers using grid search algorithm. Four classification methods have been tried: naive Bayesian classifier, logistic regression, random forest classifier and artificial neural network. The classification quality of the trained classifiers has been evaluated using a number of metrics: precision, recall and F-score. The website was also designed in order to provide a convenient information system usage.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 {VI} {International} {Conference} on {Information} {Technologies} in {Engineering} {Education} ({Inforino})},
	publisher = {IEEE},
	author = {Chelyshev, E. A. and Raskatova, M. V.},
	month = apr,
	year = {2022},
	pages = {1--4},
	file = {Chelyshev and Raskatova - 2022 - Information System for Automatic News Text Classif.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\U4VUY6ZT\\Chelyshev and Raskatova - 2022 - Information System for Automatic News Text Classif.pdf:application/pdf},
}

@inproceedings{mohammadiInvestigatingPerformanceFinetuned2020,
	address = {Yanuca Island, Cuvu, Fiji},
	title = {Investigating the {Performance} of {Fine}-tuned {Text} {Classification} {Models} {Based}-on {Bert}},
	isbn = {978-1-72817-649-9},
	url = {https://ieeexplore.ieee.org/document/9408054/},
	doi = {10.1109/HPCC-SmartCity-DSS50907.2020.00162},
	abstract = {Recently, deep learning has achieved impressive success in text mining and Natural Language Processing tasks. Bert is one of the remarkably rewarding deep learning models that is employed in a variety of NLP classiﬁcation tasks such as intent and topic detection, question answering, sentiment analysis, hate speech detection, and so on. Plenty of studies have implemented different models of classiﬁcation using pretrained Bert models. Fine-tuning is done by adding either a simple fully connected layer, BiLSTM, convolutional layers, or a combination of them. Each of those models has ﬁnetuned Bert for a speciﬁc task. The results do not always approve neither the efﬁciency of using complex ﬁne-tuned models of Bert nor the generalization of them. In this study, we extensively inspected various Bert-based ﬁne-tuning models for different text classiﬁcation tasks. Several types of ﬁnetuning Bert models varying in their classiﬁcation layer are implemented and the performance of them is meticulously investigated. The implemented ﬁne-tuning models are using alternatively deep learning networks such as convolutional networks and BiLSTM. The output layer of each model is studied to receive entirely varying inputs coming from the distinct layers of Bert. We conducted considerable experiments to ﬁnd the most general outperforming model. We discover that adding a simple dense layer to the pre-trained Bert model, as a classiﬁer, surpasses other types of deep neural network layers in the investigated tasks. We examine different values of hyperparameters to ﬁnd the optimized combination providing the highest performance.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} 22nd {International} {Conference} on {High} {Performance} {Computing} and {Communications}; {IEEE} 18th {International} {Conference} on {Smart} {City}; {IEEE} 6th {International} {Conference} on {Data} {Science} and {Systems} ({HPCC}/{SmartCity}/{DSS})},
	publisher = {IEEE},
	author = {Mohammadi, Samin and Chapon, Mathieu},
	month = dec,
	year = {2020},
	pages = {1252--1257},
	file = {Mohammadi and Chapon - 2020 - Investigating the Performance of Fine-tuned Text C.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\W3LUUUG6\\Mohammadi and Chapon - 2020 - Investigating the Performance of Fine-tuned Text C.pdf:application/pdf},
}

@inproceedings{guoLabelAwareTextRepresentation2021,
	address = {Toronto, ON, Canada},
	title = {Label-{Aware} {Text} {Representation} for {Multi}-{Label} {Text} {Classification}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9413921/},
	doi = {10.1109/ICASSP39728.2021.9413921},
	abstract = {Multi-label text classiﬁcation (MLTC) is an important task in natural language processing (NLP), which is appealing to researchers in both academia and industry. However, few of studies have been conducted on the relations among the labels. Most of existing methods tend to neglect the semantic information between labels and words. In this paper, we propose a label-aware network to obtain both the label correlation and text representation. A heterogeneous graph is built from words and labels to learn the label representation by metapath2vec, since two nearby labels or words in the graph have similar relation and the graph structure is beneﬁcial for label representation as well. Each part of the text contributes differently to label inference, therefore bidirectional attention ﬂow is exploited for label-aware text representation in two directions: from text to label and from label to text. Experimental evaluations illustrate that the proposed method outperforms various baselines on both ofﬂine benchmarks and real-world online systems.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Guo, Hao and Li, Xiangyang and Zhang, Lei and Liu, Jia and Chen, Wei},
	month = jun,
	year = {2021},
	pages = {7728--7732},
	file = {Guo et al. - 2021 - Label-Aware Text Representation for Multi-Label Te.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YB29KT29\\Guo et al. - 2021 - Label-Aware Text Representation for Multi-Label Te.pdf:application/pdf},
}

@inproceedings{wangLabelBasedConvolutionalNeural2021,
	address = {Sanya China},
	title = {Label-{Based} {Convolutional} {Neural} {Network} for {Text} {Classification}},
	isbn = {978-1-4503-8887-0},
	url = {https://dl.acm.org/doi/10.1145/3448218.3448235},
	doi = {10.1145/3448218.3448235},
	abstract = {The neural network models based on word embedding have achieved remarkable results in text classification. Even so, these models hardly consider that the importance of each word and labels for text classification is beneficial to obtain informative text representation. The attention mechanisms usually are used to measure the weights of words to improve predictive performance, but we attempt to achieve the same goal in a simple way. Since that word embedding can capture semantic regularities between words. we introduce a text representation based on label by embedding each label and the word vectors in the same space in this paper. In this label-based text representation, each word has weight information of the number of classes, which play an important role in the final performance. So we proposed a labelbased convolutional neural network (LBCNN) to obtain the importance of different word in the label-based text sequence and the most influential semantic features in the word vector respectively. The experimental results show that our proposed method outperforms the state-of-art methods on the several large text classification datasets.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Control} {Engineering} and {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Wang, Chen and Tan, Changgeng},
	month = jan,
	year = {2021},
	pages = {136--140},
	file = {Wang and Tan - 2021 - Label-Based Convolutional Neural Network for Text .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PV7IMP74\\Wang and Tan - 2021 - Label-Based Convolutional Neural Network for Text .pdf:application/pdf},
}

@inproceedings{weijieLongTextClassification2021,
	address = {Xi'an, China},
	title = {Long {Text} {Classification} {Based} on {BERT}},
	isbn = {978-1-66541-599-6},
	url = {https://ieeexplore.ieee.org/document/9587007/},
	doi = {10.1109/ITNEC52019.2021.9587007},
	abstract = {Existing text classification algorithms generally have limitations in terms of text length and yield poor classification results for long texts. To address this problem, we propose a BERT-based long text classification method. First, we slice the long text and use BERT to encode the sliced clauses to obtain the local semantic information. Second, we use BiLSTM to fuse the local semantic information and adopt the attention mechanism to increase the weight of important clauses in the long text, so as to obtain the global semantic information. Finally, the global semantic information is input to the softmax layer for classification. Experimental results show that the proposed method achieves higher accuracy than commonly used models.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {IEEE} 5th {Information} {Technology},{Networking},{Electronic} and {Automation} {Control} {Conference} ({ITNEC})},
	publisher = {IEEE},
	author = {Weijie, Ding and Yunyi, Li and Jing, Zhang and Xuchen, Shen},
	month = oct,
	year = {2021},
	pages = {1147--1151},
	file = {Weijie et al. - 2021 - Long Text Classification Based on BERT.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\573EK7VA\\Weijie et al. - 2021 - Long Text Classification Based on BERT.pdf:application/pdf},
}

@article{moreoLostTransductionTransductive2022,
	title = {Lost in {Transduction}: {Transductive} {Transfer} {Learning} in {Text} {Classification}},
	volume = {16},
	issn = {1556-4681, 1556-472X},
	shorttitle = {Lost in {Transduction}},
	url = {https://dl.acm.org/doi/10.1145/3453146},
	doi = {10.1145/3453146},
	abstract = {Obtaining high-quality labelled data for training a classifier in a new application domain is often costly.
              Transfer Learning
              (a.k.a. “Inductive Transfer”) tries to alleviate these costs by transferring, to the “target” domain of interest, knowledge available from a different “source” domain. In transfer learning the lack of labelled information from the target domain is compensated by the availability at training time of a set of unlabelled examples from the target distribution.
              Transductive Transfer Learning
              denotes the transfer learning setting in which the only set of target documents that we are interested in classifying is known and available at training time. Although this definition is indeed in line with Vapnik’s original definition of “transduction”, current terminology in the field is confused. In this article, we discuss how the term “transduction” has been misused in the transfer learning literature, and propose a clarification consistent with the original characterization of this term given by Vapnik. We go on to observe that the above terminology misuse has brought about misleading experimental comparisons, with inductive transfer learning methods that have been incorrectly compared with transductive transfer learning methods. We then, give empirical evidence that the difference in performance between the inductive version and the transductive version of a transfer learning method can indeed be statistically significant (i.e., that knowing at training time the only data one needs to classify indeed gives an advantage). Our clarification allows a reassessment of the field, and of the relative merits of the major, state-of-the-art algorithms for transfer learning in text classification.},
	language = {en},
	number = {1},
	urldate = {2022-08-03},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Moreo, Alejandro and Esuli, Andrea and Sebastiani, Fabrizio},
	month = feb,
	year = {2022},
	pages = {1--21},
	file = {Moreo et al. - 2022 - Lost in Transduction Transductive Transfer Learni.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KNA6FGJA\\Moreo et al. - 2022 - Lost in Transduction Transductive Transfer Learni.pdf:application/pdf},
}

@inproceedings{batistadossantosMetalearningAppliedMultilabel2020,
	address = {São Bernardo do Campo Brazil},
	title = {Metalearning {Applied} to {Multi}-label {Text} {Classification}},
	isbn = {978-1-4503-8873-3},
	url = {https://dl.acm.org/doi/10.1145/3411564.3411646},
	doi = {10.1145/3411564.3411646},
	abstract = {Data Mining and Machine Learning fields have many techniques that can support data analysts in the text classification task. However, finding the most adequate techniques require advanced technical knowledge, exhaustive computational experiments and, consequently, time. To address this issue, researchers have proposed different approaches for selecting such techniques to be employed in classification tasks and the dynamic selection of classifiers is one of them. Therefore, this work proposes an approach that uses metalearning to automate the process of selecting the best classifier for each instance of a given multi-label textual dataset. Experiments were performed with multi-label text datasets and showed that the proposed approach is promising.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {{XVI} {Brazilian} {Symposium} on {Information} {Systems}},
	publisher = {ACM},
	author = {Batista dos Santos, Vânia and Merschmann, Luiz Henrique de Campos},
	month = nov,
	year = {2020},
	pages = {1--8},
	file = {Batista dos Santos and Merschmann - 2020 - Metalearning Applied to Multi-label Text Classific.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EHRWQK2Y\\Batista dos Santos and Merschmann - 2020 - Metalearning Applied to Multi-label Text Classific.pdf:application/pdf},
}

@inproceedings{zhangMicroblogTextClassification2020,
	address = {Shenyang, China},
	title = {Microblog {Text} {Classification} {System} {Based} on {TextCNN} and {LSA} {Model}},
	isbn = {978-1-72818-575-0},
	url = {https://ieeexplore.ieee.org/document/9363816/},
	doi = {10.1109/ISCTT51595.2020.00090},
	urldate = {2022-08-03},
	booktitle = {2020 5th {International} {Conference} on {Information} {Science}, {Computer} {Technology} and {Transportation} ({ISCTT})},
	publisher = {IEEE},
	author = {Zhang, Weiyu and Xu, Can},
	month = nov,
	year = {2020},
	pages = {469--474},
	file = {Zhang and Xu - 2020 - Microblog Text Classification System Based on Text.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XPLVWQWL\\Zhang and Xu - 2020 - Microblog Text Classification System Based on Text.pdf:application/pdf},
}

@inproceedings{alsukhniMultiLabelArabicText2021,
	address = {Valencia, Spain},
	title = {Multi-{Label} {Arabic} {Text} {Classification} {Based} {On} {Deep} {Learning}},
	isbn = {978-1-66543-351-8},
	url = {https://ieeexplore.ieee.org/document/9464538/},
	doi = {10.1109/ICICS52457.2021.9464538},
	abstract = {Multi-label text classification is a natural extension of text classification in which each document can be assigned with a possible widespread set of labels. Natural Language Processing (NLP) helps to understand and manipulate text in natural language by using the computer. Arabic Text Classification is challenging recently because the Arabic language is under-resourced although it has many users. The aim of this paper is to build a model to classify Arabic news and help users get and display the most relevant news to their interests. In this paper, we demonstrate the efficiency of using deep learning models in solving Arabic multi-label text classification problem. Multilayer Perceptron (MLP) and Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) have been used; we build two models via python. All data has been cleaned to improve the quality of experimental data. The result of test data in LSTM was 82.03 whereas in the MLP model was 80.37, and both models were evaluated using F1 score.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 12th {International} {Conference} on {Information} and {Communication} {Systems} ({ICICS})},
	publisher = {IEEE},
	author = {alsukhni, Batool},
	month = may,
	year = {2021},
	pages = {475--477},
	file = {alsukhni - 2021 - Multi-Label Arabic Text Classification Based On De.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9XF2HP9P\\alsukhni - 2021 - Multi-Label Arabic Text Classification Based On De.pdf:application/pdf},
}

@inproceedings{panMultilabelClassificationClinical2020,
	address = {Baltimore, MD, USA},
	title = {Multi-label {Classification} for {Clinical} {Text} with {Feature}-level {Attention}},
	isbn = {978-1-72816-873-9},
	url = {https://ieeexplore.ieee.org/document/9123057/},
	doi = {10.1109/BigDataSecurity-HPSC-IDS49724.2020.00042},
	abstract = {Multi-label text classiﬁcation, which tags a given plain text with the most relevant labels from a label space, is an important task in the natural language process. To diagnose diseases, clinical researchers use a machine-learning algorithm to do multi-label clinical text classiﬁcation. However, conventional machine learning methods can neither capture deep semantic information nor the context of words strictly. Diagnostic information from the EHRs (Electronic Health Records) is mainly constructed by unstructured clinical free text which is an obstacle for clinical feature extraction. Moreover, feature engineering is time-consuming and labor-intensive. With the rapid development of deep learning, we apply neural network models to resolve this problem mentioned above. To favor multi-label classiﬁcation on EHRs, we propose FAMLC-BERT (Feature-level Attention for Multi-label classiﬁcation on BERT) to capture semantic features from different layers. The model uses feature-level attention with BERT to recognize the labels of EHRs. We empirically compared our model with other state-of-the-art models on realworld documents collected from the hospital. Experiments show that our model achieved signiﬁcant improvements compared to other selected benchmarks.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} 6th {Intl} {Conference} on {Big} {Data} {Security} on {Cloud} ({BigDataSecurity}), {IEEE} {Intl} {Conference} on {High} {Performance} and {Smart} {Computing}, ({HPSC}) and {IEEE} {Intl} {Conference} on {Intelligent} {Data} and {Security} ({IDS})},
	publisher = {IEEE},
	author = {Pan, Disheng and Zheng, Xizi and Liu, Weijie and Li, Mengya and Ma, Meng and Zhou, Ying and Yang, Li and Wang, Ping},
	month = may,
	year = {2020},
	pages = {186--191},
	file = {Pan et al. - 2020 - Multi-label Classification for Clinical Text with .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HNSVTNTJ\\Pan et al. - 2020 - Multi-label Classification for Clinical Text with .pdf:application/pdf},
}

@inproceedings{liMultilabelTextClassification2022,
	address = {Guangzhou China},
	title = {Multi-label text classification via hierarchical {Transformer}-{CNN}},
	isbn = {978-1-4503-9570-0},
	url = {https://dl.acm.org/doi/10.1145/3529836.3529912},
	doi = {10.1145/3529836.3529912},
	abstract = {Traditional multi-label text classification methods, especially deep learning, have achieved remarkable results, but most of these methods use the word2vec technique to represent continuous text information, which fails to fully capture the semantic information of the text. To solve this problem, we built a hierarchical TransformerCNN model and applied it in multi-label classification. Taking into account the characteristics of natural language, a hierarchical Transformer-CNN model is constructed to capture the semantic information of different levels of the text at the word and sentence levels using multi-headed self-attention mechanism, and a sentence convolutional neural network was used to extract key semantic features. For the hierarchical Transformer-CNN model we proposed, sufficient experiments have been conducted on the RCV1 and AAPD data sets to verify the model’s effectiveness.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 14th {International} {Conference} on {Machine} {Learning} and {Computing} ({ICMLC})},
	publisher = {ACM},
	author = {Li, Junzhe and Wang, Chenglong and Fang, Xiaohan and Yu, Kai and Zhao, Jinye and Wu, Xi and Gong, Jibing},
	month = feb,
	year = {2022},
	pages = {120--125},
	file = {Li et al. - 2022 - Multi-label text classification via hierarchical T.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TP9J9REV\\Li et al. - 2022 - Multi-label text classification via hierarchical T.pdf:application/pdf},
}

@inproceedings{yuMultilabelTextClassification2021,
	address = {Shanghai China},
	title = {Multi-label {Text} {Classification} with {Label} {Correction} under {Noise}},
	isbn = {978-1-4503-9043-9},
	url = {https://dl.acm.org/doi/10.1145/3497623.3497650},
	doi = {10.1145/3497623.3497650},
	abstract = {Multi-label text classification (MLTC) is a fundamental but difficult problem in text mining, the goal of MLTC is to assign a set of most relevant labels for the given document. While existing supervised training of deep learning models for MLTC usually requires a large number of noise-free labeled samples, which is quite expensive and time-consuming or even impractical in the real-world as label annotations are inevitable error-prone. To handle such a situation, we introduce learning multi-label text classification with noisy labels, and propose an end-to-end method called Multi-label Text Classification with Label Correction under Noise (LCN). LCN contains two modules: a label correction module and a classification module. In the label correction module, a group of prototypes for each class is learnt with the help of label semantic and feature information. These prototypes are then used to calculate the similarity between the extracted deep features to correct the labels of each training sample. In the classification module, the classifier combines the original noisy labels and corrected labels of each sample as supervised information to guide the training procedure. The two modules are combined in a unified framework and trained in an alternative manner. Extensive experimental results on two multi-label text benchmark datasets validate the effectiveness of LCN and show its advantages to the state-of-art methods.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 10th {International} {Conference} on {Computing} and {Pattern} {Recognition}},
	publisher = {ACM},
	author = {Yu, Tingting and Li, Tao and Wang, Xiaomeng},
	month = oct,
	year = {2021},
	pages = {169--174},
	file = {Yu et al. - 2021 - Multi-label Text Classification with Label Correct.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TXX8EMW7\\Yu et al. - 2021 - Multi-label Text Classification with Label Correct.pdf:application/pdf},
}

@inproceedings{linMultilevelFeatureFusion2022,
	address = {Guangzhou China},
	title = {Multi-level {Feature} {Fusion} {Method} for {Long} {Text} {Classification}},
	isbn = {978-1-4503-9570-0},
	url = {https://dl.acm.org/doi/10.1145/3529836.3529938},
	doi = {10.1145/3529836.3529938},
	abstract = {News classification task is essentially long text classification in the field of NLP (Natural Language Processing). Long text contains a lot of hidden or topic-independent information. Moreover, BERT (Bidirectional Encoder Representations from Transformer) can only process the text with a character sequence length of 512 at most, which may lose the key information and reduce the classification effectiveness. To solve above problems, the paper puts forward a model of mutli-level feature fusion based on BERT, which is suitable for the BERT through the hierarchical decomposition of long text. Then CNN (Convolutional Neural Networks) and stacked BiLSTM (Bidirectional Long Short-term Memory) based on attention mechanism are used to capture local and contextual features of text respectively. Finally, various features are spliced for classification task. The experimental results show that the model achieves 97.4\% accuracy and 97.2\% F1 score on THUCNews, 1.2\% accuracy and 1.6\% F1 score higher than that of BERT-CNN, 1.8\% accuracy and 1.4\% F1 score higher than that of BERT-BiLSTM, indicating that our model can significantly improve the effectiveness of news classification.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 14th {International} {Conference} on {Machine} {Learning} and {Computing} ({ICMLC})},
	publisher = {ACM},
	author = {Lin, RuiMing and Cheng, LiangLun and Deng, JianFeng and Wang, Tao},
	month = feb,
	year = {2022},
	pages = {532--538},
	file = {Lin et al. - 2022 - Multi-level Feature Fusion Method for Long Text Cl.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GZTCCRPE\\Lin et al. - 2022 - Multi-level Feature Fusion Method for Long Text Cl.pdf:application/pdf},
}

@inproceedings{zhangNBiLSTMBiLSTMNgram2020,
	address = {Chongqing, China},
	title = {n-{BiLSTM}: {BiLSTM} with n-gram {Features} for {Text} {Classification}},
	isbn = {978-1-72814-323-1},
	shorttitle = {n-{BiLSTM}},
	url = {https://ieeexplore.ieee.org/document/9141692/},
	doi = {10.1109/ITOEC49072.2020.9141692},
	abstract = {Text classification is widely existing in the fields of e-commerce and log message analysis. Besides, it is an essential module in text processing tasks. In this paper, we present a method to create an accurate and fast text classification system in both One-vs.-one and One-vs.-rest manner. Our approach, named n-BiLSTM, is used to convert natural text sentences into features similar to bag-of-words with n-gram techniques, and then the features are fed into a bidirectional LSTM. The two components are able to take better advantages of multi-scale feature representation and context information. Finally, the whole system is evaluated using two labeled movie review datasets, IMDB and SSTb, to test one-vs.-one and one-vs.-rest performances respectively. The results obtained show that our nBiLSTM algorithm is superior to the basic LSTM and bidirectional LSTM algorithms.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} 5th {Information} {Technology} and {Mechatronics} {Engineering} {Conference} ({ITOEC})},
	publisher = {IEEE},
	author = {Zhang, Yunxiang and Rao, Zhuyi},
	month = jun,
	year = {2020},
	pages = {1056--1059},
	file = {Zhang and Rao - 2020 - n-BiLSTM BiLSTM with n-gram Features for Text Cla.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JAHBVDCS\\Zhang and Rao - 2020 - n-BiLSTM BiLSTM with n-gram Features for Text Cla.pdf:application/pdf},
}

@inproceedings{depingNewsTextClassification2021,
	address = {Xi'an, China},
	title = {News text classification based on {Bidirectional} {Encoder} {Representation} from {Transformers}},
	isbn = {978-1-66542-490-5},
	url = {https://ieeexplore.ieee.org/document/9545942/},
	doi = {10.1109/CAIBDA53561.2021.00036},
	abstract = {In order to accurately and efficiently obtain information useful to us, people are paying more and more attention to the problem of data redundancy caused by excessive data information. In recent years, domestic and foreign researchers have proposed various frameworks for different natural language processing tasks, and different frameworks have different advantages and disadvantages. One of the classic problems in the field of natural language processing is text classification. News text classification is an important task that is easy to attract everyone's attention in our daily lives. This experiment is based on the BERT model under the Transformer framework to classify the news text data set. The same news text data set is compared with the RNN's long and short-term memory network. The evaluation index uses the general accuracy and loss value of the model classification. Experimental results show that the classification accuracy of the BERT model is significantly higher than that of the long and short-term memory network.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {International} {Conference} on {Artificial} {Intelligence}, {Big} {Data} and {Algorithms} ({CAIBDA})},
	publisher = {IEEE},
	author = {Deping, Lin and Hongjuan, Wang and Mengyang, Liu and Pei, Li},
	month = may,
	year = {2021},
	pages = {137--140},
	file = {Deping et al. - 2021 - News text classification based on Bidirectional En.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6AC2F57L\\Deping et al. - 2021 - News text classification based on Bidirectional En.pdf:application/pdf},
}

@article{steurNextGenerationNeuralNetworks2021,
	title = {Next-{Generation} {Neural} {Networks}: {Capsule} {Networks} {With} {Routing}-by-{Agreement} for {Text} {Classification}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Next-{Generation} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/9530541/},
	doi = {10.1109/ACCESS.2021.3110911},
	abstract = {These days, neural networks constantly prove their high capacity for nearly every application case and are considered as key technology for learning systems. However, neural networks need to continuously evolve for managing new arising challenges like increasing task complexity, explainability of decision making processes, expanded problem domains, providing resilient and robust systems etc. One possible enhancement of traditional neural networks constitutes the innovative Capsule Network (CapsNet) technology, which combines the expressiveness of distributed entity representations with an intelligent and interpretable signal propagation, named as routing-by-agreement. Since CapsNets represent a relatively young acquirement, further research is essential for gaining profound knowledge about CapsNet theory and best practices for diverse application areas. This paper wants to contribute to the progress of CapsNets for the task of text classiﬁcation. For this purpose, various research questions about this technology get formulated and experimentally answered with the aid of six selected datasets. In addition, this paper serves as a possible starting point for researchers as well as for practitioners to deal with CapsNets in the text domain, by supplying a survey about its theory, text classiﬁcation basics and the combination of both areas. The analysis results empirically prove the robustness of CapsNets with routing-by-agreement for a wide spectrum of net architectures, datasets and text classiﬁcation tasks. Hence, CapsNets can be viewed as a next-generation neural network technology, which offers high potential as text classiﬁcation method and should be topic of future research.},
	language = {en},
	urldate = {2022-08-03},
	journal = {IEEE Access},
	author = {Steur, Nikolai A. K. and Schwenker, Friedhelm},
	year = {2021},
	pages = {125269--125299},
	file = {Steur and Schwenker - 2021 - Next-Generation Neural Networks Capsule Networks .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JIEZGYBN\\Steur and Schwenker - 2021 - Next-Generation Neural Networks Capsule Networks .pdf:application/pdf},
}

@inproceedings{suneeraPerformanceAnalysisMachine2020,
	address = {New Delhi, India},
	title = {Performance {Analysis} of {Machine} {Learning} and {Deep} {Learning} {Models} for {Text} {Classification}},
	isbn = {978-1-72816-916-3},
	url = {https://ieeexplore.ieee.org/document/9342208/},
	doi = {10.1109/INDICON49873.2020.9342208},
	abstract = {Text classiﬁcation is the task of forming semantic groups of text documents by assigning predeﬁned class labels. It has wide range of real-life applications in various domains such as engineering, medical science, life science, social sciences and humanities, marketing, governance. Currently, machine learning and deep learning algorithms became popular and effective methods to address text classiﬁcation problems with labelled data. In this work, we analyse the performance of different machine learning and deep learning algorithms for text classiﬁcation. For this purpose, we selected six machine learning algorithms using three different vectorization techniques and ﬁve deep learning algorithms for the performance evaluation which is evaluated based on classiﬁcation accuracy. All experiments are conducted on the 20 newsgroups dataset. Results indicate that Logistic Regression outperforms over other ML algorithms and a Bichannel Convolution Neural Network model gains exciting results compared to other deep learning models.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} 17th {India} {Council} {International} {Conference} ({INDICON})},
	publisher = {IEEE},
	author = {Suneera, C M and Prakash, Jay},
	month = dec,
	year = {2020},
	pages = {1--6},
	file = {Suneera and Prakash - 2020 - Performance Analysis of Machine Learning and Deep .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NUVB43AH\\Suneera and Prakash - 2020 - Performance Analysis of Machine Learning and Deep .pdf:application/pdf},
}

@inproceedings{zhangPopulationBasedTraining2021,
	address = {Kyoto, Japan},
	title = {Population {Based} {Training} for {Text} {Classification} {Using} {Convolutional} {Neural} {Network}},
	isbn = {978-1-66543-676-2},
	url = {https://ieeexplore.ieee.org/document/9622057/},
	doi = {10.1109/GCCE53005.2021.9622057},
	abstract = {We propose Population Based Training (PBT) to dynamically adjust hyperparameters during the training of the network. Experiments on text classification show that the adapted hyperparameters provided by PBT have significant advantages over traditional fixed set of hyperparameters.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {IEEE} 10th {Global} {Conference} on {Consumer} {Electronics} ({GCCE})},
	publisher = {IEEE},
	author = {Zhang, Xie and Li, Mo and Yamane, Satoshi},
	month = oct,
	year = {2021},
	pages = {77--79},
	file = {Zhang et al. - 2021 - Population Based Training for Text Classification .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KPTR8MIA\\Zhang et al. - 2021 - Population Based Training for Text Classification .pdf:application/pdf},
}

@inproceedings{yuanResearchWeiboText2021,
	address = {Chongqing, China},
	title = {Research of {Weibo} {Text} {Classification} based on {Knowledge} {Distillation} and {Joint} {Model}},
	isbn = {978-1-72818-028-1},
	url = {https://ieeexplore.ieee.org/document/9390761/},
	doi = {10.1109/IAEAC50856.2021.9390761},
	abstract = {Text classification is a basic task in natural language processing. In 2018, the BERT(Bidirectional Encoder Representation from Transformers) was proposed. This model greatly improves the effect of natural language processing related tasks. However, the model size of the pre-trained language model is large, and because of its huge network structure, the predict time is long. In order to solve these problems, an improved model that uses knowledge distillation and adversarial perturbation is proposed. In the training phase, RoBERTa-wwm-ext is used as the teacher model, the joint model of Text-CNN and Text-RCNN is used as a student model, combined with label smoothing and adversarial perturbation method to improve the classification accuracy of the student model, compared with Text-CNN on two datasets, is improved by 1.91\% and 1.21\% respectively. Using the student model to classify texts has the advantages of easy for deployment and taking less prediction time while obtaining ideal accuracy.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {IEEE} 5th {Advanced} {Information} {Technology}, {Electronic} and {Automation} {Control} {Conference} ({IAEAC})},
	publisher = {IEEE},
	author = {Yuan, Zhengwu and Peng, Xiankang},
	month = mar,
	year = {2021},
	pages = {202--207},
	file = {Yuan and Peng - 2021 - Research of Weibo Text Classification based on Kno.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\322CAI9F\\Yuan and Peng - 2021 - Research of Weibo Text Classification based on Kno.pdf:application/pdf},
}

@inproceedings{zhonghaoResearchIntelligentClassification2022,
	address = {Beijing, China},
	title = {Research on {Intelligent} {Classification} {Method} of {Seismic} {Information} {Text} {Based} on {BERT}-{BiLSTM} {Optimization} {Algorithm}},
	isbn = {978-1-66549-663-6},
	url = {https://ieeexplore.ieee.org/document/9807785/},
	doi = {10.1109/CCAI55564.2022.9807785},
	abstract = {With the development of science and technology, it is possible to quickly obtain massive disaster information after the earthquake, but because the earthquake information not only has the characteristics of strong timeliness, but also is always in the process of dynamic change, it can quickly classify and analyze the earthquake information, which is of great significance for earthquake emergency decision-making. In this paper, an earthquake news text intelligent classification model based on the BERT-BiLSTM optimization algorithm is proposed. First, based on the BERT (Bidirectional Encoder Representation from Transformers) pre-trained model, the algorithm performs a sentence-level feature vector representation of the seismic news text, and enters the feature vector into the BiLSTM layer to extract the global features of the seismic news text, and then enters the SoftMax classifier for classification. Finally, the control experiment of earthquake news text data in Qinghai and Yunnan was passed. Experimental results show that the model is improved by 2 percentage points compared with the traditional Bert model method. Therefore, the intelligent classification model of earthquake information text proposed in this paper can effectively and accurately determine the category of earthquake news and help earthquake emergency rescue decision-making.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 {IEEE} 2nd {International} {Conference} on {Computer} {Communication} and {Artificial} {Intelligence} ({CCAI})},
	publisher = {IEEE},
	author = {Zhonghao, Wang and Chenxi, Li and Meng, Huang and Shuai, Liu},
	month = may,
	year = {2022},
	pages = {55--59},
	file = {Zhonghao et al. - 2022 - Research on Intelligent Classification Method of S.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZTCK8LDM\\Zhonghao et al. - 2022 - Research on Intelligent Classification Method of S.pdf:application/pdf},
}

@inproceedings{yuResearchNewsText2021,
	address = {Manchester United Kingdom},
	title = {Research on {News} {Text} {Classification} {Based} on {Hybrid} {Model}},
	isbn = {978-1-4503-8504-6},
	url = {https://dl.acm.org/doi/10.1145/3495018.3495372},
	doi = {10.1145/3495018.3495372},
	abstract = {With the rapid development and wide application of information technology and Internet, the amount of various information data is growing explosively. News text information is a more extensive form of text information that people have access to. Using computer to screen and classify these news information effectively can quickly and efficiently obtain valuable information content. With the development of machine learning, text classification has been gradually transferred from manual operation to machine automation. It is a basic step of machine learning to use the classified text information to learn the category features and then classify the unclassified text. Text automatic classification method are many, for the improvement of text automatic classification algorithm, its purpose lies in how to closer to the human way of thinking on the text information classification, the classification results of this can better meet the needs of people for text classification, also convenient and rapid access to valuable information. In this paper, a hybrid model is used to achieve better classification results.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 3rd {International} {Conference} on {Artificial} {Intelligence} and {Advanced} {Manufacture}},
	publisher = {ACM},
	author = {Yu, Min and Liu, Yian},
	month = oct,
	year = {2021},
	pages = {1234--1239},
	file = {Yu and Liu - 2021 - Research on News Text Classification Based on Hybr.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6E4XMGEU\\Yu and Liu - 2021 - Research on News Text Classification Based on Hybr.pdf:application/pdf},
}

@inproceedings{ruiResearchShortText2020,
	address = {Shanghai, China},
	title = {Research on {Short} {Text} {Classification} {Based} on {Word2Vec} {Microblog}},
	isbn = {978-1-72818-668-9},
	url = {https://ieeexplore.ieee.org/document/9444008/},
	doi = {10.1109/ICCSMT51754.2020.00042},
	urldate = {2022-08-03},
	booktitle = {2020 {International} {Conference} on {Computer} {Science} and {Management} {Technology} ({ICCSMT})},
	publisher = {IEEE},
	author = {Rui, Zhang and Yutai, Han},
	month = nov,
	year = {2020},
	pages = {178--182},
	file = {Rui and Yutai - 2020 - Research on Short Text Classification Based on Wor.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G6HR3HS3\\Rui and Yutai - 2020 - Research on Short Text Classification Based on Wor.pdf:application/pdf},
}

@inproceedings{qing-chaoResearchSmallSample2021,
	address = {Chengdu, China},
	title = {Research on {Small} {Sample} {Text} {Classification} {Based} on {Attribute} {Extraction} and {Data} {Augmentation}},
	isbn = {978-1-66542-311-3},
	url = {https://ieeexplore.ieee.org/document/9442500/},
	doi = {10.1109/ICCCBDA51879.2021.9442500},
	abstract = {With the development of deep learning and the progress of natural language processing technology, as well as the continuous disclosure of judicial data such as judicial documents, legal intelligence has gradually become a research hot spot. The crime classiﬁcation task is an important branch of text classiﬁcation, which can help people related to the law to improve their work efﬁciency. However, in the actual research, the sample data is small and the distribution of crime categories is not balanced. To solve these two problems, BERT was used as the encoder to solve the problem of small data volume, and attribute extraction network was added to solve the problem of unbalanced distribution. Finally, the accuracy of 90.35\% on small sample data set could be achieved, and F1 value was 67.62, which was close to the best model performance under sufﬁcient data. Finally, a text enhancement method based on back-translation technology is proposed. Different models are used to conduct experiments. Finally, it is found that LSTM model is improved to some extent, but BERT is not improved to some extent.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {IEEE} 6th {International} {Conference} on {Cloud} {Computing} and {Big} {Data} {Analytics} ({ICCCBDA})},
	publisher = {IEEE},
	author = {Qing-chao, Ni and Cong-jue, Yin and Dong-hua, Zhao},
	month = apr,
	year = {2021},
	pages = {53--57},
	file = {Qing-chao et al. - 2021 - Research on Small Sample Text Classification Based.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XGXM93YY\\Qing-chao et al. - 2021 - Research on Small Sample Text Classification Based.pdf:application/pdf},
}

@inproceedings{yuanResearchTextClassification2021,
	address = {Chongqing, China},
	title = {Research on {Text} {Classification} {Algorithm} {Based} on {BiLSTM}-{WSAttention}},
	isbn = {978-1-72818-028-1},
	url = {https://ieeexplore.ieee.org/document/9390982/},
	doi = {10.1109/IAEAC50856.2021.9390982},
	abstract = {BiLSTM has been widely used in the field of text classification, but the model still cannot accurately measure the importance of each word and cannot extract text features more effectively. In order to solve this problem, this paper proposes a BiLSTM-WSAttention model. The neural network model of BiLSTM-WSAttention is used to classify text. The BiLSTM-WSAttention neural network model combines the context of words and sentences, and extracts contextual semantic information from two perspectives: front to back and back to front. At the same time, this article introduces an attention mechanism. Since text is composed of sentences, sentences are composed of words, and the importance of words and sentences depends on context information, so this article includes word-level attention mechanisms and sentence-level. The attention mechanism assigns different weight values to different words and sentences. Finally, the method proposed in this article is compared with the classification methods of NaiveBayes, CNN, RNN, and BLSTM on the same data set. The experimental results show that: Compared with other classification methods, the neural network model BiLSTMWSAttention proposed in this article is effective on this data set.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {IEEE} 5th {Advanced} {Information} {Technology}, {Electronic} and {Automation} {Control} {Conference} ({IAEAC})},
	publisher = {IEEE},
	author = {Yuan, YingQi},
	month = mar,
	year = {2021},
	pages = {2235--2239},
	file = {Yuan - 2021 - Research on Text Classification Algorithm Based on.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GT36SYKE\\Yuan - 2021 - Research on Text Classification Algorithm Based on.pdf:application/pdf},
}

@inproceedings{zhangResearchTextClassification2021,
	address = {Dalian, China},
	title = {Research on {Text} {Classification} {Method} {Based} on {LSTM} {Neural} {Network} {Model}},
	isbn = {978-1-72819-018-1},
	url = {https://ieeexplore.ieee.org/document/9421225/},
	doi = {10.1109/IPEC51340.2021.9421225},
	abstract = {Text classification is a process of automatically classifying test data according to given rules. Word embedding technology is based on neural probabilistic language model, which can get word vectors with rich semantic information. In the task of natural language processing, a set of excellent word vectors is the basis of all researches. In order to search and extract information from massive electronic texts, this paper constructs a LSTM neural network classification model to classify text information. LSTM can extract words and sentences with different contributions, and combine LSTM's region embedding technology to classify text. Experimental results show that, compared with traditional methods, this method has obvious improvement in performance and classification accuracy.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {IEEE} {Asia}-{Pacific} {Conference} on {Image} {Processing}, {Electronics} and {Computers} ({IPEC})},
	publisher = {IEEE},
	author = {Zhang, Yanbo},
	month = apr,
	year = {2021},
	pages = {1019--1022},
	file = {Zhang - 2021 - Research on Text Classification Method Based on LS.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8HSJCJ84\\Zhang - 2021 - Research on Text Classification Method Based on LS.pdf:application/pdf},
}

@inproceedings{liuResearchCommentText2020,
	address = {Chongqing, China},
	title = {Research on {The} {Comment} {Text} {Classification} based on {Transfer} {Learning}},
	isbn = {978-1-72814-323-1},
	url = {https://ieeexplore.ieee.org/document/9141771/},
	doi = {10.1109/ITOEC49072.2020.9141771},
	abstract = {In the traditional review text classification method, in order to realize the high accuracy of classification model, there are two basic premises :(1) training data and test data must be distributed independently and uniformly; (2) there must be enough training data to learn a good classification model. However, in many cases, these two premises are not true. If a classification model already exists and classifies data from a domain well, then a classification task for a related domain exists, but only data from the source domain, then it may violate this assumption. The comment text classification method based on transfer learning refers to applying the classification knowledge learned in the source domain to the new classification task in the relevant field by using the transfer learning method in the process of classifying the comment text. Therefore, after constructing the isomorphic feature space of source domain and target domain, the TrAdaBoost migration learning framework was used to train the classification model. This model allows users to leverage old data with a small amount of new markup data to build a high-quality classification model for new data. Experimental results show that the model can effectively transfer classification knowledge from source domain to target domain.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} 5th {Information} {Technology} and {Mechatronics} {Engineering} {Conference} ({ITOEC})},
	publisher = {IEEE},
	author = {Liu, Jiangbo and He, Dongzhi},
	month = jun,
	year = {2020},
	pages = {191--195},
	file = {Liu and He - 2020 - Research on The Comment Text Classification based .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6PHZ29HN\\Liu and He - 2020 - Research on The Comment Text Classification based .pdf:application/pdf},
}

@article{xuanyuanSentimentClassificationAlgorithm2021,
	title = {Sentiment {Classification} {Algorithm} {Based} on {Multi}-{Modal} {Social} {Media} {Text} {Information}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9360796/},
	doi = {10.1109/ACCESS.2021.3061450},
	abstract = {The issue of sentiment classiﬁcation in short-term and small-scale data scenarios is considered in this paper. It is a hot topic because the text sentiment classiﬁcation task in the public opinion analysis scene has two characteristics: short time and small data scale. Existing work focused on improving the accuracy at the cost of data and training time, without considering scenarios where time and data are lacked. The most commonly used method to solve the problem of small data scale is to use multi-modal information such as pictures, sounds and videos, which will lead to unbearable training time. The shorter training time determines that the classiﬁcation model is generally selected as a deep neural network with fewer layers, such as TextCNN, TextRNN, and so on. However, such models are limited by the structure and have a low classiﬁcation accuracy. In order to solve both short-term and small-scale data problems, a common information user attribute on social media is added to the model as multimodal information, which includes twelve attributes such as user age, location, and posting time. This paper proposed a sentiment classiﬁcation algorithm based on multi-modal social media text information. The algorithm makes use of parallel convolutional neural networks (CNN) and recurrent neural network (RNN) to process text information and user attributes respectively, and combines the feature vectors of the two models for classiﬁcation, which is called User attributes Convolutional and Recurrent Neural Network (UCRNN). The addition of user attributes can improve accuracy, and the CNN network used to extract user attributes features has fewer parameters, which proves that the algorithm can achieve high accuracy under short-term and small-scale data. Experiments verify that the training time of this model is slightly less than TextRNN. The classiﬁcation accuracy can reach 90.2\%, which is the state-of-the-art in the ﬁeld of short-term and smallscale data sentiment classiﬁcation.},
	language = {en},
	urldate = {2022-08-03},
	journal = {IEEE Access},
	author = {Xuanyuan, Minzheng and Xiao, Le and Duan, Mengshi},
	year = {2021},
	pages = {33410--33418},
	file = {Xuanyuan et al. - 2021 - Sentiment Classification Algorithm Based on Multi-.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9HZBBJJD\\Xuanyuan et al. - 2021 - Sentiment Classification Algorithm Based on Multi-.pdf:application/pdf},
}

@inproceedings{wangShortTextClassification2021,
	address = {Beijing, China},
	title = {Short {Text} {Classification} {Based} on {Cross}-{Connected} {GRU} {Kernel} {Mapping} {Support} {Vector} {Machine}},
	isbn = {978-1-72819-883-5},
	url = {https://ieeexplore.ieee.org/document/9743272/},
	doi = {10.1109/ASSP54407.2021.00038},
	abstract = {Support vector machine (SVM) has achieved excellent results in short text classification. However, its performance is limited in the kernel function. This paper presents a short text classification method based on Cross-connected GRU Kernel Mapping Support Vector Machine (C-GRUKMSVM), to further improve the accuracy of short text classification. The method consists of a feature mapping module and a classification module. The feature mapping module first represents the text as a word vector using the glove method, and then explicitly maps the low-dimensional word vector to a high-dimensional space using a three-layer cross-connected GRU; the classification module uses a soft-margin support vector machine for classification. Experimental results on five publicly available short text datasets show that C-GRUKMSVM achieves better text classification performance than convolutional networks, support vector machines and Naïve Bayes. Additionally, different crossconnected methods, recurrent units and recurrent structures have an impact on the performance of C-GRUKMSVM.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 2nd {Asia} {Symposium} on {Signal} {Processing} ({ASSP})},
	publisher = {IEEE},
	author = {Wang, Qi and Liu, Zhaoying and Zhang, Ting and Li, Yujian},
	month = nov,
	year = {2021},
	pages = {201--207},
	file = {Wang et al. - 2021 - Short Text Classification Based on Cross-Connected.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3I676D6B\\Wang et al. - 2021 - Short Text Classification Based on Cross-Connected.pdf:application/pdf},
}

@inproceedings{geShortTextClassification2020,
	address = {Chongqing, China},
	title = {Short {Text} {Classification} {Method} {Combining} {Word} {Vector} and {WTTM}},
	isbn = {978-1-72814-390-3},
	url = {https://ieeexplore.ieee.org/document/9085172/},
	doi = {10.1109/ITNEC48623.2020.9085172},
	abstract = {Aimed at the problem that the characteristics of news headlines are sparse and the semantic relationship between words and words is weak, which led to the difficulty of obtaining good results in traditional text classification methods, a short text based on word vector and WTTM model is proposed. A classification method that models the topic of short text from the co-occurrence of words. Firstly, the Word2Vec tool is used to train the word vector in the short text corpus and the average word vector is synthesized by the additive averaging method. Then the theme of the short text corpus is modeled by the WTTM model to obtain the topic extended feature vector. Finally, the average word vector and the topic extended feature vector are merged. And used a random forest model to construct a classifier for classification. Compared with other short text classification methods, the short text classification method based on word vector and WTTM model has improved accuracy, recall rate and F1 value, which verified the feasibility of the method.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} 4th {Information} {Technology}, {Networking}, {Electronic} and {Automation} {Control} {Conference} ({ITNEC})},
	publisher = {IEEE},
	author = {Ge, Junwei and Wang, Hanxiao and Fang, Yiqiu},
	month = jun,
	year = {2020},
	pages = {1994--1997},
	file = {Ge et al. - 2020 - Short Text Classification Method Combining Word Ve.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N9NMTAP2\\Ge et al. - 2020 - Short Text Classification Method Combining Word Ve.pdf:application/pdf},
}

@inproceedings{baoShortTextClassification2021,
	address = {Beijing China},
	title = {Short {Text} {Classification} {Model} {Based} on {BERT} and {Fusion} {Network}},
	isbn = {978-1-4503-8415-5},
	url = {https://dl.acm.org/doi/10.1145/3507548.3507574},
	doi = {10.1145/3507548.3507574},
	abstract = {Abstract: Aiming at short texts lacking contextual information, large amount of text data, sparse features, and traditional text feature representations that cannot dynamically obtain the key classification information of a word polysemous and contextual semantics. this paper proposes a pre-trained language model based on BERT. The network model B-BAtt-MPC (BERT-BiLSTM-Attention-MaxPooling-Concat) that integrates BiLSTM, Attention mechanism and Max-Pooling mechanism. Firstly, obtain multi-dimensional and rich feature information such as text context semantics, grammar, and context through the BERT model; Secondly, use the BERT output vector to obtain the most important feature information worth noting through the BiLSTM, Attention layer and Max-Pooling layer; In order to optimize the classification model, the BERT and BiLSTM output vectors are fused and input into Max-Pooling; Finally, the classification results are obtained by fusing two feature vectors with Max-Pooling. The experimental results of two data sets show that the model proposed in this paper can obtain the importance and key rich semantic features of short text classification, and can improve the text classification effect.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 5th {International} {Conference} on {Computer} {Science} and {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Bao, Dongxue and Qin, Donghong and Liang, Xianye and Hong, Lila},
	month = dec,
	year = {2021},
	pages = {168--174},
	file = {Bao et al. - 2021 - Short Text Classification Model Based on BERT and .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L4MCZ983\\Bao et al. - 2021 - Short Text Classification Model Based on BERT and .pdf:application/pdf},
}

@inproceedings{kuchlousShortTextIntent2020,
	address = {New Delhi, India},
	title = {Short {Text} {Intent} {Classification} for {Conversational} {Agents}},
	isbn = {978-1-72816-916-3},
	url = {https://ieeexplore.ieee.org/document/9342516/},
	doi = {10.1109/INDICON49873.2020.9342516},
	abstract = {Intent classification is an important and relevant area of research in artificial intelligence and machine learning, with applications ranging from marketing and product design to intelligent communication. This paper explores the performance of various models and techniques for short text intent classification in the context of chatbots. The problem was explored for use within the mental wellness and therapy chatbot application, Wysa, to give improved responses to free-text user input. The authors looked at classifying text samples in-to 4 categories - assertions, refutations, clarifiers and transitions. For this, the suitability of the following techniques was evaluated: count vectors, TF-IDF, sentence embeddings and ngrams, as well as modifications of the same. Each technique was used to train a number of state-of-the-art classifiers, and the results have been compiled and presented. This is the first documented implementation of Arora’s modification to sentence embeddings for real world use. It also introduces a technique to generate custom stop words that gave a significant gain in performance (10 percentage points). The best pipeline, using these techniques together, gave an accuracy of 95 percent.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} 17th {India} {Council} {International} {Conference} ({INDICON})},
	publisher = {IEEE},
	author = {Kuchlous, Sahil and Kadaba, Madhura},
	month = dec,
	year = {2020},
	pages = {1--4},
	file = {Kuchlous and Kadaba - 2020 - Short Text Intent Classification for Conversationa.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FM9VNPRJ\\Kuchlous and Kadaba - 2020 - Short Text Intent Classification for Conversationa.pdf:application/pdf},
}

@inproceedings{changTamingPretrainedTransformers2020,
	address = {Virtual Event CA USA},
	title = {Taming {Pretrained} {Transformers} for {Extreme} {Multi}-label {Text} {Classification}},
	isbn = {978-1-4503-7998-4},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403368},
	doi = {10.1145/3394486.3403368},
	abstract = {We consider the extreme multi-label text classification (XMC) problem: given an input text, return the most relevant labels from a large label collection. For example, the input text could be a product description on Amazon.com and the labels could be product categories. XMC is an important yet challenging problem in the NLP community. Recently, deep pretrained transformer models have achieved state-of-the-art performance on many NLP tasks including sentence classification, albeit with small label sets. However, naively applying deep transformer models to the XMC problem leads to sub-optimal performance due to the large output space and the label sparsity issue. In this paper, we propose X-Transformer, the first scalable approach to fine-tuning deep transformer models for the XMC problem. The proposed method achieves new state-of-the-art results on four XMC benchmark datasets. In particular, on a Wiki dataset with around 0.5 million labels, the prec@1 of X-Transformer is 77.28\%, a substantial improvement over state-of-the-art XMC approaches Parabel (linear) and AttentionXML (neural), which achieve 68.70\% and 76.95\% precision@1, respectively. We further apply XTransformer to a product2query dataset from Amazon and gained 10.7\% relative improvement on prec@1 over Parabel.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Chang, Wei-Cheng and Yu, Hsiang-Fu and Zhong, Kai and Yang, Yiming and Dhillon, Inderjit S.},
	month = aug,
	year = {2020},
	pages = {3163--3171},
	file = {Chang et al. - 2020 - Taming Pretrained Transformers for Extreme Multi-l.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D2UAFNJU\\Chang et al. - 2020 - Taming Pretrained Transformers for Extreme Multi-l.pdf:application/pdf},
}

@article{zengTermbasedPoolingConvolutional2020,
	title = {Term-based pooling in convolutional neural networks for text classification},
	volume = {17},
	issn = {1673-5447},
	url = {https://ieeexplore.ieee.org/document/9089182/},
	doi = {10.23919/JCC.2020.04.011},
	abstract = {To achieve good results in convolutional neural networks (CNN) for text classification task, term-based pooling operation in CNNs is proposed. Firstly, the convolution results of several convolution kernels are combined by this method, and then the results after combination are made pooling operation, three sorts of CNN models (we named TBCNN, MCT-CNN and MMCT-CNN respectively) are constructed and then corresponding algorithmic thought are detailed on this basis. Secondly, relevant experiments and analyses are respectively designed to show the effects of three key parameters (convolution kernel, combination kernel number and word embedding) on three kinds of CNN models and to further demonstrate the effect of the models proposed. The experimental results show that compared with the traditional method of text classification in CNNs, term-based pooling method is addressed that not only the availability of the way is proved, but also the performance shows good superiority.},
	language = {en},
	number = {4},
	urldate = {2022-08-03},
	journal = {China Communications},
	author = {Zeng, Shuifei and Ma, Yan and Zhang, Xiaoyan and Du, Xiaofeng},
	month = apr,
	year = {2020},
	pages = {109--124},
	file = {Zeng et al. - 2020 - Term-based pooling in convolutional neural network.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9AZF63PR\\Zeng et al. - 2020 - Term-based pooling in convolutional neural network.pdf:application/pdf},
}

@inproceedings{kumarTextClassificationTopic2021,
	address = {Bangalore, India},
	title = {Text {Classification} and {Topic} {Modelling} of {Web} {Extracted} {Data}},
	isbn = {978-1-66541-836-2},
	url = {https://ieeexplore.ieee.org/document/9587459/},
	doi = {10.1109/GCAT52182.2021.9587459},
	abstract = {Text classiﬁcation and Topic Modelling is the backbone for the text analysis of huge amount of corpus of data. With an increase in unstructured data around us , it is very difﬁcult to analyse the data very easily.There is a need for some methods that can be applied to the data to get the sensitive and semantic information from the corpus. Text classiﬁcation is categorization of text in organised way for the interpretation of sensitive information from the text , while Topic modelling is ﬁnding the abstract topic for the collection of text or document. Topic modelling is used frequently to ﬁnd semantic information from the textual data. In this paper we applied Parsing techniques on various websites to extract the HTML and XML data which includes the textual data and also applied Preprocessing techniques to clean the data.For the text classiﬁcation purpose some of the Machine learning based classiﬁers that we have used in our experiment are Naive Bayes and also Logistic Regression Classiﬁer .The models of the document are built using three different topic modelling methods which are Latent Semantic Analysis , Probabilistic Latent Semantic Analysis and Latent Dirichlet Allocation.In the further experiment we have done analysis and also comparison based upon the performance of the models and classiﬁers on the processed textual data.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 2nd {Global} {Conference} for {Advancement} in {Technology} ({GCAT})},
	publisher = {IEEE},
	author = {Kumar, Niraj and Suman, R.R and Kumar, Sanjay},
	month = oct,
	year = {2021},
	pages = {1--8},
	file = {Kumar et al. - 2021 - Text Classification and Topic Modelling of Web Ext.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HKBVSUYL\\Kumar et al. - 2021 - Text Classification and Topic Modelling of Web Ext.pdf:application/pdf},
}

@inproceedings{wangTextClassificationBased2020,
	address = {Shanghai, China},
	title = {Text {Classification} {Based} on {GNN}},
	isbn = {978-1-72818-149-3},
	url = {https://ieeexplore.ieee.org/document/9221771/},
	doi = {10.1109/IWECAI50956.2020.00026},
	abstract = {瀥The phenomenon that AI researchers tend to transform some certain data into the form of graphs is prevailing. Usually, these graph-like data will be inputted into some certain artificial neutral networks which are dramatically disparate with the conventional CNN. The purpose of the algorithm employed the GNN is to extract much more detailed features that can be stored easily in the graph. However, these detailed features are much more difficult to extract in the raw data accumulated in the database, which requires the experiment to transfer the common database into the whole graph ahead of schedule. The dataset used in this paper, Cora, is commonly used in some papers whose targets aimed at semantic segmentation, while disparate with this paper as well. The result of this experiment has achieved to nearly 100\% accuracy accompanied with those preprocessed data. Furthermore, this paper also attaches much focus on the effects of preprocessing operation which can be reflected on the differences of accuracy. Only by preprocessing operation can this paper achieve better results accompanied with higher accuracy when compared with other experiments.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {International} {Workshop} on {Electronic} {Communication} and {Artificial} {Intelligence} ({IWECAI})},
	publisher = {IEEE},
	author = {Wang, Jingyu},
	month = jun,
	year = {2020},
	pages = {94--97},
	file = {Wang - 2020 - Text Classification Based on GNN.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QX9MMSRU\\Wang - 2020 - Text Classification Based on GNN.pdf:application/pdf},
}

@inproceedings{liuTextClassificationBased2020,
	address = {Guangzhou, China},
	title = {Text {Classification} {Based} on {Hybrid} {Neural} {Network}},
	isbn = {978-1-66540-398-6},
	url = {https://ieeexplore.ieee.org/document/9345827/},
	doi = {10.1109/CSE50738.2020.00011},
	abstract = {Aiming at the problems of insufficient feature extraction and low classification accuracy in the classification of Chinese news texts by the neural network, this paper proposes a hybrid neural network text classification model which integrates time series convolutional network -- TGNet. The new model utilizes Temporal Convolutional Network to capture the relationship between hidden features on different time scale, and uses Gated Tanh-ReLU Units (GTRU) as the activation layer to improve the expression ability of neural network to the model. Meanwhile, the Gated Recurrent Unit networks (GRU) is used to learn the semantic features of context. Finally, the extracted features are fused and input into Softmax for classification. Experimental results show that the text classification model proposed in this paper has achieved better classification results in the published Chinese news data sets SougoCS and FuDan.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} 23rd {International} {Conference} on {Computational} {Science} and {Engineering} ({CSE})},
	publisher = {IEEE},
	author = {Liu, Yapei and Ma, Jianhong and Tao, Yongcai and Shi, Lei and Wei, Lin and Li, Linna},
	month = dec,
	year = {2020},
	pages = {24--29},
	file = {Liu et al. - 2020 - Text Classification Based on Hybrid Neural Network.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LYPP5SKG\\Liu et al. - 2020 - Text Classification Based on Hybrid Neural Network.pdf:application/pdf},
}

@inproceedings{huTextClassificationBased2020,
	address = {Chongqing, China},
	title = {Text classification based recurrent neural network},
	isbn = {978-1-72814-323-1},
	url = {https://ieeexplore.ieee.org/document/9141747/},
	doi = {10.1109/ITOEC49072.2020.9141747},
	abstract = {Recurrent neural networks (RNNs) have shown outstanding performance for natural language processing tasks, influenced by the repeated multiplication of the recurrent weight matrices, the problem of gradient vanishing and explosion problem will be encountered when training RNN. Independently recurrent neural network (IndRNN) makes neurons independent and constrains recursive weights to effectively solve gradient problems. We combine IndRNN with long short-term memory (LSTM) and attention model, and test it for text classification, the results show that our models can effectively adapt to text classification task.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} 5th {Information} {Technology} and {Mechatronics} {Engineering} {Conference} ({ITOEC})},
	publisher = {IEEE},
	author = {Hu, Haojin and Liao, Mengfan and Zhang, Chao and Jing, Yanmei},
	month = jun,
	year = {2020},
	pages = {652--655},
	file = {Hu et al. - 2020 - Text classification based recurrent neural network.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y93YAD3F\\Hu et al. - 2020 - Text classification based recurrent neural network.pdf:application/pdf},
}

@inproceedings{jinbaoTextClassificationMethod2021,
	address = {Xiamen China},
	title = {Text {Classification} {Method} {Based} on {BiGRU}-{Attention} and {CNN} {Hybrid} {Model}},
	isbn = {978-1-4503-8408-7},
	url = {https://dl.acm.org/doi/10.1145/3488933.3488970},
	doi = {10.1145/3488933.3488970},
	abstract = {Aiming at the problem that traditional Gated Recurrent Unit (GRU) and Convolution Neural Network (CNN) can not reflect the importance of each word in the text when extracting features, a text classification method based on BiGRU Attention and CNN is proposed. Firstly, CNN was used to extract the local information of the text, and then the full-text semantics was integrated. Secondly, BiGRU was used to extract the context features of the text, and attention mechanism was used after BiGRU to extract the attention score of the output information. Finally, the output of BiGRU attention was fused with the output of CNN to realize the effective extraction of text features and focused on the important content words. Experimental results on three public datasets showed that the proposed model was better than GRU, CNN and other models, which can effectively improve the effect of text classification.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 4th {International} {Conference} on {Artificial} {Intelligence} and {Pattern} {Recognition}},
	publisher = {ACM},
	author = {Jinbao, Teng and Weiwei, Kong and Yidan, Chang and Qiaoxin, Tian and Chenyuan, Shi and Long, Li},
	month = sep,
	year = {2021},
	pages = {614--622},
	file = {Jinbao et al. - 2021 - Text Classification Method Based on BiGRU-Attentio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CLMNVMH6\\Jinbao et al. - 2021 - Text Classification Method Based on BiGRU-Attentio.pdf:application/pdf},
}

@inproceedings{yaoTextClassificationModel2020,
	address = {Dalian, China},
	title = {Text {Classification} {Model} {Based} on {fastText}},
	isbn = {978-1-72816-590-5},
	url = {https://ieeexplore.ieee.org/document/9194939/},
	doi = {10.1109/ICAIIS49377.2020.9194939},
	abstract = {Most text classification models based on traditional machine learning algorithms have problems such as curse of dimensionality and poor performance. In order to solve the above problems, this paper proposes a text classification model based on fastText. Our model explores the important information contained in the text through the feature engineering, and obtains the low-dimensional, continuous and high-quality text representation through the fastText algorithm. The experiment is based on Python to classify the text dataset of “user comment data emotional polarity judgment" in Baidu Dianshi platform. In the emotional polarity judgment task, the experimental results show that the precision, recall and F values of our model are superior to the model based on traditional machine learning algorithms and have excellent classification performance.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} {International} {Conference} on {Artificial} {Intelligence} and {Information} {Systems} ({ICAIIS})},
	publisher = {IEEE},
	author = {Yao, Tengjun and Zhai, Zhengang and Gao, Bingtao},
	month = mar,
	year = {2020},
	pages = {154--157},
	file = {Yao et al. - 2020 - Text Classification Model Based on fastText.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EGRKSLBJ\\Yao et al. - 2020 - Text Classification Model Based on fastText.pdf:application/pdf},
}

@inproceedings{diTextClassificationCOVID192022,
	address = {Shenyang, China},
	title = {Text classification of {COVID}-19 reviews based on pre-training language model},
	isbn = {978-1-66544-276-3},
	url = {https://ieeexplore.ieee.org/document/9719020/},
	doi = {10.1109/ICPECA53709.2022.9719020},
	abstract = {This experiment analyzed 100,000 epidemicrelated microblogs officially provided by the CCF. Using Enhanced Representation through Knowledge Integration (ERNIE), the effect of pre-training model on extracting Chinese semantic information was improved. After that, the deep pyramid network (DPCNN) was merged with ERNIE to save computing costs. Enhanced feature extraction performance for long-distance text. This model was the most effective in the comparison test of six emotional three-category tasks, which improved the accuracy of BERT pre-training model by 7\%.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2022 {IEEE} 2nd {International} {Conference} on {Power}, {Electronics} and {Computer} {Applications} ({ICPECA})},
	publisher = {IEEE},
	author = {Di, Juxing and Liu, Zixu and Yang, Yang},
	month = jan,
	year = {2022},
	pages = {1179--1183},
	file = {Di et al. - 2022 - Text classification of COVID-19 reviews based on p.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H75I43LC\\Di et al. - 2022 - Text classification of COVID-19 reviews based on p.pdf:application/pdf},
}

@inproceedings{buldinTextClassificationIllegal2020,
	address = {St. Petersburg and Moscow, Russia},
	title = {Text {Classification} of {Illegal} {Activities} on {Onion} {Sites}},
	isbn = {978-1-72815-761-0},
	url = {https://ieeexplore.ieee.org/document/9039341/},
	doi = {10.1109/EIConRus49466.2020.9039341},
	abstract = {Onion sites work using the Hidden Service Protocol, which helps to keep a double anonymity. A such system allows sites to place malicious and illegal content. An identification and tracking of such resources is an important problem, that`s why the article sets a task of developing a system for accurate thematic classification of textual content blocks of hidden web pages using k nearest neighbors method. The article presents the method of content separation placed on Russianlanguage onion-sites. The research illustrates the analysis of text categorization results based on collected dataset for the implementation of machine learning.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2020 {IEEE} {Conference} of {Russian} {Young} {Researchers} in {Electrical} and {Electronic} {Engineering} ({EIConRus})},
	publisher = {IEEE},
	author = {Buldin, Ilya D. and Ivanov, Nikita S.},
	month = jan,
	year = {2020},
	pages = {245--247},
	file = {Buldin and Ivanov - 2020 - Text Classification of Illegal Activities on Onion.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YVU64799\\Buldin and Ivanov - 2020 - Text Classification of Illegal Activities on Onion.pdf:application/pdf},
}

@inproceedings{ruchikaTextClassificationTwitter2021,
	address = {Noida, India},
	title = {Text {Classification} on {Twitter} {Data} {Using} {Machine} {Learning} {Algorithm}},
	isbn = {978-1-66541-703-7},
	url = {https://ieeexplore.ieee.org/document/9596132/},
	doi = {10.1109/ICRITO51393.2021.9596132},
	abstract = {Classification of text is the major challenge faced by the industries so that they can extract the required information from that data. With the advent of machine learning, it becomes a lot easier for the engineers to classify the data as per requirement. Writing an effective code and algorithm is required to classify the data accurately. In this paper, we have tried to solve a problem where we have taken a raw dataset which contains tweets by users where the name of the disease in every tweet begins with a hashtag (‘\#’) symbol. These disease names are extracted and stored it in a new column preceding to every tweet with the help of python.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 9th {International} {Conference} on {Reliability}, {Infocom} {Technologies} and {Optimization} ({Trends} and {Future} {Directions}) ({ICRITO})},
	publisher = {IEEE},
	author = {{Ruchika} and Sharma, Mayank and Hossain, Syed Akhtar},
	month = sep,
	year = {2021},
	pages = {1--3},
	file = {Ruchika et al. - 2021 - Text Classification on Twitter Data Using Machine .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MXVD8PBQ\\Ruchika et al. - 2021 - Text Classification on Twitter Data Using Machine .pdf:application/pdf},
}

@inproceedings{xuyangTextClassificationStudy2021,
	address = {Suzhou, China},
	title = {Text {Classification} {Study} {Based} on {Graph} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-66542-563-6},
	url = {https://ieeexplore.ieee.org/document/9526016/},
	doi = {10.1109/IEIT53597.2021.00029},
	abstract = {Because the research of machine learning and deep learning so far has presented good results in the field of text classification, but it ignores the influence of words and words, and the relationship to a certain extent between words and documents on text classification in an overall perspective. Existing text classification researches are usually based on graph convolutional neural networks, in this paper, by using Mish() activation function to deal with the problem of experimental data in the hard zero boundary and by modifying the momentum of one-step moving average in the optimizer, a certain degree of improvement is proposed to the short-term memory problem after the optimization of experimental parameters, which is called LTMTEXT-GCN. By comparison with Text-GCN, the experiment results demonstrate the effect of LTM-TEXT-GCN, and the improvement was 0.14\%, 0.64\%, 0.8\% and 0.13\% on the four data sets, respectively.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {International} {Conference} on {Internet}, {Education} and {Information} {Technology} ({IEIT})},
	publisher = {IEEE},
	author = {Xuyang, Gao and Junyang, Yu and Shuwei, Xu},
	month = apr,
	year = {2021},
	pages = {102--105},
	file = {Xuyang et al. - 2021 - Text Classification Study Based on Graph Convoluti.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SDQX8PKS\\Xuyang et al. - 2021 - Text Classification Study Based on Graph Convoluti.pdf:application/pdf},
}

@misc{devlinBERTPretrainingDeep2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	language = {en},
	urldate = {2022-08-04},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language, xrec},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2UTDN4XB\\1810.html:text/html;Devlin et al_2019_BERT.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LKC463IP\\Devlin et al_2019_BERT.pdf:application/pdf},
}

@misc{yangXLNetGeneralizedAutoregressive2020,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	shorttitle = {{XLNet}},
	url = {http://arxiv.org/abs/1906.08237},
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
	urldate = {2022-08-04},
	publisher = {arXiv},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	month = jan,
	year = {2020},
	note = {arXiv:1906.08237 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, sota, nlp, pretraining},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HV3LEEBE\\1906.html:text/html;Yang et al_2020_XLNet.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZVDILNW2\\Yang et al_2020_XLNet.pdf:application/pdf},
}

@article{uysalImpactPreprocessingText2014,
	title = {The impact of preprocessing on text classification},
	volume = {50},
	issn = {03064573},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457313000964},
	doi = {10.1016/j.ipm.2013.08.006},
	language = {en},
	number = {1},
	urldate = {2022-08-04},
	journal = {Information Processing \& Management},
	author = {Uysal, Alper Kursat and Gunal, Serkan},
	month = jan,
	year = {2014},
	pages = {104--112},
}

@inproceedings{yangTextClassificationBased2020,
	address = {Chengdu, China},
	title = {Text {Classification} {Based} on {Convolutional} {Neural} {Network} and {Attention} {Model}},
	isbn = {978-1-72819-741-8},
	url = {https://ieeexplore.ieee.org/document/9137447/},
	doi = {10.1109/ICAIBD49809.2020.9137447},
	urldate = {2022-08-04},
	booktitle = {2020 3rd {International} {Conference} on {Artificial} {Intelligence} and {Big} {Data} ({ICAIBD})},
	publisher = {IEEE},
	author = {Yang, Shuang and Tang, Yan},
	month = may,
	year = {2020},
	pages = {67--73},
}

@article{chaiComparisonTextPreprocessing2022,
	title = {Comparison of text preprocessing methods},
	issn = {1351-3249, 1469-8110},
	url = {https://www.cambridge.org/core/product/identifier/S1351324922000213/type/journal_article},
	doi = {10.1017/S1351324922000213},
	abstract = {Abstract
            Text preprocessing is not only an essential step to prepare the corpus for modeling but also a key area that directly affects the natural language processing (NLP) application results. For instance, precise tokenization increases the accuracy of part-of-speech (POS) tagging, and retaining multiword expressions improves reasoning and machine translation. The text corpus needs to be appropriately preprocessed before it is ready to serve as the input to computer models. The preprocessing requirements depend on both the nature of the corpus and the NLP application itself, that is, what researchers would like to achieve from analyzing the data. Conventional text preprocessing practices generally suffice, but there exist situations where the text preprocessing needs to be customized for better analysis results. Hence, we discuss the pros and cons of several common text preprocessing methods: removing formatting, tokenization, text normalization, handling punctuation, removing stopwords, stemming and lemmatization, n-gramming, and identifying multiword expressions. Then, we provide examples of text datasets which require special preprocessing and how previous researchers handled the challenge. We expect this article to be a starting guideline on how to select and fine-tune text preprocessing methods.},
	language = {en},
	urldate = {2022-08-04},
	journal = {Natural Language Engineering},
	author = {Chai, Christine P.},
	month = jun,
	year = {2022},
	pages = {1--45},
	file = {Chai_2022_Comparison of text preprocessing methods.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZE36QL7D\\Chai_2022_Comparison of text preprocessing methods.pdf:application/pdf},
}

@book{bittencourtArtificialIntelligenceEducation2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Artificial {Intelligence} in {Education}: 21st {International} {Conference}, {AIED} 2020, {Ifrane}, {Morocco}, {July} 6–10, 2020, {Proceedings}, {Part} {I}},
	volume = {12163},
	isbn = {978-3-030-52236-0 978-3-030-52237-7},
	shorttitle = {Artificial {Intelligence} in {Education}},
	url = {http://link.springer.com/10.1007/978-3-030-52237-7},
	language = {en},
	urldate = {2022-08-08},
	publisher = {Springer International Publishing},
	editor = {Bittencourt, Ig Ibert and Cukurova, Mutlu and Muldner, Kasia and Luckin, Rose and Millán, Eva},
	year = {2020},
	doi = {10.1007/978-3-030-52237-7},
	keywords = {no},
	file = {Artificial Intelligence in Education 21st International Conference, AIED 2020, Ifrane, Morocco, July 6–10, 2020, Proceedings, Part I.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Artificial Intelligence in Education 21st International Conference, AIED 2020, Ifrane, Morocco, July 6–10, 2020, Proceedings, Part I.md:text/plain;Bittencourt et al. - 2020 - Artificial Intelligence in Education 21st Interna.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DJIIY6EC\\Bittencourt et al. - 2020 - Artificial Intelligence in Education 21st Interna.pdf:application/pdf},
}

@book{nkambouIntelligentTutoringSystems2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Intelligent {Tutoring} {Systems}},
	volume = {10858},
	isbn = {978-3-319-91463-3 978-3-319-91464-0},
	url = {http://link.springer.com/10.1007/978-3-319-91464-0},
	language = {en},
	urldate = {2022-08-08},
	publisher = {Springer International Publishing},
	editor = {Nkambou, Roger and Azevedo, Roger and Vassileva, Julita},
	year = {2018},
	doi = {10.1007/978-3-319-91464-0},
	keywords = {no},
	file = {Intelligent Tutoring Systems.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Intelligent Tutoring Systems.md:text/plain;Nkambou et al. - 2018 - Intelligent Tutoring Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LSRKANYG\\Nkambou et al. - 2018 - Intelligent Tutoring Systems.pdf:application/pdf},
}

@inproceedings{catulayNeuralNetworkArchitectureApproach2021,
	address = {Cario, Egypt},
	title = {Neural-{Network} {Architecture} {Approach}: {An} {Automated} {Essay} {Scoring} {Using} {Bayesian} {Linear} {Ridge} {Regression} {Algorithm}},
	isbn = {978-1-72818-683-2},
	shorttitle = {Neural-{Network} {Architecture} {Approach}},
	url = {https://ieeexplore.ieee.org/document/9654801/},
	doi = {10.1109/ISCMI53840.2021.9654801},
	abstract = {Most people are relying on technology in various industries, such as education, where people are more likely to use technology as a platform for knowledge acquisition. In this study, the researchers proposed an Automated Essay Scoring or AES to help the teachers minimize the time in grading essay work of the student, prevent biased ratings and provide a feedback mechanism for the students. The study proposed a NeuralNetwork Approach Architecture and Bayesian Linear Ridge Regression Algorithm to improve the correctness and reliability of the AES system and dimensions we are going to use. We also use the new Hybrid Model or Hybrid Agile to have a detailed approach in developing our study. We used the dataset from the Hewlett Foundation, the Automated Student Assessment Prize (ASAP) from Kaggle. We also gather information in reliable sources and collect data to test the accuracy and reliability of our AES system.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {2021 8th {International} {Conference} on {Soft} {Computing} \& {Machine} {Intelligence} ({ISCMI})},
	publisher = {IEEE},
	author = {Catulay, Jeff Jojer Jones E. and Magsael, Maco E. and Ancheta, Danlord O. and Costales, Jefferson A.},
	month = nov,
	year = {2021},
	pages = {196--200},
	file = {Catulay et al. - 2021 - Neural-Network Architecture Approach An Automated.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZRR37S4I\\Catulay et al. - 2021 - Neural-Network Architecture Approach An Automated.pdf:application/pdf;Neural-Network Architecture Approach An Automated Essay Scoring Using Bayesian Linear Ridge Regression Algorithm.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Neural-Network Architecture Approach An Automated Essay Scoring Using Bayesian Linear Ridge Regression Algorithm.md:text/plain},
}

@article{kumarExplainableAutomatedEssay2020,
	title = {Explainable {Automated} {Essay} {Scoring}: {Deep} {Learning} {Really} {Has} {Pedagogical} {Value}},
	volume = {5},
	issn = {2504-284X},
	shorttitle = {Explainable {Automated} {Essay} {Scoring}},
	url = {https://www.frontiersin.org/article/10.3389/feduc.2020.572367/full},
	doi = {10.3389/feduc.2020.572367},
	language = {en},
	urldate = {2022-08-08},
	journal = {Frontiers in Education},
	author = {Kumar, Vivekanandan and Boulanger, David},
	month = oct,
	year = {2020},
	keywords = {go},
	pages = {572367},
	file = {Explainable Automated Essay Scoring Deep Learning Really Has Pedagogical Value.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Explainable Automated Essay Scoring Deep Learning Really Has Pedagogical Value.md:text/plain;Kumar and Boulanger - 2020 - Explainable Automated Essay Scoring Deep Learning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3EL5GFBM\\Kumar and Boulanger - 2020 - Explainable Automated Essay Scoring Deep Learning.pdf:application/pdf},
}

@inproceedings{liAutomatedEssayScoring2022,
	address = {Xiamen, China},
	title = {An {Automated} {Essay} {Scoring} model {Based} on {Stacking} {Method}},
	isbn = {978-1-66548-223-3},
	url = {https://ieeexplore.ieee.org/document/9832246/},
	doi = {10.1109/SEAI55746.2022.9832246},
	abstract = {In the latest two decades, thanks to AI technology, Automated Essay Scoring (AES) technology has also been rapidly developed. This technology to analyze and score essays automatically is a hot spot for the application of natural language processing in the field of education. Firstly, different encoding methods are used to obtain the lexical vectors of the text. Secondly, features of the seven aspects of the text are fully extracted. Then the comparative analysis of different ensemble learning models was studied on the English essay scoring competition dataset of Kaggle. Finally, a model based on the stacking method is proposed. The results show that the model based on the stacking method achieves the best results on all datasets. It improves the average QWK values on eight subsets by 1.0\%∼2.8\% compared to the baseline models of neural networks and feature-engineered.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {2022 {IEEE} 2nd {International} {Conference} on {Software} {Engineering} and {Artificial} {Intelligence} ({SEAI})},
	publisher = {IEEE},
	author = {Li, Chenchen and Lin, Lin and Mao, Wei and Xiong, Liu and Lin, Yongping},
	month = jun,
	year = {2022},
	keywords = {go},
	pages = {248--252},
	file = {An Automated Essay Scoring model Based on Stacking Method.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\An Automated Essay Scoring model Based on Stacking Method.md:text/plain;Li et al. - 2022 - An Automated Essay Scoring model Based on Stacking.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZWLEDDWC\\Li et al. - 2022 - An Automated Essay Scoring model Based on Stacking.pdf:application/pdf},
}

@inproceedings{nadeemAutomatedEssayScoring2019,
	address = {Florence, Italy},
	title = {Automated {Essay} {Scoring} with {Discourse}-{Aware} {Neural} {Models}},
	url = {https://www.aclweb.org/anthology/W19-4450},
	doi = {10.18653/v1/W19-4450},
	abstract = {Automated essay scoring systems typically rely on hand-crafted features to predict essay quality, but such systems are limited by the cost of feature engineering. Neural networks offer an alternative to feature engineering, but they typically require more annotated data. This paper explores network structures, contextualized embeddings and pre-training strategies aimed at capturing discourse characteristics of essays. Experiments on three essay scoring tasks show beneﬁts from all three strategies in different combinations, with simpler architectures being more effective when less training data is available.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {Proceedings of the {Fourteenth} {Workshop} on {Innovative} {Use} of {NLP} for {Building} {Educational} {Applications}},
	publisher = {Association for Computational Linguistics},
	author = {Nadeem, Farah and Nguyen, Huy and Liu, Yang and Ostendorf, Mari},
	year = {2019},
	keywords = {sota, no},
	pages = {484--493},
	file = {Automated Essay Scoring with Discourse-Aware Neural Models.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automated Essay Scoring with Discourse-Aware Neural Models.md:text/plain;Nadeem et al. - 2019 - Automated Essay Scoring with Discourse-Aware Neura.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CH49MGT8\\Nadeem et al. - 2019 - Automated Essay Scoring with Discourse-Aware Neura.pdf:application/pdf},
}

@article{xueHierarchicalBERTBasedTransfer2021,
	title = {A {Hierarchical} {BERT}-{Based} {Transfer} {Learning} {Approach} for {Multi}-{Dimensional} {Essay} {Scoring}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9530411/},
	doi = {10.1109/ACCESS.2021.3110683},
	abstract = {The task of automated essay scoring (AES) continues to attract interdisciplinary attention due to its commercial and educational importance as well as related research challenges. Traditional AES approaches rely on handcrafted features, which are time-consuming and labor-intensive. Neural network approaches have recently given fantastic results in AES without feature engineering, but they usually require extensive annotated data. Moreover, most of the existing AES models only report a single holistic score without providing diagnostic information about various dimensions of writing quality. Focusing on these issues, we develop a novel approach using multi-task learning (MTL) with ﬁne-tuning Bidirectional Encoder Representations from Transformers (BERT) for multi-dimensional AES tasks. As a state-of-theart pre-trained language model, a BERT-based approach can improve AES tasks with limited training data. Meanwhile, we deal with long texts by proposing a hierarchical method and using the attention mechanism to automatically determine the contribution of different fractions of the input essay to the ﬁnal score. For the multi-topic essay scoring tasks on the ASAP dataset, results reveal that our approach outperforms the average quadratic weighted Kappa (QWK) score by 4.5\% compared with the strong baseline. We propose a self-collected dataset of Chinese EFL Learners’ Argumentation (CELA) to provide valuable information about writing quality from multiple rating dimensions, including holistic and ﬁve analytic scales. For the multi-rating dimensional essay scoring tasks on the CELA dataset, experimental results demonstrate that our model increases the average QWK score by 8.1\% compared with the strong baseline.},
	language = {en},
	urldate = {2022-08-08},
	journal = {IEEE Access},
	author = {Xue, Jin and Tang, Xiaoyi and Zheng, Liyan},
	year = {2021},
	keywords = {no},
	pages = {125403--125415},
	file = {A Hierarchical BERT-Based Transfer Learning Approach for Multi-Dimensional Essay Scoring.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\A Hierarchical BERT-Based Transfer Learning Approach for Multi-Dimensional Essay Scoring.md:text/plain;Xue et al. - 2021 - A Hierarchical BERT-Based Transfer Learning Approa.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WSSBS6LI\\Xue et al. - 2021 - A Hierarchical BERT-Based Transfer Learning Approa.pdf:application/pdf},
}

@article{utoReviewDeepneuralAutomated2021,
	title = {A review of deep-neural automated essay scoring models},
	volume = {48},
	issn = {0385-7417, 1349-6964},
	url = {https://link.springer.com/10.1007/s41237-021-00142-y},
	doi = {10.1007/s41237-021-00142-y},
	abstract = {Automated essay scoring (AES) is the task of automatically assigning scores to essays as an alternative to grading by humans. Although traditional AES models typically rely on manually designed features, deep neural network (DNN)-based AES models that obviate the need for feature engineering have recently attracted increased attention. Various DNN-AES models with different characteristics have been proposed over the past few years. To our knowledge, however, no study has provided a comprehensive review of DNN-AES models while introducing each model in detail. Therefore, this review presents a comprehensive survey of DNNAES models, describing the main idea and detailed architecture of each model. We classify the AES task into four types and introduce existing DNN-AES models according to this classification.},
	language = {en},
	number = {2},
	urldate = {2022-08-08},
	journal = {Behaviormetrika},
	author = {Uto, Masaki},
	month = jul,
	year = {2021},
	pages = {459--484},
	file = {A review of deep-neural automated essay scoring models.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\A review of deep-neural automated essay scoring models.md:text/plain;Uto - 2021 - A review of deep-neural automated essay scoring mo.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\M7LHTRFI\\Uto - 2021 - A review of deep-neural automated essay scoring mo.pdf:application/pdf},
}

@inproceedings{chimingyangAutomaticSystemEssay2020,
	address = {Shenyang, China},
	title = {An {Automatic} {System} for {Essay} {Questions} {Scoring} based on {LSTM} and {Word} {Embedding}},
	isbn = {978-1-72818-575-0},
	url = {https://ieeexplore.ieee.org/document/9363782/},
	doi = {10.1109/ISCTT51595.2020.00068},
	urldate = {2022-08-08},
	booktitle = {2020 5th {International} {Conference} on {Information} {Science}, {Computer} {Technology} and {Transportation} ({ISCTT})},
	publisher = {IEEE},
	author = {Chimingyang, Huang},
	month = nov,
	year = {2020},
	keywords = {feature engineering, go, lstm},
	pages = {355--364},
	file = {An Automatic System for Essay Questions Scoring based on LSTM and Word Embedding.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\An Automatic System for Essay Questions Scoring based on LSTM and Word Embedding.md:text/plain;Chimingyang - 2020 - An Automatic System for Essay Questions Scoring ba.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DUVE24R6\\Chimingyang - 2020 - An Automatic System for Essay Questions Scoring ba.pdf:application/pdf},
}

@inproceedings{xiaAutomaticEssayScoring2019,
	address = {Normal IL USA},
	title = {Automatic {Essay} {Scoring} {Model} {Based} on {Two}-{Layer} {Bi}-directional {Long}-{Short} {Term} {Memory} {Network}},
	isbn = {978-1-4503-7627-3},
	url = {https://dl.acm.org/doi/10.1145/3374587.3374596},
	doi = {10.1145/3374587.3374596},
	abstract = {Automatic essay scoring provides a cost-effective and consistent alternative to human correction. However, in order to obtain good performance, human experts are needed to extract features of text manually in traditional ways. We propose a two-layer bidirectional long-short term memory model that a fully automatic essay scoring model. The agreement of essay scores which marked by our model and human raters respectively has achieved to 0.870 based on metrics of Cohen’s . This model can achieve excellent results like human beings’ professional raters.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {Proceedings of the 2019 3rd {International} {Conference} on {Computer} {Science} and {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Xia, Linzhong and Liu, Jun and Zhang, Zhenjiu},
	month = dec,
	year = {2019},
	keywords = {go},
	pages = {133--137},
	file = {Automatic Essay Scoring Model Based on Two-Layer Bi-directional Long-Short Term Memory Network.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automatic Essay Scoring Model Based on Two-Layer Bi-directional Long-Short Term Memory Network.md:text/plain;Xia et al. - 2019 - Automatic Essay Scoring Model Based on Two-Layer B.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BFKL2CC4\\Xia et al. - 2019 - Automatic Essay Scoring Model Based on Two-Layer B.pdf:application/pdf},
}

@article{beseisoNovelAutomatedEssay2021,
	title = {A novel automated essay scoring approach for reliable higher educational assessments},
	volume = {33},
	issn = {1042-1726, 1867-1233},
	url = {https://link.springer.com/10.1007/s12528-021-09283-1},
	doi = {10.1007/s12528-021-09283-1},
	abstract = {E-learning is gradually gaining prominence in higher education, with universities enlarging provision and more students getting enrolled. The effectiveness of automated essay scoring (AES) is thus holding a strong appeal to universities for managing an increasing learning interest and reducing costs associated with human raters. The growth in e-learning systems in the higher education system and the demand for consistent writing assessments has spurred research interest in improving the accuracy of AES systems. This paper presents a transformer-based neural network model for improved AES performance using Bi-LSTM and RoBERTa language model based on Kaggle’s ASAP dataset. The proposed model uses Bi-LSTM model over pre-trained RoBERTa language model to address the coherency issue in essays that is ignored by traditional essay scoring methods, including traditional NLP pipelines, deep learning-based methods, a mixture of both. The comparison of the experimental results on essay scoring with human raters concludes that the proposed model outperforms the existing methods in essay scoring in terms of QWK score. The comparative analysis of results demonstrates the applicability of the proposed model in automated essay scoring at higher education level.},
	language = {en},
	number = {3},
	urldate = {2022-08-08},
	journal = {Journal of Computing in Higher Education},
	author = {Beseiso, Majdi and Alzubi, Omar A. and Rashaideh, Hasan},
	month = dec,
	year = {2021},
	keywords = {go, BERT},
	pages = {727--746},
	file = {A novel automated essay scoring approach for reliable higher educational assessments.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\A novel automated essay scoring approach for reliable higher educational assessments.md:text/plain;Beseiso et al. - 2021 - A novel automated essay scoring approach for relia.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5NIS65ND\\Beseiso et al. - 2021 - A novel automated essay scoring approach for relia.pdf:application/pdf},
}

@article{lottridgeComparingRobustnessDeep,
	title = {Comparing the {Robustness} of {Deep} {Learning} and {Classical} {Automated} {Scoring} {Approaches} to {Gaming} {Strategies}},
	language = {en},
	author = {Lottridge, Sue and Godek, Ben and Jafari, Amir and Patel, Milan},
	keywords = {no},
	pages = {43},
	file = {Comparing the Robustness of Deep Learning and Classical Automated Scoring Approaches to Gaming Strategies.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Comparing the Robustness of Deep Learning and Classical Automated Scoring Approaches to Gaming Strategies.md:text/plain;Lottridge et al. - Comparing the Robustness of Deep Learning and Clas.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BIM5EKFA\\Lottridge et al. - Comparing the Robustness of Deep Learning and Clas.pdf:application/pdf},
}

@inproceedings{fauziAutomaticEssayScoring2017,
	address = {Bangkok Thailand},
	title = {Automatic {Essay} {Scoring} {System} {Using} {N}-{Gram} and {Cosine} {Similarity} for {Gamification} {Based} {E}-{Learning}},
	isbn = {978-1-4503-5295-6},
	url = {https://dl.acm.org/doi/10.1145/3133264.3133303},
	doi = {10.1145/3133264.3133303},
	abstract = {E-Learning is one of the great innovations in teaching methods. In the E-learning, there are several assessment methods; one of them is the essay examination. Essay assessment takes a long time if corrected manually. Therefore, researches on automatic essay scoring have been growing rapidly in recent years. The method that is usually used for automatic essay scoring is Cosine Similarity by utilizing bag of words as the feature extraction. However, the feature extraction by using bag of words did not consider to the order of words in a sentence. Meanwhile, the order of words in an essay has an important role in the assessment. In this study, an automatic essay scoring system based on n-gram and cosine similarity was proposed. N-gram was used for feature extraction and modified to split by word instead of by letter so that the word order would be considered. Based on evaluation results, this system got the best correlation of 0.66 by using unigram on questions that do not consider the order of words in the answer. For questions that consider the order of the words in the answer, bigram has the best correlation value by 0.67.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {Proceedings of the {International} {Conference} on {Advances} in {Image} {Processing}},
	publisher = {ACM},
	author = {Fauzi, M. Ali and Utomo, Djoko Cahyo and Setiawan, Budi Darma and Pramukantoro, Eko Sakti},
	month = aug,
	year = {2017},
	pages = {151--155},
	file = {Automatic Essay Scoring System Using N-Gram and Cosine Similarity for Gamification Based E-Learning.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automatic Essay Scoring System Using N-Gram and Cosine Similarity for Gamification Based E-Learning.md:text/plain;Fauzi et al. - 2017 - Automatic Essay Scoring System Using N-Gram and Co.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7SRPCVRX\\Fauzi et al. - 2017 - Automatic Essay Scoring System Using N-Gram and Co.pdf:application/pdf},
}

@article{ghantaAutomatedEssayEvaluation,
	title = {Automated {Essay} {Evaluation} {Using} {Natural} {Language} {Processing} and {Machine} {Learning}},
	language = {en},
	author = {Ghanta, Harshanthi},
	keywords = {no},
	pages = {46},
	file = {Automated Essay Evaluation Using Natural Language Processing and Machine Learning.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automated Essay Evaluation Using Natural Language Processing and Machine Learning.md:text/plain;Ghanta - Automated Essay Evaluation Using Natural Language .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C2E97CG8\\Ghanta - Automated Essay Evaluation Using Natural Language .pdf:application/pdf},
}

@article{kumarAutomatedEssayScoring2021,
	title = {Automated {Essay} {Scoring} and the {Deep} {Learning} {Black} {Box}: {How} {Are} {Rubric} {Scores} {Determined}?},
	volume = {31},
	issn = {1560-4292, 1560-4306},
	shorttitle = {Automated {Essay} {Scoring} and the {Deep} {Learning} {Black} {Box}},
	url = {https://link.springer.com/10.1007/s40593-020-00211-5},
	doi = {10.1007/s40593-020-00211-5},
	language = {en},
	number = {3},
	urldate = {2022-08-08},
	journal = {International Journal of Artificial Intelligence in Education},
	author = {Kumar, Vivekanandan S. and Boulanger, David},
	month = sep,
	year = {2021},
	keywords = {feature engineering, go},
	pages = {538--584},
	file = {Automated Essay Scoring and the Deep Learning Black Box How Are Rubric Scores Determined.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automated Essay Scoring and the Deep Learning Black Box How Are Rubric Scores Determined.md:text/plain;Kumar and Boulanger - 2021 - Automated Essay Scoring and the Deep Learning Blac:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LZJV8IPX\\Kumar and Boulanger - 2021 - Automated Essay Scoring and the Deep Learning Blac:application/pdf},
}

@inproceedings{sethiNaturalLanguageProcessing2022,
	address = {Erode, India},
	title = {Natural {Language} {Processing} based {Automated} {Essay} {Scoring} with {Parameter}-{Efficient} {Transformer} {Approach}},
	isbn = {978-1-66541-028-1},
	url = {https://ieeexplore.ieee.org/document/9753760/},
	doi = {10.1109/ICCMC53470.2022.9753760},
	abstract = {Existing automated scoring models implement layers of traditional recurrent neural networks to achieve reasonable performance. However, the models provide limited performance due to the limited capacity to encode long-term dependencies. The paper proposed a novel architecture incorporating pioneering language models of the natural language processing community. We leverage pre-trained language models and integrate it with adapter modules, which use a bottle-neck architecture to reduce the number of trainable parameters while delivering excellent performance. We also propose a model by re-purposing the bidirectional attention flow model to detect adversarial essays. The model we put forward achieves state-of-the-art performance on most essay prompts in the Automated Student Assessment Prize data set. We outline the previous methods employed to attempt this task, and show how our model outperforms them.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {2022 6th {International} {Conference} on {Computing} {Methodologies} and {Communication} ({ICCMC})},
	publisher = {IEEE},
	author = {Sethi, Angad and Singh, Kavinder},
	month = mar,
	year = {2022},
	keywords = {go},
	pages = {749--756},
	file = {Natural Language Processing based Automated Essay Scoring with Parameter-Efficient Transformer Approach.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Natural Language Processing based Automated Essay Scoring with Parameter-Efficient Transformer Approach.md:text/plain;Sethi and Singh - 2022 - Natural Language Processing based Automated Essay .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CDXACF2P\\Sethi and Singh - 2022 - Natural Language Processing based Automated Essay .pdf:application/pdf},
}

@article{rameshAutomatedEssayScoring2022,
	title = {An automated essay scoring systems: a systematic literature review},
	volume = {55},
	issn = {0269-2821, 1573-7462},
	shorttitle = {An automated essay scoring systems},
	url = {https://link.springer.com/10.1007/s10462-021-10068-2},
	doi = {10.1007/s10462-021-10068-2},
	abstract = {Assessment in the Education system plays a significant role in judging student performance. The present evaluation system is through human assessment. As the number of teachers’ student ratio is gradually increasing, the manual evaluation process becomes complicated. The drawback of manual evaluation is that it is time-consuming, lacks reliability, and many more. This connection online examination system evolved as an alternative tool for pen and paper-based methods. Present Computer-based evaluation system works only for multiple-choice questions, but there is no proper evaluation system for grading essays and short answers. Many researchers are working on automated essay grading and short answer scoring for the last few decades, but assessing an essay by considering all parameters like the relevance of the content to the prompt, development of ideas, Cohesion, and Coherence is a big challenge till now. Few researchers focused on Content-based evaluation, while many of them addressed style-based assessment. This paper provides a systematic literature review on automated essay scoring systems. We studied the Artificial Intelligence and Machine Learning techniques used to evaluate automatic essay scoring and analyzed the limitations of the current studies and research trends. We observed that the essay evaluation is not done based on the relevance of the content and coherence.},
	language = {en},
	number = {3},
	urldate = {2022-08-08},
	journal = {Artificial Intelligence Review},
	author = {Ramesh, Dadi and Sanampudi, Suresh Kumar},
	month = mar,
	year = {2022},
	keywords = {go},
	pages = {2495--2527},
	file = {An automated essay scoring systems a systematic literature review.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\An automated essay scoring systems a systematic literature review.md:text/plain;Ramesh and Sanampudi - 2022 - An automated essay scoring systems a systematic l.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8DVJP5QF\\Ramesh and Sanampudi - 2022 - An automated essay scoring systems a systematic l.pdf:application/pdf},
}

@inproceedings{contrerasAutomatedEssayScoring2018,
	address = {Shah Alam},
	title = {Automated {Essay} {Scoring} with {Ontology} based on {Text} {Mining} and {NLTK} tools},
	isbn = {978-1-5386-4836-0 978-1-5386-4838-4},
	url = {https://ieeexplore.ieee.org/document/8538399/},
	doi = {10.1109/ICSCEE.2018.8538399},
	abstract = {One of the common learning activities used in educational levels and disciplines is essay writing. The problems of the essay writing activities are time-consuming, concerns in producing immediate result and/or feedback from teachers to students, and the teachers tend to be subjective in grading the essay activities. The study aims to apply the preliminary approach for automatically generating the domain concept ontology in essays using OntoGen and applied natural language processing algorithms using NLTK (Natural Language Tool Kit) that enhance the teachers essay grading.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {2018 {International} {Conference} on {Smart} {Computing} and {Electronic} {Enterprise} ({ICSCEE})},
	publisher = {IEEE},
	author = {Contreras, Jennifer O. and Hilles, Shadi and Abubakar, Zainab Binti},
	month = jul,
	year = {2018},
	keywords = {no},
	pages = {1--6},
	file = {Automated Essay Scoring with Ontology based on Text Mining and NLTK tools.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automated Essay Scoring with Ontology based on Text Mining and NLTK tools.md:text/plain;Contreras et al. - 2018 - Automated Essay Scoring with Ontology based on Tex.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MFDL3ZSL\\Contreras et al. - 2018 - Automated Essay Scoring with Ontology based on Tex.pdf:application/pdf},
}

@inproceedings{prabhuHybridApproachAutomated2022,
	address = {Mumbai, India},
	title = {A {Hybrid} {Approach} {Towards} {Automated} {Essay} {Evaluation} based on {Bert} and {Feature} {Engineering}},
	isbn = {978-1-66542-168-3},
	url = {https://ieeexplore.ieee.org/document/9824999/},
	doi = {10.1109/I2CT54291.2022.9824999},
	abstract = {Educational institutions often assess a student's critical thinking and communication skills based on essay responses. Manual Evaluation is time-consuming, and there may be wide variations when multiple evaluators rate batches of essays. In the last few years, the automated grading of essay scripts has emerged as a new area of research. Most studies essentially focus on visible attributes such as length, vocabulary, sentiment or spelling. The use of neural networks requires the conversion of text into some vector representations. However solely using handcrafted attributes or text encodings implies primarily operating on word granularity. On the other hand, Transformers can handle dependencies between words of the text. In this paper, we propose a hybrid model that can capture the interaction of words in the essay using the BERT self-attention transformer, along with handcrafted syntactical features. While previous studies have built individual models for every essay topic, our model has been incrementally trained on multiple essay topics to test its generalizability. The validation of the model uses quadratic weighted kappa to compare human-rated scores and model scores.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {2022 {IEEE} 7th {International} conference for {Convergence} in {Technology} ({I2CT})},
	publisher = {IEEE},
	author = {Prabhu, Shreya and Akhila, Kara and S, Sanriya},
	month = apr,
	year = {2022},
	keywords = {no},
	pages = {1--4},
	file = {A Hybrid Approach Towards Automated Essay Evaluation based on Bert and Feature Engineering.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\A Hybrid Approach Towards Automated Essay Evaluation based on Bert and Feature Engineering.md:text/plain;Prabhu et al. - 2022 - A Hybrid Approach Towards Automated Essay Evaluati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4PA4A9T2\\Prabhu et al. - 2022 - A Hybrid Approach Towards Automated Essay Evaluati.pdf:application/pdf},
}

@inproceedings{lagakisAutomatedEssayScoring2021,
	address = {Istanbul, Turkey},
	title = {Automated essay scoring: {A} review of the field},
	isbn = {978-1-66544-913-7},
	shorttitle = {Automated essay scoring},
	url = {https://ieeexplore.ieee.org/document/9618476/},
	doi = {10.1109/CITS52676.2021.9618476},
	abstract = {This paper critically reviews the recently published scientific literature on the task of Automated Essay Scoring (AES), by examining the various systems and approaches used. Automated Essay Scoring, which is the process of automating the evaluation of answers to open-ended questions, most usually in educational settings, by utilizing NLP techniques, gathers an increasing amount of interest, due to its potential applications both commercially and as part of the learn-ing process. The focus of this paper is to analyze the most popular approaches in recently published AES systems, categorize the systems with respect to certain characteristics in their design, the datasets that they use and the evaluation schemas that are used to evaluate them, and finally discuss the recent trends and challenges of the field of AES.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {2021 {International} {Conference} on {Computer}, {Information} and {Telecommunication} {Systems} ({CITS})},
	publisher = {IEEE},
	author = {Lagakis, Paraskevas and Demetriadis, Stavros},
	month = nov,
	year = {2021},
	keywords = {no},
	pages = {1--6},
	file = {Automated essay scoring A review of the field.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automated essay scoring A review of the field.md:text/plain;Lagakis and Demetriadis - 2021 - Automated essay scoring A review of the field.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9JT4V9QW\\Lagakis and Demetriadis - 2021 - Automated essay scoring A review of the field.pdf:application/pdf},
}

@article{AutomaticEssayScoring,
	title = {Automatic {Essay} {Scoring}},
	language = {en},
	keywords = {no},
	pages = {8},
	file = {Automatic Essay Scoring.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automatic Essay Scoring.md:text/plain;Automatic Essay Scoring.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2T7AF4HW\\Automatic Essay Scoring.pdf:application/pdf},
}

@inproceedings{hellmanScalingWritingCurriculum2019,
	address = {Chicago IL USA},
	title = {Scaling {Up} {Writing} in the {Curriculum}: {Batch} {Mode} {Active} {Learning} for {Automated} {Essay} {Scoring}},
	isbn = {978-1-4503-6804-9},
	shorttitle = {Scaling {Up} {Writing} in the {Curriculum}},
	url = {https://dl.acm.org/doi/10.1145/3330430.3333629},
	doi = {10.1145/3330430.3333629},
	abstract = {Automated essay scoring (AES) allows writing to be assigned in large courses and can provide instant formative feedback to students. However, creating models for AES can be costly, requiring the collection and human scoring of hundreds of essays. We have developed and are piloting a web-based tool that allows instructors to incrementally score responses to enable AES scoring while minimizing the number of essays the instructors must score. Previous work has shown that techniques from the machine learning subﬁeld of active learning can reduce the amount of training data required to create effective AES models. We extend those results to a less idealized scenario: one driven by the instructor’s need to score sets of essays, in which the model is trained iteratively using batch mode active learning. We propose a novel approach inspired by a class of topological methods, but with reduced computational requirements, which we refer to as topological maxima. Using actual student data, we show that batch mode active learning is a practical approach to training AES models. Finally, we discuss implications of using this technology for automated customized scoring of writing across the curriculum.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {Proceedings of the {Sixth} (2019) {ACM} {Conference} on {Learning} @ {Scale}},
	publisher = {ACM},
	author = {Hellman, Scott and Rosenstein, Mark and Gorman, Andrew and Murray, William and Becker, Lee and Baikadi, Alok and Budden, Jill and Foltz, Peter W.},
	month = jun,
	year = {2019},
	pages = {1--10},
	file = {Hellman et al. - 2019 - Scaling Up Writing in the Curriculum Batch Mode A.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3SVVEAJF\\Hellman et al. - 2019 - Scaling Up Writing in the Curriculum Batch Mode A.pdf:application/pdf;Scaling Up Writing in the Curriculum Batch Mode Active Learning for Automated Essay Scoring.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Scaling Up Writing in the Curriculum Batch Mode Active Learning for Automated Essay Scoring.md:text/plain},
}

@inproceedings{wangkriangkriComparativeStudyPretrained2020,
	address = {Osaka, Japan},
	title = {A {Comparative} {Study} of {Pretrained} {Language} {Models} for {Automated} {Essay} {Scoring} with {Adversarial} {Inputs}},
	isbn = {978-1-72818-455-5},
	url = {https://ieeexplore.ieee.org/document/9293930/},
	doi = {10.1109/TENCON50793.2020.9293930},
	abstract = {Automated Essay Scoring (AES) is a task that deals with grading written essays automatically without human intervention. This study compares the performance of three AES models which utilize different text embedding methods, namely Global Vectors for Word Representation (GloVe), Embeddings from Language Models (ELMo), and Bidirectional Encoder Representations from Transformers (BERT). We used two evaluation metrics: Quadratic Weighted Kappa (QWK) and a novel “robustness”, which quantiﬁes the models’ ability to detect adversarial essays created by modifying normal essays to cause them to be less coherent. We found that: (1) the BERTbased model achieved the greatest robustness, followed by the GloVe-based and ELMo-based models, respectively, and (2) ﬁnetuning the embeddings improves QWK but lowers robustness. These ﬁndings could be informative on how to choose, and whether to ﬁne-tune, an appropriate model based on how much the AES program places emphasis on proper grading of adversarial essays.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {2020 {IEEE} {REGION} 10 {CONFERENCE} ({TENCON})},
	publisher = {IEEE},
	author = {Wangkriangkri, Phakawat and Viboonlarp, Chanissara and Rutherford, Attapol T. and Chuangsuwanich, Ekapol},
	month = nov,
	year = {2020},
	keywords = {no},
	pages = {875--880},
	file = {A Comparative Study of Pretrained Language Models for Automated Essay Scoring with Adversarial Inputs.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\A Comparative Study of Pretrained Language Models for Automated Essay Scoring with Adversarial Inputs.md:text/plain;Wangkriangkri et al. - 2020 - A Comparative Study of Pretrained Language Models .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QSJLTTS5\\Wangkriangkri et al. - 2020 - A Comparative Study of Pretrained Language Models .pdf:application/pdf},
}

@misc{liuExploringDiscourseStructures2021,
	title = {Exploring {Discourse} {Structures} for {Argument} {Impact} {Classification}},
	url = {http://arxiv.org/abs/2106.00976},
	abstract = {Discourse relations among arguments reveal logical structures of a debate conversation. However, no prior work has explicitly studied how the sequence of discourse relations inﬂuence a claim’s impact. This paper empirically shows that the discourse relations between two arguments along the context path are essential factors for identifying the persuasive power of an argument. We further propose DISCOC to inject and fuse the sentence-level structural discourse information with contextualized features derived from large-scale language models. Experimental results and extensive analysis show that the attention and gate mechanisms that explicitly model contexts and texts can indeed help the argument impact classiﬁcation task deﬁned by Durmus et al. (2019), and discourse structures among the context path of the claim to be classiﬁed can further boost the performance.},
	language = {en},
	urldate = {2022-08-08},
	publisher = {arXiv},
	author = {Liu, Xin and Ou, Jiefu and Song, Yangqiu and Jiang, Xin},
	month = jun,
	year = {2021},
	note = {arXiv:2106.00976 [cs]},
	keywords = {Computer Science - Computation and Language, no},
	file = {Exploring Discourse Structures for Argument Impact Classification - Comment Accepted by ACL 2021.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Exploring Discourse Structures for Argument Impact Classification - Comment Accepted by ACL 2021.md:text/plain;Exploring Discourse Structures for Argument Impact Classification.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Exploring Discourse Structures for Argument Impact Classification.md:text/plain;Liu et al. - 2021 - Exploring Discourse Structures for Argument Impact.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I3E897GN\\Liu et al. - 2021 - Exploring Discourse Structures for Argument Impact.pdf:application/pdf},
}

@inproceedings{chenRelevanceBasedAutomatedEssay2018,
	address = {Bandung, Indonesia},
	title = {Relevance-{Based} {Automated} {Essay} {Scoring} via {Hierarchical} {Recurrent} {Model}},
	isbn = {978-1-72811-175-9},
	url = {https://ieeexplore.ieee.org/document/8629256/},
	doi = {10.1109/IALP.2018.8629256},
	abstract = {In recent years, neural network models have been used in automated essay scoring task and achieved good performance. However, few studies investigated using the prompt information into the neural network. We know that there is a close relevance between the essay content and the topic. Therefore, the relevance between the essay and the topic can aid to represent the relationship between the essay and its score. That is to say, the degree of relevance between the high score essay and the topic will be higher while the low score essay is less similar to the topic. Inspired by this idea, we propose to use the similarity of the essay and the topic as auxiliary information which can be concatenated into the final representation of the essay. We first use a hierarchical recurrent neural network combined with attention mechanism to learn the content representation of the essay and the topic on sentence-level and document-level. Then, we multiply the essay representation and the topic representation to get a similarity representation between them. In the end, we concatenate the similarity representation into the essay’s representation to get a final representation of the essay. We tested our model on ASAP dataset and the experimental results show that our model outperformed the existing state-of-the-art models.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {2018 {International} {Conference} on {Asian} {Language} {Processing} ({IALP})},
	publisher = {IEEE},
	author = {Chen, Minping and Li, Xia},
	month = nov,
	year = {2018},
	keywords = {no},
	pages = {378--383},
	file = {Chen and Li - 2018 - Relevance-Based Automated Essay Scoring via Hierar.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\57QPLD63\\Chen and Li - 2018 - Relevance-Based Automated Essay Scoring via Hierar.pdf:application/pdf;Relevance-Based Automated Essay Scoring via Hierarchical Recurrent Model.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Relevance-Based Automated Essay Scoring via Hierarchical Recurrent Model.md:text/plain},
}

@inproceedings{caiAutomaticEssayScoring2019,
	address = {Xi'an China},
	title = {Automatic essay scoring with recurrent neural network},
	isbn = {978-1-4503-6638-0},
	url = {https://dl.acm.org/doi/10.1145/3318265.3318296},
	doi = {10.1145/3318265.3318296},
	abstract = {As deep learning has developed rapidly in recent years, the automatic essay scoring system, based on deep learning models, has become more reliable than previous feature-based systems. Recent researchers have developed an approach based on recurrent neural networks to learn the relationship between an essay and its assigned score, without any feature engineering. In this paper, we use an ASAP essay dataset, combining feature scoring and a recurrent neural network. The results show that we can compare the result of quadratic weighted Kappa of each experience to get the best model. GloVe significantly improves the results, and feature extraction can affect the result slightly. In future work, we will apply transfer learning, one-shot learning, and adversarial inputs in our model to get better performance.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {High} {Performance} {Compilation}, {Computing} and {Communications}},
	publisher = {ACM},
	author = {Cai, Changzhi},
	month = mar,
	year = {2019},
	keywords = {no},
	pages = {1--7},
	file = {Automatic essay scoring with recurrent neural network.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automatic essay scoring with recurrent neural network.md:text/plain;Cai - 2019 - Automatic essay scoring with recurrent neural netw.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5W5TISXV\\Cai - 2019 - Automatic essay scoring with recurrent neural netw.pdf:application/pdf},
}

@article{chistovaDiscourseawareTextClassification,
	title = {Discourse-aware text classification for argument mining},
	abstract = {We show that using the rhetorical structure automatically generated by the discourse parser is beneficial for paragraph-level argument mining in Russian. First, we improve the structure awareness of the current RST discourse parser for Russian by employing the recent top-down approach for unlabeled tree construction on a paragraph level. Then we demonstrate the utility of this parser in two classification argument mining subtasks of the RuARG-2022 shared task. Our approach leverages a structured LSTM module to compute a text representation that reflects the composition of discourse units in the rhetorical structure. We show that: (i) the inclusion of discourse analysis improves paragraph-level text classification; (ii) a novel TreeLSTM-based approach performs well for the computation of the complex text hidden representation using both a language model and an end-to-end RST parser; (iii) structures predicted by the proposed RST parser reflect the argumentative structures in texts in Russian.},
	language = {en},
	author = {Chistova, Elena and Smirnov, Ivan},
	keywords = {no},
	pages = {13},
	file = {Chistova and Smirnov - Discourse-aware text classification for argument m.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E7RQJEQ4\\Chistova and Smirnov - Discourse-aware text classification for argument m.pdf:application/pdf;Discourse-aware text classification for argument mining.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Discourse-aware text classification for argument mining.md:text/plain},
}

@inproceedings{h.beseisoEssayScoringTool2021,
	address = {Ma'an Jordan},
	title = {Essay {Scoring} {Tool} by {Employing} {RoBERTa} {Architecture}},
	isbn = {978-1-4503-8838-2},
	url = {https://dl.acm.org/doi/10.1145/3460620.3460630},
	doi = {10.1145/3460620.3460630},
	abstract = {The automated essay scoring (AES) has significant importance in machine grading of student essays particularly in standardized exams like the Graduate Record Examination (GRE). However, some issues in AES have remained unsolved over the past several years. The current approaches have scrutinized AES from both classification and regression perspectives. This study discusses the cutting edge architectures such as RoBERTa, XLNet, and BERT and compares their automated essay scoring performance. ASAP, a publicly accessible dataset is used for this purpose. The obtained results indicate that the natural language understanding (NLU) model proposed in this paper depicts significantly improved performance than all the other existing approaches.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {International {Conference} on {Data} {Science}, {E}-learning and {Information} {Systems} 2021},
	publisher = {ACM},
	author = {H. Beseiso, Majdi},
	month = apr,
	year = {2021},
	keywords = {no},
	pages = {54--57},
	file = {Essay Scoring Tool by Employing RoBERTa Architecture.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Essay Scoring Tool by Employing RoBERTa Architecture.md:text/plain;H. Beseiso - 2021 - Essay Scoring Tool by Employing RoBERTa Architectu.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N5UD9SW5\\H. Beseiso - 2021 - Essay Scoring Tool by Employing RoBERTa Architectu.pdf:application/pdf},
}

@misc{liuAutomatedEssayScoring2019,
	title = {Automated {Essay} {Scoring} based on {Two}-{Stage} {Learning}},
	url = {http://arxiv.org/abs/1901.07744},
	abstract = {Current state-of-the-art feature-engineered and endto-end Automated Essay Score (AES) methods are proven to be unable to detect adversarial samples, e.g. the essays composed of permuted sentences and the prompt-irrelevant essays. Focusing on the problem, we develop a Two-Stage Learning Framework (TSLF) which integrates the advantages of both feature-engineered and end-to-end AES methods. In experiments, we compare TSLF against a number of strong baselines, and the results demonstrate the effectiveness and robustness of our models. TSLF surpasses all the baselines on ﬁve-eighths of prompts and achieves new state-of-the-art average performance when without negative samples. After adding some adversarial eassys to the original datasets, TSLF outperforms the featuresengineered and end-to-end baselines to a great extent, and shows great robustness.},
	language = {en},
	urldate = {2022-08-08},
	publisher = {arXiv},
	author = {Liu, Jiawei and Xu, Yang and Zhu, Yaguang},
	month = dec,
	year = {2019},
	note = {arXiv:1901.07744 [cs]},
	keywords = {Computer Science - Computation and Language, sota, go},
	file = {Automated Essay Scoring based on Two-Stage Learning - Comment 7 pages, 4 figures.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automated Essay Scoring based on Two-Stage Learning - Comment 7 pages, 4 figures.md:text/plain;Automated Essay Scoring based on Two-Stage Learning.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automated Essay Scoring based on Two-Stage Learning.md:text/plain;Liu et al. - 2019 - Automated Essay Scoring based on Two-Stage Learnin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y2PNHVTG\\Liu et al. - 2019 - Automated Essay Scoring based on Two-Stage Learnin.pdf:application/pdf},
}

@inproceedings{gunawansyahAutomatedEssayScoring2020,
	address = {Bandung, Indonesia},
	title = {Automated {Essay} {Scoring} {Using} {Natural} {Language} {Processing} {And} {Text} {Mining} {Method}},
	isbn = {978-1-72817-598-0},
	url = {https://ieeexplore.ieee.org/document/9310845/},
	doi = {10.1109/TSSA51342.2020.9310845},
	abstract = {The use of technology really helps to maximized the effectiveness and efficiency of work expecially in the education field. Elearning is the concept of education that has begun to be widely implemented at this covid-19 pandemic to avoid the spread of transmission through social distancing. One of elearning types is essay but for large participants, it need much effort for evaluate by human rater. The inconsistency of assessment by the rater due to fatigue can also affect the quality of the assessment. Developing a system that can learn and understand on its own without having to be repeatedly programmed by humans used machine learning and computational linguistics to study the interaction between computers and human natural language used natural language processing proposed in this research. Natural language processing and text mining methods are able to provide a good assessment which is influenced by several processes, namely tokenization, stopword, stemming and support with the number of keywords, and the synonym of more complex keywords. The automated essay scoring system is proven to provide consistent and objective assessments and is able to approach human raters assessments.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {2020 14th {International} {Conference} on {Telecommunication} {Systems}, {Services}, and {Applications} ({TSSA}},
	publisher = {IEEE},
	author = {{Gunawansyah} and Rahayu, Riska and {Nurwathi} and Sugiarto, Bambang and {Gunawan}},
	month = nov,
	year = {2020},
	pages = {1--4},
	file = {Automated Essay Scoring Using Natural Language Processing And Text Mining Method.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automated Essay Scoring Using Natural Language Processing And Text Mining Method.md:text/plain;Gunawansyah et al. - 2020 - Automated Essay Scoring Using Natural Language Pro.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7PXVZWWA\\Gunawansyah et al. - 2020 - Automated Essay Scoring Using Natural Language Pro.pdf:application/pdf},
}

@misc{tashuDeepLearningArchitecture2022,
	title = {Deep {Learning} {Architecture} for {Automatic} {Essay} {Scoring}},
	url = {http://arxiv.org/abs/2206.08232},
	abstract = {Automatic evaluation of essay (AES) and also called automatic essay scoring has become a severe problem due to the rise of online learning and evaluation platforms such as Coursera, Udemy, Khan academy, and so on. Researchers have recently proposed many techniques for automatic evaluation. However, many of these techniques use hand-crafted features and thus are limited from the feature representation point of view. Deep learning has emerged as a new paradigm in machine learning which can exploit the vast data and identify the features useful for essay evaluation. To this end, we propose a novel architecture based on recurrent networks (RNN) and convolution neural network (CNN). In the proposed architecture, the multichannel convolutional layer learns and captures the contextual features of the word n-gram from the word embedding vectors and the essential semantic concepts to form the feature vector at essay level using max-pooling operation. A variant of RNN called Bi-gated recurrent unit (BGRU) is used to access both previous and subsequent contextual representations. The experiment was carried out on eight data sets available on Kaggle for the task of AES. The experimental results show that our proposed system achieves significantly higher grading accuracy than other deep learning-based AES systems and also other state-of-the-art AES systems.},
	language = {en},
	urldate = {2022-08-08},
	publisher = {arXiv},
	author = {Tashu, Tsegaye Misikir and Maurya, Chandresh Kumar and Horvath, Tomas},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08232 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, no},
	file = {Deep Learning Architecture for Automatic Essay Scoring.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Deep Learning Architecture for Automatic Essay Scoring.md:text/plain;Tashu et al. - 2022 - Deep Learning Architecture for Automatic Essay Sco.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UVD2TS3T\\Tashu et al. - 2022 - Deep Learning Architecture for Automatic Essay Sco.pdf:application/pdf},
}

@inproceedings{taghipourNeuralApproachAutomated2016,
	address = {Austin, Texas},
	title = {A {Neural} {Approach} to {Automated} {Essay} {Scoring}},
	url = {http://aclweb.org/anthology/D16-1193},
	doi = {10.18653/v1/D16-1193},
	abstract = {Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6\% in terms of quadratic weighted Kappa, without requiring any feature engineering.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Taghipour, Kaveh and Ng, Hwee Tou},
	year = {2016},
	keywords = {sota, go},
	pages = {1882--1891},
	file = {A Neural Approach to Automated Essay Scoring.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\A Neural Approach to Automated Essay Scoring.md:text/plain;Taghipour and Ng - 2016 - A Neural Approach to Automated Essay Scoring.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BTP59L97\\Taghipour and Ng - 2016 - A Neural Approach to Automated Essay Scoring.pdf:application/pdf},
}

@inproceedings{dongAttentionbasedRecurrentConvolutional2017,
	address = {Vancouver, Canada},
	title = {Attention-based {Recurrent} {Convolutional} {Neural} {Network} for {Automatic} {Essay} {Scoring}},
	url = {http://aclweb.org/anthology/K17-1017},
	doi = {10.18653/v1/K17-1017},
	abstract = {Neural network models have recently been applied to the task of automatic essay scoring, giving promising results. Existing work used recurrent neural networks and convolutional neural networks to model input essays, giving grades based on a single vector representation of the essay. On the other hand, the relative advantages of RNNs and CNNs have not been compared. In addition, different parts of the essay can contribute differently for scoring, which is not captured by existing models. We address these issues by building a hierarchical sentence-document model to represent essays, using the attention mechanism to automatically decide the relative weights of words and sentences. Results show that our model outperforms the previous stateof-the-art methods, demonstrating the effectiveness of the attention mechanism.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {Proceedings of the 21st {Conference} on {Computational} {Natural} {Language}           {Learning} ({CoNLL} 2017)},
	publisher = {Association for Computational Linguistics},
	author = {Dong, Fei and Zhang, Yue and Yang, Jie},
	year = {2017},
	keywords = {go, cnn, rnn},
	pages = {153--162},
	file = {Attention-based Recurrent Convolutional Neural Network for Automatic Essay Scoring.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Attention-based Recurrent Convolutional Neural Network for Automatic Essay Scoring.md:text/plain;Dong et al. - 2017 - Attention-based Recurrent Convolutional Neural Net.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z8MGTVWM\\Dong et al. - 2017 - Attention-based Recurrent Convolutional Neural Net.pdf:application/pdf},
}

@inproceedings{eidAutomatedEssayScoring2017,
	address = {Alexandria},
	title = {Automated essay scoring linguistic feature: {Comparative} study},
	isbn = {978-1-5386-6407-0},
	shorttitle = {Automated essay scoring linguistic feature},
	url = {http://ieeexplore.ieee.org/document/8303043/},
	doi = {10.1109/ACCS-PEIT.2017.8303043},
	abstract = {Automated Essay Scoring (AES) is the solution to a tedious and time consuming activity of manually scoring students’ essays. AES is usually treated as a supervised machine learning problem where feature extraction plays an important role. In an attempt to investigate the importance of lexical features in AES systems, a new extended feature set is developed by combining popularly known features. The combined feature set contains 22 features that captures five different aspects of writing qualities. The importance of each feature in the combined feature set is tested by eliminating each feature separately. It was found that using the number of nouns in the essay slightly degrades the AES system performance. The significance of the combined feature set is compared against three state-of- the-art AES commercial systems and its performance was found comparable.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {2017 {Intl} {Conf} on {Advanced} {Control} {Circuits} {Systems} ({ACCS}) {Systems} \& 2017 {Intl} {Conf} on {New} {Paradigms} in {Electronics} \& {Information} {Technology} ({PEIT})},
	publisher = {IEEE},
	author = {Eid, Soha M. and Wanas, Nayer M.},
	month = nov,
	year = {2017},
	keywords = {no},
	pages = {212--217},
	file = {Automated essay scoring linguistic feature Comparative study.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automated essay scoring linguistic feature Comparative study.md:text/plain;Eid and Wanas - 2017 - Automated essay scoring linguistic feature Compar.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XC4XBK4R\\Eid and Wanas - 2017 - Automated essay scoring linguistic feature Compar.pdf:application/pdf},
}

@inproceedings{chaddaEngineeringIntelligentEssay2021,
	address = {Madrid, Spain},
	title = {Engineering an {Intelligent} {Essay} {Scoring} and {Feedback} {System}: {An} {Experience} {Report}},
	isbn = {978-1-66544-470-5},
	shorttitle = {Engineering an {Intelligent} {Essay} {Scoring} and {Feedback} {System}},
	url = {https://ieeexplore.ieee.org/document/9474379/},
	doi = {10.1109/WAIN52551.2021.00029},
	abstract = {Artificial Intelligence (AI) / Machine Learning (ML)-based systems are widely sought-after commercial solutions that can automate and augment core business services. Intelligent systems can improve the quality of services offered and support scalability through automation. In this paper we describe our experience in engineering an exploratory system for assessing the quality of essays supplied by customers of a specialized recruitment support service. The problem domain is challenging because the open-ended customer-supplied source text has considerable scope for ambiguity and error, making models for analysis hard to build. There is also a need to incorporate specialized business domain knowledge into the intelligent processing systems. To address these challenges, we experimented with and exploited a number of cloud-based machine learning models and composed them into an application-specific processing pipeline. This design allows for modification of the underlying algorithms as more data and improved techniques become available. We describe our design, and the main challenges we faced, namely keeping a check on the quality control of the models, testing the software and deploying the computationally expensive ML models on the cloud.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {2021 {IEEE}/{ACM} 1st {Workshop} on {AI} {Engineering} - {Software} {Engineering} for {AI} ({WAIN})},
	publisher = {IEEE},
	author = {Chadda, Akriti and Song, Kelly and Chandrasekar, Raman and Gorton, Ian},
	month = may,
	year = {2021},
	pages = {141--144},
	file = {Chadda et al. - 2021 - Engineering an Intelligent Essay Scoring and Feedb.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4P2KNCLE\\Chadda et al. - 2021 - Engineering an Intelligent Essay Scoring and Feedb.pdf:application/pdf;Engineering an Intelligent Essay Scoring and Feedback System An Experience Report.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Engineering an Intelligent Essay Scoring and Feedback System An Experience Report.md:text/plain},
}

@article{scholkopfNewSupportVector2000,
	title = {New {Support} {Vector} {Algorithms}},
	volume = {12},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/12/5/1207-1245/6368},
	doi = {10.1162/089976600300015565},
	abstract = {We built an automated essay scoring system to score approximately 13,000 essay from an online Machine Learning competition on Kaggle.com. There are 8 diﬀerent essay topics and as such, the essays were divided into 8 sets which diﬀered signiﬁcantly in their responses to the our features and evaluation. Our focus for this essay grading was the style of the essay, to which we extended by adding the category of maturity. We evaluated Linear Regression, Regression Tree, Linear Discriminant Analysis, and Support Vector Machines on our features and discovered that SVM achieved the best results with an average κ = 0.78.},
	language = {en},
	number = {5},
	urldate = {2022-08-08},
	journal = {Neural Computation},
	author = {Schölkopf, Bernhard and Smola, Alex J. and Williamson, Robert C. and Bartlett, Peter L.},
	month = may,
	year = {2000},
	pages = {1207--1245},
	file = {New Support Vector Algorithms.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\New Support Vector Algorithms.md:text/plain;Schölkopf et al. - 2000 - New Support Vector Algorithms.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YBIFJSXJ\\Schölkopf et al. - 2000 - New Support Vector Algorithms.pdf:application/pdf},
}

@inproceedings{goffredoFallaciousArgumentClassification2022,
	address = {Vienna, Austria},
	title = {Fallacious {Argument} {Classification} in {Political} {Debates}},
	isbn = {978-1-956792-00-3},
	url = {https://www.ijcai.org/proceedings/2022/575},
	doi = {10.24963/ijcai.2022/575},
	abstract = {Fallacies play a prominent role in argumentation since antiquity due to their contribution to argumentation in critical thinking education. Their role is even more crucial nowadays as contemporary argumentation technologies face challenging tasks as misleading and manipulative information detection in news articles and political discourse, and counter-narrative generation. Despite some work in this direction, the issue of classifying arguments as being fallacious largely remains a challenging and an unsolved task. Our contribution is twofold: first, we present a novel annotated resource of 31 political debates from the U.S. Presidential Campaigns, where we annotated six main categories of fallacious arguments (i.e., ad hominem, appeal to authority, appeal to emotion, false cause, slogan, slippery slope) leading to 1628 annotated fallacious arguments; second, we tackle this novel task of fallacious argument classification and we define a neural architecture based on transformers outperforming state-of-the-art results and standard baselines. Our results show the important role played by argument components and relations in this task.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Goffredo, Pierpaolo and Haddadan, Shohreh and Vorakitphan, Vorakit and Cabrio, Elena and Villata, Serena},
	month = jul,
	year = {2022},
	pages = {4143--4149},
	file = {Fallacious Argument Classification in Political Debates.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Fallacious Argument Classification in Political Debates.md:text/plain;Goffredo et al. - 2022 - Fallacious Argument Classification in Political De.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P7S7PRQ5\\Goffredo et al. - 2022 - Fallacious Argument Classification in Political De.pdf:application/pdf},
}

@article{luIntegratingDeepLearning2021,
	title = {Integrating {Deep} {Learning} into {An} {Automated} {Feedback} {Generation} {System} for {Automated} {Essay} {Scoring}},
	abstract = {Digitalization and automation of test administration, score reporting, and feedback provision have the potential to benefit large-scale and formative assessments. Many studies on automated essay scoring (AES) and feedback generation systems were published in the last decade, but few connected AES and feedback generation within a unified framework. Recent advancements in machine learning algorithms enable researchers to develop more models that explore the potential of automated assessments in education. This study makes the following contributions. First, it implements, compares, and contrasts three AES algorithms with word-embedding and deep learning models (CNN, LSTM, and BiLSTM). Second, it proposes a novel automated feedback generation algorithm based on the Constrained MetropolisHastings Sampling (CGMH). Third, it builds a classifier to integrate AES and feedback generation into a systematic framework. Results show that (1) the scoring accuracy of the AES algorithm outperforms that of state-of-the-art models; and (2) the CGMH method generates semantically-related feedback sentences. The findings support the feasibility of an automated system that combines essay scoring with feedback generation. Implications may lead to the development of models that reveal linguistic features, while achieving high scoring accuracy, as well as to the creation of feedback corpora to generate more semantically-related and sentiment-appropriate feedback.},
	language = {en},
	author = {Lu, Chang and Cutumisu, Maria},
	year = {2021},
	keywords = {go},
	pages = {7},
	file = {Integrating Deep Learning into An Automated Feedback Generation System for Automated Essay Scoring.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Integrating Deep Learning into An Automated Feedback Generation System for Automated Essay Scoring.md:text/plain;Lu and Cutumisu - 2021 - Integrating Deep Learning into An Automated Feedba.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UZIBVPBK\\Lu and Cutumisu - 2021 - Integrating Deep Learning into An Automated Feedba.pdf:application/pdf},
}

@misc{chengIAMComprehensiveLargeScale2022,
	title = {{IAM}: {A} {Comprehensive} and {Large}-{Scale} {Dataset} for {Integrated} {Argument} {Mining} {Tasks}},
	shorttitle = {{IAM}},
	url = {http://arxiv.org/abs/2203.12257},
	abstract = {Traditionally, a debate usually requires a manual preparation process, including reading plenty of articles, selecting the claims, identifying the stances of the claims, seeking the evidence for the claims, etc. As the AI debate attracts more attention these years, it is worth exploring the methods to automate the tedious process involved in the debating system. In this work, we introduce a comprehensive and large dataset named IAM, which can be applied to a series of argument mining tasks, including claim extraction, stance classification, evidence extraction, etc. Our dataset is collected from over 1k articles related to 123 topics. Near 70k sentences in the dataset are fully annotated based on their argument properties (e.g., claims, stances, evidence, etc.). We further propose two new integrated argument mining tasks associated with the debate preparation process: (1) claim extraction with stance classification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a pipeline approach and an end-to-end method for each integrated task separately. Promising experimental results are reported to show the values and challenges of our proposed tasks, and motivate future research on argument mining.},
	language = {en},
	urldate = {2022-08-08},
	publisher = {arXiv},
	author = {Cheng, Liying and Bing, Lidong and He, Ruidan and Yu, Qian and Zhang, Yan and Si, Luo},
	month = jul,
	year = {2022},
	note = {arXiv:2203.12257 [cs]},
	keywords = {Computer Science - Computation and Language, no},
	file = {Cheng et al. - 2022 - IAM A Comprehensive and Large-Scale Dataset for I.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3S6JCEZF\\Cheng et al. - 2022 - IAM A Comprehensive and Large-Scale Dataset for I.pdf:application/pdf;IAM A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks - Comment 11 pages, 3 figures, accepted by ACL 2022.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\IAM A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks - Comment 11 pages, 3 figures, accepted by ACL 2022.md:text/plain;IAM A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\IAM A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks.md:text/plain},
}

@misc{ollingerSameSideStance2020,
	title = {Same {Side} {Stance} {Classification} {Task}: {Facilitating} {Argument} {Stance} {Classification} by {Fine}-tuning a {BERT} {Model}},
	shorttitle = {Same {Side} {Stance} {Classification} {Task}},
	url = {http://arxiv.org/abs/2004.11163},
	abstract = {Research on computational argumentation is currently being intensively investigated. The goal of this community is to ﬁnd the best pro and con arguments for a user given topic either to form an opinion for oneself, or to persuade others to adopt a certain standpoint. While existing argument mining methods can ﬁnd appropriate arguments for a topic, a correct classiﬁcation into pro and con is not yet reliable. The same side stance classiﬁcation task provides a dataset of argument pairs classiﬁed by whether or not both arguments share the same stance and does not need to distinguish between topic-speciﬁc pro and con vocabulary but only the argument similarity within a stance needs to be assessed. The results of our contribution to the task are build on a setup based on the BERT architecture. We ﬁne-tuned a pre-trained BERT model for three epochs and used the ﬁrst 512 tokens of each argument to predict if two arguments share the same stance.},
	language = {en},
	urldate = {2022-08-08},
	publisher = {arXiv},
	author = {Ollinger, Stefan and Dumani, Lorik and Sahitaj, Premtim and Bergmann, Ralph and Schenkel, Ralf},
	month = apr,
	year = {2020},
	note = {arXiv:2004.11163 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Ollinger et al. - 2020 - Same Side Stance Classification Task Facilitating.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SWP7PKPG\\Ollinger et al. - 2020 - Same Side Stance Classification Task Facilitating.pdf:application/pdf;Same Side Stance Classification Task Facilitating Argument Stance Classification by Fine-tuning a BERT Model.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Same Side Stance Classification Task Facilitating Argument Stance Classification by Fine-tuning a BERT Model.md:text/plain},
}

@article{mahanaAutomatedEssayGrading,
	title = {Automated {Essay} {Grading} {Using} {Machine} {Learning}},
	abstract = {The project aims to build an automated essay scoring system using a data set of ≈13000 essays from kaggle.com. These essays were divided into 8 dierent sets based on context. We extracted features such as total word count per essay, sentence count, number of long words, part of speech counts etc from the training set essays. We used a linear regression model to learn from these features and generate parameters for testing and validation. We used 5-fold cross validation to train and test our model rigorously. Further, we used a forward feature selection algorithm to arrive at a combination of features that gives the best score prediction. Quadratic Weighted Kappa, which measures agreement between predicted scores and human scores, was used as an error metric. Our nal model was able to achieve a kappa score of 0.73 across all 8 essay sets. We also got a good insight into what kind of features could improve our model, for example N-Grams and content testing features.},
	language = {en},
	author = {Mahana, Manvi and Johns, Mishel and Apte, Ashwin},
	keywords = {no},
	pages = {5},
	file = {Automated Essay Grading Using Machine Learning.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automated Essay Grading Using Machine Learning.md:text/plain;Mahana et al. - Automated Essay Grading Using Machine Learning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I7AN59GR\\Mahana et al. - Automated Essay Grading Using Machine Learning.pdf:application/pdf},
}

@misc{cozmaAutomatedEssayScoring2018,
	title = {Automated essay scoring with string kernels and word embeddings},
	url = {http://arxiv.org/abs/1804.07954},
	abstract = {In this work, we present an approach based on combining string kernels and word embeddings for automatic essay scoring. String kernels capture the similarity among strings based on counting common character ngrams, which are a low-level yet powerful type of feature, demonstrating state-of-theart results in various text classiﬁcation tasks such as Arabic dialect identiﬁcation or native language identiﬁcation. To our best knowledge, we are the ﬁrst to apply string kernels to automatically score essays. We are also the ﬁrst to combine them with a high-level semantic feature representation, namely the bag-of-super-word-embeddings. We report the best performance on the Automated Student Assessment Prize data set, in both indomain and cross-domain settings, surpassing recent state-of-the-art deep learning approaches.},
	language = {en},
	urldate = {2022-08-08},
	publisher = {arXiv},
	author = {Cozma, Mădălina and Butnaru, Andrei M. and Ionescu, Radu Tudor},
	month = jul,
	year = {2018},
	note = {arXiv:1804.07954 [cs]},
	keywords = {Computer Science - Computation and Language, no},
	file = {Automated essay scoring with string kernels and word embeddings - Comment Accepted at ACL 2018.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automated essay scoring with string kernels and word embeddings - Comment Accepted at ACL 2018.md:text/plain;Automated essay scoring with string kernels and word embeddings.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automated essay scoring with string kernels and word embeddings.md:text/plain;Cozma et al. - 2018 - Automated essay scoring with string kernels and wo.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WQN3QURY\\Cozma et al. - 2018 - Automated essay scoring with string kernels and wo.pdf:application/pdf},
}

@inproceedings{alikaniotisAutomaticTextScoring2016,
	title = {Automatic {Text} {Scoring} {Using} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1606.04289},
	doi = {10.18653/v1/P16-1068},
	abstract = {Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which speciﬁc words contribute to the text’s score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative.},
	language = {en},
	urldate = {2022-08-09},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	author = {Alikaniotis, Dimitrios and Yannakoudakis, Helen and Rei, Marek},
	year = {2016},
	note = {arXiv:1606.04289 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, I.2.6, I.2.7, I.5.1, no},
	pages = {715--725},
	file = {Alikaniotis et al. - 2016 - Automatic Text Scoring Using Neural Networks.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TPVBX3JU\\Alikaniotis et al. - 2016 - Automatic Text Scoring Using Neural Networks.pdf:application/pdf;Automatic Text Scoring Using Neural Networks.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automatic Text Scoring Using Neural Networks.md:text/plain},
}

@inproceedings{wangAutomaticEssayScoring2018,
	address = {Brussels, Belgium},
	title = {Automatic {Essay} {Scoring} {Incorporating} {Rating} {Schema} via {Reinforcement} {Learning}},
	url = {http://aclweb.org/anthology/D18-1090},
	doi = {10.18653/v1/D18-1090},
	abstract = {Automatic essay scoring (AES) is the task of assigning grades to essays without human interference. Existing systems for AES are typically trained to predict the score of each single essay at a time without considering the rating schema. In order to address this issue, we propose a reinforcement learning framework for essay scoring that incorporates quadratic weighted kappa as guidance to optimize the scoring system. Experiment results on benchmark datasets show the effectiveness of our framework.},
	language = {en},
	urldate = {2022-08-09},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Yucheng and Wei, Zhongyu and Zhou, Yaqian and Huang, Xuanjing},
	year = {2018},
	keywords = {no},
	pages = {791--797},
	file = {Automatic Essay Scoring Incorporating Rating Schema via Reinforcement Learning.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Automatic Essay Scoring Incorporating Rating Schema via Reinforcement Learning.md:text/plain;Wang et al. - 2018 - Automatic Essay Scoring Incorporating Rating Schem.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9VRTS6CX\\Wang et al. - 2018 - Automatic Essay Scoring Incorporating Rating Schem.pdf:application/pdf},
}

@inproceedings{mesgarNeuralLocalCoherence2018,
	address = {Brussels, Belgium},
	title = {A {Neural} {Local} {Coherence} {Model} for {Text} {Quality} {Assessment}},
	url = {http://aclweb.org/anthology/D18-1464},
	doi = {10.18653/v1/D18-1464},
	abstract = {We propose a local coherence model that captures the ﬂow of what semantically connects adjacent sentences in a text. We represent the semantics of a sentence by a vector and capture its state at each word of the sentence. We model what relates two adjacent sentences based on the two most similar semantic states, each of which is in one of the sentences. We encode the perceived coherence of a text by a vector, which represents patterns of changes in salient information that relates adjacent sentences. Our experiments demonstrate that our approach is beneﬁcial for two downstream tasks: Readability assessment, in which our model achieves new state-of-the-art results; and essay scoring, in which the combination of our coherence vectors and other taskdependent features signiﬁcantly improves the performance of a strong essay scorer.},
	language = {en},
	urldate = {2022-08-09},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mesgar, Mohsen and Strube, Michael},
	year = {2018},
	keywords = {sota},
	pages = {4328--4339},
	file = {A Neural Local Coherence Model for Text Quality Assessment.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\A Neural Local Coherence Model for Text Quality Assessment.md:text/plain;Mesgar and Strube - 2018 - A Neural Local Coherence Model for Text Quality As.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7ZR86G4K\\Mesgar and Strube - 2018 - A Neural Local Coherence Model for Text Quality As.pdf:application/pdf},
}

@inproceedings{jinTDNNTwostageDeep2018,
	address = {Melbourne, Australia},
	title = {{TDNN}: {A} {Two}-stage {Deep} {Neural} {Network} for {Prompt}-independent {Automated} {Essay} {Scoring}},
	shorttitle = {{TDNN}},
	url = {http://aclweb.org/anthology/P18-1100},
	doi = {10.18653/v1/P18-1100},
	abstract = {Existing automated essay scoring (AES) models rely on rated essays for the target prompt as training data. Despite their successes in prompt-dependent AES, how to effectively predict essay ratings under a prompt-independent setting remains a challenge, where the rated essays for the target prompt are not available. To close this gap, a two-stage deep neural network (TDNN) is proposed. In particular, in the ﬁrst stage, using the rated essays for nontarget prompts as the training data, a shallow model is learned to select essays with an extreme quality for the target prompt, serving as pseudo training data; in the second stage, an end-to-end hybrid deep model is proposed to learn a prompt-dependent rating model consuming the pseudo training data from the ﬁrst step. Evaluation of the proposed TDNN on the standard ASAP dataset demonstrates a promising improvement for the prompt-independent AES task.},
	language = {en},
	urldate = {2022-08-09},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Jin, Cancan and He, Ben and Hui, Kai and Sun, Le},
	year = {2018},
	keywords = {no},
	pages = {1088--1097},
	file = {Jin et al. - 2018 - TDNN A Two-stage Deep Neural Network for Prompt-i.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\V35MY5A4\\Jin et al. - 2018 - TDNN A Two-stage Deep Neural Network for Prompt-i.pdf:application/pdf;TDNN A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\TDNN A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring.md:text/plain},
}

@inproceedings{faragNeuralAutomatedEssay2018,
	address = {New Orleans, Louisiana},
	title = {Neural {Automated} {Essay} {Scoring} and {Coherence} {Modeling} for {Adversarially} {Crafted} {Input}},
	url = {http://aclweb.org/anthology/N18-1024},
	doi = {10.18653/v1/N18-1024},
	abstract = {We demonstrate that current state-of-theart approaches to Automated Essay Scoring (AES) are not well-suited to capturing adversarially crafted input of grammatical but incoherent sequences of sentences. We develop a neural model of local coherence that can effectively learn connectedness features between sentences, and propose a framework for integrating and jointly training the local coherence model with a state-of-the-art AES model. We evaluate our approach against a number of baselines and experimentally demonstrate its effectiveness on both the AES task and the task of ﬂagging adversarial input, further contributing to the development of an approach that strengthens the validity of neural essay scoring models.},
	language = {en},
	urldate = {2022-08-09},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Human} {Language}           {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Farag, Youmna and Yannakoudakis, Helen and Briscoe, Ted},
	year = {2018},
	keywords = {sota, no},
	pages = {263--271},
	file = {Farag et al. - 2018 - Neural Automated Essay Scoring and Coherence Model.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UFTWEVL3\\Farag et al. - 2018 - Neural Automated Essay Scoring and Coherence Model.pdf:application/pdf;Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input.md:text/plain},
}

@inproceedings{dasguptaAugmentingTextualQualitative2018,
	address = {Melbourne, Australia},
	title = {Augmenting {Textual} {Qualitative} {Features} in {Deep} {Convolution} {Recurrent} {Neural} {Network} for {Automatic} {Essay} {Scoring}},
	url = {http://aclweb.org/anthology/W18-3713},
	doi = {10.18653/v1/W18-3713},
	abstract = {In this paper we present a qualitatively enhanced deep convolution recurrent neural network for computing the quality of a text in an automatic essay scoring task. The novelty of the work lies in the fact that instead of considering only the word and sentence representation of a text, we try to augment the different complex linguistic, cognitive and psychological features associated within a text document along with a hierarchical convolution recurrent neural network framework. Our preliminary investigation shows that incorporation of such qualitative feature vectors along with standard word/sentence embeddings can give us better understanding about improving the overall evaluation of the input essays.},
	language = {en},
	urldate = {2022-08-09},
	booktitle = {Proceedings of the 5th {Workshop} on {Natural} {Language} {Processing} {Techniques} for {Educational} {Applications}},
	publisher = {Association for Computational Linguistics},
	author = {Dasgupta, Tirthankar and Naskar, Abir and Dey, Lipika and Saha, Rupsa},
	year = {2018},
	keywords = {sota, go},
	pages = {93--102},
	file = {Augmenting Textual Qualitative Features in Deep Convolution Recurrent Neural Network for Automatic Essay Scoring.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Augmenting Textual Qualitative Features in Deep Convolution Recurrent Neural Network for Automatic Essay Scoring.md:text/plain;Augmenting Textual Qualitative Features in Deep Convolution Recurrent Neural Network for Automatic Essay Scoring.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Augmenting Textual Qualitative Features in Deep Convolution Recurrent Neural Network for Automatic Essay Scoring.md:text/plain;Dasgupta et al. - 2018 - Augmenting Textual Qualitative Features in Deep Co.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9N2WMT3T\\Dasgupta et al. - 2018 - Augmenting Textual Qualitative Features in Deep Co.pdf:application/pdf},
}

@inproceedings{yangEnhancingAutomatedEssay2020,
	address = {Online},
	title = {Enhancing {Automated} {Essay} {Scoring} {Performance} via {Fine}-tuning {Pre}-trained {Language} {Models} with {Combination} of {Regression} and {Ranking}},
	url = {https://www.aclweb.org/anthology/2020.findings-emnlp.141},
	doi = {10.18653/v1/2020.findings-emnlp.141},
	abstract = {Automated Essay Scoring (AES) is a critical text regression task that automatically assigns scores to essays based on their writing quality. Recently, the performance of sentence prediction tasks has been largely improved by using Pre-trained Language Models via fusing representations from different layers, constructing an auxiliary sentence, using multitask learning, etc. However, to solve the AES task, previous works utilize shallow neural networks to learn essay representations and constrain calculated scores with regression loss or ranking loss, respectively. Since shallow neural networks trained on limited samples show poor performance to capture deep semantic of texts. And without an accurate scoring function, ranking loss and regression loss measures two different aspects of the calculated scores. To improve AES’s performance, we ﬁnd a new way to ﬁne-tune pre-trained language models with multiple losses of the same task. In this paper, we propose to utilize a pretrained language model to learn text representations ﬁrst. With scores calculated from the representations, mean square error loss and the batch-wise ListNet loss with dynamic weights constrain the scores simultaneously. We utilize Quadratic Weighted Kappa to evaluate our model on the Automated Student Assessment Prize dataset. Our model outperforms not only state-of-the-art neural models near 3 percent but also the latest statistic model. Especially on the two narrative prompts, our model performs much better than all other state-of-theart models.},
	language = {en},
	urldate = {2022-08-09},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Ruosong and Cao, Jiannong and Wen, Zhiyuan and Wu, Youzheng and He, Xiaodong},
	year = {2020},
	keywords = {sota, go, BERT},
	pages = {1560--1569},
	file = {Yang et al. - 2020 - Enhancing Automated Essay Scoring Performance via .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EGL33IXA\\Yang et al. - 2020 - Enhancing Automated Essay Scoring Performance via .pdf:application/pdf},
}

@article{utoNeuralAutomatedEssay,
	title = {Neural {Automated} {Essay} {Scoring} {Incorporating} {Handcrafted} {Features}},
	abstract = {Automated essay scoring (AES) is the task of automatically assigning scores to essays as an alternative to grading by human raters. Conventional AES typically relies on handcrafted features, whereas recent studies have proposed AES models based on deep neural networks (DNNs) to obviate the need for feature engineering. Furthermore, hybrid methods that integrate handcrafted features in a DNN-AES model have been recently developed and have achieved state-of-the-art accuracy. One of the most popular hybrid methods is formulated as a DNN-AES model with an additional recurrent neural network (RNN) that processes a sequence of handcrafted sentencelevel features. However, this method has the following problems: 1) It cannot incorporate effective essay-level features developed in previous AES research. 2) It greatly increases the numbers of model parameters and tuning parameters, increasing the difﬁculty of model training. 3) It has an additional RNN to process sentence-level features, enabling extension to various DNN-AES models complex. To resolve these problems, we propose a new hybrid method that integrates handcrafted essay-level features into a DNN-AES model. Speciﬁcally, our method concatenates handcrafted essay-level features to a distributed essay representation vector, which is obtained from an intermediate layer of a DNN-AES model. Our method is a simple DNN-AES extension, but signiﬁcantly improves scoring accuracy.},
	language = {en},
	author = {Uto, Masaki and Xie, Yikuan and Ueno, Maomi},
	keywords = {sota, go, BERT},
	pages = {12},
	file = {Uto et al. - Neural Automated Essay Scoring Incorporating Handc.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F2FRNH6F\\Uto et al. - Neural Automated Essay Scoring Incorporating Handc.pdf:application/pdf},
}

@misc{taySkipFlowIncorporatingNeural2017,
	title = {{SkipFlow}: {Incorporating} {Neural} {Coherence} {Features} for {End}-to-{End} {Automatic} {Text} {Scoring}},
	shorttitle = {{SkipFlow}},
	url = {http://arxiv.org/abs/1711.04981},
	abstract = {Deep learning has demonstrated tremendous potential for Automatic Text Scoring (ATS) tasks. In this paper, we describe a new neural architecture that enhances vanilla neural network models with auxiliary neural coherence features. Our new method proposes a new {\textbackslash}textsc\{SkipFlow\} mechanism that models relationships between snapshots of the hidden representations of a long short-term memory (LSTM) network as it reads. Subsequently, the semantic relationships between multiple snapshots are used as auxiliary features for prediction. This has two main benefits. Firstly, essays are typically long sequences and therefore the memorization capability of the LSTM network may be insufficient. Implicit access to multiple snapshots can alleviate this problem by acting as a protection against vanishing gradients. The parameters of the {\textbackslash}textsc\{SkipFlow\} mechanism also acts as an auxiliary memory. Secondly, modeling relationships between multiple positions allows our model to learn features that represent and approximate textual coherence. In our model, we call this {\textbackslash}textit\{neural coherence\} features. Overall, we present a unified deep learning architecture that generates neural coherence features as it reads in an end-to-end fashion. Our approach demonstrates state-of-the-art performance on the benchmark ASAP dataset, outperforming not only feature engineering baselines but also other deep learning models.},
	urldate = {2022-08-09},
	publisher = {arXiv},
	author = {Tay, Yi and Phan, Minh C. and Tuan, Luu Anh and Hui, Siu Cheung},
	month = nov,
	year = {2017},
	note = {arXiv:1711.04981 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, go},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XXTHM7JJ\\1711.html:text/html;Tay et al_2017_SkipFlow.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PM9HEN6Z\\Tay et al_2017_SkipFlow.pdf:application/pdf},
}

@inproceedings{razonAutomatedEssayContent2010,
	address = {Kuala Lumpur, Malaysia},
	title = {Automated essay content analysis based on {Concept} {Indexing} with {Fuzzy} {C}-means clustering},
	isbn = {978-1-4244-7454-7},
	url = {http://ieeexplore.ieee.org/document/5775058/},
	doi = {10.1109/APCCAS.2010.5775058},
	urldate = {2022-08-09},
	booktitle = {2010 {IEEE} {Asia} {Pacific} {Conference} on {Circuits} and {Systems}},
	publisher = {IEEE},
	author = {Razon, Abigail R. and Vargas, Ma. Lourdes J. and Guevara, Rowena Cristina L. and Naval, Prospero C.},
	month = dec,
	year = {2010},
	pages = {1167--1170},
}

@inproceedings{wangIntelligentAutogradingSystem2018,
	address = {Nanjing, China},
	title = {Intelligent {Auto}-grading {System}},
	isbn = {978-1-5386-6005-8},
	url = {https://ieeexplore.ieee.org/document/8691244/},
	doi = {10.1109/CCIS.2018.8691244},
	urldate = {2022-08-09},
	booktitle = {2018 5th {IEEE} {International} {Conference} on {Cloud} {Computing} and {Intelligence} {Systems} ({CCIS})},
	publisher = {IEEE},
	author = {Wang, Zining and Liu, Jianli and Dong, Ruihai},
	month = nov,
	year = {2018},
	keywords = {go},
	pages = {430--435},
}

@article{Honnibal_spaCy_Industrial-strength_Natural_2020,
	title = {{spaCy}: {Industrial}-strength natural language processing in python},
	doi = {10.5281/zenodo.1212303},
	author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
	year = {2020},
}

@article{zenkerInvestigatingMinimumText2021,
	title = {Investigating minimum text lengths for lexical diversity indices},
	volume = {47},
	issn = {10752935},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1075293520300660},
	doi = {10.1016/j.asw.2020.100505},
	language = {en},
	urldate = {2022-08-13},
	journal = {Assessing Writing},
	author = {Zenker, Fred and Kyle, Kristopher},
	month = jan,
	year = {2021},
	pages = {100505},
}

@article{jandaSyntacticSemanticSentiment2019a,
	title = {Syntactic, {Semantic} and {Sentiment} {Analysis}: {The} {Joint} {Effect} on {Automated} {Essay} {Evaluation}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Syntactic, {Semantic} and {Sentiment} {Analysis}},
	url = {https://ieeexplore.ieee.org/document/8788526/},
	doi = {10.1109/ACCESS.2019.2933354},
	abstract = {Manual grading of essays by humans is time-consuming and likely to be susceptible to inconsistencies and inaccuracies. In recent years, an abundance of research has been done to automate essay evaluation processes, yet little has been done to take into consideration the syntax, semantic coherence and sentiments of the essay’s text together. Our proposed system incorporates not just the rule-based grammar and surface level coherence check but also includes the semantic similarity of the sentences. We propose to use Graph-based relationships within the essay’s content and polarity of opinion expressions. Semantic similarity is determined between each statement of the essay to form these Graph-based spatial relationships and novel features are obtained from it. Our algorithm uses 23 salient features with high predictive power, which is less than the current systems while considering every aspect to cover the dimensions that a human grader focuses on. Fewer features help us get rid of the redundancies of the data so that the predictions are based on more representative features and are robust to noisy data. The prediction of the scores is done with neural networks using the data released by the ASAP competition held by Kaggle. The resulting agreement between human grader’s score and the system’s prediction is measured using Quadratic Weighted Kappa (QWK). Our system produces a QWK of 0.793.},
	language = {en},
	urldate = {2022-08-13},
	journal = {IEEE Access},
	author = {Janda, Harneet Kaur and Pawar, Atish and Du, Shan and Mago, Vijay},
	year = {2019},
	keywords = {feature engineering, go},
	pages = {108486--108503},
	file = {Janda et al_2019_Syntactic, Semantic and Sentiment Analysis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2R5GRF3Z\\Janda et al_2019_Syntactic, Semantic and Sentiment Analysis.pdf:application/pdf;Syntactic, Semantic and Sentiment Analysis The Joint Effect on Automated Essay Evaluation.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Syntactic, Semantic and Sentiment Analysis The Joint Effect on Automated Essay Evaluation.md:text/plain},
}

@article{luAutomaticAnalysisSyntactic2010,
	title = {Automatic analysis of syntactic complexity in second language writing},
	volume = {15},
	issn = {1384-6655, 1569-9811},
	url = {http://www.jbe-platform.com/content/journals/10.1075/ijcl.15.4.02lu},
	doi = {10.1075/ijcl.15.4.02lu},
	abstract = {We describe a computational system for automatic analysis of syntactic complexity in second language writing using fourteen different measures that have been explored or proposed in studies of second language development. The system takes a written language sample as input and produces fourteen indices of syntactic complexity of the sample based on these measures. The system is designed with advanced second language proficiency research in mind, and is therefore developed and evaluated using college-level second language writing data from the Written English Corpus of Chinese Learners (Wen et al. 2005). Experimental results show that the system achieves very high reliability on unseen test data from the corpus. We illustrate how the system is used in an example application to investigate whether and to what extent each of these measures significantly differentiate between different proficiency levels},
	language = {en},
	number = {4},
	urldate = {2022-08-14},
	journal = {International Journal of Corpus Linguistics},
	author = {Lu, Xiaofei},
	month = nov,
	year = {2010},
	pages = {474--496},
}

@article{kyleAutomaticallyAssessingLexical2015,
	title = {Automatically {Assessing} {Lexical} {Sophistication}: {Indices}, {Tools}, {Findings}, and {Application}},
	volume = {49},
	issn = {00398322},
	shorttitle = {Automatically {Assessing} {Lexical} {Sophistication}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/tesq.194},
	doi = {10.1002/tesq.194},
	language = {en},
	number = {4},
	urldate = {2022-08-14},
	journal = {TESOL Quarterly},
	author = {Kyle, Kristopher and Crossley, Scott A.},
	month = dec,
	year = {2015},
	pages = {757--786},
}

@article{kyleToolAutomaticAnalysis2018,
	title = {The tool for the automatic analysis of lexical sophistication ({TAALES}): version 2.0},
	volume = {50},
	issn = {1554-3528},
	shorttitle = {The tool for the automatic analysis of lexical sophistication ({TAALES})},
	url = {http://link.springer.com/10.3758/s13428-017-0924-4},
	doi = {10.3758/s13428-017-0924-4},
	language = {en},
	number = {3},
	urldate = {2022-08-14},
	journal = {Behavior Research Methods},
	author = {Kyle, Kristopher and Crossley, Scott and Berger, Cynthia},
	month = jun,
	year = {2018},
	pages = {1030--1046},
	file = {Kyle et al_2018_The tool for the automatic analysis of lexical sophistication (TAALES).pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2BAAYNYP\\Kyle et al_2018_The tool for the automatic analysis of lexical sophistication (TAALES).pdf:application/pdf},
}

@article{kyleAssessingValidityLexical2021,
	title = {Assessing the {Validity} of {Lexical} {Diversity} {Indices} {Using} {Direct} {Judgements}},
	volume = {18},
	issn = {1543-4303, 1543-4311},
	url = {https://www.tandfonline.com/doi/full/10.1080/15434303.2020.1844205},
	doi = {10.1080/15434303.2020.1844205},
	language = {en},
	number = {2},
	urldate = {2022-08-14},
	journal = {Language Assessment Quarterly},
	author = {Kyle, Kristopher and Crossley, Scott A. and Jarvis, Scott},
	month = mar,
	year = {2021},
	pages = {154--170},
}

@article{crossleyAnalyzingDiscourseProcessing2014,
	title = {Analyzing {Discourse} {Processing} {Using} a {Simple} {Natural} {Language} {Processing} {Tool}},
	volume = {51},
	issn = {0163-853X, 1532-6950},
	url = {http://www.tandfonline.com/doi/abs/10.1080/0163853X.2014.910723},
	doi = {10.1080/0163853X.2014.910723},
	language = {en},
	number = {5-6},
	urldate = {2022-09-05},
	journal = {Discourse Processes},
	author = {Crossley, Scott A. and Allen, Laura K. and Kyle, Kristopher and McNamara, Danielle S.},
	month = jul,
	year = {2014},
	pages = {511--534},
}

@article{crossleyUsingHumanJudgments2019,
	title = {Using human judgments to examine the validity of automated grammar, syntax, and mechanical errors in writing},
	volume = {11},
	issn = {2030-1006, 2294-3307},
	url = {https://www.jowr.org/index.php/jowr/article/view/591},
	doi = {10.17239/jowr-2019.11.02.01},
	language = {en},
	number = {vol. 11 issue 2},
	urldate = {2022-09-05},
	journal = {Journal of Writing Research},
	author = {Crossley, Scott},
	month = oct,
	year = {2019},
	pages = {251--270},
	file = {Crossley_2019_Using human judgments to examine the validity of automated grammar, syntax, and.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\R9ANZN58\\Crossley_2019_Using human judgments to examine the validity of automated grammar, syntax, and.pdf:application/pdf},
}

@article{crossleyToolAutomaticAnalysis2016,
	title = {The tool for the automatic analysis of text cohesion ({TAACO}): {Automatic} assessment of local, global, and text cohesion},
	volume = {48},
	issn = {1554-3528},
	shorttitle = {The tool for the automatic analysis of text cohesion ({TAACO})},
	url = {http://link.springer.com/10.3758/s13428-015-0651-7},
	doi = {10.3758/s13428-015-0651-7},
	language = {en},
	number = {4},
	urldate = {2022-09-05},
	journal = {Behavior Research Methods},
	author = {Crossley, Scott A. and Kyle, Kristopher and McNamara, Danielle S.},
	month = dec,
	year = {2016},
	pages = {1227--1237},
	file = {Crossley et al_2016_The tool for the automatic analysis of text cohesion (TAACO).pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XCQPQAEZ\\Crossley et al_2016_The tool for the automatic analysis of text cohesion (TAACO).pdf:application/pdf},
}

@phdthesis{kyleMeasuringSyntacticDevelopment,
	title = {Measuring {Syntactic} {Development} in {L2} {Writing}: {Fine} {Grained} {Indices} of {Syntactic} {Complexity} and {Usage}-{Based} {Indices} of {Syntactic} {Sophistication}},
	shorttitle = {Measuring {Syntactic} {Development} in {L2} {Writing}},
	url = {https://scholarworks.gsu.edu/alesl_diss/35},
	abstract = {Syntactic complexity has been an area of significant interest in L2 writing development studies over the past 45 years. Despite the regularity in which syntactic complexity measures have been employed, the construct is still relatively under-developed, and, as a result, the cumulative results of syntactic complexity studies can appear opaque. At least three reasons exist for the current state of affairs, namely the lack of consistency and clarity by which indices of syntactic complexity have been described, the overly broad nature of the indices that have been regularly employed, and the omission of indices that focus on usage-based perspectives. This study seeks to address these three gaps through the development and validation of the Tool for the Automatic Assessment of Syntactic Sophistication and Complexity (TAASSC). TAASSC measures large and fined grained clausal and phrasal indices of syntactic complexity and usage-based frequency/contingency indices of syntactic sophistication. Using TAASSC, this study will address L2 writing development in two main ways: through the examination of syntactic development longitudinally and through the examination of human judgments of writing proficiency (e.g., expert ratings of TOEFL essays). This study will have important implications for second language acquisition, second language writing, and language assessment.},
	urldate = {2022-09-05},
	school = {Georgia State University},
	author = {Kyle, Kristopher},
	doi = {10.57709/8501051},
}

@article{weigleEnglishLanguageLearners2013,
	title = {English language learners and automated scoring of essays: {Critical} considerations},
	volume = {18},
	issn = {10752935},
	shorttitle = {English language learners and automated scoring of essays},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1075293512000499},
	doi = {10.1016/j.asw.2012.10.006},
	language = {en},
	number = {1},
	urldate = {2022-09-05},
	journal = {Assessing Writing},
	author = {Weigle, Sara Cushing},
	month = jan,
	year = {2013},
	pages = {85--99},
}

@article{geneseeEnglishLanguageLearners2005,
	title = {English {Language} {Learners} in {U}.{S}. {Schools}: {An} {Overview} of {Research} {Findings}},
	volume = {10},
	issn = {1082-4669, 1532-7671},
	shorttitle = {English {Language} {Learners} in {U}.{S}. {Schools}},
	url = {http://www.tandfonline.com/doi/abs/10.1207/s15327671espr1004_2},
	doi = {10.1207/s15327671espr1004_2},
	language = {en},
	number = {4},
	urldate = {2022-09-06},
	journal = {Journal of Education for Students Placed at Risk (JESPAR)},
	author = {Genesee, Fred and Lindholm-Leary, Kathryn and Saunders, William and Christian, Donna},
	month = aug,
	year = {2005},
	pages = {363--385},
}

@article{saundersGapThatCan2013,
	title = {The {Gap} {That} {Can}’t {Go} {Away}: {The} {Catch}-22 of {Reclassification} in {Monitoring} the {Progress} of {English} {Learners}},
	volume = {35},
	issn = {0162-3737, 1935-1062},
	shorttitle = {The {Gap} {That} {Can}’t {Go} {Away}},
	url = {http://journals.sagepub.com/doi/10.3102/0162373712461849},
	doi = {10.3102/0162373712461849},
	abstract = {When English Learners (ELs) demonstrate English language proficiency, they are reclassified as Fluent English Proficient (RFEP). Subsequently they are often left out of the analysis of EL progress because they are, technically, no longer ELs. This article examines the effects of including and excluding RFEPs from the analysis of EL progress. Based on statewide achievement data from California including ELs, RFEPs, IELs (all initially identified English Learners: ELs + RFEPs), and English-only students (EOs), the analysis demonstrates that focusing on current ELs and excluding RFEPs (a) underestimates the population of IELs, (b) overestimates the achievement gap between IELs and EOs, and (c) decreases the likelihood of detecting progress when positive changes in achievement have taken place over time. Implications are discussed.},
	language = {en},
	number = {2},
	urldate = {2022-09-06},
	journal = {Educational Evaluation and Policy Analysis},
	author = {Saunders, William M. and Marcelletti, David J.},
	month = jun,
	year = {2013},
	pages = {139--156},
}

@misc{FeedbackPrizeEnglish,
	title = {Feedback {Prize} - {English} {Language} {Learning}},
	url = {https://kaggle.com/competitions/feedback-prize-english-language-learning},
	abstract = {Evaluating language knowledge of ELL students from grades 8-12},
	language = {en},
	urldate = {2022-09-06},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FS29VLC6\\overview.html:text/html},
}

@techreport{yannakoudakisAutomatedAssessmentEnglishlearner,
	title = {Automated assessment of {English}-learner writing},
	url = {https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-842.html},
	abstract = {In this thesis, we investigate automated assessment (AA) systems of free text that automatically analyse and score the quality of writing of learners of English as a second (or other) language. Previous research has employed techniques that measure, in addition to writing competence, the semantic relevance of a text written in response to a given prompt. We argue that an approach which does not rely on task-dependent components or data, and directly assesses learner English, can produce results as good as prompt-specific models. Furthermore, it has the advantage that it may not require re-training or tuning for new prompts or assessment tasks. We evaluate the performance of our models against human scores, manually annotated in the Cambridge Learner Corpus, a subset of which we have released in the public domain to facilitate further research on the task.

We address AA as a supervised discriminative machine learning problem, investigate methods for assessing different aspects of writing prose, examine their generalisation to different corpora, and present state-of-the-art models. We focus on scoring general linguistic competence and discourse coherence and cohesion, and report experiments on detailed analysis of appropriate techniques and feature types derived automatically from generic text processing tools, on their relative importance and contribution to performance, and on comparison with different discriminative models, whilst also experimentally motivating novel feature types for the task. Using outlier texts, we examine and address validity issues of AA systems and, more specifically, their robustness to subversion by writers who understand something of their workings. Finally, we present a user interface that visualises and uncovers the ‘marking criteria’ represented in AA models, that is, textual features identified as highly predictive of a learner’s level of attainment. We demonstrate how the tool can support their linguistic interpretation and enhance hypothesis formation about learner grammars, in addition to informing the development of AA systems and further improving their performance.},
	urldate = {2022-09-06},
	institution = {Computer Laboratory, University of Cambridge},
	author = {Yannakoudakis, Helen},
	doi = {10.48456/TR-842},
	note = {Artwork Size: 151 pages
Medium: PDF},
	pages = {151 pages},
}

@misc{CohesionLinguistics2022,
	title = {Cohesion (linguistics)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Cohesion_(linguistics)&oldid=1093429674},
	abstract = {Cohesion is the grammatical and lexical linking within a text or sentence that holds a text together and gives it meaning. It is related to the broader concept of coherence.
There are two main types of cohesion:

grammatical cohesion: based on structural content
lexical cohesion: based on lexical content and background knowledge.A cohesive text is created in many different ways. In Cohesion in English, M.A.K. Halliday and Ruqaiya Hasan identify five general categories of cohesive devices that create coherence in texts: reference, ellipsis, substitution, lexical cohesion and conjunction.},
	language = {en},
	urldate = {2022-09-06},
	journal = {Wikipedia},
	month = jun,
	year = {2022},
	note = {Page Version ID: 1093429674},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Q6TDIC5D\\Cohesion_(linguistics).html:text/html},
}

@misc{Syntax2022,
	title = {Syntax},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Syntax&oldid=1108035811},
	abstract = {In linguistics, syntax () is the study of how words and morphemes combine to form larger units such as phrases and sentences. Central concerns of syntax include word order, grammatical relations, hierarchical sentence structure (constituency), agreement, the nature of crosslinguistic variation, and the relationship between form and meaning (semantics). There are numerous approaches to syntax that differ in their central assumptions and goals.},
	language = {en},
	urldate = {2022-09-06},
	journal = {Wikipedia},
	month = sep,
	year = {2022},
	note = {Page Version ID: 1108035811},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D3UANEP9\\Syntax.html:text/html},
}

@misc{Phraseology2021,
	title = {Phraseology},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Phraseology&oldid=1043391934},
	abstract = {In linguistics, phraseology is the study of set or fixed expressions, such as idioms, phrasal verbs, and other types of multi-word lexical units (often collectively referred to as phrasemes), in which the component parts of the expression take on a meaning more specific than, or otherwise not predictable from, the sum of their meanings when used independently. For example, ‘Dutch auction’ is composed of the words Dutch ‘of or pertaining to the Netherlands’ and auction ‘a public sale in which goods are sold to the highest bidder’, but its meaning is not ‘a sale in the Netherlands where goods are sold to the highest bidder’; instead, the phrase has a conventionalized meaning referring to any auction where, instead of rising, the prices fall.},
	language = {en},
	urldate = {2022-09-06},
	journal = {Wikipedia},
	month = sep,
	year = {2021},
	note = {Page Version ID: 1043391934},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\G4EPR7VN\\Phraseology.html:text/html},
}

@misc{Grammar2022,
	title = {Grammar},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Grammar&oldid=1108166280},
	abstract = {In linguistics, the grammar of a natural language is its set of structural constraints on speakers' or writers' composition of clauses, phrases, and words. The term can also refer to the study of such constraints, a field that includes domains such as phonology, morphology, and syntax, often complemented by phonetics, semantics, and pragmatics. There are currently two different approaches to the study of grammar: traditional grammar and theoretical grammar.
Fluent speakers of a language variety or lect have effectively internalized these constraints, the vast majority of which – at least in the case of one's native language(s) – are acquired not by conscious study or instruction but by hearing other speakers. Much of this internalization occurs during early childhood; learning a language later in life usually involves more explicit instruction. In this view, grammar is understood as the cognitive information underlying a specific instance of language production.
The term "grammar" can also describe the linguistic behavior of groups of speakers and writers, rather than individuals. Differences in scales are important to this sense of the word: for example, the term "English grammar" could refer to the whole of English grammar (that is, to the grammars of all the speakers of the language), in which case the term encompasses a great deal of variation. At a smaller scale, it may refer only to what is shared among the grammars of all or most English speakers (such as subject–verb–object word order in simple declarative sentences). At the smallest scale, this sense of "grammar" can describe the conventions of just one relatively well-defined form of English (such as standard English for a region).
A description, study, or analysis of such rules may also be referred to as grammar. A reference book describing the grammar of a language is called a "reference grammar" or simply "a grammar" (see History of English grammars). A fully explicit grammar which exhaustively describes the grammatical constructions of a particular speech variety is called descriptive grammar. This kind of linguistic description contrasts with linguistic prescription, an attempt to actively discourage or suppress some grammatical constructions, while codifying and promoting others, either in an absolute sense or about a standard variety. For example, some prescriptivists maintain that sentences in English should not end with prepositions, a prohibition that has been traced to John Dryden (13 April 1668 – January 1688) whose unexplained objection to the practice perhaps led other English speakers to avoid the construction and discourage its use. Yet preposition stranding has a long history in Germanic languages like English, where it is so widespread as to be a standard usage.
Outside linguistics, the term grammar is often used in a rather different sense. It may be used more broadly to include conventions of spelling and punctuation, which linguists would not typically consider as part of grammar but rather as part of orthography, the conventions used for writing a language. It may also be used more narrowly to refer to a set of prescriptive norms only, excluding those aspects of a language's grammar which are not subject to variation or debate on their normative acceptability. Jeremy Butterfield claimed that, for non-linguists, "Grammar is often a generic way of referring to any aspect of English that people object to."},
	language = {en},
	urldate = {2022-09-06},
	journal = {Wikipedia},
	month = sep,
	year = {2022},
	note = {Page Version ID: 1108166280},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PVPVBGXT\\Grammar.html:text/html},
}

@article{robinsonKnowingLinguisticConventions2014,
	title = {Knowing linguistic conventions},
	volume = {33},
	issn = {0258-0136, 2073-4867},
	url = {http://www.tandfonline.com/doi/full/10.1080/02580136.2014.923692},
	doi = {10.1080/02580136.2014.923692},
	language = {en},
	number = {2},
	urldate = {2022-09-06},
	journal = {South African Journal of Philosophy},
	author = {Robinson, Carin},
	month = apr,
	year = {2014},
	pages = {167--176},
}

@article{husseinAutomatedLanguageEssay2019,
	title = {Automated language essay scoring systems: a literature review},
	volume = {5},
	issn = {2376-5992},
	shorttitle = {Automated language essay scoring systems},
	url = {https://peerj.com/articles/cs-208},
	doi = {10.7717/peerj-cs.208},
	abstract = {Background
              Writing composition is a significant factor for measuring test-takers’ ability in any language exam. However, the assessment (scoring) of these writing compositions or essays is a very challenging process in terms of reliability and time. The need for objective and quick scores has raised the need for a computer system that can automatically grade essay questions targeting specific prompts. Automated Essay Scoring (AES) systems are used to overcome the challenges of scoring writing tasks by using Natural Language Processing (NLP) and machine learning techniques. The purpose of this paper is to review the literature for the AES systems used for grading the essay questions.
            
            
              Methodology
              We have reviewed the existing literature using Google Scholar, EBSCO and ERIC to search for the terms “AES”, “Automated Essay Scoring”, “Automated Essay Grading”, or “Automatic Essay” for essays written in English language. Two categories have been identified: handcrafted features and automatically featured AES systems. The systems of the former category are closely bonded to the quality of the designed features. On the other hand, the systems of the latter category are based on the automatic learning of the features and relations between an essay and its score without any handcrafted features. We reviewed the systems of the two categories in terms of system primary focus, technique(s) used in the system, the need for training data, instructional application (feedback system), and the correlation between e-scores and human scores. The paper includes three main sections. First, we present a structured literature review of the available Handcrafted Features AES systems. Second, we present a structured literature review of the available Automatic Featuring AES systems. Finally, we draw a set of discussions and conclusions.
            
            
              Results
              AES models have been found to utilize a broad range of manually-tuned shallow and deep linguistic features. AES systems have many strengths in reducing labor-intensive marking activities, ensuring a consistent application of scoring criteria, and ensuring the objectivity of scoring. Although many techniques have been implemented to improve the AES systems, three primary challenges have been identified. The challenges are lacking of the sense of the rater as a person, the potential that the systems can be deceived into giving a lower or higher score to an essay than it deserves, and the limited ability to assess the creativity of the ideas and propositions and evaluate their practicality. Many techniques have only been used to address the first two challenges.},
	language = {en},
	urldate = {2022-09-06},
	journal = {PeerJ Computer Science},
	author = {Hussein, Mohamed Abdellatif and Hassan, Hesham and Nassef, Mohammad},
	month = aug,
	year = {2019},
	pages = {e208},
	file = {Hussein et al_2019_Automated language essay scoring systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MBPMVG6Z\\Hussein et al_2019_Automated language essay scoring systems.pdf:application/pdf},
}

@inproceedings{chenResearchAutomaticEssay2019,
	address = {Chengdu, China},
	title = {Research on {Automatic} {Essay} {Scoring} of {Composition} {Based} on {CNN} and {OR}},
	isbn = {978-1-72810-829-2 978-1-72810-831-5},
	url = {https://ieeexplore.ieee.org/document/8837007/},
	doi = {10.1109/ICAIBD.2019.8837007},
	urldate = {2022-09-06},
	booktitle = {2019 2nd {International} {Conference} on {Artificial} {Intelligence} and {Big} {Data} ({ICAIBD})},
	publisher = {IEEE},
	author = {Chen, Zhiyun and Zhou, Yuxin},
	month = may,
	year = {2019},
	pages = {13--18},
}

@article{hacohen-kernerInfluencePreprocessingText2020,
	title = {The influence of preprocessing on text classification using a bag-of-words representation},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0232525},
	doi = {10.1371/journal.pone.0232525},
	language = {en},
	number = {5},
	urldate = {2022-09-07},
	journal = {PLOS ONE},
	author = {HaCohen-Kerner, Yaakov and Miller, Daniel and Yigal, Yair},
	editor = {Zhang, Weinan},
	month = may,
	year = {2020},
	pages = {e0232525},
	file = {HaCohen-Kerner et al_2020_The influence of preprocessing on text classification using a bag-of-words.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DWXUVF9C\\HaCohen-Kerner et al_2020_The influence of preprocessing on text classification using a bag-of-words.pdf:application/pdf},
}

@article{mohapatraDistributionPreservingTraintest2022,
	title = {Distribution preserving train-test split directed ensemble classifier for heart disease prediction},
	volume = {14},
	issn = {2511-2104, 2511-2112},
	url = {https://link.springer.com/10.1007/s41870-022-00868-2},
	doi = {10.1007/s41870-022-00868-2},
	language = {en},
	number = {4},
	urldate = {2022-09-07},
	journal = {International Journal of Information Technology},
	author = {Mohapatra, Debasis and Bhoi, Sourav Kumar and Mallick, Chittaranjan and Jena, Kalyan Kumar and Mishra, Satrujit},
	month = jun,
	year = {2022},
	pages = {1763--1769},
}

@book{kublerDependencyParsing2009,
	address = {Cham},
	title = {Dependency {Parsing}},
	isbn = {978-3-031-01003-3 978-3-031-02131-2},
	url = {https://link.springer.com/10.1007/978-3-031-02131-2},
	language = {en},
	urldate = {2022-09-07},
	publisher = {Springer International Publishing},
	author = {Kübler, Sandra and McDonald, Ryan and Nivre, Joakim},
	year = {2009},
	doi = {10.1007/978-3-031-02131-2},
}

@article{zhengAssessingReadabilityMedical2018,
	title = {Assessing the {Readability} of {Medical} {Documents}: {A} {Ranking} {Approach}},
	volume = {6},
	issn = {2291-9694},
	shorttitle = {Assessing the {Readability} of {Medical} {Documents}},
	url = {http://medinform.jmir.org/2018/1/e17/},
	doi = {10.2196/medinform.8611},
	language = {en},
	number = {1},
	urldate = {2022-09-08},
	journal = {JMIR Medical Informatics},
	author = {Zheng, Jiaping and Yu, Hong},
	month = mar,
	year = {2018},
	pages = {e17},
	file = {Zheng_Yu_2018_Assessing the Readability of Medical Documents.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QXBH7Q8F\\Zheng_Yu_2018_Assessing the Readability of Medical Documents.pdf:application/pdf},
}

@book{2021IEEE18th2021,
	address = {Piscataway, New Jersey},
	title = {2021 {IEEE} 18th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI}): {April} 13-16, 2021, {Nice}, {France}},
	shorttitle = {2021 {IEEE} 18th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	language = {eng},
	publisher = {IEEE},
	year = {2021},
	note = {OCLC: 1259214401},
}

@inproceedings{shaSpinalFractureLesions2020,
	address = {Dalian, China},
	title = {Spinal fracture lesions segmentation based on {U}-net},
	isbn = {978-1-72817-005-3},
	url = {https://ieeexplore.ieee.org/document/9182574/},
	doi = {10.1109/ICAICA50127.2020.9182574},
	urldate = {2022-09-08},
	booktitle = {2020 {IEEE} {International} {Conference} on {Artificial} {Intelligence} and {Computer} {Applications} ({ICAICA})},
	publisher = {IEEE},
	author = {Sha, Gang and Wu, Junsheng and Yu, Bin},
	month = jun,
	year = {2020},
	pages = {142--144},
}

@book{Proceedings2020IEEE2020,
	address = {Piscataway, New Jersey},
	title = {Proceedings of 2020 {IEEE} {International} {Conference} on {Artificial} {Intelligence} and {Computer} {Applications}: {ICAICA} 2020 : {Dalian}, {China}, {June} 27-29, 2020},
	shorttitle = {Proceedings of 2020 {IEEE} {International} {Conference} on {Artificial} {Intelligence} and {Computer} {Applications}},
	language = {eng},
	publisher = {IEEE},
	year = {2020},
	note = {OCLC: 1198175632},
}

@book{Proceedings2020IEEE2020a,
	address = {Piscataway, New Jersey},
	title = {Proceedings of 2020 {IEEE} {International} {Conference} on {Artificial} {Intelligence} and {Information} {Systems}: {ICAIIS} : {March} 20-22, 2020, {Dalian}, {China}},
	shorttitle = {Proceedings of 2020 {IEEE} {International} {Conference} on {Artificial} {Intelligence} and {Information} {Systems}},
	language = {eng},
	publisher = {IEEE},
	year = {2020},
	note = {OCLC: 1200517217},
}

@article{smallCTCervicalSpine2021,
	title = {{CT} {Cervical} {Spine} {Fracture} {Detection} {Using} a {Convolutional} {Neural} {Network}},
	volume = {42},
	issn = {0195-6108, 1936-959X},
	url = {http://www.ajnr.org/lookup/doi/10.3174/ajnr.A7094},
	doi = {10.3174/ajnr.A7094},
	language = {en},
	number = {7},
	urldate = {2022-09-08},
	journal = {American Journal of Neuroradiology},
	author = {Small, J.E. and Osler, P. and Paul, A.B. and Kunst, M.},
	month = jul,
	year = {2021},
	pages = {1341--1347},
	file = {Small et al_2021_CT Cervical Spine Fracture Detection Using a Convolutional Neural Network.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PNGQYSNZ\\Small et al_2021_CT Cervical Spine Fracture Detection Using a Convolutional Neural Network.pdf:application/pdf},
}

@article{voterDiagnosticAccuracyFailure2021,
	title = {Diagnostic {Accuracy} and {Failure} {Mode} {Analysis} of a {Deep} {Learning} {Algorithm} for the {Detection} of {Cervical} {Spine} {Fractures}},
	volume = {42},
	issn = {0195-6108, 1936-959X},
	url = {http://www.ajnr.org/lookup/doi/10.3174/ajnr.A7179},
	doi = {10.3174/ajnr.A7179},
	language = {en},
	number = {8},
	urldate = {2022-09-08},
	journal = {American Journal of Neuroradiology},
	author = {Voter, A.F. and Larson, M.E. and Garrett, J.W. and Yu, J.-P.J.},
	month = aug,
	year = {2021},
	pages = {1550--1556},
	file = {Voter et al_2021_Diagnostic Accuracy and Failure Mode Analysis of a Deep Learning Algorithm for.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3NZB6ZKT\\Voter et al_2021_Diagnostic Accuracy and Failure Mode Analysis of a Deep Learning Algorithm for.pdf:application/pdf},
}

@article{linkUsingAIImprove2022,
	title = {Using {AI} to {Improve} {Radiographic} {Fracture} {Detection}},
	volume = {302},
	issn = {0033-8419, 1527-1315},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.212364},
	doi = {10.1148/radiol.212364},
	language = {en},
	number = {3},
	urldate = {2022-09-08},
	journal = {Radiology},
	author = {Link, Thomas M. and Pedoia, Valentina},
	month = mar,
	year = {2022},
	pages = {637--638},
}

@misc{nicolaesDetectionVertebralFractures2019,
	title = {Detection of vertebral fractures in {CT} using {3D} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1911.01816},
	abstract = {Osteoporosis induced fractures occur worldwide about every 3 seconds. Vertebral compression fractures are early signs of the disease and considered risk predictors for secondary osteoporotic fractures. We present a detection method to opportunistically screen spine-containing CT images for the presence of these vertebral fractures. Inspired by radiology practice, existing methods are based on 2D and 2.5D features but we present, to the best of our knowledge, the first method for detecting vertebral fractures in CT using automatically learned 3D feature maps. The presented method explicitly localizes these fractures allowing radiologists to interpret its results. We train a voxel-classification 3D Convolutional Neural Network (CNN) with a training database of 90 cases that has been semi-automatically generated using radiologist readings that are readily available in clinical practice. Our 3D method produces an Area Under the Curve (AUC) of 95\% for patient-level fracture detection and an AUC of 93\% for vertebra-level fracture detection in a five-fold cross-validation experiment.},
	urldate = {2022-09-08},
	publisher = {arXiv},
	author = {Nicolaes, Joeri and Raeymaeckers, Steven and Robben, David and Wilms, Guido and Vandermeulen, Dirk and Libanati, Cesar and Debois, Marc},
	month = nov,
	year = {2019},
	note = {arXiv:1911.01816 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5WJ5KU2V\\1911.html:text/html;Nicolaes et al_2019_Detection of vertebral fractures in CT using 3D Convolutional Neural Networks.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XTVVNHVI\\Nicolaes et al_2019_Detection of vertebral fractures in CT using 3D Convolutional Neural Networks.pdf:application/pdf},
}

@article{derkatchIdentificationVertebralFractures2019,
	title = {Identification of {Vertebral} {Fractures} by {Convolutional} {Neural} {Networks} to {Predict} {Nonvertebral} and {Hip} {Fractures}: {A} {Registry}-based {Cohort} {Study} of {Dual} {X}-ray {Absorptiometry}},
	volume = {293},
	issn = {0033-8419, 1527-1315},
	shorttitle = {Identification of {Vertebral} {Fractures} by {Convolutional} {Neural} {Networks} to {Predict} {Nonvertebral} and {Hip} {Fractures}},
	url = {http://pubs.rsna.org/doi/10.1148/radiol.2019190201},
	doi = {10.1148/radiol.2019190201},
	language = {en},
	number = {2},
	urldate = {2022-09-08},
	journal = {Radiology},
	author = {Derkatch, Sheldon and Kirby, Christopher and Kimelman, Douglas and Jozani, Mohammad Jafari and Davidson, J. Michael and Leslie, William D.},
	month = nov,
	year = {2019},
	pages = {405--411},
	file = {Derkatch et al_2019_Identification of Vertebral Fractures by Convolutional Neural Networks to.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ENKW35F5\\Derkatch et al_2019_Identification of Vertebral Fractures by Convolutional Neural Networks to.pdf:application/pdf},
}

@article{liaoJointVertebraeIdentification2018,
	title = {Joint {Vertebrae} {Identification} and {Localization} in {Spinal} {CT} {Images} by {Combining} {Short}- and {Long}-{Range} {Contextual} {Information}},
	volume = {37},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8269371/},
	doi = {10.1109/TMI.2018.2798293},
	number = {5},
	urldate = {2022-09-08},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Liao, Haofu and Mesfin, Addisu and Luo, Jiebo},
	month = may,
	year = {2018},
	pages = {1266--1275},
	file = {Liao et al_2018_Joint Vertebrae Identification and Localization in Spinal CT Images by.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DMNCJMW7\\Liao et al_2018_Joint Vertebrae Identification and Localization in Spinal CT Images by.pdf:application/pdf},
}

@article{chenApplicationDeepLearning2021,
	title = {Application of deep learning algorithm to detect and visualize vertebral fractures on plain frontal radiographs},
	volume = {16},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0245992},
	doi = {10.1371/journal.pone.0245992},
	abstract = {Background
              Identification of vertebral fractures (VFs) is critical for effective secondary fracture prevention owing to their association with the increasing risks of future fractures. Plain abdominal frontal radiographs (PARs) are a common investigation method performed for a variety of clinical indications and provide an ideal platform for the opportunistic identification of VF. This study uses a deep convolutional neural network (DCNN) to identify the feasibility for the screening, detection, and localization of VFs using PARs.
            
            
              Methods
              A DCNN was pretrained using ImageNet and retrained with 1306 images from the PARs database obtained between August 2015 and December 2018. The accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) were evaluated. The visualization algorithm gradient-weighted class activation mapping (Grad-CAM) was used for model interpretation.
            
            
              Results
              Only 46.6\% (204/438) of the VFs were diagnosed in the original PARs reports. The algorithm achieved 73.59\% accuracy, 73.81\% sensitivity, 73.02\% specificity, and an AUC of 0.72 in the VF identification.
            
            
              Conclusion
              Computer driven solutions integrated with the DCNN have the potential to identify VFs with good accuracy when used opportunistically on PARs taken for a variety of clinical purposes. The proposed model can help clinicians become more efficient and economical in the current clinical pathway of fragile fracture treatment.},
	language = {en},
	number = {1},
	urldate = {2022-09-08},
	journal = {PLOS ONE},
	author = {Chen, Hsuan-Yu and Hsu, Benny Wei-Yun and Yin, Yu-Kai and Lin, Feng-Huei and Yang, Tsung-Han and Yang, Rong-Sen and Lee, Chih-Kuo and Tseng, Vincent S.},
	editor = {Hum, Yan Chai},
	month = jan,
	year = {2021},
	pages = {e0245992},
	file = {Chen et al_2021_Application of deep learning algorithm to detect and visualize vertebral.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TY9FNNT7\\Chen et al_2021_Application of deep learning algorithm to detect and visualize vertebral.pdf:application/pdf},
}

@article{macarthurWhichLinguisticFeatures2019,
	title = {Which linguistic features predict quality of argumentative writing for college basic writers, and how do those features change with instruction?},
	volume = {32},
	issn = {0922-4777, 1573-0905},
	url = {http://link.springer.com/10.1007/s11145-018-9853-6},
	doi = {10.1007/s11145-018-9853-6},
	language = {en},
	number = {6},
	urldate = {2022-09-09},
	journal = {Reading and Writing},
	author = {MacArthur, Charles A. and Jennings, Amanda and Philippakos, Zoi A.},
	month = jun,
	year = {2019},
	pages = {1553--1574},
}

@inproceedings{xuStrategyproofConferencePeer2019,
	address = {Macao, China},
	title = {On {Strategyproof} {Conference} {Peer} {Review}},
	isbn = {978-0-9992411-4-1},
	url = {https://www.ijcai.org/proceedings/2019/87},
	doi = {10.24963/ijcai.2019/87},
	abstract = {We consider peer review under a conference setting where there are conflicts between the reviewers and the submissions. Under such conflicts, reviewers can manipulate their reviews in a strategic manner to influence the final rankings of their own papers. Present-day peer-review systems are not designed to guard against such strategic behavior, beyond minimal (and insufficient) checks such as not assigning a paper to a conflicted reviewer. In this work, we address this problem through the lens of social choice, and present a theoretical framework for strategyproof and efficient peer review. Given the conflict graph which satisfies a simple property, we first present and analyze a flexible framework for reviewer-assignment and aggregation for the reviews that guarantees not only strategyproofness but also a natural efficiency property (unanimity). Our framework is based on the so-called partitioning method, and can be treated as a generalization of this type of method to conference peer review settings. We then empirically show that the requisite property on the (authorship) conflict graph is indeed satisfied in the ICLR-17 submissions data, and further demonstrate a simple trick to make the partitioning method more practically appealing under conference peer-review settings. Finally, we complement our positive results with negative theoretical results where we prove that under slightly stronger requirements, it is impossible for any algorithm to be both strategyproof and efficient.},
	language = {en},
	urldate = {2022-09-09},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Xu, Yichong and Zhao, Han and Shi, Xiaofei and Shah, Nihar B.},
	month = aug,
	year = {2019},
	pages = {616--622},
	file = {Xu et al_2019_On Strategyproof Conference Peer Review.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N9DMD2HN\\Xu et al_2019_On Strategyproof Conference Peer Review.pdf:application/pdf},
}

@inproceedings{keAutomatedEssayScoring2019,
	address = {Macao, China},
	title = {Automated {Essay} {Scoring}: {A} {Survey} of the {State} of the {Art}},
	isbn = {978-0-9992411-4-1},
	shorttitle = {Automated {Essay} {Scoring}},
	url = {https://www.ijcai.org/proceedings/2019/879},
	doi = {10.24963/ijcai.2019/879},
	abstract = {Despite being investigated for over 50 years, the task of automated essay scoring is far from being solved. Nevertheless, it continues to draw a lot of attention in the natural language processing community in part because of its commercial and educational values as well as the associated research challenges. This paper presents an overview of the major milestones made in automated essay scoring research since its inception.},
	language = {en},
	urldate = {2022-09-09},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Ke, Zixuan and Ng, Vincent},
	month = aug,
	year = {2019},
	pages = {6300--6308},
}

@inproceedings{mayfieldShouldYouFineTune2020,
	address = {Seattle, WA, USA → Online},
	title = {Should {You} {Fine}-{Tune} {BERT} for {Automated} {Essay} {Scoring}?},
	url = {https://www.aclweb.org/anthology/2020.bea-1.15},
	doi = {10.18653/v1/2020.bea-1.15},
	language = {en},
	urldate = {2022-09-09},
	booktitle = {Proceedings of the {Fifteenth} {Workshop} on {Innovative} {Use} of {NLP} for {Building} {Educational} {Applications}},
	publisher = {Association for Computational Linguistics},
	author = {Mayfield, Elijah and Black, Alan W},
	year = {2020},
	pages = {151--162},
	file = {Mayfield_Black_2020_Should You Fine-Tune BERT for Automated Essay Scoring.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z8RWRLK4\\Mayfield_Black_2020_Should You Fine-Tune BERT for Automated Essay Scoring.pdf:application/pdf},
}

@article{vajjalaAutomatedAssessmentNonNative2018,
	title = {Automated {Assessment} of {Non}-{Native} {Learner} {Essays}: {Investigating} the {Role} of {Linguistic} {Features}},
	volume = {28},
	issn = {1560-4292, 1560-4306},
	shorttitle = {Automated {Assessment} of {Non}-{Native} {Learner} {Essays}},
	url = {http://link.springer.com/10.1007/s40593-017-0142-3},
	doi = {10.1007/s40593-017-0142-3},
	language = {en},
	number = {1},
	urldate = {2022-09-09},
	journal = {International Journal of Artificial Intelligence in Education},
	author = {Vajjala, Sowmya},
	month = mar,
	year = {2018},
	pages = {79--105},
	file = {Vajjala_2018_Automated Assessment of Non-Native Learner Essays.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D8ANLBBC\\Vajjala_2018_Automated Assessment of Non-Native Learner Essays.pdf:application/pdf},
}

@article{nguyenArgumentMiningImproving2018,
	title = {Argument {Mining} for {Improving} the {Automated} {Scoring} of {Persuasive} {Essays}},
	volume = {32},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12046},
	doi = {10.1609/aaai.v32i1.12046},
	abstract = {End-to-end argument mining has enabled the development of new automated essay scoring (AES) systems that use argumentative features (e.g., number of claims, number of support relations) in addition to traditional legacy features (e.g., grammar, discourse structure) when scoring persuasive essays. While prior research has proposed different argumentative features as well as empirically demonstrated their utility for AES, these studies have all had important limitations.  In this paper we identify a set of desiderata for evaluating the use of argument mining for AES, introduce an end-to-end argument mining system and associated argumentative feature sets, and present the results of several studies that both satisfy the desiderata and demonstrate the value-added of argument mining for scoring persuasive essays.},
	number = {1},
	urldate = {2022-09-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Nguyen, Huy and Litman, Diane},
	month = apr,
	year = {2018},
	file = {Nguyen_Litman_2018_Argument Mining for Improving the Automated Scoring of Persuasive Essays.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ISF585E9\\Nguyen_Litman_2018_Argument Mining for Improving the Automated Scoring of Persuasive Essays.pdf:application/pdf},
}

@article{ramalingamAutomatedEssayGrading2018,
	title = {Automated {Essay} {Grading} using {Machine} {Learning} {Algorithm}},
	volume = {1000},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1000/1/012030},
	doi = {10.1088/1742-6596/1000/1/012030},
	urldate = {2022-09-09},
	journal = {Journal of Physics: Conference Series},
	author = {Ramalingam, V. V. and Pandian, A and Chetry, Prateek and Nigam, Himanshu},
	month = apr,
	year = {2018},
	pages = {012030},
}

@article{dengFeatureSelectionText2019,
	title = {Feature selection for text classification: {A} review},
	volume = {78},
	issn = {1380-7501, 1573-7721},
	shorttitle = {Feature selection for text classification},
	url = {http://link.springer.com/10.1007/s11042-018-6083-5},
	doi = {10.1007/s11042-018-6083-5},
	language = {en},
	number = {3},
	urldate = {2022-09-09},
	journal = {Multimedia Tools and Applications},
	author = {Deng, Xuelian and Li, Yuqing and Weng, Jian and Zhang, Jilian},
	month = feb,
	year = {2019},
	pages = {3797--3816},
}

@article{gharehchopoghChaoticVortexSearch2022,
	title = {Chaotic vortex search algorithm: metaheuristic algorithm for feature selection},
	volume = {15},
	issn = {1864-5909, 1864-5917},
	shorttitle = {Chaotic vortex search algorithm},
	url = {https://link.springer.com/10.1007/s12065-021-00590-1},
	doi = {10.1007/s12065-021-00590-1},
	language = {en},
	number = {3},
	urldate = {2022-09-09},
	journal = {Evolutionary Intelligence},
	author = {Gharehchopogh, Farhad Soleimanian and Maleki, Isa and Dizaji, Zahra Asheghi},
	month = sep,
	year = {2022},
	pages = {1777--1808},
}

@article{linMULFEMultiLabelLearning2022,
	title = {{MULFE}: {Multi}-{Label} {Learning} via {Label}-{Specific} {Feature} {Space} {Ensemble}},
	volume = {16},
	issn = {1556-4681, 1556-472X},
	shorttitle = {{MULFE}},
	url = {https://dl.acm.org/doi/10.1145/3451392},
	doi = {10.1145/3451392},
	abstract = {In multi-label learning, label correlations commonly exist in the data. Such correlation not only provides useful information, but also imposes significant challenges for multi-label learning. Recently, label-specific feature embedding has been proposed to explore label-specific features from the training data, and uses feature highly customized to the multi-label set for learning. While such feature embedding methods have demonstrated good performance, the creation of the feature embedding space is only based on a single label, without considering label correlations in the data. In this article, we propose to combine multiple label-specific feature spaces, using label correlation, for multi-label learning. The proposed algorithm,
              mu
              lti-
              l
              abel-specific
              f
              eature space
              e
              nsemble (MULFE), takes consideration label-specific features, label correlation, and weighted ensemble principle to form a learning framework. By conducting clustering analysis on each label’s negative and positive instances, MULFE first creates features customized to each label. After that, MULFE utilizes the label correlation to optimize the margin distribution of the base classifiers which are induced by the related label-specific feature spaces. By combining multiple label-specific features, label correlation based weighting, and ensemble learning, MULFE achieves maximum margin multi-label classification goal through the underlying optimization framework. Empirical studies on 10 public data sets manifest the effectiveness of MULFE.},
	language = {en},
	number = {1},
	urldate = {2022-09-09},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Lin, Yaojin and Hu, Qinghua and Liu, Jinghua and Zhu, Xingquan and Wu, Xindong},
	month = feb,
	year = {2022},
	pages = {1--24},
}

@incollection{liDefendingAdversarialAttacks2021,
	title = {Defending {Against} {Adversarial} {Attacks} {On} {Medical} {Imaging} {Ai} {System}, {Classification} {Or} {Detection}?},
	abstract = {Medical imaging AI systems such as disease classification and segmentation are increasingly inspired and transformed from computer vision based AI systems. Although an array of defense techniques have been developed and proved to be effective in computer vision, defending against adversarial attacks on medical images remains largely an uncharted territory due to their unique challenges: 1) label scarcity limits adversarial generalizability; 2) vastly similar and dominant fore- and background make it difficult for learning the discriminating features; and 3) crafted adversarial noises added to a highly standardized medical image can make it a hard sample for model to predict. In this paper, we propose a novel robust medical imaging AI framework based on Semi-Supervised Adversarial Training (SSAT) and Unsupervised Adversarial Detection (UAD), followed by a new measure for assessing systems adversarial risk. We systematically demonstrate the advantages of our robust medical imaging AI system over the existing adversarial defense techniques under diverse real-world settings of adversarial attacks using a benchmark OCT imaging data set},
	author = {Li, Xin and Pan, Deng and Zhu, Dongxiao},
	year = {2021},
	note = {OCLC: 9055296023},
}

@article{tomitaDeepNeuralNetworks2018,
	title = {Deep neural networks for automatic detection of osteoporotic vertebral fractures on {CT} scans},
	volume = {98},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482518301185},
	doi = {10.1016/j.compbiomed.2018.05.011},
	language = {en},
	urldate = {2022-09-11},
	journal = {Computers in Biology and Medicine},
	author = {Tomita, Naofumi and Cheung, Yvonne Y. and Hassanpour, Saeed},
	month = jul,
	year = {2018},
	pages = {8--15},
}

@misc{salehinejadDeepSequentialLearning2021,
	title = {Deep {Sequential} {Learning} for {Cervical} {Spine} {Fracture} {Detection} on {Computed} {Tomography} {Imaging}},
	url = {http://arxiv.org/abs/2010.13336},
	abstract = {Fractures of the cervical spine are a medical emergency and may lead to permanent paralysis and even death. Accurate diagnosis in patients with suspected fractures by computed tomography (CT) is critical to patient management. In this paper, we propose a deep convolutional neural network (DCNN) with a bidirectional long-short term memory (BLSTM) layer for the automated detection of cervical spine fractures in CT axial images. We used an annotated dataset of 3,666 CT scans (729 positive and 2,937 negative cases) to train and validate the model. The validation results show a classification accuracy of 70.92\% and 79.18\% on the balanced (104 positive and 104 negative cases) and imbalanced (104 positive and 419 negative cases) test datasets, respectively.},
	urldate = {2022-09-11},
	publisher = {arXiv},
	author = {Salehinejad, Hojjat and Ho, Edward and Lin, Hui-Ming and Crivellaro, Priscila and Samorodova, Oleksandra and Arciniegas, Monica Tafur and Merali, Zamir and Suthiphosuwan, Suradech and Bharatha, Aditya and Yeom, Kristen and Mamdani, Muhammad and Wilson, Jefferson and Colak, Errol},
	month = feb,
	year = {2021},
	note = {arXiv:2010.13336 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3QFKC3Q9\\2010.html:text/html;Salehinejad et al_2021_Deep Sequential Learning for Cervical Spine Fracture Detection on Computed.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XWYBYLCS\\Salehinejad et al_2021_Deep Sequential Learning for Cervical Spine Fracture Detection on Computed.pdf:application/pdf},
}

@article{huPredictionSubsequentOsteoporotic2022,
	title = {Prediction of subsequent osteoporotic vertebral compression fracture on {CT} radiography via deep learning},
	issn = {2688-268X, 2688-268X},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/VIW.20220012},
	doi = {10.1002/VIW.20220012},
	language = {en},
	urldate = {2022-09-11},
	journal = {VIEW},
	author = {Hu, Xiao and Zhu, Yanjing and Qian, Yadong and Huang, Ruiqi and Yin, Shuai and Zeng, Zhili and Xie, Ning and Ma, Bin and Yu, Yan and Zhao, Qing and Wu, Zhourui and Wang, Jianjie and Xu, Wei and Ren, Yilong and Li, Chen and Zhu, Rongrong and Cheng, Liming},
	month = jul,
	year = {2022},
	pages = {20220012},
}

@techreport{rosenbergARTIFICIALINTELLIGENCEACCURATELY2021,
	type = {preprint},
	title = {{ARTIFICIAL} {INTELLIGENCE} {ACCURATELY} {DETECTS} {TRAUMATIC} {THORACOLUMBAR} {FRACTURES} {ON} {SAGITTAL} {RADIOGRAPHS}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2021.05.09.21256762},
	abstract = {Abstract
          
            Background context
            Traumatic thoracolumbar (TL) fractures are frequently encountered in emergency rooms. Sagittal and anteroposterior radiographs are the first step in the trauma routine imaging. Up to 30\% of TL fractures are missed in this imaging modality, thus requiring a CT and/or MRI to confirm the diagnosis. A delay in treatment leads to increased morbidity, mortality, exposure to ionizing radiation and financial burden. Fracture detection with Machine Learning models has achieved expert level performance in previous studies. Reliably detecting vertebral fractures in simple radiographic projections would have a significant clinical and financial impact.
          
          
            Purpose
            To develop a deep learning model that detects traumatic fractures on sagittal radiographs of the TL spine.
          
          
            Study design/setting
            Retrospective Cohort study.
          
          
            Methods
            We collected sagittal radiographs, CT and MRI scans of the TL spine of 362 patients exhibiting traumatic vertebral fractures. Cases were excluded when CT and/or MRI where not available. The reference standard was set by an expert group of three spine surgeons who conjointly annotated the sagittal radiographs of 171 cases. CT and/or MRI were reviewed to confirm the presence and type of the fracture in all cases. 302 cropped vertebral images were labelled ‘fracture’ and 328 ‘no fracture’. After augmentation, this dataset was then used to train, validate, and test deep learning classifiers based on ResNet18 and VGG16 architectures. To ensure that the model’s prediction was based on the correct identification of the fracture zone, an Activation Map analysis was conducted.
          
          
            Results
            Vertebras T12 to L2 were the most frequently involved, accounting for 48\% of the fractures. A4, A3 and A1 were the most frequent AO Spine fracture types. Accuracies of 88\% and 84\% were obtained with ResNet18 and VGG16 respectively. The sensitivity was 89\% with both architectures but ResNet18 showed a higher specificity (88\%) compared to VGG16 (79\%). The fracture zone was precisely identified in 81\% of the heatmaps.
          
          
            Conclusions
            Our AI model can accurately identify anomalies suggestive of vertebral fractures in sagittal radiographs by precisely identifying the fracture zone within the vertebral body.
          
          
            Clinical significance
            Clinical implementation of a diagnosis aid tool specifically trained for TL fracture identification is anticipated to reduce the rate of missed vertebral fractures in emergency rooms.},
	language = {en},
	urldate = {2022-09-11},
	institution = {Orthopedics},
	author = {Rosenberg, Guillermo Sanchez and Cina, Andrea and Schirò, Giuseppe Rosario and Giorgi, Pietro Domenico and Gueorguiev, Boyko and Alini, Mauro and Varga, Peter and Galbusera, Fabio and Gallazzi, Enrico},
	month = may,
	year = {2021},
	doi = {10.1101/2021.05.09.21256762},
	file = {Rosenberg et al_2021_ARTIFICIAL INTELLIGENCE ACCURATELY DETECTS TRAUMATIC THORACOLUMBAR FRACTURES ON.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TCAS5MVM\\Rosenberg et al_2021_ARTIFICIAL INTELLIGENCE ACCURATELY DETECTS TRAUMATIC THORACOLUMBAR FRACTURES ON.pdf:application/pdf},
}

@article{liCanDeeplearningModel2021,
	title = {Can a {Deep}-learning {Model} for the {Automated} {Detection} of {Vertebral} {Fractures} {Approach} the {Performance} {Level} of {Human} {Subspecialists}?},
	volume = {479},
	issn = {0009-921X, 1528-1132},
	url = {https://journals.lww.com/10.1097/CORR.0000000000001685},
	doi = {10.1097/CORR.0000000000001685},
	language = {en},
	number = {7},
	urldate = {2022-09-11},
	journal = {Clinical Orthopaedics \& Related Research},
	author = {Li, Yi-Chu and Chen, Hung-Hsun and Horng-Shing Lu, Henry and Hondar Wu, Hung-Ta and Chang, Ming-Chau and Chou, Po-Hsin},
	month = jul,
	year = {2021},
	pages = {1598--1612},
	file = {Li et al_2021_Can a Deep-learning Model for the Automated Detection of Vertebral Fractures.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LBEDL4LY\\Li et al_2021_Can a Deep-learning Model for the Automated Detection of Vertebral Fractures.pdf:application/pdf},
}

@article{alholaSleepDeprivationImpact2007,
	title = {Sleep deprivation: {Impact} on cognitive performance},
	volume = {3},
	issn = {1176-6328},
	shorttitle = {Sleep deprivation},
	abstract = {Today, prolonged wakefulness is a widespread phenomenon. Nevertheless, in the field of sleep and wakefulness, several unanswered questions remain. Prolonged wakefulness can be due to acute total sleep deprivation (SD) or to chronic partial sleep restriction. Although the latter is more common in everyday life, the effects of total SD have been examined more thoroughly. Both total and partial SD induce adverse changes in cognitive performance. First and foremost, total SD impairs attention and working memory, but it also affects other functions, such as long-term memory and decision-making. Partial SD is found to influence attention, especially vigilance. Studies on its effects on more demanding cognitive functions are lacking. Coping with SD depends on several factors, especially aging and gender. Also interindividual differences in responses are substantial. In addition to coping with SD, recovering from it also deserves attention. Cognitive recovery processes, although insufficiently studied, seem to be more demanding in partial sleep restriction than in total SD.},
	language = {eng},
	number = {5},
	journal = {Neuropsychiatric Disease and Treatment},
	author = {Alhola, Paula and Polo-Kantola, Päivi},
	year = {2007},
	pmid = {19300585},
	pmcid = {PMC2656292},
	keywords = {aging, cognitive performance, gender differences, recovery, Sleep deprivation, sleep restriction},
	pages = {553--567},
}

@article{mccarthyDecreasedAttentionalResponsivity1997,
	title = {Decreased {Attentional} {Responsivity} {During} {Sleep} {Deprivation}: {Orienting} {Response} {Latency}, {Amplitude}, and {Habituation}},
	volume = {20},
	issn = {1550-9109, 0161-8105},
	shorttitle = {Decreased {Attentional} {Responsivity} {During} {Sleep} {Deprivation}},
	url = {https://academic.oup.com/sleep/article/20/2/115/2731607},
	doi = {10.1093/sleep/20.2.115},
	language = {en},
	number = {2},
	urldate = {2022-09-17},
	journal = {Sleep},
	author = {McCarthy, Michele E. and Waters, William F.},
	month = feb,
	year = {1997},
	pages = {115--123},
	file = {McCarthy_Waters_1997_Decreased Attentional Responsivity During Sleep Deprivation.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YJ99JDN2\\McCarthy_Waters_1997_Decreased Attentional Responsivity During Sleep Deprivation.pdf:application/pdf},
}

@incollection{killgoreEffectsSleepDeprivation2010,
	title = {Effects of sleep deprivation on cognition},
	volume = {185},
	isbn = {978-0-444-53702-7},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444537027000075},
	language = {en},
	urldate = {2022-09-17},
	booktitle = {Progress in {Brain} {Research}},
	publisher = {Elsevier},
	author = {Killgore, William D.S.},
	year = {2010},
	doi = {10.1016/B978-0-444-53702-7.00007-5},
	pages = {105--129},
}

@article{horneSleepLossDivergent1988,
	title = {Sleep {Loss} and “{Divergent}” {Thinking} {Ability}},
	volume = {11},
	issn = {0161-8105, 1550-9109},
	url = {https://academic.oup.com/sleep/article-lookup/doi/10.1093/sleep/11.6.528},
	doi = {10.1093/sleep/11.6.528},
	language = {en},
	number = {6},
	urldate = {2022-09-17},
	journal = {Sleep},
	author = {Horne, J. A.},
	month = sep,
	year = {1988},
	pages = {528--536},
}

@article{harrisonSleepDeprivationAffects1997,
	title = {Sleep {Deprivation} {Affects} {Speech}},
	volume = {20},
	issn = {1550-9109, 0161-8105},
	url = {https://academic.oup.com/sleep/article-lookup/doi/10.1093/sleep/20.10.871},
	doi = {10.1093/sleep/20.10.871},
	language = {en},
	number = {10},
	urldate = {2022-09-17},
	journal = {Sleep},
	author = {Harrison, Yvonne and Horne, James A.},
	month = oct,
	year = {1997},
	pages = {871--877},
}

@article{altiniSegmentationIdentificationVertebrae2021,
	title = {Segmentation and {Identification} of {Vertebrae} in {CT} {Scans} {Using} {CNN}, k-{Means} {Clustering} and k-{NN}},
	volume = {8},
	issn = {2227-9709},
	url = {https://www.mdpi.com/2227-9709/8/2/40},
	doi = {10.3390/informatics8020040},
	abstract = {The accurate segmentation and identification of vertebrae presents the foundations for spine analysis including fractures, malfunctions and other visual insights. The large-scale vertebrae segmentation challenge (VerSe), organized as a competition at the Medical Image Computing and Computer Assisted Intervention (MICCAI), is aimed at vertebrae segmentation and labeling. In this paper, we propose a framework that addresses the tasks of vertebrae segmentation and identification by exploiting both deep learning and classical machine learning methodologies. The proposed solution comprises two phases: a binary fully automated segmentation of the whole spine, which exploits a 3D convolutional neural network, and a semi-automated procedure that allows locating vertebrae centroids using traditional machine learning algorithms. Unlike other approaches, the proposed method comes with the added advantage of no requirement for single vertebrae-level annotations to be trained. A dataset of 214 CT scans has been extracted from VerSe’20 challenge data, for training, validating and testing the proposed approach. In addition, to evaluate the robustness of the segmentation and labeling algorithms, 12 CT scans from subjects affected by severe, moderate and mild scoliosis have been collected from a local medical clinic. On the designated test set from Verse’20 data, the binary spine segmentation stage allowed to obtain a binary Dice coefficient of 89.17\%, whilst the vertebrae identification one reached an average multi-class Dice coefficient of 90.09\%. In order to ensure the reproducibility of the algorithms hereby developed, the code has been made publicly available.},
	language = {en},
	number = {2},
	urldate = {2022-10-04},
	journal = {Informatics},
	author = {Altini, Nicola and De Giosa, Giuseppe and Fragasso, Nicola and Coscia, Claudia and Sibilano, Elena and Prencipe, Berardino and Hussain, Sardar Mehboob and Brunetti, Antonio and Buongiorno, Domenico and Guerriero, Andrea and Tatò, Ilaria Sabina and Brunetti, Gioacchino and Triggiani, Vito and Bevilacqua, Vitoantonio},
	month = jun,
	year = {2021},
	pages = {40},
	file = {Altini et al_2021_Segmentation and Identification of Vertebrae in CT Scans Using CNN, k-Means.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HMWSSDTQ\\Altini et al_2021_Segmentation and Identification of Vertebrae in CT Scans Using CNN, k-Means.pdf:application/pdf},
}

@misc{wasserthalTotalSegmentatorRobustSegmentation2022,
	title = {{TotalSegmentator}: robust segmentation of 104 anatomical structures in {CT} images},
	shorttitle = {{TotalSegmentator}},
	url = {http://arxiv.org/abs/2208.05868},
	abstract = {In this work we focus on automatic segmentation of multiple anatomical structures in (whole body) CT images. Many segmentation algorithms exist for this task. However, in most cases they suffer from 3 problems: 1. They are difficult to use (the code and data is not publicly available or difficult to use). 2. They do not generalize (often the training dataset was curated to only contain very clean images which do not reflect the image distribution found during clinical routine), 3. The algorithm can only segment one anatomical structure. For more structures several algorithms have to be used which increases the effort required to set up the system. In this work we publish a new dataset and segmentation toolkit which solves all three of these problems: In 1204 CT images we segmented 104 anatomical structures (27 organs, 59 bones, 10 muscles, 8 vessels) covering a majority of relevant classes for most use cases. We show an improved workflow for the creation of ground truth segmentations which speeds up the process by over 10x. The CT images were randomly sampled from clinical routine, thus representing a real world dataset which generalizes to clinical application. The dataset contains a wide range of different pathologies, scanners, sequences and sites. Finally, we train a segmentation algorithm on this new dataset. We call this algorithm TotalSegmentator and make it easily available as a pretrained python pip package (pip install totalsegmentator). Usage is as simple as TotalSegmentator -i ct.nii.gz -o seg and it works well for most CT images. The code is available at https://github.com/wasserth/TotalSegmentator and the dataset at https://doi.org/10.5281/zenodo.6802613.},
	urldate = {2022-10-04},
	publisher = {arXiv},
	author = {Wasserthal, Jakob and Meyer, Manfred and Breit, Hanns-Christian and Cyriac, Joshy and Yang, Shan and Segeroth, Martin},
	month = aug,
	year = {2022},
	note = {arXiv:2208.05868 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XPE7GSET\\2208.html:text/html;Wasserthal et al_2022_TotalSegmentator.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9IQ3P7C3\\Wasserthal et al_2022_TotalSegmentator.pdf:application/pdf},
}

@inproceedings{zhouReviewDeepLearning2022a,
	address = {Newark NJ USA},
	title = {Review of {Deep} {Learning} {Models} for {Spine} {Segmentation}},
	isbn = {978-1-4503-9238-9},
	url = {https://dl.acm.org/doi/10.1145/3512527.3531356},
	doi = {10.1145/3512527.3531356},
	language = {en},
	urldate = {2022-10-04},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {ACM},
	author = {Zhou, Neng and Wen, Hairu and Wang, Yi and Liu, Yang and Zhou, Longfei},
	month = jun,
	year = {2022},
	keywords = {csf},
	pages = {498--507},
}

@article{qinVertebraeLabelingEndtoEnd2022,
	title = {Vertebrae {Labeling} via {End}-to-{End} {Integral} {Regression} {Localization} and {Multi}-{Label} {Classification} {Network}},
	volume = {33},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9319556/},
	doi = {10.1109/TNNLS.2020.3045601},
	number = {6},
	urldate = {2022-10-04},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Qin, Chunli and Zhou, Ji and Yao, Demin and Zhuang, Han and Wang, Hui and Chen, Shiyao and Shi, Yonghong and Song, Zhijian},
	month = jun,
	year = {2022},
	pages = {2726--2736},
}

@article{shenDeepLearningMedical2017,
	title = {Deep {Learning} in {Medical} {Image} {Analysis}},
	volume = {19},
	issn = {1523-9829, 1545-4274},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-bioeng-071516-044442},
	doi = {10.1146/annurev-bioeng-071516-044442},
	abstract = {This review covers computer-assisted analysis of images in the field of medical imaging. Recent advances in machine learning, especially with regard to deep learning, are helping to identify, classify, and quantify patterns in medical images. At the core of these advances is the ability to exploit hierarchical feature representations learned solely from data, instead of features designed by hand according to domain-specific knowledge. Deep learning is rapidly becoming the state of the art, leading to enhanced performance in various medical applications. We introduce the fundamentals of deep learning methods and review their successes in image registration, detection of anatomical and cellular structures, tissue segmentation, computer-aided disease diagnosis and prognosis, and so on. We conclude by discussing research issues and suggesting future directions for further improvement.},
	language = {en},
	number = {1},
	urldate = {2022-10-13},
	journal = {Annual Review of Biomedical Engineering},
	author = {Shen, Dinggang and Wu, Guorong and Suk, Heung-Il},
	month = jun,
	year = {2017},
	pages = {221--248},
	file = {Shen et al_2017_Deep Learning in Medical Image Analysis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2UG65SQF\\Shen et al_2017_Deep Learning in Medical Image Analysis.pdf:application/pdf},
}

@article{zhangMACRMultiinformationAugmented2023,
	title = {{MACR}: {Multi}-information {Augmented} {Conversational} {Recommender}},
	volume = {213},
	issn = {09574174},
	shorttitle = {{MACR}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422019996},
	doi = {10.1016/j.eswa.2022.118981},
	abstract = {Conversational recommender systems (CRS) aim to provide high-quality recommendations through fewer multiturn conversations. However, because short conversation histories lack sufficient item information, CRSs not only struggle to make accurate recommendations but also lack diversity in the generated responses. Existing CRSs mainly alleviate these problems by introducing external information (e.g., reviews) while ignoring information inside the conversations (e.g., potential category preferences in user utterances). Besides, item introduction is a kind of external information that is more objective and contains more entities than reviews. Therefore, we propose a Multi-information Augmented Conversational Recommender (MACR), which improves the performance of recommendation and response generation by mining the underlying category preferences in users’ utterances and incorporating item introductions. Specifically, we enhance the category associations among entities by constructing a knowledge graph DBMG with category nodes, extracting and encoding the item categories that match the user preferences into the user representation. For item introductions, we extract the entities in them and fuse them into the conversation using an introduction-attentive encoder–decoder. Extensive experiments on the dataset REDIAL show that our MACR significantly outperforms previous state-of-the-art approaches. The source code will be available at https://github.com/zcy-cqut/MACR.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Expert Systems with Applications},
	author = {Zhang, Chengyang and Huang, Xianying and An, Jiahao},
	month = mar,
	year = {2023},
	pages = {118981},
	file = {Zhang et al. - 2023 - MACR Multi-information Augmented Conversational R.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JX2E9I7R\\Zhang et al. - 2023 - MACR Multi-information Augmented Conversational R.pdf:application/pdf},
}

@article{gaoAdvancesChallengesConversational2021,
	title = {Advances and challenges in conversational recommender systems: {A} survey},
	volume = {2},
	issn = {26666510},
	shorttitle = {Advances and challenges in conversational recommender systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666651021000164},
	doi = {10.1016/j.aiopen.2021.06.002},
	abstract = {Recommender systems exploit interaction history to estimate user preference, having been heavily used in a wide range of industry applications. However, static recommendation models are difficult to answer two important questions well due to inherent shortcomings: (a) What exactly does a user like? (b) Why does a user like an item? The shortcomings are due to the way that static models learn user preference, i.e., without explicit instructions and active feedback from users. The recent rise of conversational recommender systems (CRSs) changes this situation fundamentally. In a CRS, users and the system can dynamically communicate through natural language interactions, which provide unprecedented opportunities to explicitly obtain the exact preference of users. Considerable efforts, spread across disparate settings and applications, have been put into developing CRSs. Existing models, technologies, and evaluation methods for CRSs are far from mature. In this paper, we provide a systematic review of the techniques used in current CRSs. We summarize the key challenges of developing CRSs in five directions: (1) Question-based user preference elicitation. (2) Multi-turn conversational recommendation strategies. (3) Dialogue understanding and generation. (4) Exploitation-exploration trade-offs. (5) Evaluation and user simulation. These research directions involve multiple research fields like information retrieval (IR), natural language processing (NLP), and human-computer interaction (HCI). Based on these research directions, we discuss some future challenges and opportunities. We provide a road map for researchers from multiple com­ munities to get started in this area. We hope this survey can help to identify and address challenges in CRSs and inspire future research.},
	language = {en},
	urldate = {2022-11-05},
	journal = {AI Open},
	author = {Gao, Chongming and Lei, Wenqiang and He, Xiangnan and de Rijke, Maarten and Chua, Tat-Seng},
	year = {2021},
	pages = {100--126},
	file = {Gao et al. - 2021 - Advances and challenges in conversational recommen.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WQ8T3XML\\Gao et al. - 2021 - Advances and challenges in conversational recommen.pdf:application/pdf},
}

@article{kotAIactivatedValueCocreation2022,
	title = {{AI}-activated value co-creation. {An} exploratory study of conversational agents},
	volume = {107},
	issn = {00198501},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0019850122002528},
	doi = {10.1016/j.indmarman.2022.10.013},
	abstract = {This paper aims to provide in-depth insight into the value co-creation around conversational agents — a solution based on Artificial Intelligence (AI) that replaces customer service employees. Using the concepts of value-in-use from information technology and the ARA Model as a theoretical lens, we investigated which resources are combined to create an AI-based solution, how providers and clients cooperate, and what value is an outcome of co-creation in the case of advanced technologies in the B2B context. Based on multiple case studies on leading providers of conversational agents, this research demonstrates the complexities of the process. Our findings reveal significant differences in informational, strategic, transactional, and transformational dimensions of valuein-use of conversational agents (CA) from other technologies. It shows that AI-activated value is dynamic, context-dependent, and fuzzy. This paper also highlights the role of resource interaction and non-technological factors that enable conversational agents to reflect and replace human activities.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Industrial Marketing Management},
	author = {Kot, Mateusz and Leszczyński, Grzegorz},
	month = nov,
	year = {2022},
	pages = {287--299},
	file = {Kot and Leszczyński - 2022 - AI-activated value co-creation. An exploratory stu.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8E8U99ZS\\Kot and Leszczyński - 2022 - AI-activated value co-creation. An exploratory stu.pdf:application/pdf},
}

@article{dinoiaConversationalRecommendationTheoretical2022,
	title = {Conversational recommendation: {Theoretical} model and complexity analysis},
	issn = {00200255},
	shorttitle = {Conversational recommendation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025522008647},
	doi = {10.1016/j.ins.2022.07.169},
	abstract = {Recommender systems help users ﬁnd items of interest in situations of information overload in a personalized way, using needs and preferences of individual users. In conversational recommendation approaches, the system acquires needs and preferences in an interactive, multi-turn dialog. This is usually driven by incrementally asking users about their preferences about item features or individual items. A central research goal in this context is efﬁciency, evaluated concerning the number of required interactions until a satisfying item is found. Today, research on dialog efﬁciency is almost entirely empirical, aiming to demonstrate, for example, that one strategy for selecting questions to ask the user is better than another one in a given application. This work complements empirical research with a theoretical, domain-independent model of conversational recommendation. This model, designed to cover a range of application scenarios, allows us to investigate the efﬁciency of conversational approaches in a formal way, particularly concerning the computational complexity of devising optimal interaction strategies. An experimental evaluation empirically conﬁrms our ﬁndings.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Information Sciences},
	author = {Di Noia, Tommaso and Donini, Francesco Maria and Jannach, Dietmar and Narducci, Fedelucio and Pomo, Claudio},
	month = aug,
	year = {2022},
	pages = {S0020025522008647},
	file = {Di Noia et al. - 2022 - Conversational recommendation Theoretical model a.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I3HX2BNG\\Di Noia et al. - 2022 - Conversational recommendation Theoretical model a.pdf:application/pdf},
}

@article{pramodConversationalRecommenderSystems2022,
	title = {Conversational recommender systems techniques, tools, acceptance, and adoption: {A} state of the art review},
	volume = {203},
	issn = {09574174},
	shorttitle = {Conversational recommender systems techniques, tools, acceptance, and adoption},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422008612},
	doi = {10.1016/j.eswa.2022.117539},
	abstract = {Conversational recommender systems(CRS) have become popular in recent years and have gained the attention of researchers. More emphasis is given to user choices and experiences in recent research studies. This study presents a systematic review and uses PRISMA methodology for the search strategy, study selection, and extraction. The algorithmic aspects and evolution of the most common methods used in the design of conver­ sational recommender system components are discussed to provide a comprehensive picture. The study unveils the major advantages and limitations of the user preference elicitation and recommendation techniques. It also summarizes the recommendation tools and framework for different application domains with the techniques adopted. The study further aimed to find the implementation challenges, and the same is highlighted with the recommended solutions. As the users are the key focus for the conversational recommender systems, this paper also presents the role of effective user acceptance and adoption models. The statistical analysis provided in the study gives an insight into the growing popularity of CRS research areas and their application in various do­ mains. The results of this study provide insights to practitioners, which may help in better CRS implementations and contribute to the literature by providing a comprehensive viewpoint. The study also postulates the topics that need attention and gives future research trends.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Expert Systems with Applications},
	author = {Pramod, Dhanya and Bafna, Prafulla},
	month = oct,
	year = {2022},
	pages = {117539},
	file = {Pramod and Bafna - 2022 - Conversational recommender systems techniques, too.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DA8S39RH\\Pramod and Bafna - 2022 - Conversational recommender systems techniques, too.pdf:application/pdf},
}

@article{rheeEffectsPersonalizationSocial2020,
	title = {Effects of personalization and social role in voice shopping: {An} experimental study on product recommendation by a conversational voice agent},
	volume = {109},
	issn = {07475632},
	shorttitle = {Effects of personalization and social role in voice shopping},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0747563220301126},
	doi = {10.1016/j.chb.2020.106359},
	abstract = {This study examines the persuasion mechanism in product recommendations made by a voice-based conversa­ tional agent and explores whether the personalized content reflecting the customer’s preferences and the agent’s social role of a friend, rather than a secretary, generate a more positive attitude toward the product in the context of voice shopping. With the framework of dual modes of information-processing models, we hypothesized that the personalization of messages would be a central route with a greater impact on attitude for products with high involvement. By contrast, the social role of the conversational agent was expected to represent a peripheral route with a greater impact on products with low involvement. An experimental study was designed to test the effects of personalized content that reflected individual preferences for product attributes and a friend role of a voice agent with high and low product involvement. The results showed main effects of both personalization and the social role on building attitudes toward the product. Although no interaction effect for personalization and involvement was found, there was a significant interaction effect for the social role and involvement. This study contributes to persuasion theory by extending it to the interaction with a conversational agent. For practitioners, the study provides insights into the importance of the personalized content of recommendations and the need for consideration of an alternative social role in the design of voice shopping through a conversational agent.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Computers in Human Behavior},
	author = {Rhee, Chong Eun and Choi, Junho},
	month = aug,
	year = {2020},
	pages = {106359},
	file = {Rhee and Choi - 2020 - Effects of personalization and social role in voic.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AJE3BUIJ\\Rhee and Choi - 2020 - Effects of personalization and social role in voic.pdf:application/pdf},
}

@article{adikariEmpathicConversationalAgents2022,
	title = {Empathic conversational agents for real-time monitoring and co-facilitation of patient-centered healthcare},
	volume = {126},
	issn = {0167739X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X2100323X},
	doi = {10.1016/j.future.2021.08.015},
	abstract = {Healthcare systems across the world are transitioning into patient-centered healthcare models to ensure improved health outcomes, increased operational efficiencies and respectful patient engagement. Digital health technologies are at the forefront of this transition in facilitating a role for the patient in the clinical dimensions of the healthcare trajectory, from diagnosis and interventions to treatment and recovery. Despite this prevalence in the clinical space, the non-clinical needs of patient mental health and wellbeing are frequently overlooked by contemporary patient-centered healthcare models. Conversational agents (or chatbots) are digital dialogue systems that are widespread and widely used in sequential information provision and information acquisition tasks. Given the intimate nature of this human-machine interaction, conversational agents can be effectively utilized to support and sustain patient mental health and wellbeing. In this paper, we propose an empathic conversational agent framework based on an ensemble of natural language processing techniques and artificial intelligence algorithms for real-time monitoring and co-facilitation of patient-centered healthcare for improved mental health and wellbeing outcomes. The technical contributions of this framework are; detection of patient emotions, prediction of patient emotion transitions, detection of group emotions, formulation of patient behavioral metrics, and resource recommendations based on patient concerns. The architectural contributions of the framework are intelligent communication channels that stream empathic conversational elements and resource recommendations for the multi-user conversations and co-facilitation updates for the human healthcare provider interface. The framework was empirically evaluated on a benchmark dataset and further validated based on a clinical protocol designed for its application in an online support group setting for cancer patients and caregivers in Canada. The results of these experiments confirm the effectiveness of this framework, its contributory role and practical value in realizing a patient-centered healthcare model for improved mental health and wellbeing outcomes.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Future Generation Computer Systems},
	author = {Adikari, Achini and de Silva, Daswin and Moraliyage, Harsha and Alahakoon, Damminda and Wong, Jiahui and Gancarz, Mathew and Chackochan, Suja and Park, Bomi and Heo, Rachel and Leung, Yvonne},
	month = jan,
	year = {2022},
	pages = {318--329},
	file = {Adikari et al. - 2022 - Empathic conversational agents for real-time monit.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3L7LBF2P\\Adikari et al. - 2022 - Empathic conversational agents for real-time monit.pdf:application/pdf},
}

@article{gardinerEngagingWomenEmbodied2017,
	title = {Engaging women with an embodied conversational agent to deliver mindfulness and lifestyle recommendations: {A} feasibility randomized control trial},
	volume = {100},
	issn = {07383991},
	shorttitle = {Engaging women with an embodied conversational agent to deliver mindfulness and lifestyle recommendations},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0738399117302495},
	doi = {10.1016/j.pec.2017.04.015},
	abstract = {Objective: This randomized controlled trial evaluates the feasibility of using an Embodied Conversational Agent (ECA) to teach lifestyle modiﬁcations to urban women.
Methods: Women were randomized to either 1) an ECA (content included: mindfulness, stress management, physical activity, and healthy eating) or 2) patient education sheets mirroring same content plus a meditation CD/MP3 once a day for one month. General outcome measures included: number of stress management techniques used, physical activity levels, and eating patterns.
Results: Sixty-one women ages 18 to 50 were enrolled. On average, 51\% identiﬁed as white, 26\% as black, 23\% as other races; and 20\% as Hispanic. The major stress management techniques reported at baseline were: exercise (69\%), listening to music (70\%), and social support (66\%). After one month, women randomized to the ECA signiﬁcantly decreased alcohol consumption to reduce stress (p = 0.03) and increased daily fruit consumption by an average of 2 servings compared to the control (p = 0.04).
Conclusion: It is feasible to use an ECA to promote health behaviors on stress management and healthy eating among diverse urban women. Practice implications: Compared to patient information sheets, ECAs provide promise as a way to teach healthy lifestyle behaviors to diverse urban women.},
	language = {en},
	number = {9},
	urldate = {2022-11-05},
	journal = {Patient Education and Counseling},
	author = {Gardiner, Paula M. and McCue, Kelly D. and Negash, Lily M. and Cheng, Teresa and White, Laura F. and Yinusa-Nyahkoon, Leanne and Jack, Brian W. and Bickmore, Timothy W.},
	month = sep,
	year = {2017},
	pages = {1720--1729},
	file = {Gardiner et al. - 2017 - Engaging women with an embodied conversational age.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XZ38R8ZI\\Gardiner et al. - 2017 - Engaging women with an embodied conversational age.pdf:application/pdf},
}

@article{kumarsharmaEfficientApproachProduct2021,
	title = {An {Efficient} {Approach} of {Product} {Recommendation} {System} using {NLP} {Technique}},
	issn = {22147853},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2214785321052305},
	doi = {10.1016/j.matpr.2021.07.371},
	abstract = {As we are moving toward an age of digital globalization and online shopping, there is an increasing need for an efﬁcient and reliable system that can help the consumers and the visitors to ﬁnd their suitable products. Currently, various websites display the searched product when a visitor comes to their website. What we need is a system, which can recommend the products which are like the searched products. This will help the consumer to ﬁnd out another product in case the item is unavailable, or the searched product is not good enough, or when they would like to look through different similar products. A good recommendation system has been found out to be ﬁnancially beneﬁcial for the companies also. It is found out that consumer is 35\% more likely to buy a product if the recommendation is good enough for consumers.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Materials Today: Proceedings},
	author = {Kumar Sharma, Akhilesh and Bajpai, Bhavna and Adhvaryu, Rachit and Dhruvi Pankajkumar, Suthar and Parthkumar Gordhanbhai, Prajapati and Kumar, Atul},
	month = aug,
	year = {2021},
	pages = {S2214785321052305},
	file = {Kumar Sharma et al. - 2021 - An Efficient Approach of Product Recommendation Sy.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RBFXLYSK\\Kumar Sharma et al. - 2021 - An Efficient Approach of Product Recommendation Sy.pdf:application/pdf},
}

@article{sejwalHybridRecommendationTechnique2022,
	title = {A hybrid recommendation technique using topic embedding for rating prediction and to handle cold-start problem},
	volume = {209},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422014385},
	doi = {10.1016/j.eswa.2022.118307},
	abstract = {Recommender systems aim to estimate item ratings and recommend items based on the users’ interests. The traditional recommender systems generally consider user–item rating information for rating prediction, but they suffer from various limitations, such as data sparsity, black-box recommendation, and cold-start problems. As a result, researchers have proposed amalgamating contextual information with rating data to provide effective recommendations. Although user-generated data in the form of reviews are a rich source of contextual information, they are rarely utilized in recommender algorithms. This study presents a hybrid recommendation technique, called RecTE, using rating data and topic embedding, which is an amalgamation of word embedding and topic modeling techniques. The novelty of RecTE lies in predicting item ratings using topic embeddings learned by incorporating local and global contextual information and integrating them with user-based collaborative filtering. RecTE is empirically evaluated over three real-world datasets – YelpNYC, YelpZip and TripAdvisor. This technique performs significantly better in comparison to nine baselines and five state-of-the-art recommendation techniques. On empirical analysis, we found that incorporating topic embedding in RecTE makes it capable of performing significantly better and handle cold-start problems effectively in comparison to the existing recommendation approaches.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Expert Systems with Applications},
	author = {Sejwal, Vineet K. and Abulaish, Muhammad},
	month = dec,
	year = {2022},
	pages = {118307},
	file = {Sejwal and Abulaish - 2022 - A hybrid recommendation technique using topic embe.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\78S37FGQ\\Sejwal and Abulaish - 2022 - A hybrid recommendation technique using topic embe.pdf:application/pdf},
}

@article{linContextawareReinforcementLearning2022,
	title = {Context-aware reinforcement learning for course recommendation},
	volume = {125},
	issn = {15684946},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S156849462200432X},
	doi = {10.1016/j.asoc.2022.109189},
	abstract = {Online course recommendation is an extremely relevant ingredient for the efficiency of e-learning. The current recommendation methods cannot guarantee the effectiveness and accuracy of course recommendation, especially when a user has enrolled in many different courses. Because these methods fail to distinguish the most relevant historical courses, which can contribute to predicting the target course that indeed reflects the user’s interests from her sequential learning behaviors. In this paper, we propose a context-aware reinforcement learning method, named Hierarchical and Recurrent Reinforcement Learning (HRRL), to efficiently reconstruct user profiles for course recommendation. The key ingredient of our scheme is the novel interaction between an attention-based recommendation model and a profile reviser with Recurrent Reinforcement Learning (RRL) that exploits temporal context. To this aim, a contextual policy gradient with approximation is proposed for RRL. By employing RRL in hierarchical tasks of revising user profiles, the proposed HRRL model enables reliable convergence in revising policy learning and improves the recommendation accuracy. We demonstrate the effectiveness of our proposed method by experiments on two open online courses datasets. Empirical results show that HRRL significantly outperforms state-of-the-art baselines.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Applied Soft Computing},
	author = {Lin, Yuanguo and Lin, Fan and Yang, Lvqing and Zeng, Wenhua and Liu, Yong and Wu, Pengcheng},
	month = aug,
	year = {2022},
	pages = {109189},
	file = {Lin et al. - 2022 - Context-aware reinforcement learning for course re.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UGAPP57I\\Lin et al. - 2022 - Context-aware reinforcement learning for course re.pdf:application/pdf},
}

@article{zhangKGANKnowledgeGrouping2023,
	title = {{KGAN}: {Knowledge} {Grouping} {Aggregation} {Network} for course recommendation in {MOOCs}},
	volume = {211},
	issn = {09574174},
	shorttitle = {{KGAN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422014646},
	doi = {10.1016/j.eswa.2022.118344},
	abstract = {Massive open online courses (MOOCs) are dedicated to providing learners with large-scale and open-access boutique courses. Recently, the course recommendation algorithm in MOOCs has attracted many researchers’ attention. Compared with ordinary recommendation scenarios, course interaction density in MOOCs is more tenuous, since it demands lots of time and gumption of learners. Meanwhile, the connections and semantic information among courses are much more diversiform and plentiful. Therefore, knowledge graph enhanced recommendation algorithms are appropriate for addressing the course recommendation problem in MOOCs scenario. According to relevant research on KG-based algorithms, utilization efficiency of edge information seems to be the key to boosting model effectiveness. In this paper, we propose a high-performance course recommendation model named Knowledge Grouping Aggregation Network (KGAN), which uses the course graph (a heterogeneous graph for describing the relations between courses and facts) to estimate learners’ potential interests automatically and iteratively. More precisely, KGAN constructs an end-to-end recommendation model, which projects the learner’s behavior and course graph into a unified space naturally, which alleviates difficulties in course recommendation such as interaction tenuous, course relevance, and intention diversity. In addition, we proposed intra-group and inter-group attention operator, which packages the propagation set according to the relation-links and obtains the corresponding attention priorities of different entities under different paths for constructing a reasonable and explicit encoding of users. We apply the proposed model on the real-world datasets, and the empirical results demonstrate that KGAN outperforms compelling state-of-the-art baselines. Our implementations are available at https://github.com/StZHY/KGAN.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Expert Systems with Applications},
	author = {Zhang, Huanyu and Shen, Xiaoxuan and Yi, Baolin and Wang, Wei and Feng, Yong},
	month = jan,
	year = {2023},
	pages = {118344},
	file = {Zhang et al. - 2023 - KGAN Knowledge Grouping Aggregation Network for c.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RHZGGHD3\\Zhang et al. - 2023 - KGAN Knowledge Grouping Aggregation Network for c.pdf:application/pdf},
}

@article{zhangSelfSupervisedReinforcementLearning2022,
	title = {Self-{Supervised} {Reinforcement} {Learning} with dual-reward for knowledge-aware recommendation},
	issn = {15684946},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494622007943},
	doi = {10.1016/j.asoc.2022.109745},
	abstract = {To improve the recommendation accuracy and oﬀer explanations for recommendations, Reinforcement Learning (RL) has been applied to path reasoning over knowledge graphs. However, in recommendation tasks, most existing RL methods learn the path-ﬁnding policy using only a shortterm or single reward, leading to a local optimum and losing some potential paths. To address these issues, we propose a Self-Supervised Reinforcement Learning (SSRL) framework combined with dual-reward for knowledgeaware recommendation reasoning over knowledge graphs. Then, we improve Actor-Critic algorithm by using a dual-reward driven strategy, which combines short-term reward with long-term incremental evaluation. The improved algorithm helps the policy guide path reasoning in an overall situation. In addition, to ﬁnd the most potential paths, in the improved Actor-Critic algorithm, a loss constraint of each sample is used as a reinforced signal to update the gradients. With some improvements against baselines, experimental results demonstrate the eﬀectiveness of our framework.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Applied Soft Computing},
	author = {Zhang, Wei and Lin, Yuanguo and Liu, Yong and You, Huanyu and Wu, Pengcheng and Lin, Fan and Zhou, Xiuze},
	month = oct,
	year = {2022},
	pages = {109745},
	file = {Zhang et al. - 2022 - Self-Supervised Reinforcement Learning with dual-r.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I5TLY4YP\\Zhang et al. - 2022 - Self-Supervised Reinforcement Learning with dual-r.pdf:application/pdf},
}

@article{reyesAdaptableRecommendationSystem2021,
	title = {Adaptable {Recommendation} {System} for {Outfit} {Selection} with {Deep} {Learning} {Approach}},
	volume = {54},
	issn = {24058963},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2405896321019534},
	doi = {10.1016/j.ifacol.2021.10.516},
	language = {en},
	number = {13},
	urldate = {2022-11-05},
	journal = {IFAC-PapersOnLine},
	author = {Reyes, Laura J. Padilla and Oviedo, Natalia Bonifaz and Camacho, Edgar C. and Calderon, Juan M.},
	year = {2021},
	pages = {605--610},
	file = {Reyes et al. - 2021 - Adaptable Recommendation System for Outfit Selecti.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LTT8XBP7\\Reyes et al. - 2021 - Adaptable Recommendation System for Outfit Selecti.pdf:application/pdf},
}

@article{montenegroSurveyConversationalAgents2019,
	title = {Survey of conversational agents in health},
	volume = {129},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417419302283},
	doi = {10.1016/j.eswa.2019.03.054},
	abstract = {Method
Results Discussion
Conclusion},
	language = {en},
	urldate = {2022-11-05},
	journal = {Expert Systems with Applications},
	author = {Montenegro, Joao Luis Zeni and da Costa, Cristiano André and da Rosa Righi, Rodrigo},
	month = sep,
	year = {2019},
	pages = {56--67},
	file = {Montenegro et al. - 2019 - Survey of conversational agents in health.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HT9SC5CA\\Montenegro et al. - 2019 - Survey of conversational agents in health.pdf:application/pdf},
}

@incollection{hernandezrizzardiniIntegratingConversationalPedagogical2021,
	title = {Integrating a conversational pedagogical agent into the instructional activities of a {Massive} {Open} {Online} {Course}},
	isbn = {978-0-12-823410-5},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B978012823410500005X},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Intelligent {Systems} and {Learning} {Data} {Analytics} in {Online} {Education}},
	publisher = {Elsevier},
	author = {Hernández Rizzardini, Rocael and Amado-Salvatierra, Héctor R. and Morales Chan, Miguel},
	year = {2021},
	doi = {10.1016/B978-0-12-823410-5.00005-X},
	pages = {31--45},
	file = {Hernández Rizzardini et al. - 2021 - Integrating a conversational pedagogical agent int.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2SN56SSV\\Hernández Rizzardini et al. - 2021 - Integrating a conversational pedagogical agent int.pdf:application/pdf},
}

@incollection{berubeVoicebasedConversationalAgents2023,
	title = {Voice-based conversational agents for sensing and support: {Examples} from academia and industry},
	isbn = {978-0-323-90045-4},
	shorttitle = {Voice-based conversational agents for sensing and support},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780323900454000174},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Digital {Therapeutics} for {Mental} {Health} and {Addiction}},
	publisher = {Elsevier},
	author = {Bérubé, Caterina and Fleisch, Elgar},
	year = {2023},
	doi = {10.1016/B978-0-323-90045-4.00017-4},
	pages = {113--134},
	file = {Bérubé and Fleisch - 2023 - Voice-based conversational agents for sensing and .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XELDSGCM\\Bérubé and Fleisch - 2023 - Voice-based conversational agents for sensing and .pdf:application/pdf},
}

@article{baizalComputationalModelGenerating2020,
	title = {Computational model for generating interactions in conversational recommender system based on product functional requirements},
	volume = {128},
	issn = {0169023X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169023X1830524X},
	doi = {10.1016/j.datak.2020.101813},
	abstract = {Conversational recommender system is a tool to help customer in deciding products they are going to buy, by conversational mechanism. By this mechanism, the system is able to imitate natural conversation between customer and professional sales support, for eliciting customer preference. However, many customers are not familiar with the technical features of multifunction and multi-feature products. A more natural way to explore customer preferences is by asking what they want to use with the product they are looking for (product functional requirements). Therefore, this paper proposes a computational model incorporating product functional requirements for interaction. The proposed model covers ontology and its structure as well as algorithms for generating interaction that comprises asking question, recommending products and presenting explanation of why a product is recommended. Based on our user studies, both expert users (familiar with product technical features) and novice users (not familiar with product technical feature) prefer our proposed interaction model than that of the flat interaction model (interaction model based on technical features). Meanwhile, functional requirements-based explanation is able to improve user trust in recommended products by 30\% for novice users and 17\% for expert users.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Data \& Knowledge Engineering},
	author = {Baizal, Z.K.A. and Widyantoro, Dwi H. and Maulidevi, Nur Ulfa},
	month = jul,
	year = {2020},
	pages = {101813},
	file = {Baizal et al. - 2020 - Computational model for generating interactions in.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZJWDY35X\\Baizal et al. - 2020 - Computational model for generating interactions in.pdf:application/pdf},
}

@article{bavarescoConversationalAgentsBusiness2020,
	title = {Conversational agents in business: {A} systematic literature review and future research directions},
	volume = {36},
	issn = {15740137},
	shorttitle = {Conversational agents in business},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574013719303193},
	doi = {10.1016/j.cosrev.2020.100239},
	abstract = {The field of business shows an increasing interest in exploring conversational agents to improve service quality and market competitiveness. Furthermore, the advances in machine learning capabilities leverage the natural language processing towards natural and straightforward dialogue experiences for industries. However, in the best of our knowledge, no literature review outlines conversational agents in the business industry, primarily taking into account computational learning capabilities. This article presents a systematic literature review that encompasses these areas looking through the use of machine learning to improve the field of business. The review followed a guideline for systematic reviews to present the literature of the last decade, emphasizing business perspectives such as domains, goals, and challenges, and computational methods for self-learning, personalization, and response generation of conversational agents. As a result, the article provides the answers of three general, three focused, and two statistical questions to address the role of artificial intelligence in conversational agents applied to business domains. In this regard, the results show that no study combines self-learning, personalization, and generative-based responses for the same business solution. Additionally, the article describes the organization of the state-of-the-art, highlighting the correlation of business perspectives and machine learning methods. The contributions of this review focus on opportunities and future research directions towards human-like conversational agents for business.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Computer Science Review},
	author = {Bavaresco, Rodrigo and Silveira, Diórgenes and Reis, Eduardo and Barbosa, Jorge and Righi, Rodrigo and Costa, Cristiano and Antunes, Rodolfo and Gomes, Marcio and Gatti, Clauter and Vanzin, Mariangela and Junior, Saint Clair and Silva, Elton and Moreira, Carlos},
	month = may,
	year = {2020},
	pages = {100239},
	file = {Bavaresco et al. - 2020 - Conversational agents in business A systematic li.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SA6N8D99\\Bavaresco et al. - 2020 - Conversational agents in business A systematic li.pdf:application/pdf},
}

@article{binderConversationalEngagementMobile2019,
	title = {Conversational engagement and mobile technology use},
	volume = {99},
	issn = {07475632},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0747563219301918},
	doi = {10.1016/j.chb.2019.05.016},
	abstract = {The present work investigated eﬀects of mobility constraints on gesturing and other aspects of conversational engagement when using mobile technology. Based on studies of non-verbal communication and interaction quality, we compared mobile and static forms of video call in structured interviews. Using real-time motion capture and content coding of recordings as well as self-reports, several data sources were considered: hand velocity as an indicator of gesturing, observed levels of engagement and remembered conversational content. Gesturing, in contrast to other indicators of engagement, was reduced under high mobility constraints and was more pronounced under low constraints. Gesturing was further positively related to observed engagement. Additionally, lower constraints resulted in more memories of conversational content. These ﬁndings emphasise the relevance of mobility constraints and bodily movement during mediated interaction and call for further model development in computer-mediated communication. Potential for application lies in interactive scenarios such as remote interviewing, testimonials, and relationship maintenance.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Computers in Human Behavior},
	author = {Binder, Jens F. and Cebula, Klaudia and Metwally, Sondos and Vernon, Michael and Atkin, Christopher and Mitra, Suvobrata},
	month = oct,
	year = {2019},
	pages = {66--75},
	file = {Binder et al. - 2019 - Conversational engagement and mobile technology us.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TB2ZBQVH\\Binder et al. - 2019 - Conversational engagement and mobile technology us.pdf:application/pdf},
}

@article{fuLearningConversationalAI2022,
	title = {Learning towards conversational {AI}: {A} survey},
	volume = {3},
	issn = {26666510},
	shorttitle = {Learning towards conversational {AI}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666651022000079},
	doi = {10.1016/j.aiopen.2022.02.001},
	abstract = {Recent years have witnessed a surge of interest in the field of open-domain dialogue. Thanks to the rapid development of social media, large dialogue corpus from the Internet builds up a fundamental premise for datadriven dialogue model. The breakthrough in neural network also brings new ideas to researchers in AI and NLP. A great number of new techniques and methods therefore came into being. In this paper, we review some of the most representative works in recent years and divide existing prevailing frameworks for a dialogue model into three categories. We further analyze the trend of development for open-domain dialogue and summarize the goal of an open-domain dialogue system in two aspects, informative and controllable. The methods we review in this paper are selected according to our unique perspectives and by no means complete. Rather, we hope this servery could benefit NLP community for future research in open-domain dialogue.},
	language = {en},
	urldate = {2022-11-05},
	journal = {AI Open},
	author = {Fu, Tingchen and Gao, Shen and Zhao, Xueliang and Wen, Ji-rong and Yan, Rui},
	year = {2022},
	pages = {14--28},
	file = {Fu et al. - 2022 - Learning towards conversational AI A survey.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NXWZ25NT\\Fu et al. - 2022 - Learning towards conversational AI A survey.pdf:application/pdf},
}

@inproceedings{sachdevaHowUsefulAre2020,
	title = {How {Useful} are {Reviews} for {Recommendation}? {A} {Critical} {Review} and {Potential} {Improvements}},
	shorttitle = {How {Useful} are {Reviews} for {Recommendation}?},
	url = {http://arxiv.org/abs/2005.12210},
	doi = {10.1145/3397271.3401281},
	abstract = {We investigate a growing body of work that seeks to improve recommender systems through the use of review text. Generally, these papers argue that since reviews ‘explain’ users’ opinions, they ought to be useful to infer the underlying dimensions that predict ratings or purchases. Schemes to incorporate reviews range from simple regularizers to neural network approaches. Our initial findings reveal several discrepancies in reported results, partly due to (e.g.) copying results across papers despite changes in experimental settings or data pre-processing. First, we attempt a comprehensive analysis to resolve these ambiguities. Further investigation calls for discussion on a much larger problem about the “importance" of user reviews for recommendation. Through a wide range of experiments, we observe several cases where state-of-the-art methods fail to outperform existing baselines, especially as we deviate from a few narrowly-defined settings where reviews are useful. We conclude by providing hypotheses for our observations, that seek to characterize under what conditions reviews are likely to be helpful. Through this work, we aim to evaluate the direction in which the field is progressing and encourage robust empirical evaluation.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Sachdeva, Noveen and McAuley, Julian},
	month = jul,
	year = {2020},
	note = {arXiv:2005.12210 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval, Computer Science - Social and Information Networks},
	pages = {1845--1848},
	file = {Sachdeva and McAuley - 2020 - How Useful are Reviews for Recommendation A Criti.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EUYWYMF7\\Sachdeva and McAuley - 2020 - How Useful are Reviews for Recommendation A Criti.pdf:application/pdf},
}

@misc{guIncorporatingCopyingMechanism2016,
	title = {Incorporating {Copying} {Mechanism} in {Sequence}-to-{Sequence} {Learning}},
	url = {http://arxiv.org/abs/1603.06393},
	abstract = {We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural networkbased Seq2Seq learning and propose a new model called COPYNET with encoderdecoder structure. COPYNET can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose subsequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efﬁcacy of COPYNET. For example, COPYNET can outperform regular RNN-based model with remarkable margins on text summarization tasks.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Gu, Jiatao and Lu, Zhengdong and Li, Hang and Li, Victor O. K.},
	month = jun,
	year = {2016},
	note = {arXiv:1603.06393 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {Gu et al. - 2016 - Incorporating Copying Mechanism in Sequence-to-Seq.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KUVMCAQV\\Gu et al. - 2016 - Incorporating Copying Mechanism in Sequence-to-Seq.pdf:application/pdf},
}

@inproceedings{niPersonalizedReviewGeneration2018a,
	address = {Melbourne, Australia},
	title = {Personalized {Review} {Generation} {By} {Expanding} {Phrases} and {Attending} on {Aspect}-{Aware} {Representations}},
	url = {http://aclweb.org/anthology/P18-2112},
	doi = {10.18653/v1/P18-2112},
	abstract = {In this paper, we focus on the problem of building assistive systems that can help users to write reviews. We cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries, product titles) provided as input to the system. We incorporate aspect-level information via an aspect encoder that learns ‘aspect-aware’ user and item representations. An attention fusion layer is applied to control generation by attending on the outputs of multiple encoders. Experimental results show that our model is capable of generating coherent and diverse reviews that expand the contents of input phrases. In addition, the learned aspectaware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ni, Jianmo and McAuley, Julian},
	year = {2018},
	pages = {706--711},
	file = {Ni and McAuley - 2018 - Personalized Review Generation By Expanding Phrase.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FQQ3VM8W\\Ni and McAuley - 2018 - Personalized Review Generation By Expanding Phrase.pdf:application/pdf},
}

@misc{gaoGeneratingMultipleDiverse2019,
	title = {Generating {Multiple} {Diverse} {Responses} for {Short}-{Text} {Conversation}},
	url = {http://arxiv.org/abs/1811.05696},
	abstract = {Neural generative models have become popular and achieved promising performance on short-text conversation tasks. They are generally trained to build a 1-to-1 mapping from the input post to its output response. However, a given post is often associated with multiple replies simultaneously in real applications. Previous research on this task mainly focuses on improving the relevance and informativeness of the top one generated response for each post. Very few works study generating multiple accurate and diverse responses for the same post. In this paper, we propose a novel response generation model, which considers a set of responses jointly and generates multiple diverse responses simultaneously. A reinforcement learning algorithm is designed to solve our model. Experiments on two short-text conversation tasks validate that the multiple responses generated by our model obtain higher quality and larger diversity compared with various state-ofthe-art generative models.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Gao, Jun and Bi, Wei and Liu, Xiaojiang and Li, Junhui and Shi, Shuming},
	month = feb,
	year = {2019},
	note = {arXiv:1811.05696 [cs]},
	keywords = {Computer Science - Computation and Language, xrec},
	file = {Gao et al. - 2019 - Generating Multiple Diverse Responses for Short-Te.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9H846A9C\\Gao et al. - 2019 - Generating Multiple Diverse Responses for Short-Te.pdf:application/pdf},
}

@misc{bahetiGeneratingMoreInteresting2018a,
	title = {Generating {More} {Interesting} {Responses} in {Neural} {Conversation} {Models} with {Distributional} {Constraints}},
	url = {http://arxiv.org/abs/1809.01215},
	abstract = {Neural conversation models tend to generate safe, generic responses for most inputs. This is due to the limitations of likelihoodbased decoding objectives in generation tasks with diverse outputs, such as conversation. To address this challenge, we propose a simple yet effective approach for incorporating side information in the form of distributional constraints over the generated responses. We propose two constraints that help generate more content rich responses that are based on a model of syntax and topics (Grifﬁths et al., 2005) and semantic similarity (Arora et al., 2016). We evaluate our approach against a variety of competitive baselines, using both automatic metrics and human judgments, showing that our proposed approach generates responses that are much less generic without sacriﬁcing plausibility. A working demo of our code can be found at https://github.com/abaheti95/ DC-NeuralConversation.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Baheti, Ashutosh and Ritter, Alan and Li, Jiwei and Dolan, Bill},
	month = sep,
	year = {2018},
	note = {arXiv:1809.01215 [cs]},
	keywords = {Computer Science - Computation and Language, xrec},
	file = {Baheti et al. - 2018 - Generating More Interesting Responses in Neural Co.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3QH9WS4H\\Baheti et al. - 2018 - Generating More Interesting Responses in Neural Co.pdf:application/pdf},
}

@misc{wangBERTHasMouth2019,
	title = {{BERT} has a {Mouth}, and {It} {Must} {Speak}: {BERT} as a {Markov} {Random} {Field} {Language} {Model}},
	shorttitle = {{BERT} has a {Mouth}, and {It} {Must} {Speak}},
	url = {http://arxiv.org/abs/1902.04094},
	abstract = {We show that BERT (Devlin et al., 2018) is a Markov random ﬁeld language model. This formulation gives way to a natural procedure to sample sentences from BERT. We generate from BERT and ﬁnd that it can produce highquality, ﬂuent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Wang, Alex and Cho, Kyunghyun},
	month = apr,
	year = {2019},
	note = {arXiv:1902.04094 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, xrec},
	file = {Wang and Cho - 2019 - BERT has a Mouth, and It Must Speak BERT as a Mar.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VJLIIN3E\\Wang and Cho - 2019 - BERT has a Mouth, and It Must Speak BERT as a Mar.pdf:application/pdf},
}

@misc{wuGoogleNeuralMachine2016,
	title = {Google's {Neural} {Machine} {Translation} {System}: {Bridging} the {Gap} between {Human} and {Machine} {Translation}},
	shorttitle = {Google's {Neural} {Machine} {Translation} {System}},
	url = {http://arxiv.org/abs/1609.08144},
	abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Łukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	month = oct,
	year = {2016},
	note = {arXiv:1609.08144 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, xrec},
	file = {Wu et al. - 2016 - Google's Neural Machine Translation System Bridgi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3A6VUED5\\Wu et al. - 2016 - Google's Neural Machine Translation System Bridgi.pdf:application/pdf},
}

@misc{choLearningPhraseRepresentations2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder–Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a ﬁxedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv:1406.1078 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, xrec},
	file = {Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4KDLI87E\\Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:application/pdf},
}

@misc{sutskeverSequenceSequenceLearning2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = dec,
	year = {2014},
	note = {arXiv:1409.3215 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, xrec},
	file = {Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7MKAHJQN\\Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf},
}

@inproceedings{wangFastAccurateNeural2018a,
	address = {Brussels, Belgium},
	title = {Toward {Fast} and {Accurate} {Neural} {Discourse} {Segmentation}},
	url = {http://aclweb.org/anthology/D18-1116},
	doi = {10.18653/v1/D18-1116},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Yizhong and Li, Sujian and Yang, Jingfeng},
	year = {2018},
	keywords = {xrec},
	pages = {962--967},
	file = {Wang et al. - 2018 - Toward Fast and Accurate Neural Discourse Segmenta.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KYLBXNSF\\Wang et al. - 2018 - Toward Fast and Accurate Neural Discourse Segmenta.pdf:application/pdf},
}

@article{wooPredictingRatingsAmazon2021a,
	title = {Predicting the ratings of {Amazon} products using {Big} {Data}},
	volume = {11},
	issn = {1942-4787, 1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/widm.1400},
	doi = {10.1002/widm.1400},
	abstract = {This paper aims to apply several machine learning (ML) models to the massive dataset present in the area of e-commerce from Amazon to analyze and predict ratings and to recommend products. For this purpose, we have used both traditional and Big Data algorithms. As the Amazon product review dataset is large, we present Big Data architecture suitable massive dataset for storing and computation, which is not possible with the traditional architecture. Furthermore, the dataset contains 15 attributes and has about 7 million records. With the dataset, we develop several models in Oracle Big Data and Azure Cloud Computing services to predict the review rating and recommendation for the items at Amazon. We present a comparative conclusion in terms of the accuracy as well as the efficiency with Spark ML—the Big Data architecture, and Azure ML—the traditional architecture.},
	language = {en},
	number = {3},
	urldate = {2022-11-05},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Woo, Jongwook and Mishra, Monika},
	month = may,
	year = {2021},
	file = {Woo and Mishra - 2021 - Predicting the ratings of Amazon products using Bi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5R878DTD\\Woo and Mishra - 2021 - Predicting the ratings of Amazon products using Bi.pdf:application/pdf},
}

@article{huangReportInternationalWorkshop2020,
	title = {Report on the international workshop on natural language processing for recommendations ({NLP4REC} 2020) workshop held at {WSDM} 2020},
	volume = {54},
	issn = {0163-5840},
	url = {https://dl.acm.org/doi/10.1145/3451964.3451970},
	doi = {10.1145/3451964.3451970},
	abstract = {This paper summarizes the outcomes of the International Workshop on Natural Language Processing for Recommendations (NLP4REC 2020), held in Houston, USA, on February 7, 2020, during WSDM 2020. The purpose of this workshop was to explore the potential research topics and industrial applications in leveraging natural language processing techniques to tackle the challenges in constructing more intelligent recommender systems. Speciﬁc topics included, but were not limited to knowledge-aware recommendation, explainable recommendation, conversational recommendation, and sequential recommendation.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {ACM SIGIR Forum},
	author = {Huang, Xiao and Ren, Pengjie and Ren, Zhaochun and Sun, Fei and He, Xiangnan and Yin, Dawei and de Rijke, Maarten},
	month = jun,
	year = {2020},
	pages = {1--5},
	file = {Huang et al. - 2020 - Report on the international workshop on natural la.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AMIWIP38\\Huang et al. - 2020 - Report on the international workshop on natural la.pdf:application/pdf},
}

@inproceedings{hadaReXPlugExplainableRecommendation2021,
	address = {Virtual Event Canada},
	title = {{ReXPlug}: {Explainable} {Recommendation} using {Plug}-and-{Play} {Language} {Model}},
	isbn = {978-1-4503-8037-9},
	shorttitle = {{ReXPlug}},
	url = {https://dl.acm.org/doi/10.1145/3404835.3462939},
	doi = {10.1145/3404835.3462939},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Hada, Deepesh V. and M., Vijaikumar and Shevade, Shirish K.},
	month = jul,
	year = {2021},
	pages = {81--91},
	file = {Hada et al. - 2021 - ReXPlug Explainable Recommendation using Plug-and.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IG6DELQI\\Hada et al. - 2021 - ReXPlug Explainable Recommendation using Plug-and.pdf:application/pdf},
}

@misc{Niu2018,
	title = {Image-based {Recommendations} on {Styles} and {Substitutes}},
	url = {http://arxiv.org/abs/1506.04757},
	abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {McAuley, Julian and Targett, Christopher and Shi, Qinfeng and Hengel, Anton van den},
	month = jun,
	year = {2015},
	note = {ISBN: 9781450364195},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval},
	file = {McAuley et al. - 2015 - Image-based Recommendations on Styles and Substitu.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZWYHJFUC\\McAuley et al. - 2015 - Image-based Recommendations on Styles and Substitu.pdf:application/pdf},
}

@book{ricciRecommenderSystemsHandbook2011a,
	address = {Boston, MA},
	title = {Recommender {Systems} {Handbook}},
	isbn = {978-0-387-85819-7 978-0-387-85820-3},
	url = {http://link.springer.com/10.1007/978-0-387-85820-3},
	language = {en},
	urldate = {2022-11-05},
	publisher = {Springer US},
	editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
	year = {2011},
	doi = {10.1007/978-0-387-85820-3},
	file = {Ricci et al. - 2011 - Recommender Systems Handbook.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TPGAU24U\\Ricci et al. - 2011 - Recommender Systems Handbook.pdf:application/pdf},
}

@misc{costaAutomaticGenerationNatural2017,
	title = {Automatic {Generation} of {Natural} {Language} {Explanations}},
	url = {http://arxiv.org/abs/1707.01561},
	abstract = {An important task for recommender system is to generate explanations according to a user’s preferences. Most of the current methods for explainable recommendations use structured sentences to provide descriptions along with the recommendations they produce. However, those methods have neglected the review-oriented way of writing a text, even though it is known that these reviews have a strong influence over user’s decision.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Costa, Felipe and Ouyang, Sixun and Dolog, Peter and Lawlor, Aonghus},
	month = jul,
	year = {2017},
	note = {arXiv:1707.01561 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Costa et al. - 2017 - Automatic Generation of Natural Language Explanati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I75HRFUI\\Costa et al. - 2017 - Automatic Generation of Natural Language Explanati.pdf:application/pdf},
}

@inproceedings{liNeuralRatingRegression2017,
	title = {Neural {Rating} {Regression} with {Abstractive} {Tips} {Generation} for {Recommendation}},
	url = {http://arxiv.org/abs/1708.00154},
	doi = {10.1145/3077136.3080822},
	abstract = {Recently, some E-commerce sites launch a new interaction box called Tips on their mobile apps. Users can express their experience and feelings or provide suggestions using short texts typically several words or one sentence. In essence, writing some tips and giving a numerical rating are two facets of a user’s product assessment action, expressing the user experience and feelings. Jointly modeling these two facets is helpful for designing a better recommendation system. While some existing models integrate text information such as item specifications or user reviews into user and item latent factors for improving the rating prediction, no existing works consider tips for improving recommendation quality. We propose a deep learning based framework named NRT which can simultaneously predict precise ratings and generate abstractive tips with good linguistic quality simulating user experience and feelings. For abstractive tips generation, gated recurrent neural networks are employed to “translate” user and item latent representations into a concise sentence. Extensive experiments on benchmark datasets from different domains show that NRT achieves significant improvements over the state-of-the-art methods. Moreover, the generated tips can vividly predict the user experience and feelings.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 40th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Li, Piji and Wang, Zihao and Ren, Zhaochun and Bing, Lidong and Lam, Wai},
	month = aug,
	year = {2017},
	note = {arXiv:1708.00154 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	pages = {345--354},
	file = {Li et al. - 2017 - Neural Rating Regression with Abstractive Tips Gen.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UXHAVJGB\\Li et al. - 2017 - Neural Rating Regression with Abstractive Tips Gen.pdf:application/pdf},
}

@misc{chenGenerateNaturalLanguage2021,
	title = {Generate {Natural} {Language} {Explanations} for {Recommendation}},
	url = {http://arxiv.org/abs/2101.03392},
	abstract = {Providing personalized explanations for recommendations can help users to understand the underlying insight of the recommendation results, which is helpful to the e ectiveness, transparency, persuasiveness and trustworthiness of recommender systems. Current explainable recommendation models mostly generate textual explanations based on pre-de ned sentence templates. However, the expressiveness power of template-based explanation sentences are limited to the pre-de ned expressions, and manually de ning the expressions require signi cant human e orts.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Chen, Hanxiong and Chen, Xu and Shi, Shaoyun and Zhang, Yongfeng},
	month = jan,
	year = {2021},
	note = {arXiv:2101.03392 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Chen et al. - 2021 - Generate Natural Language Explanations for Recomme.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9YQDGV8I\\Chen et al. - 2021 - Generate Natural Language Explanations for Recomme.pdf:application/pdf},
}

@misc{zhengJointDeepModeling2017,
	title = {Joint {Deep} {Modeling} of {Users} and {Items} {Using} {Reviews} for {Recommendation}},
	url = {http://arxiv.org/abs/1701.04783},
	abstract = {A large amount of information exists in reviews written by users. This source of information has been ignored by most of the current recommender systems while it can potentially alleviate the sparsity problem and improve the quality of recommendations. In this paper, we present a deep model to learn item properties and user behaviors jointly from review text. The proposed model, named Deep Cooperative Neural Networks (DeepCoNN), consists of two parallel neural networks coupled in the last layers. One of the networks focuses on learning user behaviors exploiting reviews written by the user, and the other one learns item properties from the reviews written for the item. A shared layer is introduced on the top to couple these two networks together. The shared layer enables latent factors learned for users and items to interact with each other in a manner similar to factorization machine techniques. Experimental results demonstrate that DeepCoNN signiﬁcantly outperforms all baseline recommender systems on a variety of datasets.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Zheng, Lei and Noroozi, Vahid and Yu, Philip S.},
	month = jan,
	year = {2017},
	note = {arXiv:1701.04783 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	file = {Zheng et al. - 2017 - Joint Deep Modeling of Users and Items Using Revie.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H93QFM43\\Zheng et al. - 2017 - Joint Deep Modeling of Users and Items Using Revie.pdf:application/pdf},
}

@inproceedings{wangExplainableRecommendationMultiTask2018,
	title = {Explainable {Recommendation} via {Multi}-{Task} {Learning} in {Opinionated} {Text} {Data}},
	url = {http://arxiv.org/abs/1806.03568},
	doi = {10.1145/3209978.3210010},
	abstract = {Explaining automatically generated recommendations allows users to make more informed and accurate decisions about which results to utilize, and therefore improves their satisfaction. In this work, we develop a multi-task learning solution for explainable recommendation. Two companion learning tasks of user preference modeling for recommendation and opinionated content modeling for explanation are integrated via a joint tensor factorization. As a result, the algorithm predicts not only a user’s preference over a list of items, i.e., recommendation, but also how the user would appreciate a particular item at the feature level, i.e., opinionated textual explanation. Extensive experiments on two large collections of Amazon and Yelp reviews confirmed the effectiveness of our solution in both recommendation and explanation tasks, compared with several existing recommendation algorithms. And our extensive user study clearly demonstrates the practical value of the explainable recommendations generated by our algorithm.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {The 41st {International} {ACM} {SIGIR} {Conference} on {Research} \& {Development} in {Information} {Retrieval}},
	author = {Wang, Nan and Wang, Hongning and Jia, Yiling and Yin, Yue},
	month = jun,
	year = {2018},
	note = {arXiv:1806.03568 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	pages = {165--174},
	file = {Wang et al. - 2018 - Explainable Recommendation via Multi-Task Learning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DNJVWYJ5\\Wang et al. - 2018 - Explainable Recommendation via Multi-Task Learning.pdf:application/pdf},
}

@misc{liCapsuleNetworkRecommendation2019,
	title = {A {Capsule} {Network} for {Recommendation} and {Explaining} {What} {You} {Like} and {Dislike}},
	url = {http://arxiv.org/abs/1907.00687},
	abstract = {User reviews contain rich semantics towards the preference of users to features of items. Recently, many deep learning based solutions have been proposed by exploiting reviews for recommendation. The attention mechanism is mainly adopted in these works to identify words or aspects that are important for rating prediction. However, it is still hard to understand whether a user likes or dislikes an aspect of an item according to what viewpoint the user holds and to what extent, without examining the review details. Here, we consider a pair of a viewpoint held by a user and an aspect of an item as a logic unit. Reasoning a rating behavior by discovering the informative logic units from the reviews and resolving their corresponding sentiments could enable a better rating prediction with explanation.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Li, Chenliang and Quan, Cong and Peng, Li and Qi, Yunwei and Deng, Yuming and Wu, Libing},
	month = jul,
	year = {2019},
	note = {arXiv:1907.00687 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {Li et al. - 2019 - A Capsule Network for Recommendation and Explainin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\A3ZZGYBZ\\Li et al. - 2019 - A Capsule Network for Recommendation and Explainin.pdf:application/pdf},
}

@inproceedings{chenSequentialRecommendationUser2018a,
	address = {Marina Del Rey CA USA},
	title = {Sequential {Recommendation} with {User} {Memory} {Networks}},
	isbn = {978-1-4503-5581-0},
	url = {https://dl.acm.org/doi/10.1145/3159652.3159668},
	doi = {10.1145/3159652.3159668},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {Eleventh} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Xu and Xu, Hongteng and Zhang, Yongfeng and Tang, Jiaxi and Cao, Yixin and Qin, Zheng and Zha, Hongyuan},
	month = feb,
	year = {2018},
	pages = {108--116},
	file = {Chen et al. - 2018 - Sequential Recommendation with User Memory Network.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\X8NV75HG\\Chen et al. - 2018 - Sequential Recommendation with User Memory Network.pdf:application/pdf;Chen et al. - 2018 - Sequential Recommendation with User Memory Network.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QLVYHNVE\\Chen et al. - 2018 - Sequential Recommendation with User Memory Network.pdf:application/pdf;Chen et al. - 2018 - Sequential Recommendation with User Memory Network.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YQZR4U5E\\Chen et al. - 2018 - Sequential Recommendation with User Memory Network.pdf:application/pdf},
}

@misc{taoFacTTamingLatent2019a,
	title = {The {FacT}: {Taming} {Latent} {Factor} {Models} for {Explainability} with {Factorization} {Trees}},
	shorttitle = {The {FacT}},
	url = {http://arxiv.org/abs/1906.02037},
	abstract = {Latent factor models have achieved great success in personalized recommendations, but they are also notoriously difficult to explain. In this work, we integrate regression trees to guide the learning of latent factor models for recommendation, and use the learnt tree structure to explain the resulting latent factors. Specifically, we build regression trees on users and items respectively with usergenerated reviews, and associate a latent profile to each node on the trees to represent users and items. With the growth of regression tree, the latent factors are gradually refined under the regularization imposed by the tree structure. As a result, we are able to track the creation of latent profiles by looking into the path of each factor on regression trees, which thus serves as an explanation for the resulting recommendations. Extensive experiments on two large collections of Amazon and Yelp reviews demonstrate the advantage of our model over several competitive baseline algorithms. Besides, our extensive user study also confirms the practical value of explainable recommendations generated by our model.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Tao, Yiyi and Jia, Yiling and Wang, Nan and Wang, Hongning},
	month = jun,
	year = {2019},
	note = {arXiv:1906.02037 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {Tao et al. - 2019 - The FacT Taming Latent Factor Models for Explaina.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FHXE7KB3\\Tao et al. - 2019 - The FacT Taming Latent Factor Models for Explaina.pdf:application/pdf},
}

@inproceedings{wangTEMTreeenhancedEmbedding2018a,
	address = {Lyon, France},
	title = {{TEM}: {Tree}-enhanced {Embedding} {Model} for {Explainable} {Recommendation}},
	isbn = {978-1-4503-5639-8},
	shorttitle = {{TEM}},
	url = {http://dl.acm.org/citation.cfm?doid=3178876.3186066},
	doi = {10.1145/3178876.3186066},
	abstract = {While collaborative filtering is the dominant technique in personalized recommendation, it models user-item interactions only and cannot provide concrete reasons for a recommendation. Meanwhile, the rich side information affiliated with user-item interactions (e.g., user demographics and item attributes), which provide valuable evidence that why a recommendation is suitable for a user, has not been fully explored in providing explanations.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 2018 {World} {Wide} {Web} {Conference} on {World} {Wide} {Web} - {WWW} '18},
	publisher = {ACM Press},
	author = {Wang, Xiang and He, Xiangnan and Feng, Fuli and Nie, Liqiang and Chua, Tat-Seng},
	year = {2018},
	pages = {1543--1552},
	file = {Wang et al. - 2018 - TEM Tree-enhanced Embedding Model for Explainable.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IA4IE9DN\\Wang et al. - 2018 - TEM Tree-enhanced Embedding Model for Explainable.pdf:application/pdf;Wang et al. - 2018 - TEM Tree-enhanced Embedding Model for Explainable.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KW3GC465\\Wang et al. - 2018 - TEM Tree-enhanced Embedding Model for Explainable.pdf:application/pdf},
}

@misc{kondylidisCategoryAwareExplainable2021,
	title = {Category {Aware} {Explainable} {Conversational} {Recommendation}},
	url = {http://arxiv.org/abs/2103.08733},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Kondylidis, Nikolaos and Zou, Jie and Kanoulas, Evangelos},
	month = mar,
	year = {2021},
	note = {arXiv:2103.08733 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Human-Computer Interaction},
	file = {Kondylidis et al. - 2021 - Category Aware Explainable Conversational Recommen.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JK75SPL4\\Kondylidis et al. - 2021 - Category Aware Explainable Conversational Recommen.pdf:application/pdf},
}

@inproceedings{xianReinforcementKnowledgeGraph2019a,
	address = {Paris France},
	title = {Reinforcement {Knowledge} {Graph} {Reasoning} for {Explainable} {Recommendation}},
	isbn = {978-1-4503-6172-9},
	url = {https://dl.acm.org/doi/10.1145/3331184.3331203},
	doi = {10.1145/3331184.3331203},
	abstract = {Recent advances in personalized recommendation have sparked great interest in the exploitation of rich structured information provided by knowledge graphs. Unlike most existing approaches that only focus on leveraging knowledge graphs for more accurate recommendation, we perform explicit reasoning with knowledge for decision making so that the recommendations are generated and supported by an interpretable causal inference procedure. To this end, we propose a method called Policy-Guided Path Reasoning (PGPR), which couples recommendation and interpretability by providing actual paths in a knowledge graph. Our contributions include four aspects. We first highlight the significance of incorporating knowledge graphs into recommendation to formally define and interpret the reasoning process. Second, we propose a reinforcement learning (RL) approach featuring an innovative soft reward strategy, user-conditional action pruning and a multi-hop scoring function. Third, we design a policy-guided graph search algorithm to efficiently and effectively sample reasoning paths for recommendation. Finally, we extensively evaluate our method on several large-scale real-world benchmark datasets, obtaining favorable results compared with state-of-the-art methods.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 42nd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Xian, Yikun and Fu, Zuohui and Muthukrishnan, S. and de Melo, Gerard and Zhang, Yongfeng},
	month = jul,
	year = {2019},
	pages = {285--294},
	file = {Reinforcement Knowledge Graph Reasoning for Explainable Recommendation.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Notes\\Reinforcement Knowledge Graph Reasoning for Explainable Recommendation.md:text/plain;Xian et al. - 2019 - Reinforcement Knowledge Graph Reasoning for Explai.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LWFACISN\\Xian et al. - 2019 - Reinforcement Knowledge Graph Reasoning for Explai.pdf:application/pdf;Xian et al. - 2019 - Reinforcement Knowledge Graph Reasoning for Explai.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2XYKMN43\\Xian et al. - 2019 - Reinforcement Knowledge Graph Reasoning for Explai.pdf:application/pdf;Xian et al. - 2019 - Reinforcement Knowledge Graph Reasoning for Explai.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N5IAQZEN\\Xian et al. - 2019 - Reinforcement Knowledge Graph Reasoning for Explai.pdf:application/pdf},
}

@inproceedings{caiPredictingUserIntents2020,
	address = {Genoa Italy},
	title = {Predicting {User} {Intents} and {Satisfaction} with {Dialogue}-based {Conversational} {Recommendations}},
	isbn = {978-1-4503-6861-2},
	url = {https://dl.acm.org/doi/10.1145/3340631.3394856},
	doi = {10.1145/3340631.3394856},
	abstract = {To develop a multi-turn dialogue-based conversational recommender system (DCRS), it is important to predict users’ intents behind their utterances and their satisfaction with the recommendation, so as to allow the system to incrementally refine user preference model and adjust its dialogue strategy. However, little work has investigated these issues so far. In this paper, we first contribute with two hierarchical taxonomies for classifying user intents and recommender actions respectively based on grounded theory. We then define various categories of feature considering content, discourse, sentiment, and context to predict users’ intents and satisfaction by comparing different machine learning methods. The experimental results for user intent prediction task show that some models (such as XGBoost and SVM) can perform well in predicting user intents, and incorporating context features into the prediction model can significantly boost the performance. Our empirical study also demonstrates that leveraging dialogue behavior features (i.e., including both user intents and recommender actions) can achieve good results in predicting user satisfaction.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 28th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {ACM},
	author = {Cai, Wanling and Chen, Li},
	month = jul,
	year = {2020},
	pages = {33--42},
	file = {Cai and Chen - 2020 - Predicting User Intents and Satisfaction with Dial.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZTQZEY2E\\Cai and Chen - 2020 - Predicting User Intents and Satisfaction with Dial.pdf:application/pdf;Cai and Chen - 2020 - Predicting User Intents and Satisfaction with Dial.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EYRZ5BAQ\\Cai and Chen - 2020 - Predicting User Intents and Satisfaction with Dial.pdf:application/pdf;Cai and Chen - 2020 - Predicting User Intents and Satisfaction with Dial.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8UHX74VN\\Cai and Chen - 2020 - Predicting User Intents and Satisfaction with Dial.pdf:application/pdf},
}

@inproceedings{chenTemporalMetapathGuided2021a,
	address = {Virtual Event Israel},
	title = {Temporal {Meta}-path {Guided} {Explainable} {Recommendation}},
	isbn = {978-1-4503-8297-7},
	url = {https://dl.acm.org/doi/10.1145/3437963.3441762},
	doi = {10.1145/3437963.3441762},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Hongxu and Li, Yicong and Sun, Xiangguo and Xu, Guandong and Yin, Hongzhi},
	month = mar,
	year = {2021},
	pages = {1056--1064},
	file = {Chen et al. - 2021 - Temporal Meta-path Guided Explainable Recommendati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9SIQ9QRG\\Chen et al. - 2021 - Temporal Meta-path Guided Explainable Recommendati.pdf:application/pdf;Chen et al. - 2021 - Temporal Meta-path Guided Explainable Recommendati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8S2AI26Y\\Chen et al. - 2021 - Temporal Meta-path Guided Explainable Recommendati.pdf:application/pdf;Chen et al. - 2021 - Temporal Meta-path Guided Explainable Recommendati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TWISSUW7\\Chen et al. - 2021 - Temporal Meta-path Guided Explainable Recommendati.pdf:application/pdf;Temporal Meta-path Guided Explainable Recommendation.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Notes\\Temporal Meta-path Guided Explainable Recommendation.md:text/plain},
}

@misc{zhengPriceawareRecommendationGraph2020a,
	title = {Price-aware {Recommendation} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/2003.03975},
	abstract = {In recent years, much research effort on recommendation has been devoted to mining user behaviors, i.e., collaborative ﬁltering, along with the general information which describes users or items, e.g., textual attributes, categorical demographics, product images, and so on. Price, an important factor in marketing — which determines whether a user will make the ﬁnal purchase decision on an item — surprisingly, has received relatively little scrutiny.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Zheng, Yu and Gao, Chen and He, Xiangnan and Li, Yong and Jin, Depeng},
	month = mar,
	year = {2020},
	note = {arXiv:2003.03975 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {Zheng et al. - 2020 - Price-aware Recommendation with Graph Convolutiona.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\M8TTXV3F\\Zheng et al. - 2020 - Price-aware Recommendation with Graph Convolutiona.pdf:application/pdf},
}

@inproceedings{chengWideDeepLearning2016a,
	address = {Boston MA USA},
	title = {Wide \& {Deep} {Learning} for {Recommender} {Systems}},
	isbn = {978-1-4503-4795-2},
	url = {https://dl.acm.org/doi/10.1145/2988450.2988454},
	doi = {10.1145/2988450.2988454},
	abstract = {Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classiﬁcation problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are eﬀective and interpretable, while generalization requires more feature engineering eﬀort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide \& Deep learning—jointly trained wide linear models and deep neural networks—to combine the beneﬁts of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide \& Deep signiﬁcantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 1st {Workshop} on {Deep} {Learning} for {Recommender} {Systems}},
	publisher = {ACM},
	author = {Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and Anil, Rohan and Haque, Zakaria and Hong, Lichan and Jain, Vihan and Liu, Xiaobing and Shah, Hemal},
	month = sep,
	year = {2016},
	pages = {7--10},
	file = {Cheng et al. - 2016 - Wide & Deep Learning for Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4GFYIWG3\\Cheng et al. - 2016 - Wide & Deep Learning for Recommender Systems.pdf:application/pdf;Cheng et al. - 2016 - Wide & Deep Learning for Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\X6LBKMQL\\Cheng et al. - 2016 - Wide & Deep Learning for Recommender Systems.pdf:application/pdf},
}

@inproceedings{zhongReviewDeepLearningBased2020a,
	address = {Osaka Japan},
	title = {Review of {Deep} {Learning}-{Based} {Personalized} {Learning} {Recommendation}},
	isbn = {978-1-4503-7294-7},
	url = {https://dl.acm.org/doi/10.1145/3377571.3377587},
	doi = {10.1145/3377571.3377587},
	abstract = {Online learning has become a significant way for learners to acquire knowledge, which enables anybody to learn anywhere and anytime. However, how to choose appropriate content for learning is an interesting issue, especially when one faces massive online learning resources. With this regard, deep learning-based personalized learning resources recommendation has become an effective approach to handle this problem. In this paper, deep learning-based personalized learning recommendation technology in the field of education is thoroughly analyzed, and some limitations of recent studies as well as future works, then, are reported and predicted respectively. The study would provide suggestions for future research on deep learning-based personalized learning recommendations.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 2020 11th {International} {Conference} on {E}-{Education}, {E}-{Business}, {E}-{Management}, and {E}-{Learning}},
	publisher = {ACM},
	author = {Zhong, Ling and Wei, Yantao and Yao, Huang and Deng, Wei and Wang, Zhifeng and Tong, Mingwen},
	month = jan,
	year = {2020},
	pages = {145--149},
	file = {Zhong et al. - 2020 - Review of Deep Learning-Based Personalized Learnin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7IHWLB6Z\\Zhong et al. - 2020 - Review of Deep Learning-Based Personalized Learnin.pdf:application/pdf;Zhong et al. - 2020 - Review of Deep Learning-Based Personalized Learnin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6T85MP87\\Zhong et al. - 2020 - Review of Deep Learning-Based Personalized Learnin.pdf:application/pdf},
}

@inproceedings{fuTutorialConversationalRecommendation2020,
	address = {Virtual Event Brazil},
	title = {Tutorial on {Conversational} {Recommendation} {Systems}},
	isbn = {978-1-4503-7583-2},
	url = {https://dl.acm.org/doi/10.1145/3383313.3411548},
	doi = {10.1145/3383313.3411548},
	abstract = {Learning feature interactions is crucial for click-through rate (CTR) prediction in recommender systems. In most existing deep learning models, feature interactions are either manually designed or simply enumerated. However, enumerating all feature interactions brings large memory and computation cost. Even worse, useless interactions may introduce noise and complicate the training process. In this work, we propose a two-stage algorithm called Automatic Feature Interaction Selection (AutoFIS). AutoFIS can automatically identify important feature interactions for factorization models with computational cost just equivalent to training the target model to convergence. In the {\textbackslash}emph\{search stage\}, instead of searching over a discrete set of candidate feature interactions, we relax the choices to be continuous by introducing the architecture parameters. By implementing a regularized optimizer over the architecture parameters, the model can automatically identify and remove the redundant feature interactions during the training process of the model. In the {\textbackslash}emph\{re-train stage\}, we keep the architecture parameters serving as an attention unit to further boost the performance. Offline experiments on three large-scale datasets (two public benchmarks, one private) demonstrate that AutoFIS can significantly improve various FM based models. AutoFIS has been deployed onto the training platform of Huawei App Store recommendation service, where a 10-day online A/B test demonstrated that AutoFIS improved the DeepFM model by 20.3{\textbackslash}\% and 20.1{\textbackslash}\% in terms of CTR and CVR respectively.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Fourteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Fu, Zuohui and Xian, Yikun and Zhang, Yongfeng and Zhang, Yi},
	month = sep,
	year = {2020},
	note = {arXiv: 2003.11235
ISBN: 9781450379984},
	pages = {751--753},
	file = {Fu et al. - 2020 - Tutorial on Conversational Recommendation Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VV7II3I3\\Fu et al. - 2020 - Tutorial on Conversational Recommendation Systems.pdf:application/pdf;Fu et al. - 2020 - Tutorial on Conversational Recommendation Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UIRJV292\\Fu et al. - 2020 - Tutorial on Conversational Recommendation Systems.pdf:application/pdf},
}

@misc{zhangKECRSKnowledgeEnrichedConversational2021,
	title = {{KECRS}: {Towards} {Knowledge}-{Enriched} {Conversational} {Recommendation} {System}},
	shorttitle = {{KECRS}},
	url = {http://arxiv.org/abs/2105.08261},
	abstract = {The chit-chat-based conversational recommendation systems (CRS) provide item recommendations to users through natural language interactions. To better understand user’s intentions, external knowledge graphs (KG) have been introduced into chit-chat-based CRS. However, existing chit-chat-based CRS usually generate repetitive item recommendations, and they cannot properly infuse knowledge from KG into CRS to generate informative responses. To remedy these issues, we first reformulate the conversational recommendation task to highlight that the recommended items should be new and possibly interested by users. Then, we propose the KnowledgeEnriched Conversational Recommendation System (KECRS). Specifically, we develop the Bag-of-Entity (BOE) loss and the infusion loss to better integrate KG with CRS for generating more diverse and informative responses. BOE loss provides an additional supervision signal to guide CRS to learn from both human-written utterances and KG. Infusion loss bridges the gap between the word embeddings and entity embeddings by minimizing distances of the same words in these two embeddings. Moreover, we facilitate our study by constructing a high-quality KG, i.e., The Movie Domain Knowledge Graph (TMDKG). Experimental results on a large-scale dataset demonstrate that KECRS outperforms state-of-the-art chitchat-based CRS, in terms of both recommendation accuracy and response generation quality.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Zhang, Tong and Liu, Yong and Zhong, Peixiang and Zhang, Chen and Wang, Hao and Miao, Chunyan},
	month = may,
	year = {2021},
	note = {arXiv:2105.08261 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Zhang et al. - 2021 - KECRS Towards Knowledge-Enriched Conversational R.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\C5I95F9P\\Zhang et al. - 2021 - KECRS Towards Knowledge-Enriched Conversational R.pdf:application/pdf},
}

@misc{wuContextAwareUserItemRepresentation2017,
	title = {A {Context}-{Aware} {User}-{Item} {Representation} {Learning} for {Item} {Recommendation}},
	url = {http://arxiv.org/abs/1712.02342},
	abstract = {Both reviews and user-item interactions (i.e., rating scores) have been widely adopted for user rating prediction. However, these existing techniques mainly extract the latent representations for users and items in an independent and static manner. That is, a single static feature vector is derived to encode her preference without considering the particular characteristics of each candidate item. We argue that this static encoding scheme is difﬁcult to fully capture the users’ preference. In this paper, we propose a novel context-aware user-item representation learning model for rating prediction, named CARL. Namely, CARL derives a joint representation for a given user-item pair based on their individual latent features and latent feature interactions. Then, CARL adopts Factorization Machines to further model higher-order feature interactions on the basis of the user-item pair for rating prediction. Speciﬁcally, two separate learning components are devised in CARL to exploit review data and interaction data respectively: review-based feature learning and interaction-based feature learning. In review-based learning component, with convolution operations and attention mechanism, the relevant features for a user-item pair are extracted by jointly considering their corresponding reviews. However, these features are only reivew-driven and may not be comprehensive. Hence, interaction-based learning component further extracts complementary features from interaction data alone, also on the basis of user-item pairs. The ﬁnal rating score is then derived with a dynamic linear fusion mechanism. Experiments on ﬁve real-world datasets show that CARL achieves signiﬁcantly better rating predication accuracy than existing state-of-the-art alternatives. Also, with attention mechanism, we show that the relevant information in reviews can be highlighted to interpret the rating prediction.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Wu, Libing and Quan, Cong and Li, Chenliang and Wang, Qian and Zheng, Bolong},
	month = dec,
	year = {2017},
	note = {arXiv:1712.02342 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {Wu et al. - 2017 - A Context-Aware User-Item Representation Learning .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\94FBXG2M\\Wu et al. - 2017 - A Context-Aware User-Item Representation Learning .pdf:application/pdf},
}

@inproceedings{liuNRPANeuralRecommendation2019a,
	title = {{NRPA}: {Neural} {Recommendation} with {Personalized} {Attention}},
	shorttitle = {{NRPA}},
	url = {http://arxiv.org/abs/1905.12480},
	doi = {10.1145/3331184.3331371},
	abstract = {Existing review-based recommendation methods usually use the same model to learn the representations of all users/items from reviews posted by users towards items. However, different users have different preference and different items have different characteristics. Thus, the same word or the similar reviews may have different informativeness for different users and items. In this paper we propose a neural recommendation approach with personalized attention to learn personalized representations of users and items from reviews. We use a review encoder to learn representations of reviews from words, and a user/item encoder to learn representations of users or items from reviews. We propose a personalized attention model, and apply it to both review and user/item encoders to select different important words and reviews for different users/items. Experiments on five datasets validate our approach can effectively improve the performance of neural recommendation.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 42nd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Liu, Hongtao and Wu, Fangzhao and Wang, Wenjun and Wang, Xianchen and Jiao, Pengfei and Wu, Chuhan and Xie, Xing},
	month = jul,
	year = {2019},
	note = {arXiv:1905.12480 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	pages = {1233--1236},
	file = {Liu et al. - 2019 - NRPA Neural Recommendation with Personalized Atte.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L87RQFEI\\Liu et al. - 2019 - NRPA Neural Recommendation with Personalized Atte.pdf:application/pdf},
}

@article{aamirRecommendationSystemState2015a,
	title = {Recommendation {System}: {State} of the {Art} {Approach}},
	volume = {120},
	issn = {09758887},
	shorttitle = {Recommendation {System}},
	url = {http://research.ijcaonline.org/volume120/number12/pxc3904200.pdf},
	doi = {10.5120/21281-4200},
	abstract = {A Recommender System (RS) is a composition of software tools and machine learning techniques that provides valuable piece of advice for items or services chosen by a user. Recommender systems are currently useful in both the research and in the commercial areas. Numerous approaches have been proposed for providing recommendations. Certainly, recommendation systems have an assortment of properties that may entail experiences of user such as user preference, prediction accuracy, confidence, trust, etc. In this paper we present a categorical reassess of the field of recommender systems and Approaches for Evaluation of Recommendation System to propose the recommendation method that would further help to enhance opinion mining through recommendations.},
	language = {en},
	number = {12},
	urldate = {2022-11-05},
	journal = {International Journal of Computer Applications},
	author = {Aamir, Mohammad and Bhusry, Mamta},
	month = jun,
	year = {2015},
	pages = {25--32},
	file = {Aamir and Bhusry - 2015 - Recommendation System State of the Art Approach.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XCLCIAEP\\Aamir and Bhusry - 2015 - Recommendation System State of the Art Approach.pdf:application/pdf;Aamir and Bhusry - 2015 - Recommendation System State of the Art Approach.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZWYFJ6H4\\Aamir and Bhusry - 2015 - Recommendation System State of the Art Approach.pdf:application/pdf},
}

@inproceedings{zouQuestionbasedRecommenderSystems2020,
	address = {Virtual Event China},
	title = {Towards {Question}-based {Recommender} {Systems}},
	isbn = {978-1-4503-8016-4},
	url = {https://dl.acm.org/doi/10.1145/3397271.3401180},
	doi = {10.1145/3397271.3401180},
	abstract = {Conversational and question-based recommender systems have gained increasing attention in recent years, with users enabled to converse with the system and better control recommendations. Nevertheless, research in the field is still limited, compared to traditional recommender systems. In this work, we propose a novel Question-based recommendation method, Qrec, to assist users to find items interactively, by answering automatically constructed and algorithmically chosen questions. Previous conversational recommender systems ask users to express their preferences over items or item facets. Our model, instead, asks users to express their preferences over descriptive item features. The model is first trained offline by a novel matrix factorization algorithm, and then iteratively updates the user and item latent factors online by a closed-form solution based on the user answers. Meanwhile, our model infers the underlying user belief and preferences over items to learn an optimal question-asking strategy by using Generalized Binary Search, so as to ask a sequence of questions to the user. Our experimental results demonstrate that our proposed matrix factorization model outperforms the traditional Probabilistic Matrix Factorization model. Further, our proposed Qrec model can greatly improve the performance of state-of-the-art baselines, and it is also effective in the case of cold-start user and item recommendations.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Zou, Jie and Chen, Yifan and Kanoulas, Evangelos},
	month = jul,
	year = {2020},
	pages = {881--890},
	file = {Zou et al. - 2020 - Towards Question-based Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5GWI3HP3\\Zou et al. - 2020 - Towards Question-based Recommender Systems.pdf:application/pdf;Zou et al. - 2020 - Towards Question-based Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HQBLNFCN\\Zou et al. - 2020 - Towards Question-based Recommender Systems.pdf:application/pdf},
}

@inproceedings{zouReinforcementLearningOptimize2019a,
	address = {Anchorage AK USA},
	title = {Reinforcement {Learning} to {Optimize} {Long}-term {User} {Engagement} in {Recommender} {Systems}},
	isbn = {978-1-4503-6201-6},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330668},
	doi = {10.1145/3292500.3330668},
	abstract = {Recommender systems play a crucial role in our daily lives. Feed streaming mechanism has been widely used in the recommender system, especially on the mobile Apps. The feed streaming setting provides users the interactive manner of recommendation in never-ending feeds. In such a manner, a good recommender system should pay more attention to user stickiness, which is far beyond classical instant metrics and typically measured by long-term user engagement. Directly optimizing long-term user engagement is a non-trivial problem, as the learning target is usually not available for conventional supervised learning methods. Though reinforcement learning (RL) naturally fits the problem of maximizing the long term rewards, applying RL to optimize long-term user engagement is still facing challenges: user behaviors are versatile to model, which typically consists of both instant feedback (e.g., clicks) and delayed feedback (e.g., dwell time, revisit); in addition, performing effective off-policy learning is still immature, especially when combining bootstrapping and function approximation. To address these issues, in this work, we introduce a RL framework — FeedRec to optimize the long-term user engagement. FeedRec includes two components: 1) a Q-Network which designed in hierarchical LSTM takes charge of modeling complex user behaviors, and 2) a S-Network, which simulates the environment, assists the Q-Network and voids the instability of convergence in policy learning. Extensive experiments on synthetic data and a real-world large scale data show that FeedRec effectively optimizes the long-term user engagement and outperforms state-of-the-arts.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Zou, Lixin and Xia, Long and Ding, Zhuoye and Song, Jiaxing and Liu, Weidong and Yin, Dawei},
	month = jul,
	year = {2019},
	pages = {2810--2818},
	file = {Zou et al. - 2019 - Reinforcement Learning to Optimize Long-term User .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\86SAK89B\\Zou et al. - 2019 - Reinforcement Learning to Optimize Long-term User .pdf:application/pdf;Zou et al. - 2019 - Reinforcement Learning to Optimize Long-term User .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PI9RI9ST\\Zou et al. - 2019 - Reinforcement Learning to Optimize Long-term User .pdf:application/pdf},
}

@misc{liuAutoFISAutomaticFeature2020,
	title = {{AutoFIS}: {Automatic} {Feature} {Interaction} {Selection} in {Factorization} {Models} for {Click}-{Through} {Rate} {Prediction}},
	shorttitle = {{AutoFIS}},
	url = {http://arxiv.org/abs/2003.11235},
	abstract = {Learning feature interactions is crucial for click-through rate (CTR) prediction in recommender systems. In most existing deep learning models, feature interactions are either manually designed or simply enumerated. However, enumerating all feature interactions brings large memory and computation cost. Even worse, useless interactions may introduce noise and complicate the training process. In this work, we propose a two-stage algorithm called Automatic Feature Interaction Selection (AutoFIS). AutoFIS can automatically identify important feature interactions for factorization models with computational cost just equivalent to training the target model to convergence. In the {\textbackslash}emph\{search stage\}, instead of searching over a discrete set of candidate feature interactions, we relax the choices to be continuous by introducing the architecture parameters. By implementing a regularized optimizer over the architecture parameters, the model can automatically identify and remove the redundant feature interactions during the training process of the model. In the {\textbackslash}emph\{re-train stage\}, we keep the architecture parameters serving as an attention unit to further boost the performance. Offline experiments on three large-scale datasets (two public benchmarks, one private) demonstrate that AutoFIS can significantly improve various FM based models. AutoFIS has been deployed onto the training platform of Huawei App Store recommendation service, where a 10-day online A/B test demonstrated that AutoFIS improved the DeepFM model by 20.3{\textbackslash}\% and 20.1{\textbackslash}\% in terms of CTR and CVR respectively.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Liu, Bin and Zhu, Chenxu and Li, Guilin and Zhang, Weinan and Lai, Jincai and Tang, Ruiming and He, Xiuqiang and Li, Zhenguo and Yu, Yong},
	month = jul,
	year = {2020},
	note = {arXiv: 2003.11235
ISBN: 9781450379984},
	keywords = {feature selection, factorization machine, neural, recommendation, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval, no},
	file = {Liu et al. - 2020 - AutoFIS Automatic Feature Interaction Selection i.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\K6UR9ZXW\\Liu et al. - 2020 - AutoFIS Automatic Feature Interaction Selection i.pdf:application/pdf},
}

@inproceedings{lianXDeepFMCombiningExplicit2018,
	title = {{xDeepFM}: {Combining} {Explicit} and {Implicit} {Feature} {Interactions} for {Recommender} {Systems}},
	shorttitle = {{xDeepFM}},
	url = {http://arxiv.org/abs/1803.05170},
	doi = {10.1145/3219819.3220023},
	abstract = {Combinatorial features are essential for the success of many commercial models. Manually crafting these features usually comes with high cost due to the variety, volume and velocity of raw data in web-scale systems. Factorization based models, which measure interactions in terms of vector product, can learn patterns of combinatorial features automatically and generalize to unseen features as well. With the great success of deep neural networks (DNNs) in various fields, recently researchers have proposed several DNN-based factorization model to learn both low- and high-order feature interactions. Despite the powerful ability of learning an arbitrary function from data, plain DNNs generate feature interactions implicitly and at the bit-wise level. In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level. We show that the CIN share some functionalities with convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We further combine a CIN and a classical DNN into one unified model, and named this new model eXtreme Deep Factorization Machine (xDeepFM). On one hand, the xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it can learn arbitrary low- and high-order feature interactions implicitly. We conduct comprehensive experiments on three real-world datasets. Our results demonstrate that xDeepFM outperforms state-of-the-art models. We have released the source code of xDeepFM at {\textbackslash}url\{https://github.com/Leavingseason/xDeepFM\}.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Lian, Jianxun and Zhou, Xiaohuan and Zhang, Fuzheng and Chen, Zhongxia and Xie, Xing and Sun, Guangzhong},
	month = jul,
	year = {2018},
	note = {arXiv:1803.05170 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	pages = {1754--1763},
	file = {Lian et al. - 2018 - xDeepFM Combining Explicit and Implicit Feature I.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\84HZ4FFB\\Lian et al. - 2018 - xDeepFM Combining Explicit and Implicit Feature I.pdf:application/pdf},
}

@inproceedings{pacukRecSysChallenge20162016,
	title = {{RecSys} {Challenge} 2016: job recommendations based on preselection of offers and gradient boosting},
	shorttitle = {{RecSys} {Challenge} 2016},
	url = {http://arxiv.org/abs/1612.00959},
	doi = {10.1145/2987538.2987544},
	abstract = {We present the Mim-Solution's approach to the RecSys Challenge 2016, which ranked 2nd. The goal of the competition was to prepare job recommendations for the users of the website Xing.com. Our two phase algorithm consists of candidate selection followed by the candidate ranking. We ranked the candidates by the predicted probability that the user will positively interact with the job offer. We have used Gradient Boosting Decision Trees as the regression tool.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {Recommender} {Systems} {Challenge} on - {RecSys} {Challenge} '16},
	author = {Pacuk, Andrzej and Sankowski, Piotr and Węgrzycki, Karol and Witkowski, Adam and Wygocki, Piotr},
	year = {2016},
	note = {ISBN: 9781450348010
arXiv: 1612.00959},
	keywords = {Recommender Systems, RecSys Challenge 2016, Recommender systems, Gradient Boosting, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, I.2.6, D.2.8, H.3.3},
	pages = {1--4},
	file = {Pacuk et al. - 2016 - RecSys Challenge 2016 job recommendations based o.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JHFF28Z7\\Pacuk et al. - 2016 - RecSys Challenge 2016 job recommendations based o.pdf:application/pdf},
}

@inproceedings{tramontinRecommenderSystemsSocial2018a,
	address = {Caxias do Sul, Brazil},
	title = {Recommender {Systems} with {Social} {Elements}: {A} {Systematic} {Mapping}},
	isbn = {978-1-4503-6559-8},
	shorttitle = {Recommender {Systems} with {Social} {Elements}},
	url = {http://dl.acm.org/citation.cfm?doid=3229345.3229350},
	doi = {10.1145/3229345.3229350},
	abstract = {Recommendation Systems (RS) deal of the overload of information online, allowing the user to find desirable items quickly, without being surprised by irrelevant information. Individual preference and interpersonal influence are important contextual factors for social recommendations, as they affect users' decisions about information retention. The goal of this paper is to identify the state of the art in RS with the use of social elements. For this, a systematic mapping of the literature was conducted, revealing a growing trend in the number of articles published in the last ten years, especially in China, and with more frequent proposals for new models, systems and frameworks for recommendation. Almost half of the mapped articles present as a domain Entertainment or Product Review/Evaluation, with the collaborative filtering approach being the most common of the approaches used, and the similarity of friends as the most common of the social components considered. As an evaluation strategy, more than half of the mapped articles use offline experiments in a previously populated database to simulate user actions. The mapping showed that although RSs are considering social elements, there is still a lack of works that explore these elements in real contexts of use.},
	language = {pt},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {XIV} {Brazilian} {Symposium} on {Information} {Systems} - {SBSI}'18},
	publisher = {ACM Press},
	author = {Tramontin, Aline de Paula A. and Gasparini, Isabela and Pereira, Roberto},
	year = {2018},
	pages = {1--8},
	file = {Tramontin et al. - 2018 - Recommender Systems with Social Elements A System.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\27XVFFGH\\Tramontin et al. - 2018 - Recommender Systems with Social Elements A System.pdf:application/pdf;Tramontin et al. - 2018 - Recommender Systems with Social Elements A System.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DTWNZAJP\\Tramontin et al. - 2018 - Recommender Systems with Social Elements A System.pdf:application/pdf},
}

@inproceedings{NoTitleFound2012,
	address = {Istanbul, Turkey},
	title = {[{No} title found]},
	isbn = {978-1-4673-2497-7},
	abstract = {This paper presents a content-based recommender system which proposes jobs to Facebook and LinkedIn users. A variant of this recommender system is currently used by Work4, a San Francisco-based software company that offers Facebook recruitment solutions.},
	language = {en},
	booktitle = {2012 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}},
	publisher = {IEEE},
	year = {2012},
	file = {2012 - [No title found].pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EYWG9VZG\\2012 - [No title found].pdf:application/pdf},
}

@article{bansalTopicModelingDriven2017a,
	title = {Topic {Modeling} {Driven} {Content} {Based} {Jobs} {Recommendation} {Engine} for {Recruitment} {Industry}},
	volume = {122},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050917326960},
	doi = {10.1016/j.procs.2017.11.448},
	abstract = {A number of postings for different job roles and job positions are posted at numerous sources in the recruitment industry. Therefore, this is a challenging and time-consuming task to collate the information and find out most relevant user-job connection mapping according to the skills and preferences of a user. This research work has been done to cover up this same problem and efforts have been made to provide a feasible and efficient solution for the same. We suggest a content-based recommendation engine, which automatically provides best suggestions to users by matching their interests and skills with the features of a job posting. In order to produce an intended recommendation, the proposed engine applies various text filters and feature similarity measurements. Similarity techniques use the bag of n-grams and topic models as the elements of feature vectors. The validations and testing of the model on real data obtained from a top job posting website show the applicability and efficiency of using topic models as features. The approach is generic and can be replicated to different industries.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Procedia Computer Science},
	author = {Bansal, Shivam and Srivastava, Aman and Arora, Anuja},
	year = {2017},
	pages = {865--872},
	file = {Bansal et al. - 2017 - Topic Modeling Driven Content Based Jobs Recommend.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DB6WM7RG\\Bansal et al. - 2017 - Topic Modeling Driven Content Based Jobs Recommend.pdf:application/pdf;Bansal et al. - 2017 - Topic Modeling Driven Content Based Jobs Recommend.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6MWATTRN\\Bansal et al. - 2017 - Topic Modeling Driven Content Based Jobs Recommend.pdf:application/pdf},
}

@inproceedings{caiSPMCSociallyAwarePersonalized2017a,
	address = {Melbourne, Australia},
	title = {{SPMC}: {Socially}-{Aware} {Personalized} {Markov} {Chains} for {Sparse} {Sequential} {Recommendation}},
	isbn = {978-0-9992411-0-3},
	shorttitle = {{SPMC}},
	url = {https://www.ijcai.org/proceedings/2017/204},
	doi = {10.24963/ijcai.2017/204},
	abstract = {Dealing with sparse, long-tailed datasets, and coldstart problems is always a challenge for recommender systems. These issues can partly be dealt with by making predictions not in isolation, but by leveraging information from related events; such information could include signals from social relationships or from the sequence of recent activities. Both types of additional information can be used to improve the performance of state-of-the-art matrix factorization-based techniques. In this paper, we propose new methods to combine both social and sequential information simultaneously, in order to further improve recommendation performance. We show these techniques to be particularly effective when dealing with sparsity and cold-start issues in several large, real-world datasets.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Cai, Chenwei and He, Ruining and McAuley, Julian},
	month = aug,
	year = {2017},
	pages = {1476--1482},
	file = {Cai et al. - 2017 - SPMC Socially-Aware Personalized Markov Chains fo.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NLB6QJJ5\\Cai et al. - 2017 - SPMC Socially-Aware Personalized Markov Chains fo.pdf:application/pdf;Cai et al. - 2017 - SPMC Socially-Aware Personalized Markov Chains fo.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\EFUU4ST8\\Cai et al. - 2017 - SPMC Socially-Aware Personalized Markov Chains fo.pdf:application/pdf},
}

@article{roffoPersonalityAwareRecommendationa,
	title = {Towards {Personality}-{Aware} {Recommendation}},
	abstract = {In the last decade new ways of shopping online have increased the possibility of buying products and services more easily and faster than ever. In this new context, personality is a key determinant in the decision making of the consumer when shopping. The two main reasons are: ﬁrstly, a person’s buying choices are inﬂuenced by psychological factors like impulsiveness, and secondly, some consumers may be more susceptible to making impulse purchases than others. To the best of our knowledge, the impact of personality factors on advertisements has been largely neglected at the level of recommender systems. This work proposes a highly innovative research which uses a personality perspective to determine the unique associations among the consumer’s buying tendency and advert recommendations. As a matter of fact, the lack of a publicly available benchmark for computational advertising do not allow both the exploration of this intriguing research direction and the evaluation of state-of-the-art algorithms. We present the ADS Dataset, a publicly available benchmark for computational advertising enriched with Big-Five users’ personality factors and 1,200 personal users’ pictures. The proposed benchmark allows two main tasks: rating prediction over 300 real advertisements (i.e., Rich Media Ads, Image Ads, Text Ads) and click-through rate prediction. Moreover, this work carries out experiments, reviews various evaluation criteria used in the literature, and provides a library for each one of them within one integrated toolbox.},
	language = {en},
	author = {Roffo, Giorgio},
	pages = {4},
	file = {Roffo - Towards Personality-Aware Recommendation.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4URQQ74C\\Roffo - Towards Personality-Aware Recommendation.pdf:application/pdf;Roffo - Towards Personality-Aware Recommendation.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P2VGDSZX\\Roffo - Towards Personality-Aware Recommendation.pdf:application/pdf},
}

@inproceedings{zhangPersonalizedRecommendationUsing2019a,
	address = {Sanya China},
	title = {Personalized {Recommendation} using {Similarity} {Powered} {Pairwise} {Amplifier} {Network}},
	isbn = {978-1-4503-7261-9},
	url = {http://dl.acm.org/doi/10.1145/3377713.3377728},
	doi = {10.1145/3377713.3377728},
	abstract = {Online advertising, one typical application of recommendation system, calls for effective and accurate recommendations of keywords. Extreme sparse and large scale data makes online advertising a challenging problem. To achieve better performance and accuracy of the recommendation, a better model with a short turnaround time is needed. In this paper, we address the problem of personalized online advertising for extreme sparse and large scale data. We develop a novel machine learning model (Similarity Powered Pairwise Amplifier Network, SPPAN for short). The complexity of this model (a.k.a. the number of parameters) grows with the amount of observed data, which makes it suitable to extremely sparse data. The training algorithm based on gradient descent makes it easy to parallelize. The similarity model combines the user neighborhood and item neighborhood ideas in collaborative filtering smartly, obtaining a cost-effective way to handle large scale data. The proposed framework is evaluated on a large set of real-world data set from a large internet company (expressed by “Company A”). The experiment results demonstrate that the proposed SPPAN model can greatly improve the prediction and recommendation accuracy on that extreme sparse data set compared with existing approaches.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 2019 2nd {International} {Conference} on {Algorithms}, {Computing} and {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Zhang, Tongda and Qian, Jun and Sun, Xiao and Guo, Daqiang},
	month = dec,
	year = {2019},
	pages = {138--146},
	file = {Zhang et al. - 2019 - Personalized Recommendation using Similarity Power.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\RAMVFMGL\\Zhang et al. - 2019 - Personalized Recommendation using Similarity Power.pdf:application/pdf;Zhang et al. - 2019 - Personalized Recommendation using Similarity Power.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PFZXLNY8\\Zhang et al. - 2019 - Personalized Recommendation using Similarity Power.pdf:application/pdf},
}

@inproceedings{jainTrendsProblemsSolutions2015a,
	address = {Greater Noida, India},
	title = {Trends, problems and solutions of recommender system},
	isbn = {978-1-4799-8890-7},
	url = {http://ieeexplore.ieee.org/document/7148534/},
	doi = {10.1109/CCAA.2015.7148534},
	abstract = {In this era of web, we have a huge amount of information overloaded over Internet. It becomes a herculean task for the user to get the relevant information. To some extent, the problem is being solved by the search engines, but they do not provide the personalization of data. So, to further filter the information, we need a recommendation engine. In this paper, we have described the various web recommender systems in use by some popular web sites on the internet like Amazon.com, LinkedIn.com, and YouTube.com etc. Further, we have described the various approaches used in the various recommender systems such as Content based, Collaborative and Hybrid recommender system. At the end of this paper, we focus on some of the main challenges faced by the web recommender systems and analyze some techniques to overcome them.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {International {Conference} on {Computing}, {Communication} \& {Automation}},
	publisher = {IEEE},
	author = {Jain, Sarika and Grover, Anjali and Thakur, Praveen Singh and Choudhary, Sourabh Kumar},
	month = may,
	year = {2015},
	pages = {955--958},
	file = {Jain et al. - 2015 - Trends, problems and solutions of recommender syst.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZRV85YH2\\Jain et al. - 2015 - Trends, problems and solutions of recommender syst.pdf:application/pdf;Jain et al. - 2015 - Trends, problems and solutions of recommender syst.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GU96HVX6\\Jain et al. - 2015 - Trends, problems and solutions of recommender syst.pdf:application/pdf},
}

@article{burkeRecommenderSystemsOverview2011,
	title = {Recommender {Systems}: {An} {Overview}},
	volume = {32},
	doi = {10.1609/aimag.v32i3.2361},
	abstract = {We present the Mim-Solution's approach to the RecSys Challenge 2016, which ranked 2nd. The goal of the competition was to prepare job recommendations for the users of the website Xing.com. Our two phase algorithm consists of candidate selection followed by the candidate ranking. We ranked the candidates by the predicted probability that the user will positively interact with the job offer. We have used Gradient Boosting Decision Trees as the regression tool.},
	language = {en},
	journal = {Ai Magazine},
	author = {Burke, Robin and Felfernig, Alexander and Göker, Mehmet H},
	year = {2011},
	note = {arXiv: 1612.00959
ISBN: 9781450348010},
	pages = {13--18},
	file = {Burke et al. - Recommender Systems An Overview.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\2FG6FDHV\\Burke et al. - Recommender Systems An Overview.pdf:application/pdf;Burke et al. - Recommender Systems An Overview.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XLP36K9V\\Burke et al. - Recommender Systems An Overview.pdf:application/pdf},
}

@incollection{candillierStateoftheArtRecommenderSystems2009a,
	title = {State-of-the-{Art} {Recommender} {Systems}:},
	isbn = {978-1-60566-306-7 978-1-60566-307-4},
	shorttitle = {State-of-the-{Art} {Recommender} {Systems}},
	url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60566-306-7.ch001},
	abstract = {The aim of Recommender Systems is to help users to find items that they should appreciate from huge catalogues. In that field, collaborative filtering approaches can be distinguished from content-based ones. The former is based on a set of user ratings on items while the latter uses item content descriptions and user thematic profiles.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Collaborative and {Social} {Information} {Retrieval} and {Access}},
	publisher = {IGI Global},
	author = {Candillier, Laurent and Jack, Kris and Fessant, Françoise and Meyer, Frank},
	editor = {Chevalier, Max and Julien, Christine and Soule-Dupuy, Chantal},
	year = {2009},
	doi = {10.4018/978-1-60566-306-7.ch001},
	pages = {1--22},
	file = {Candillier et al. - 2009 - State-of-the-Art Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZYPAIXVE\\Candillier et al. - 2009 - State-of-the-Art Recommender Systems.pdf:application/pdf},
}

@article{zhangRecommendationSystemSocial2019c,
	title = {Recommendation system in social networks with topical attention and probabilistic matrix factorization},
	volume = {14},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0223967},
	doi = {10.1371/journal.pone.0223967},
	abstract = {Collaborative filtering (CF) is a common recommendation mechanism that relies on useritem ratings. However, the intrinsic sparsity of user-item rating data can be problematic in many domains and settings, limiting the ability to generate accurate predictions and effective recommendations. At present, most algorithms use two-valued trust relationship of social network to improve recommendation quality but fail to take into account the difference of trust intensity of each friend and user’s comment information. To this end, the recommendation system within a social network adopts topical attention and probabilistic matrix factorization (STAPMF) is proposed. We combine the trust information in social networks and the topical information from review documents by proposing a novel algorithm combining probabilistic matrix factorization and attention-based recurrent neural networks to extract item underlying feature vectors, user’s personal potential feature vectors, and user’s social hidden feature vectors, which represent the features extracted from the user’s trusted network. Using real-world datasets, we show a significant improvement in recommendation performance comparing with the prevailing state-of-the-art algorithms for social network-based recommendation.},
	language = {en},
	number = {10},
	urldate = {2022-11-05},
	journal = {PLOS ONE},
	author = {Zhang, Weiwei and Liu, Fangai and Xu, Daomeng and Jiang, Lu},
	editor = {Wang, Hua},
	month = oct,
	year = {2019},
	pages = {e0223967},
	file = {Zhang et al. - 2019 - Recommendation system in social networks with topi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3KBDY5KE\\Zhang et al. - 2019 - Recommendation system in social networks with topi.pdf:application/pdf;Zhang et al. - 2019 - Recommendation system in social networks with topi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5XRFSDEY\\Zhang et al. - 2019 - Recommendation system in social networks with topi.pdf:application/pdf;Zhang et al. - 2019 - Recommendation system in social networks with topi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QGISSXDB\\Zhang et al. - 2019 - Recommendation system in social networks with topi.pdf:application/pdf;Zhang et al. - 2019 - Recommendation system in social networks with topi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5SRKAYB3\\Zhang et al. - 2019 - Recommendation system in social networks with topi.pdf:application/pdf;Zhang et al. - 2019 - Recommendation system in social networks with topi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P7ZUD5BN\\Zhang et al. - 2019 - Recommendation system in social networks with topi.pdf:application/pdf;Zhang et al. - 2019 - Recommendation system in social networks with topi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SJIN3QIX\\Zhang et al. - 2019 - Recommendation system in social networks with topi.pdf:application/pdf},
}

@article{hwangboRecommendationSystemDevelopment2018,
	title = {Recommendation system development for fashion retail e-commerce},
	volume = {28},
	issn = {15674223},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1567422318300152},
	doi = {10.1016/j.elerap.2018.01.012},
	abstract = {This study presents a real-world collaborative ﬁltering recommendation system implemented in a large Korean fashion company that sells fashion products through both online and ofﬂine shopping malls. The company’s recommendation environment displays the following unique characteristics: First, the company’s online and ofﬂine stores sell the same products. Second, fashion products are usually seasonal, so customers’ general preference changes according to the time of year. Last, customers usually purchase items to replace previously preferred items or purchase items to complement those already bought. We propose a new system called K-RecSys, which extends the typical item-based collaborative ﬁltering algorithm by reﬂecting the above domain characteristics. K-RecSys combines online product click data and ofﬂine product sale data weighted to reﬂect the online and ofﬂine preferences of customers. It also adopts a preference decay function to reﬂect changes in preferences over time, and ﬁnally recommends substitute and complementary products using product category information. We conducted an A/B test in the actual operating environment to compare K-RecSys with the existing collaborative ﬁltering system implemented with only online data. Our experimental results show that the proposed system is superior in terms of product clicks and sales in the online shopping mall and its substitute recommendations are adopted more frequently than complementary recommendations.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Electronic Commerce Research and Applications},
	author = {Hwangbo, Hyunwoo and Kim, Yang Sok and Cha, Kyung Jin},
	month = mar,
	year = {2018},
	pages = {94--101},
	file = {Hwangbo et al. - 2018 - Recommendation system development for fashion reta.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DTQTCITG\\Hwangbo et al. - 2018 - Recommendation system development for fashion reta.pdf:application/pdf},
}

@article{laenenComparativeStudyOutfit2020,
	title = {A {Comparative} {Study} of {Outfit} {Recommendation} {Methods} with a {Focus} on {Attention}-based {Fusion}},
	volume = {57},
	issn = {03064573},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457320308116},
	doi = {10.1016/j.ipm.2020.102316},
	abstract = {In recent years, deep learning-based recommender systems have received increasing attention, as deep neural networks can detect important product features in images and text descriptions and capture them in semantic vector representations of items. This is especially relevant for outfit recommendation, since a variety of fashion product features play a role in creating outfits. This work is a comparative study of fusion methods for outfit recommendation that combine relevant product features extracted from visual and textual data in semantic, multimodal item representations. We compare traditional fusion methods with attention-based fusion methods, which are designed to focus on the fine-grained product features of items. We evaluate the fusion methods on four benchmark datasets for outfit recommendation and provide insights into the importance of the multimodality and granularity of the fashion item representations. We find that the visual and textual item data not only share product features but also contain complementary product features for the outfit recommendation task, confirming the need to effectively combine them into multimodal item representations. Furthermore, we show that the average performance of attention-based fusion methods surpasses the average performance of traditional fusion methods on three out of the four benchmark datasets, demonstrating the ability of attention to learn relevant correlations among fine-grained fashion attributes.},
	language = {en},
	number = {6},
	urldate = {2022-11-05},
	journal = {Information Processing \& Management},
	author = {Laenen, Katrien and Moens, Marie-Francine},
	month = nov,
	year = {2020},
	pages = {102316},
	file = {Laenen and Moens - 2020 - A Comparative Study of Outfit Recommendation Metho.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y4AACGAY\\Laenen and Moens - 2020 - A Comparative Study of Outfit Recommendation Metho.pdf:application/pdf},
}

@article{alirezazadehDeepLearningLoss2022,
	title = {A deep learning loss based on additive cosine margin: {Application} to fashion style and face recognition},
	issn = {15684946},
	shorttitle = {A deep learning loss based on additive cosine margin},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494622008250},
	doi = {10.1016/j.asoc.2022.109776},
	abstract = {Recently, loss functions based on angular spans improved the performance of deep 10 visual recognition. These losses converted Euclidean cross entropy to angular cross entropy loss. Fashion style recognition deals with the problem of assigning a person’s outﬁt to a fashion style category. Due to the high similarity between different clothing items and the use of softmax-based loss functions, many of the current methods that address this problem show relatively poor performance and cannot guarantee sufﬁcient 15 inter-class margins in the fashion domain. In this work, we propose an end-to-end method for deep visual recognition by combining a standard CNN architecture with a novel loss function, which we call Additive Cosine Margin Loss (ACML). The proposed function not only projects feature vectors of different classes into different regions of the embedding, but also enforces compactness of the projections within each 20 class. Our experiments were conducted on two public and well-known fashion style recognition datasets FashionStyle14 and HipsterWars, and on the face veriﬁcation and identiﬁcation datasets LFW, YTF, and MegaFace. These experiments demonstrate the superiority of the proposed loss function over: 1) existing angular margin-based loss functions 2) state-of-the-art methods for clothing style recognition as well as face anal25 ysis tasks.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Applied Soft Computing},
	author = {Alirezazadeh, P. and Dornaika, F. and Moujahid, A.},
	month = nov,
	year = {2022},
	pages = {109776},
	file = {Alirezazadeh et al. - 2022 - A deep learning loss based on additive cosine marg.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VQIP2HMH\\Alirezazadeh et al. - 2022 - A deep learning loss based on additive cosine marg.pdf:application/pdf},
}

@incollection{ayVisualSimilaritybasedFashion2021,
	title = {Visual similarity-based fashion recommendation system},
	isbn = {978-0-12-823519-5},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128235195000233},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Generative {Adversarial} {Networks} for {Image}-to-{Image} {Translation}},
	publisher = {Elsevier},
	author = {Ay, Betul and Aydin, Galip},
	year = {2021},
	doi = {10.1016/B978-0-12-823519-5.00023-3},
	pages = {185--203},
	file = {Ay and Aydin - 2021 - Visual similarity-based fashion recommendation sys.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QJJII7YU\\Ay and Aydin - 2021 - Visual similarity-based fashion recommendation sys.pdf:application/pdf},
}

@article{zhouFashionRecommendationsCrossmedia2019,
	title = {Fashion recommendations through cross-media information retrieval},
	volume = {61},
	issn = {10473203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047320319300963},
	doi = {10.1016/j.jvcir.2019.03.003},
	abstract = {Fashion recommendation has attracted much attention given its ready applications to e-commerce. Traditional methods usually recommend clothing products to users on the basis of their textual descriptions. Product images, although covering a large resource of information, are often ignored in the recommendation processes. In this study, we propose a novel fashion product recommendation method based on both text and image mining techniques. Our model facilitates two kinds of fashion recommendation, namely, similar product and mix-and-match, by leveraging text-based product attributes and image features. To suggest similar products, we construct a new similarity measure to compare the image colour and texture descriptors. For mix-and-match recommendation, we ﬁrstly adopt convolutional neural network (CNN) to classify ﬁne-grained clothing categories and ﬁne-grained clothing attributes from product images. Algorithm is developed to make mix-and-match recommendations by integrating the image extracted categories and attributes information are with text-based product attributes. Our comprehensive experimental work on a real-life online dataset has demonstrated the effectiveness of the proposed method.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Journal of Visual Communication and Image Representation},
	author = {Zhou, Wei and Mok, P.Y. and Zhou, Yanghong and Zhou, Yangping and Shen, Jialie and Qu, Qiang and Chau, K.P.},
	month = may,
	year = {2019},
	pages = {112--120},
	file = {Zhou et al. - 2019 - Fashion recommendations through cross-media inform.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ETKE8EWE\\Zhou et al. - 2019 - Fashion recommendations through cross-media inform.pdf:application/pdf},
}

@article{wangLearningCompatibilityKnowledge2022,
	title = {Learning compatibility knowledge for outfit recommendation with complementary clothing matching},
	volume = {181},
	issn = {01403664},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S014036642100400X},
	doi = {10.1016/j.comcom.2021.10.022},
	abstract = {With the rapid development of mobile networks and e-commerce, clothing recommendation has achieved considerable success in recent years. Fashion outfit matching has become an essential component to users while shopping, which helps users to select and present items to individuals in a personalized fashion recommendation. Apparently, it is an arduous task to guide complementary clothing matching due to the complexity and subjectivity of fashion items. Some existing solutions have been presented in recent years, which are tending to discover a series of visual cues to establish the matching relations. However, it would be mismatched easily due to these methods being hard to represent all the potential semantic information from the appearance of clothes. To thoroughly make use of the visual characteristics of clothing products and the related description information, we propose a complementary clothing matching method with some compatibility knowledge, named it CCMCK shortly. For visual compatibility, we adopt the graph neural network to model the visual relationship between items. To generate an outfit that satisfies the requirement of fashion compatibility, we propose a matching way under the compatibility constraint and seek to recommend compatible items based on multi-modal compatibility. Finally, we performed a qualitative investigation on the fill-in-the-blank and fashion outfit compatibility tasks to evaluate the proposed method.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Computer Communications},
	author = {Wang, Ruomei and Wang, Jianfeng and Su, Zhuo},
	month = jan,
	year = {2022},
	keywords = {sota},
	pages = {320--328},
	file = {Wang et al. - 2022 - Learning compatibility knowledge for outfit recomm.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NUMZ4CRG\\Wang et al. - 2022 - Learning compatibility knowledge for outfit recomm.pdf:application/pdf},
}

@article{sunLearningFashionCompatibility2020,
	title = {Learning fashion compatibility across categories with deep multimodal neural networks},
	volume = {395},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219309142},
	doi = {10.1016/j.neucom.2018.06.098},
	language = {en},
	urldate = {2022-11-05},
	journal = {Neurocomputing},
	author = {Sun, Guang-Lu and He, Jun-Yan and Wu, Xiao and Zhao, Bo and Peng, Qiang},
	month = jun,
	year = {2020},
	pages = {237--246},
	file = {Sun et al. - 2020 - Learning fashion compatibility across categories w.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\APY2Y7MJ\\Sun et al. - 2020 - Learning fashion compatibility across categories w.pdf:application/pdf},
}

@article{moNeuralStylistOnline2022,
	title = {Neural stylist: {Towards} online styling service},
	volume = {203},
	issn = {09574174},
	shorttitle = {Neural stylist},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095741742200690X},
	doi = {10.1016/j.eswa.2022.117333},
	abstract = {Online stylist service enjoys huge economic potentials due to the trend of transformation of the fashion industry to digitalisation. Existing works either predict the fashion compatibility from the overall aspect or evaluate the compatibility with type-conditional representations. The prediction is hard to interpret due to the abstractive forecast. This paper proposes a visual and semantic representation model for explainable evaluation and recommendation. The model considers fashion compatibility from different factors, such as colour, material and style, by leveraging low to high-level features from former to later layers of CNN. The colour correlation and the pairwise relationship of fashion items in the same outfit are considered during the prediction stage. Instead of just predicting an outfit as compatible or incompatible, the model can classify an outfit as three precise evaluation levels: Good, Normal and Bad. The detailed compatible level is more consistent with the fashion sense of our human brain as Good or Bad outfits may have specific characteristics while Normal outfits tend to be ordinary. Additionally, the model can diagnose and recommend substitutions of the problematic fashion items from overall compatibility or colour-specific aspects by tracking the prediction matrices’ backpropagation gradients during the recommendation stage. Experiments in terms of outfit compatibility prediction and fill in the blank are conducted to evaluate the prediction ability of the proposed model. In contrast, fashion substitution recommendation experiments are conducted to assess the compatibility diagnosis and recommendation ability. Quantitative and qualitative results show that the model enables online stylist services with excellent explainability and generalisation on fashion prediction and recommendation.},
	language = {en},
	urldate = {2022-11-05},
	journal = {Expert Systems with Applications},
	author = {Mo, Dongmei and Zou, Xingxing and Wong, WaiKeung},
	month = oct,
	year = {2022},
	pages = {117333},
	file = {Mo et al. - 2022 - Neural stylist Towards online styling service.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LLUH27A9\\Mo et al. - 2022 - Neural stylist Towards online styling service.pdf:application/pdf},
}

@misc{vasilevaLearningTypeAwareEmbeddings2018,
	title = {Learning {Type}-{Aware} {Embeddings} for {Fashion} {Compatibility}},
	url = {http://arxiv.org/abs/1803.09196},
	abstract = {Outﬁts in online fashion data are composed of items of many diﬀerent types (e.g. top, bottom, shoes) that share some stylistic relationship with one another. A representation for building outﬁts requires a method that can learn both notions of similarity (for example, when two tops are interchangeable) and compatibility (items of possibly different type that can go together in an outﬁt). This paper presents an approach to learning an image embedding that respects item type, and jointly learns notions of item similarity and compatibility in an end-toend model. To evaluate the learned representation, we crawled 68,306 outﬁts created by users on the Polyvore website. Our approach obtains 3-5\% improvement over the state-of-the-art on outﬁt compatibility prediction and ﬁll-in-the-blank tasks using our dataset, as well as an established smaller dataset, while supporting a variety of useful queries1.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Vasileva, Mariya I. and Plummer, Bryan A. and Dusad, Krishna and Rajpal, Shreya and Kumar, Ranjitha and Forsyth, David},
	month = jul,
	year = {2018},
	note = {arXiv:1803.09196 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Vasileva et al. - 2018 - Learning Type-Aware Embeddings for Fashion Compati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JE6EL87N\\Vasileva et al. - 2018 - Learning Type-Aware Embeddings for Fashion Compati.pdf:application/pdf},
}

@inproceedings{wuMultimodalConversationalFashion2022,
	address = {Glasgow United Kingdom},
	title = {Multimodal {Conversational} {Fashion} {Recommendation} with {Positive} and {Negative} {Natural}-{Language} {Feedback}},
	isbn = {978-1-4503-9739-1},
	url = {https://dl.acm.org/doi/10.1145/3543829.3543837},
	doi = {10.1145/3543829.3543837},
	abstract = {In a real-world shopping scenario, users can express their naturallanguage feedback when communicating with a shopping assistant by stating their satisfactions positively with “I like” or negatively with “I dislike” according to the quality of the recommended fashion products. A multimodal conversational recommender system (using text and images in particular) aims to replicate this process by eliciting the dynamic preferences of users from their natural-language feedback and updating the visual recommendations so as to satisfy the users’ current needs through multi-turn interactions. However, the impact of positive and negative natural-language feedback on the effectiveness of multimodal conversational recommendation has not yet been fully explored. Since there are no datasets of conversational recommendation with both positive and negative natural-language feedback, the existing research on multimodal conversational recommendation imposed several constraints on the users’ natural-language expressions (i.e. either only describing their preferred attributes as positive feedback or rejecting the undesired recommendations without any natural-language critiques) to simplify the multimodal conversational recommendation task. To further explore the multimodal conversational recommendation with positive and negative natural-language feedback, we investigate the effectiveness of the recent multimodal conversational recommendation models for effectively incorporating the users’ preferences over time from both positively and negatively natural-language oriented feedback corresponding to the visual recommendations. We also propose an approach to generate both positive and negative natural-language critiques about the recommendations within an existing user simulator. Following previous work, we train and evaluate the two existing conversational recommendation models by using the user simulator with positive and negative feedback as a surrogate for real human users. Extensive experiments conducted on a well-known fashion dataset demonstrate that positive natural-language feedback is more informative relating to the users’ preferences in comparison to negative natural-language feedback.},
	language = {en},
	urldate = {2022-11-06},
	booktitle = {4th {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {ACM},
	author = {Wu, Yaxiong and Macdonald, Craig and Ounis, Iadh},
	month = jul,
	year = {2022},
	pages = {1--10},
	file = {Wu et al. - 2022 - Multimodal Conversational Fashion Recommendation w.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AZMMSZ34\\Wu et al. - 2022 - Multimodal Conversational Fashion Recommendation w.pdf:application/pdf},
}

@article{chengFashionMeetsComputer2022,
	title = {Fashion {Meets} {Computer} {Vision}: {A} {Survey}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Fashion {Meets} {Computer} {Vision}},
	url = {https://dl.acm.org/doi/10.1145/3447239},
	doi = {10.1145/3447239},
	abstract = {Fashion is the way we present ourselves to the world and has become one of the world’s largest industries. Fashion, mainly conveyed by vision, has thus attracted much attention from computer vision researchers in recent years. Given the rapid development, this article provides a comprehensive survey of more than 200 major fashion-related works covering four main aspects for enabling intelligent fashion: (1) Fashion detection includes landmark detection, fashion parsing, and item retrieval; (2) Fashion analysis contains attribute recognition, style learning, and popularity prediction; (3) Fashion synthesis involves style transfer, pose transformation, and physical simulation; and (4) Fashion recommendation comprises fashion compatibility, outfit matching, and hairstyle suggestion. For each task, the benchmark datasets and the evaluation protocols are summarized. Furthermore, we highlight promising directions for future research.},
	language = {en},
	number = {4},
	urldate = {2022-11-06},
	journal = {ACM Computing Surveys},
	author = {Cheng, Wen-Huang and Song, Sijie and Chen, Chieh-Yun and Hidayati, Shintami Chusnul and Liu, Jiaying},
	month = may,
	year = {2022},
	pages = {1--41},
	file = {Cheng et al. - 2022 - Fashion Meets Computer Vision A Survey.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GW8G7JYK\\Cheng et al. - 2022 - Fashion Meets Computer Vision A Survey.pdf:application/pdf},
}

@article{russoTutorialThompsonSampling,
	title = {A {Tutorial} on {Thompson} {Sampling}},
	abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally eﬃcient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, product recommendation, assortment, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not eﬀective and relations to alternative algorithms.},
	language = {en},
	author = {Russo, Daniel J and Roy, Benjamin Van and Kazerouni, Abbas and Wen, Zheng},
	pages = {96},
	file = {Russo et al. - A Tutorial on Thompson Sampling.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NBQQ6VU7\\Russo et al. - A Tutorial on Thompson Sampling.pdf:application/pdf},
}

@inproceedings{saidComparativeRecommenderSystem2014,
	address = {Foster City, Silicon Valley, California, USA},
	title = {Comparative recommender system evaluation: benchmarking recommendation frameworks},
	isbn = {978-1-4503-2668-1},
	shorttitle = {Comparative recommender system evaluation},
	url = {http://dl.acm.org/citation.cfm?doid=2645710.2645746},
	doi = {10.1145/2645710.2645746},
	language = {en},
	urldate = {2022-12-06},
	booktitle = {Proceedings of the 8th {ACM} {Conference} on {Recommender} systems - {RecSys} '14},
	publisher = {ACM Press},
	author = {Said, Alan and Bellogín, Alejandro},
	year = {2014},
	pages = {129--136},
	file = {Said_Bellogín_2014_Comparative recommender system evaluation.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N44XQI64\\Said_Bellogín_2014_Comparative recommender system evaluation.pdf:application/pdf},
}

@inproceedings{yuan2022tenrec,
	title = {Tenrec: {A} large-scale multipurpose benchmark dataset for recommender systems},
	url = {https://openreview.net/forum?id=PfuW84q25y9},
	booktitle = {Thirty-sixth conference on neural information processing systems datasets and benchmarks track},
	author = {Yuan, Guanghu and Yuan, Fajie and Li, Yudong and Kong, Beibei and Li, Shujie and Chen, Lei and Yang, Min and YU, Chenyun and Hu, Bo and Li, Zang and Xu, Yu and Qie, Xiaohu},
	year = {2022},
}

@article{tudor-lockeHowManySteps2011,
	title = {How many steps/day are enough? {For} older adults and special populations},
	volume = {8},
	issn = {1479-5868},
	shorttitle = {How many steps/day are enough?},
	url = {http://ijbnpa.biomedcentral.com/articles/10.1186/1479-5868-8-80},
	doi = {10.1186/1479-5868-8-80},
	language = {en},
	number = {1},
	urldate = {2022-12-17},
	journal = {International Journal of Behavioral Nutrition and Physical Activity},
	author = {Tudor-Locke, Catrine and Craig, Cora L and Aoyagi, Yukitoshi and Bell, Rhonda C and Croteau, Karen A and De Bourdeaudhuij, Ilse and Ewald, Ben and Gardner, Andrew W and Hatano, Yoshiro and Lutes, Lesley D and Matsudo, Sandra M and Ramirez-Marrero, Farah A and Rogers, Laura Q and Rowe, David A and Schmidt, Michael D and Tully, Mark A and Blair, Steven N},
	year = {2011},
	pages = {80},
	file = {Tudor-Locke et al_2011_How many steps-day are enough.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9374ZPH4\\Tudor-Locke et al_2011_How many steps-day are enough.pdf:application/pdf},
}

@article{tanSpinalSurgerySafe2019,
	title = {Is {Spinal} {Surgery} {Safe} for {Elderly} {Patients} {Aged} 80 and {Above}? {Predictors} of {Mortality} and {Morbidity} in an {Asian} {Population}},
	volume = {16},
	issn = {2586-6583},
	shorttitle = {Is {Spinal} {Surgery} {Safe} for {Elderly} {Patients} {Aged} 80 and {Above}?},
	doi = {10.14245/ns.1836336.168},
	abstract = {OBJECTIVE: We aimed to determine the 2-year mortality and morbidity rates following spine surgery in elderly patients (age ≥80 years) and to study the associated risk factors.
METHODS: The records of patients ≥80 years of age who underwent spine surgery during the years 2003-2015 at Tan Tock Seng Hospital, Singapore were retrospectively reviewed. Information was collected on their demographic characteristics, comorbidities, diagnosis, general and neurological status, type of surgery, and outcomes. The mortality and morbidity rates over a 2-year period were analyzed. Bivariate analyses were carried out to identify factors associated with mortality.
RESULTS: We selected 47 patients (mean age, 83.3 years; range, 80-91 years) who were followed up for a mean duration of 27.7 months. The mortality rates at 30 days, 6 months, 1 year, and 2 years following surgery were 2.1\%, 8.5\%, 10.6\%, and 12.8\%, respectively. The factors significantly associated with mortality included multiple comorbidities, nondegenerative aetiology, and vertebral fractures. The overall morbidity rate was 48.9\%, and 17\% of this cohort had major complications.
CONCLUSION: Surgeons should strategize management protocols with due consideration of the mortality and morbidity rates, and be wary of operating on patients with multiple comorbidities, nondegenerative conditions, and vertebral fractures.},
	language = {eng},
	number = {4},
	journal = {Neurospine},
	author = {Tan, Joshua Yuan-Wang and Kaliya-Perumal, Arun-Kumar and Oh, Jacob Yoong-Leong},
	month = dec,
	year = {2019},
	pmid = {31284337},
	pmcid = {PMC6945003},
	keywords = {80 and over aged, Morbidity, Mortality, Risk factors, Spine},
	pages = {764--769},
	file = {Tan et al_2019_Is Spinal Surgery Safe for Elderly Patients Aged 80 and Above.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\N9DQP84S\\Tan et al_2019_Is Spinal Surgery Safe for Elderly Patients Aged 80 and Above.pdf:application/pdf},
}

@article{sharmaDoesStage3Chronic2010,
	title = {Does stage-3 chronic kidney disease matter?: {A} systematic literature review},
	volume = {60},
	issn = {0960-1643, 1478-5242},
	shorttitle = {Does stage-3 chronic kidney disease matter?},
	url = {https://bjgp.org/lookup/doi/10.3399/bjgp10X502173},
	doi = {10.3399/bjgp10X502173},
	language = {en},
	number = {575},
	urldate = {2023-01-09},
	journal = {British Journal of General Practice},
	author = {Sharma, Pawana and McCullough, Keith and Scotland, Graham and McNamee, Paul and Prescott, Gordon and MacLeod, Alison and Fluck, Nick and Smith, William Cairns and Black, Corri},
	month = jun,
	year = {2010},
	pages = {e266--e276},
	file = {Sharma et al_2010_Does stage-3 chronic kidney disease matter.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YAKL5YQL\\Sharma et al_2010_Does stage-3 chronic kidney disease matter.pdf:application/pdf},
}

@misc{suekunkelActuarialStudiesHttps,
	title = {Actuarial {Studies} - https://www.ssa.gov/oact/{NOTES}/s2000s.html},
	shorttitle = {Actuarial {Studies} - https},
	url = {https://www.ssa.gov/oact/NOTES/s2000s.html},
	language = {en},
	urldate = {2023-01-11},
	author = {SueKunkel},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UHGZN23H\\s2000s.html:text/html},
}

@book{brooksEvolutionEntropyUnified1988,
	address = {Chicago},
	edition = {2nd ed},
	series = {Science and its conceptual foundations},
	title = {Evolution as entropy: toward a unified theory of biology},
	isbn = {978-0-226-07573-0 978-0-226-07574-7},
	shorttitle = {Evolution as entropy},
	publisher = {University of Chicago Press},
	author = {Brooks, D. R. and Wiley, E. O.},
	year = {1988},
	keywords = {Biology, Entropy, Evolution (Biology), Philosophy},
}

@article{barwaisPhysicalActivitySedentary2013,
	title = {Physical activity, sedentary behavior and total wellness changes among sedentary adults: a 4-week randomized controlled trial},
	volume = {11},
	issn = {1477-7525},
	shorttitle = {Physical activity, sedentary behavior and total wellness changes among sedentary adults},
	url = {http://hqlo.biomedcentral.com/articles/10.1186/1477-7525-11-183},
	doi = {10.1186/1477-7525-11-183},
	language = {en},
	number = {1},
	urldate = {2023-01-11},
	journal = {Health and Quality of Life Outcomes},
	author = {Barwais, Faisal A and Cuddihy, Thomas F and Tomson, L},
	year = {2013},
	pages = {183},
	file = {Barwais et al_2013_Physical activity, sedentary behavior and total wellness changes among.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\D2Q56FT6\\Barwais et al_2013_Physical activity, sedentary behavior and total wellness changes among.pdf:application/pdf},
}

@article{aldenLogisticalAspectsHuman2009,
	title = {Logistical aspects of human surgical tissue management in a hospital setting},
	volume = {25},
	issn = {0894-203X},
	abstract = {Many hospital transfusion services have assumed responsibility for the coordinated management of human allograft tissue. This overview summarizes logistical aspects of tissue management based on the experience of a centralized tissue service at a large academic hospital, in which tissue is stored in a location remote from patient care areas. Operational aspects include determination of which personnel classifications will perform the necessary functions, establishment and maintenance of the standing tissue inventory (including pros and cons of alternative approaches to inventory acquisition), and necessary considerations for making tissue available for surgical cases in the hospital. The nature of communications regarding tissue orders for individual surgical cases is discussed, as well as mechanisms for storage of tissue and transportation and delivery of tissue to the surgical suites. Finally, options for the disposition of tissue that has been dispensed from the tissue service but was not used during the surgical procedure are summarized. With attention to these details, a tissue service can provide reliable, high-quality tissue in a timely fashion.},
	language = {eng},
	number = {3},
	journal = {Immunohematology},
	author = {Alden, B. M. and Schlueter, A. J.},
	year = {2009},
	pmid = {20406016},
	keywords = {Humans, Hospital Departments, Personnel, Hospital, Tissue Transplantation, Transplantation, Homologous},
	pages = {107--111},
}

@article{criswellShippingLogisticsConsiderations2022,
	title = {Shipping and {Logistics} {Considerations} for {Regenerative} {Medicine} {Therapies}},
	volume = {11},
	issn = {2157-6564, 2157-6580},
	url = {https://academic.oup.com/stcltm/article/11/2/107/6542876},
	doi = {10.1093/stcltm/szab025},
	abstract = {Abstract
            Advances in regenerative medicine manufacturing continue to be a priority for achieving the full commercial potential of important breakthrough therapies. Equally important will be the establishment of distribution chains that support the transport of live cells and engineered tissues and organs resulting from these advanced biomanufacturing processes. The importance of a well-managed distribution chain for products requiring specialized handling procedures was highlighted during the COVID-19 pandemic and serves as a reminder of the critical role of logistics and distribution in the success of breakthrough therapies. This perspective article will provide insight into current practices and future considerations for creating global distribution chains that facilitate the successful deployment of regenerative medicine therapies to the vast number of patients that would benefit from them worldwide.},
	language = {en},
	number = {2},
	urldate = {2023-01-24},
	journal = {Stem Cells Translational Medicine},
	author = {Criswell, Tracy and Swart, Corné and Stoudemire, Jana and Brockbank, Kelvin and Floren, Michael and Eaker, Shannon and Hunsberger, Joshua},
	month = mar,
	year = {2022},
	pages = {107--113},
	file = {Criswell et al_2022_Shipping and Logistics Considerations for Regenerative Medicine Therapies.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\43WQBPNH\\Criswell et al_2022_Shipping and Logistics Considerations for Regenerative Medicine Therapies.pdf:application/pdf},
}

@misc{ExaminationClinicalTrial,
	title = {Examination of {Clinical} {Trial} {Costs} and {Barriers} for {Drug} {Development}},
	url = {https://aspe.hhs.gov/reports/examination-clinical-trial-costs-barriers-drug-development-0},
	abstract = {TASK ORDER NO. HHSP23337007T CONTRACT NO. HHSP23320095634WC Submitted to:Hui-Hsing WongAmber JessupU.S. Department of Health and Human Services Assistant Secretary of Planning and Evaluation (ASPE) 200 Independence Avenue, SW Washington, DC 20201},
	language = {en},
	urldate = {2023-01-25},
	journal = {ASPE},
	file = {Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JSUSEPWJ\\examination-clinical-trial-costs-barriers-drug-development-0.html:text/html},
}

@article{weberAIStartupBusiness2022,
	title = {{AI} {Startup} {Business} {Models}: {Key} {Characteristics} and {Directions} for {Entrepreneurship} {Research}},
	volume = {64},
	issn = {2363-7005, 1867-0202},
	shorttitle = {{AI} {Startup} {Business} {Models}},
	url = {https://link.springer.com/10.1007/s12599-021-00732-w},
	doi = {10.1007/s12599-021-00732-w},
	abstract = {Abstract
            We currently observe the rapid emergence of startups that use Artificial Intelligence (AI) as part of their business model. While recent research suggests that AI startups employ novel or different business models, one could argue that AI technology has been used in business models for a long time already—questioning the novelty of those business models. Therefore, this study investigates how AI startup business models potentially differ from common IT-related business models. First, a business model taxonomy of AI startups is developed from a sample of 100 AI startups and four archetypal business model patterns are derived: AI-charged Product/Service Provider, AI Development Facilitator, Data Analytics Provider, and Deep Tech Researcher. Second, drawing on this descriptive analysis, three distinctive aspects of AI startup business models are discussed: (1) new value propositions through AI capabilities, (2) different roles of data for value creation, and (3) the impact of AI technology on the overall business logic. This study contributes to our fundamental understanding of AI startup business models by identifying their key characteristics, common instantiations, and distinctive aspects. Furthermore, this study proposes promising directions for future entrepreneurship research. For practice, the taxonomy and patterns serve as structured tools to support entrepreneurial action.},
	language = {en},
	number = {1},
	urldate = {2023-01-25},
	journal = {Business \& Information Systems Engineering},
	author = {Weber, Michael and Beutter, Moritz and Weking, Jörg and Böhm, Markus and Krcmar, Helmut},
	month = feb,
	year = {2022},
	pages = {91--109},
	file = {Weber et al_2022_AI Startup Business Models.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FGHFHR2X\\Weber et al_2022_AI Startup Business Models.pdf:application/pdf},
}

@book{ricciRecommenderSystemsHandbook2015,
	address = {Boston, MA},
	title = {Recommender {Systems} {Handbook}},
	isbn = {978-1-4899-7636-9 978-1-4899-7637-6},
	url = {https://link.springer.com/10.1007/978-1-4899-7637-6},
	language = {en},
	urldate = {2023-01-26},
	publisher = {Springer US},
	editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha},
	year = {2015},
	doi = {10.1007/978-1-4899-7637-6},
	file = {Recommender Systems Handbook.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Recommender Systems Handbook.md:text/plain;Ricci et al_2015_Recommender Systems Handbook.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7K85V8ZB\\Ricci et al_2015_Recommender Systems Handbook.pdf:application/pdf},
}

@article{davenportPotentialArtificialIntelligence2019,
	title = {The potential for artificial intelligence in healthcare},
	volume = {6},
	issn = {2514-6645},
	doi = {10.7861/futurehosp.6-2-94},
	abstract = {The complexity and rise of data in healthcare means that artificial intelligence (AI) will increasingly be applied within the field. Several types of AI are already being employed by payers and providers of care, and life sciences companies. The key categories of applications involve diagnosis and treatment recommendations, patient engagement and adherence, and administrative activities. Although there are many instances in which AI can perform healthcare tasks as well or better than humans, implementation factors will prevent large-scale automation of healthcare professional jobs for a considerable period. Ethical issues in the application of AI to healthcare are also discussed.},
	language = {eng},
	number = {2},
	journal = {Future Healthcare Journal},
	author = {Davenport, Thomas and Kalakota, Ravi},
	month = jun,
	year = {2019},
	pmid = {31363513},
	pmcid = {PMC6616181},
	keywords = {Artificial intelligence, clinical decision support, electronic health record systems},
	pages = {94--98},
	file = {Davenport_Kalakota_2019_The potential for artificial intelligence in healthcare.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LN6G4QBS\\Davenport_Kalakota_2019_The potential for artificial intelligence in healthcare.pdf:application/pdf},
}

@article{kammounGenerativeAdversarialNetworks2022,
	title = {Generative {Adversarial} {Networks} for face generation: {A} survey},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Generative {Adversarial} {Networks} for face generation},
	url = {https://dl.acm.org/doi/10.1145/1122445.1122456},
	doi = {10.1145/1122445.1122456},
	abstract = {Recently, Generative Adversarial Networks (GANs) have received enormous progress, which makes them able to learn complex data distributions in particular faces. More and more efficient GAN architectures have been designed and proposed to learn the different variations of faces, such as cross pose, age, expression and style. These GAN based approaches need to be reviewed, discussed, and categorized in terms of architectures, applications, and metrics. Several reviews that focus on the use and advances of GAN in general have been proposed. However, the GAN models applied to the face, that we call facial GANs, have never been addressed. In this article, we review facial GANs and their different applications. We mainly focus on architectures, problems and performance evaluation with respect to each application and used datasets. More precisely, we reviewed the progress of architectures and we discussed the contributions and limits of each. Then, we exposed the encountered problems of facial GANs and proposed solutions to handle them. Additionally, as GANs evaluation has become a notable current defiance, we investigate the state of the art quantitative and qualitative evaluation metrics and their applications. We concluded the article with a discussion on the face generation challenges and proposed open research issues.},
	language = {en},
	urldate = {2023-01-26},
	journal = {ACM Computing Surveys},
	author = {Kammoun, Amina and Slama, Rim and Tabia, Hedi and Ouni, Tarek and Abid, Mohmed},
	month = mar,
	year = {2022},
	pages = {1122445.1122456},
	file = {Kammoun et al_2022_Generative Adversarial Networks for face generation.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PETQL2NY\\Kammoun et al_2022_Generative Adversarial Networks for face generation.pdf:application/pdf},
}

@article{panagiotakisImprovingRecommenderSystems2021,
	title = {Improving recommender systems via a {Dual} {Training} {Error} based {Correction} approach},
	volume = {183},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417421008101},
	doi = {10.1016/j.eswa.2021.115386},
	language = {en},
	urldate = {2023-01-26},
	journal = {Expert Systems with Applications},
	author = {Panagiotakis, Costas and Papadakis, Harris and Papagrigoriou, Antonis and Fragopoulou, Paraskevi},
	month = nov,
	year = {2021},
	pages = {115386},
	file = {Panagiotakis et al. - 2021 - Improving recommender systems via a Dual Training .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\73YMWX34\\Panagiotakis et al. - 2021 - Improving recommender systems via a Dual Training .pdf:application/pdf},
}

@misc{afsarReinforcementLearningBased2022,
	title = {Reinforcement learning based recommender systems: {A} survey},
	shorttitle = {Reinforcement learning based recommender systems},
	url = {http://arxiv.org/abs/2101.06286},
	abstract = {Recommender systems (RSs) have become an inseparable part of our everyday lives. They help us find our favorite items to purchase, our friends on social networks, and our favorite movies to watch. Traditionally, the recommendation problem was considered to be a classification or prediction problem, but it is now widely agreed that formulating it as a sequential decision problem can better reflect the user-system interaction. Therefore, it can be formulated as a Markov decision process (MDP) and be solved by reinforcement learning (RL) algorithms. Unlike traditional recommendation methods, including collaborative filtering and content-based filtering, RL is able to handle the sequential, dynamic user-system interaction and to take into account the long-term user engagement. Although the idea of using RL for recommendation is not new and has been around for about two decades, it was not very practical, mainly because of scalability problems of traditional RL algorithms. However, a new trend has emerged in the field since the introduction of deep reinforcement learning (DRL), which made it possible to apply RL to the recommendation problem with large state and action spaces. In this paper, a survey on reinforcement learning based recommender systems (RLRSs) is presented. Our aim is to present an outlook on the field and to provide the reader with a fairly complete knowledge of key concepts of the field. We first recognize and illustrate that RLRSs can be generally classified into RL- and DRL-based methods. Then, we propose an RLRS framework with four components, i.e., state representation, policy optimization, reward formulation, and environment building, and survey RLRS algorithms accordingly. We highlight emerging topics and depict important trends using various graphs and tables. Finally, we discuss important aspects and challenges that can be addressed in the future.},
	urldate = {2023-01-26},
	publisher = {arXiv},
	author = {Afsar, M. Mehdi and Crump, Trafford and Far, Behrouz},
	month = jun,
	year = {2022},
	note = {arXiv:2101.06286 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {Afsar et al_2022_Reinforcement learning based recommender systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8WC7J5W8\\Afsar et al_2022_Reinforcement learning based recommender systems.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\48ZEIPAD\\2101.html:text/html},
}

@article{daraSurveyGroupRecommender2020,
	title = {A survey on group recommender systems},
	volume = {54},
	issn = {0925-9902, 1573-7675},
	url = {http://link.springer.com/10.1007/s10844-018-0542-3},
	doi = {10.1007/s10844-018-0542-3},
	language = {en},
	number = {2},
	urldate = {2023-01-26},
	journal = {Journal of Intelligent Information Systems},
	author = {Dara, Sriharsha and Chowdary, C. Ravindranath and Kumar, Chintoo},
	month = apr,
	year = {2020},
	pages = {271--295},
}

@article{herlockerEvaluatingCollaborativeFiltering2004,
	title = {Evaluating collaborative filtering recommender systems},
	volume = {22},
	issn = {1046-8188, 1558-2868},
	url = {https://dl.acm.org/doi/10.1145/963770.963772},
	doi = {10.1145/963770.963772},
	abstract = {Recommender systems have been evaluated in many, often incomparable, ways. In this article, we review the key decisions in evaluating collaborative filtering recommender systems: the user tasks being evaluated, the types of analysis and datasets being used, the ways in which prediction quality is measured, the evaluation of prediction attributes other than quality, and the user-based evaluation of the system as a whole. In addition to reviewing the evaluation strategies used by prior researchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metrics within each equivalency class were strongly correlated, while metrics from different equivalency classes were uncorrelated.},
	language = {en},
	number = {1},
	urldate = {2023-01-26},
	journal = {ACM Transactions on Information Systems},
	author = {Herlocker, Jonathan L. and Konstan, Joseph A. and Terveen, Loren G. and Riedl, John T.},
	month = jan,
	year = {2004},
	pages = {5--53},
	file = {Evaluating collaborative filtering recommender systems.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Evaluating collaborative filtering recommender systems.md:text/plain;Herlocker et al. - 2004 - Evaluating collaborative filtering recommender sys.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\L7HWB3WY\\Herlocker et al. - 2004 - Evaluating collaborative filtering recommender sys.pdf:application/pdf},
}

@incollection{burkeHybridWebRecommender2007,
	address = {Berlin, Heidelberg},
	title = {Hybrid {Web} {Recommender} {Systems}},
	volume = {4321},
	isbn = {978-3-540-72078-2},
	url = {http://link.springer.com/10.1007/978-3-540-72079-9_12},
	language = {en},
	urldate = {2023-01-26},
	booktitle = {The {Adaptive} {Web}},
	publisher = {Springer Berlin Heidelberg},
	author = {Burke, Robin},
	editor = {Brusilovsky, Peter and Kobsa, Alfred and Nejdl, Wolfgang},
	year = {2007},
	doi = {10.1007/978-3-540-72079-9_12},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {377--408},
	file = {Burke - 2007 - Hybrid Web Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9F8MWUEH\\Burke - 2007 - Hybrid Web Recommender Systems.pdf:application/pdf;Burke_2007_Hybrid Web Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ID38ZIH4\\Burke_2007_Hybrid Web Recommender Systems.pdf:application/pdf;Hybrid Web Recommender Systems.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Hybrid Web Recommender Systems.md:text/plain},
}

@book{littleStatisticalAnalysisMissing2020,
	address = {Hoboken, NJ},
	edition = {Third edition},
	series = {Wiley series in probability and statistics},
	title = {Statistical analysis with missing data},
	isbn = {978-1-118-59601-2 978-0-470-52679-8},
	language = {en},
	publisher = {Wiley},
	author = {Little, Roderick J. A. and Rubin, Donald B.},
	year = {2020},
	keywords = {Mathematical statistics, Missing observations (Statistics), Problems, exercises, etc},
	file = {Little and Rubin - 2020 - Statistical analysis with missing data.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\P87DV6G3\\Little and Rubin - 2020 - Statistical analysis with missing data.pdf:application/pdf},
}

@article{korenBellKorSolutionNetflix,
	title = {The {BellKor} {Solution} to the {Netﬂix} {Grand} {Prize}},
	language = {en},
	author = {Koren, Yehuda},
	file = {Koren - The BellKor Solution to the Netﬂix Grand Prize.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PT8M6W2E\\Koren - The BellKor Solution to the Netﬂix Grand Prize.pdf:application/pdf},
}

@article{luRecommenderSystems2012,
	title = {Recommender {Systems}},
	volume = {519},
	issn = {03701573},
	url = {http://arxiv.org/abs/1202.1112},
	doi = {10.1016/j.physrep.2012.02.006},
	abstract = {The ongoing rapid expansion of the Internet greatly increases the necessity of eﬀective recommender systems for ﬁltering the abundant information. Extensive research for recommender systems is conducted by a broad range of communities including social and computer scientists, physicists, and interdisciplinary researchers. Despite substantial theoretical and practical achievements, uniﬁcation and comparison of diﬀerent approaches are lacking, which impedes further advances. In this article, we review recent developments in recommender systems and discuss the major challenges. We compare and evaluate available algorithms and examine their roles in the future developments. In addition to algorithms, physical aspects are described to illustrate macroscopic behavior of recommender systems. Potential impacts and future directions are discussed. We emphasize that recommendation has a great scientiﬁc depth and combines diverse research ﬁelds which makes it of interests for physicists as well as interdisciplinary researchers.},
	language = {en},
	number = {1},
	urldate = {2023-01-26},
	journal = {Physics Reports},
	author = {Lü, Linyuan and Medo, Matus and Yeung, Chi Ho and Zhang, Yi-Cheng and Zhang, Zi-Ke and Zhou, Tao},
	month = oct,
	year = {2012},
	note = {arXiv:1202.1112 [cond-mat, physics:physics]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Social and Information Networks, Condensed Matter - Statistical Mechanics, Physics - Physics and Society},
	pages = {1--49},
	file = {Lü et al. - 2012 - Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DWSZ88CD\\Lü et al. - 2012 - Recommender Systems.pdf:application/pdf},
}

@article{bobadillaDeepLearningArchitecture2020,
	title = {Deep {Learning} {Architecture} for {Collaborative} {Filtering} {Recommender} {Systems}},
	volume = {10},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/7/2441},
	doi = {10.3390/app10072441},
	abstract = {This paper provides an innovative deep learning architecture to improve collaborative ﬁltering results in recommender systems. It exploits the potential of the reliability concept to raise predictions and recommendations quality by incorporating prediction errors (reliabilities) in the deep learning layers. The underlying idea is to recommend highly predicted items that also have been found as reliable ones. We use the deep learning architecture to extract the existing non-linear relations between predictions, reliabilities, and accurate recommendations. The proposed architecture consists of three related stages, providing three stacked abstraction levels: (a) real prediction errors, (b) predicted errors (reliabilities), and (c) predicted ratings (predictions). In turn, each abstraction level requires a learning process: (a) Matrix Factorization from ratings, (b) Multilayer Neural Network fed with real prediction errors and hidden factors, and (c) Multilayer Neural Network fed with reliabilities and hidden factors. A complete set of experiments has been run involving three representative and open datasets and a state-of-the-art baseline. The results show strong prediction improvements and also important recommendation improvements, particularly for the recall quality measure.},
	language = {en},
	number = {7},
	urldate = {2023-01-26},
	journal = {Applied Sciences},
	author = {Bobadilla, Jesus and Alonso, Santiago and Hernando, Antonio},
	month = apr,
	year = {2020},
	pages = {2441},
	file = {Bobadilla et al. - 2020 - Deep Learning Architecture for Collaborative Filte.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7A3IQHF2\\Bobadilla et al. - 2020 - Deep Learning Architecture for Collaborative Filte.pdf:application/pdf},
}

@article{bobadillaRecommenderSystemsSurvey2013,
	title = {Recommender systems survey},
	volume = {46},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705113001044},
	doi = {10.1016/j.knosys.2013.03.012},
	abstract = {Recommender systems have developed in parallel with the web. They were initially based on demographic, content-based and collaborative ﬁltering. Currently, these systems are incorporating social information. In the future, they will use implicit, local and personal information from the Internet of things. This article provides an overview of recommender systems as well as collaborative ﬁltering methods and algorithms; it also explains their evolution, provides an original classiﬁcation for these systems, identiﬁes areas of future implementation and develops certain areas selected for past, present or future importance.},
	language = {en},
	urldate = {2023-01-26},
	journal = {Knowledge-Based Systems},
	author = {Bobadilla, J. and Ortega, F. and Hernando, A. and Gutiérrez, A.},
	month = jul,
	year = {2013},
	pages = {109--132},
	file = {Bobadilla et al. - 2013 - Recommender systems survey.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DKNMNYIF\\Bobadilla et al. - 2013 - Recommender systems survey.pdf:application/pdf},
}

@article{tiwalolaCOMPREHENSIVESTUDYRECOMMENDER2015,
	title = {A {COMPREHENSIVE} {STUDY} {OF} {RECOMMENDER} {SYSTEMS}: {PROSPECTS} {AND} {CHALLENGES}},
	volume = {6},
	abstract = {Recommender systems (RSs) automate some of these strategies with the goal of providing affordable, personal, and high-quality recommendations. Recommender Systems are software tools and techniques aimed at providing suggestion to support users in various decision-making processes. Development of recommender systems is a multi-disciplinary effort which involves experts from various fields such as Artificial intelligence (AI), Human Computer Interaction (HCI), Information Technology (IT), Data Mining, Statistics, Adaptive User Interfaces, Decision Support Systems (DSS), Marketing, or Consumer Behaviour. Recommender systems have proven to be valuable means for online users to cope with information overload and various techniques for recommendation algorithms have been proposed and successfully deployed in commercial environments. In this paper, a comprehensive study of recommendation systems and various approaches are provided with their major strengths and limitations thereby providing future research possibilities in recommendation systems.},
	language = {en},
	number = {8},
	author = {Tiwalola, Adetoba B and Asafe, Yekini N},
	year = {2015},
	file = {Tiwalola and Asafe - 2015 - A COMPREHENSIVE STUDY OF RECOMMENDER SYSTEMS PROS.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ARMIPR2Y\\Tiwalola and Asafe - 2015 - A COMPREHENSIVE STUDY OF RECOMMENDER SYSTEMS PROS.pdf:application/pdf},
}

@article{chicaizaComprehensiveSurveyKnowledge2021,
	title = {A {Comprehensive} {Survey} of {Knowledge} {Graph}-{Based} {Recommender} {Systems}: {Technologies}, {Development}, and {Contributions}},
	volume = {12},
	issn = {2078-2489},
	shorttitle = {A {Comprehensive} {Survey} of {Knowledge} {Graph}-{Based} {Recommender} {Systems}},
	url = {https://www.mdpi.com/2078-2489/12/6/232},
	doi = {10.3390/info12060232},
	abstract = {In recent years, the use of recommender systems has become popular on the web. To improve recommendation performance, usage, and scalability, the research has evolved by producing several generations of recommender systems. There is much literature about it, although most proposals focus on traditional methods’ theories and applications. Recently, knowledge graphbased recommendations have attracted attention in academia and the industry because they can alleviate information sparsity and performance problems. We found only two studies that analyze the recommendation system’s role over graphs, but they focus on speciﬁc recommendation methods. This survey attempts to cover a broader analysis from a set of selected papers. In summary, the contributions of this paper are as follows: (1) we explore traditional and more recent developments of ﬁltering methods for a recommender system, (2) we identify and analyze proposals related to knowledge graph-based recommender systems, (3) we present the most relevant contributions using an application domain, and (4) we outline future directions of research in the domain of recommender systems. As the main survey result, we found that the use of knowledge graphs for recommendations is an efﬁcient way to leverage and connect a user’s and an item’s knowledge, thus providing more precise results for users.},
	language = {en},
	number = {6},
	urldate = {2023-01-26},
	journal = {Information},
	author = {Chicaiza, Janneth and Valdiviezo-Diaz, Priscila},
	month = may,
	year = {2021},
	pages = {232},
	file = {Chicaiza and Valdiviezo-Diaz - 2021 - A Comprehensive Survey of Knowledge Graph-Based Re.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4EMPRMZ4\\Chicaiza and Valdiviezo-Diaz - 2021 - A Comprehensive Survey of Knowledge Graph-Based Re.pdf:application/pdf},
}

@article{adomaviciusNextGenerationRecommender2005,
	title = {Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions},
	volume = {17},
	issn = {1041-4347},
	shorttitle = {Toward the next generation of recommender systems},
	url = {http://ieeexplore.ieee.org/document/1423975/},
	doi = {10.1109/TKDE.2005.99},
	abstract = {This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multcriteria ratings, and a provision of more flexible and less intrusive types of recommendations.},
	language = {en},
	number = {6},
	urldate = {2023-01-26},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Adomavicius, G. and Tuzhilin, A.},
	month = jun,
	year = {2005},
	pages = {734--749},
	file = {Adomavicius and Tuzhilin - 2005 - Toward the next generation of recommender systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VIC5BMVB\\Adomavicius and Tuzhilin - 2005 - Toward the next generation of recommender systems.pdf:application/pdf},
}

@article{shahbaziImprovingPredictionAccuracy2020,
	title = {Toward {Improving} the {Prediction} {Accuracy} of {Product} {Recommendation} {System} {Using} {Extreme} {Gradient} {Boosting} and {Encoding} {Approaches}},
	volume = {12},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/12/9/1566},
	doi = {10.3390/sym12091566},
	abstract = {With the spread of COVID-19, the “untact” culture in South Korea is expanding and customers are increasingly seeking for online services. A recommendation system serves as a decisionmaking indicator that helps users by suggesting items to be purchased in the future by exploring the symmetry between multiple user activity characteristics. A plethora of approaches are employed by the scientific community to design recommendation systems, including collaborative filtering, stereotyping, and content-based filtering, etc. The current paradigm of recommendation systems favors collaborative filtering due to its significant potential to closely capture the interest of a user as compared to other approaches. The collaborative filtering harnesses features like user-profile details, visited pages, and click information to determine the interest of a user, thereby recommending the items that are related to the user’s interest. The existing collaborative filtering approaches exploit implicit and explicit features and report either good classification or prediction outcome. These systems fail to exhibit good results for both measures at the same time. We believe that avoiding the recommendation of those items that have already been purchased could contribute to overcoming the said issue. In this study, we present a collaborative filtering-based algorithm to tackle big data of user with symmetric purchasing order and repetitive purchased products. The proposed algorithm relies on combining extreme gradient boosting machine learning architecture with word2vec mechanism to explore the purchased products based on the click patterns of users. Our algorithm improves the accuracy of predicting the relevant products to be recommended to the customers that are likely to be bought. The results are evaluated on the dataset that contains click-based features of users from an online shopping mall in Jeju Island, South Korea. We have evaluated Mean Absolute Error, Mean Square Error, and Root Mean Square Error for our proposed methodology and also other machine learning algorithms. Our proposed model generated the least error rate and enhanced the prediction accuracy of the recommendation system compared to other traditional approaches.},
	language = {en},
	number = {9},
	urldate = {2023-01-26},
	journal = {Symmetry},
	author = {Shahbazi, Zeinab and Hazra, Debapriya and Park, Sejoon and Byun, Yung Cheol},
	month = sep,
	year = {2020},
	pages = {1566},
	file = {Shahbazi et al. - 2020 - Toward Improving the Prediction Accuracy of Produc.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z6ILI7WE\\Shahbazi et al. - 2020 - Toward Improving the Prediction Accuracy of Produc.pdf:application/pdf},
}

@article{departmentcomputersciencealigarhmuslimuniversityaligarhindiaHttpWwwJestr2017,
	title = {http://www.jestr.org/downloads/{Volume10Issue4}/fulltext171042017.pdf},
	volume = {10},
	issn = {17919320, 17912377},
	shorttitle = {http},
	url = {http://www.jestr.org/downloads/Volume10Issue4/fulltext181042017.pdf},
	doi = {10.25103/jestr.104.18},
	abstract = {This paper presents the state of art techniques in recommender systems (RS). The various techniques are diagrammatically illustrated which on one hand helps a naïve researcher in this field to accommodate the on-going researches and establish a strong base, on the other hand it focuses on different categories of the recommender systems with deep technical discussions. The review studies on RS are highlighted which helps in understanding the previous review works and their directions. 8 different main categories of recommender techniques and 19 sub categories have been identified and stated. Further, soft computing approach for recommendation is emphasized which have not been well studied earlier. The major problems of the existing area is reviewed and presented from different perspectives. However, solutions to these issues are rarely discussed in the previous works, in this study future direction for possible solutions are also addressed.},
	language = {en},
	number = {4},
	urldate = {2023-01-26},
	journal = {JOURNAL OF ENGINEERING SCIENCE AND TECHNOLOGY REVIEW},
	author = {{Department Computer Science Aligarh Muslim University, Aligarh, India} and Sohail, Shahab Saquib and Siddiqui, Jamshed and {Department Computer Science Aligarh Muslim University, Aligarh, India} and Ali, Rashid and {Department of Computer Engineering, Aligarh Muslim University, Aligarh, India}},
	year = {2017},
	pages = {132--153},
	file = {Department Computer Science Aligarh Muslim University, Aligarh, India et al. - 2017 - httpwww.jestr.orgdownloadsVolume10Issue4full.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T22PCMQG\\Department Computer Science Aligarh Muslim University, Aligarh, India et al. - 2017 - httpwww.jestr.orgdownloadsVolume10Issue4full.pdf:application/pdf},
}

@misc{chenSurveyDeepReinforcement2021,
	title = {A {Survey} of {Deep} {Reinforcement} {Learning} in {Recommender} {Systems}: {A} {Systematic} {Review} and {Future} {Directions}},
	shorttitle = {A {Survey} of {Deep} {Reinforcement} {Learning} in {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2109.03540},
	abstract = {In light of the emergence of deep reinforcement learning (DRL) in recommender systems research and several fruitful results in recent years, this survey aims to provide a timely and comprehensive overview of the recent trends of deep reinforcement learning in recommender systems. We start with the motivation of applying DRL in recommender systems. Then, we provide a taxonomy of current DRL-based recommender systems and a summary of existing methods. We discuss emerging topics and open issues, and provide our perspective on advancing the domain. This survey serves as introductory material for readers from academia and industry into the topic and identifies notable opportunities for further research.},
	language = {en},
	urldate = {2023-01-26},
	publisher = {arXiv},
	author = {Chen, Xiaocong and Yao, Lina and McAuley, Julian and Zhou, Guanglin and Wang, Xianzhi},
	month = sep,
	year = {2021},
	note = {arXiv:2109.03540 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Chen et al. - 2021 - A Survey of Deep Reinforcement Learning in Recomme.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JF45FBGU\\Chen et al. - 2021 - A Survey of Deep Reinforcement Learning in Recomme.pdf:application/pdf},
}

@article{agRecommenderSystems2023,
	title = {Recommender {Systems}},
	language = {en},
	journal = {. RECOMMENDER SYSTEMS},
	author = {Ag, Peter Lang},
	year = {2023},
	file = {Ag - 2023 - Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AMP26BBW\\Ag - 2023 - Recommender Systems.pdf:application/pdf},
}

@misc{CollaborativeFilteringAdvantages,
	title = {Collaborative {Filtering} {Advantages} \& {Disadvantages} {\textbar} {Machine} {Learning}},
	url = {https://developers.google.com/machine-learning/recommendation/collaborative/summary},
	language = {en},
	urldate = {2023-01-27},
	journal = {Google Developers},
	file = {Collaborative Filtering Advantages & Disadvantages  Machine Learning.md:C\:\\Users\\John\\Documents\\Knowledge Centre\\Obsidian\\Data Science\\references\\Collaborative Filtering Advantages & Disadvantages  Machine Learning.md:text/plain;Collaborative Filtering Advantages & Disadvantages  Machine Learning.md:C\:\\Users\\John\\iCloudDrive\\iCloud~md~obsidian\\AIStudio\\references\\Collaborative Filtering Advantages & Disadvantages  Machine Learning.md:text/plain;Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MJDAWTLK\\summary.html:text/html},
}

@article{kumarApproachesIssuesChallenges2016,
	title = {Approaches, {Issues} and {Challenges} in {Recommender} {Systems}: {A} {Systematic} {Review}},
	volume = {9},
	issn = {0974-5645, 0974-6846},
	shorttitle = {Approaches, {Issues} and {Challenges} in {Recommender} {Systems}},
	url = {http://www.indjst.org/index.php/indjst/article/view/94892},
	doi = {10.17485/ijst/2015/v8i1/94892},
	abstract = {Objectives: Today the recommendation technology has managed to achieve a distinct place in the modern and fascinating world of e-commerce applications as it helps the user in selecting items or products of his interest from a large pool. The present article aims to provide a comprehensive and systematic review of the state-of-the-art recommender systems. Methods/Statistical Analysis: The entire literature review process was divided into six research questions keeping in view the different perspectives of recommendation field. The methodology adopted here, consists of the search plan and the paper selection criteria. The search plan attempts to retrieve the research studies through several digital libraries and the paper selection criteria help filter out the most relevant studies further to gather evidence against each of the research questions. Findings: The literature review process provides a thorough discussion on different techniques deployed in recommender system literature such as collaborative filtering, content-based filtering, social filtering, demographic and knowledge-based and utility based systems. It also explores their strengths and weaknesses. The recommender systems face certain challenges in their deployment such as cold-start, sparsity, scalability, user privacy, etc. The different application domains where recommender systems are being adopted these days include movie, music, books, news, tourism etc. The gap analysis conducted during literature review, focuses on improving the traditional recommendation approaches, the precise blend of existing approaches with different types of information, modeling of user profiles and recommended items, standardization of non-standard evaluation techniques etc. Application/Improvements: This paper also throws some light on certain application fields such as television, research grants, restaurant, job search, etc. that need to grab the attention of scientific and research communities to promote more research in those areas.},
	language = {en},
	number = {47},
	urldate = {2023-01-27},
	journal = {Indian Journal of Science and Technology},
	author = {Kumar, Balraj and Sharma, Neeraj},
	month = dec,
	year = {2016},
	file = {Kumar and Sharma - 2016 - Approaches, Issues and Challenges in Recommender S.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WRPB4J7S\\Kumar and Sharma - 2016 - Approaches, Issues and Challenges in Recommender S.pdf:application/pdf},
}

@book{aggarwalRecommenderSystems2016a,
	address = {Cham},
	title = {Recommender {Systems}},
	isbn = {978-3-319-29657-9 978-3-319-29659-3},
	url = {http://link.springer.com/10.1007/978-3-319-29659-3},
	language = {en},
	urldate = {2023-01-27},
	publisher = {Springer International Publishing},
	author = {Aggarwal, Charu C.},
	year = {2016},
	doi = {10.1007/978-3-319-29659-3},
	file = {Aggarwal_2016_Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\B4PCNKRJ\\Aggarwal_2016_Recommender Systems.pdf:application/pdf},
}

@article{tranRecommenderSystemsHealthcare2021,
	title = {Recommender systems in the healthcare domain: state-of-the-art and research issues},
	volume = {57},
	issn = {0925-9902, 1573-7675},
	shorttitle = {Recommender systems in the healthcare domain},
	url = {https://link.springer.com/10.1007/s10844-020-00633-6},
	doi = {10.1007/s10844-020-00633-6},
	abstract = {Nowadays, a vast amount of clinical data scattered across different sites on the Internet hinders users from finding helpful information for their well-being improvement. Besides, the overload of medical information (e.g., on drugs, medical tests, and treatment suggestions) have brought many difficulties to medical professionals in making patient-oriented decisions. These issues raise the need to apply recommender systems in the healthcare domain to help both, end-users and medical professionals, make more efficient and accurate healthrelated decisions. In this article, we provide a systematic overview of existing research on healthcare recommender systems. Different from existing related overview papers, our article provides insights into recommendation scenarios and recommendation approaches. Examples thereof are food recommendation, drug recommendation, health status prediction, healthcare service recommendation, and healthcare professional recommendation. Additionally, we develop working examples to give a deep understanding of recommendation algorithms. Finally, we discuss challenges concerning the development of healthcare recommender systems in the future.},
	language = {en},
	number = {1},
	urldate = {2023-01-27},
	journal = {Journal of Intelligent Information Systems},
	author = {Tran, Thi Ngoc Trang and Felfernig, Alexander and Trattner, Christoph and Holzinger, Andreas},
	month = aug,
	year = {2021},
	pages = {171--201},
	file = {Tran et al. - 2021 - Recommender systems in the healthcare domain stat.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BYZFSU9L\\Tran et al. - 2021 - Recommender systems in the healthcare domain stat.pdf:application/pdf},
}

@article{zhangArtificialIntelligenceRecommender2021,
	title = {Artificial intelligence in recommender systems},
	volume = {7},
	issn = {2199-4536, 2198-6053},
	url = {http://link.springer.com/10.1007/s40747-020-00212-w},
	doi = {10.1007/s40747-020-00212-w},
	abstract = {Recommender systems provide personalized service support to users by learning their previous behaviors and predicting their current preferences for particular products. Artiﬁcial intelligence (AI), particularly computational intelligence and machine learning methods and algorithms, has been naturally applied in the development of recommender systems to improve prediction accuracy and solve data sparsity and cold start problems. This position paper systematically discusses the basic methodologies and prevailing techniques in recommender systems and how AI can effectively improve the technological development and application of recommender systems. The paper not only reviews cutting-edge theoretical and practical contributions, but also identiﬁes current research issues and indicates new research directions. It carefully surveys various issues related to recommender systems that use AI, and also reviews the improvements made to these systems through the use of such AI approaches as fuzzy techniques, transfer learning, genetic algorithms, evolutionary algorithms, neural networks and deep learning, and active learning. The observations in this paper will directly support researchers and professionals to better understand current developments and new directions in the ﬁeld of recommender systems using AI.},
	language = {en},
	number = {1},
	urldate = {2023-01-27},
	journal = {Complex \& Intelligent Systems},
	author = {Zhang, Qian and Lu, Jie and Jin, Yaochu},
	month = feb,
	year = {2021},
	pages = {439--457},
	file = {Zhang et al. - 2021 - Artificial intelligence in recommender systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SPAJ2QYN\\Zhang et al. - 2021 - Artificial intelligence in recommender systems.pdf:application/pdf},
}

@article{sarkerDeepLearningComprehensive2021,
	title = {Deep {Learning}: {A} {Comprehensive} {Overview} on {Techniques}, {Taxonomy}, {Applications} and {Research} {Directions}},
	volume = {2},
	issn = {2661-8907},
	shorttitle = {Deep {Learning}},
	doi = {10.1007/s42979-021-00815-1},
	abstract = {Deep learning (DL), a branch of machine learning (ML) and artificial intelligence (AI) is nowadays considered as a core technology of today's Fourth Industrial Revolution (4IR or Industry 4.0). Due to its learning capabilities from data, DL technology originated from artificial neural network (ANN), has become a hot topic in the context of computing, and is widely applied in various application areas like healthcare, visual recognition, text analytics, cybersecurity, and many more. However, building an appropriate DL model is a challenging task, due to the dynamic nature and variations in real-world problems and data. Moreover, the lack of core understanding turns DL methods into black-box machines that hamper development at the standard level. This article presents a structured and comprehensive view on DL techniques including a taxonomy considering various types of real-world tasks like supervised or unsupervised. In our taxonomy, we take into account deep networks for supervised or discriminative learning, unsupervised or generative learning as well as hybrid learning and relevant others. We also summarize real-world application areas where deep learning techniques can be used. Finally, we point out ten potential aspects for future generation DL modeling with research directions. Overall, this article aims to draw a big picture on DL modeling that can be used as a reference guide for both academia and industry professionals.},
	language = {eng},
	number = {6},
	journal = {SN computer science},
	author = {Sarker, Iqbal H.},
	year = {2021},
	pmid = {34426802},
	pmcid = {PMC8372231},
	keywords = {Artificial intelligence, Deep learning, Artificial neural network, Discriminative learning, Generative learning, Hybrid learning, Intelligent systems},
	pages = {420},
	file = {Sarker_2021_Deep Learning.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NWB7AZ4T\\Sarker_2021_Deep Learning.pdf:application/pdf},
}

@article{hadsellEmbracingChangeContinual2020,
	title = {Embracing {Change}: {Continual} {Learning} in {Deep} {Neural} {Networks}},
	volume = {24},
	issn = {13646613},
	shorttitle = {Embracing {Change}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661320302199},
	doi = {10.1016/j.tics.2020.09.004},
	language = {en},
	number = {12},
	urldate = {2023-01-27},
	journal = {Trends in Cognitive Sciences},
	author = {Hadsell, Raia and Rao, Dushyant and Rusu, Andrei A. and Pascanu, Razvan},
	month = dec,
	year = {2020},
	pages = {1028--1040},
	file = {Hadsell et al_2020_Embracing Change.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3PGUGREW\\Hadsell et al_2020_Embracing Change.pdf:application/pdf},
}

@inproceedings{faggioliEfficientSimilarityBased2018,
	address = {Vancouver BC Canada},
	title = {Efficient {Similarity} {Based} {Methods} {For} {The} {Playlist} {Continuation} {Task}},
	isbn = {978-1-4503-6586-4},
	url = {https://dl.acm.org/doi/10.1145/3267471.3267486},
	doi = {10.1145/3267471.3267486},
	language = {en},
	urldate = {2023-01-30},
	booktitle = {Proceedings of the {ACM} {Recommender} {Systems} {Challenge} 2018},
	publisher = {ACM},
	author = {Faggioli, Guglielmo and Polato, Mirko and Aiolli, Fabio},
	month = oct,
	year = {2018},
	pages = {1--6},
}

@article{symeonidisCollaborativeRecommenderSystems2008,
	title = {Collaborative recommender systems: {Combining} effectiveness and efficiency},
	volume = {34},
	issn = {09574174},
	shorttitle = {Collaborative recommender systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417407001959},
	doi = {10.1016/j.eswa.2007.05.013},
	abstract = {Recommender systems base their operation on past user ratings over a collection of items, for instance, books, CDs, etc. Collaborative ﬁltering (CF) is a successful recommendation technique that confronts the ‘‘information overload’’ problem. Memory-based algorithms recommend according to the preferences of nearest neighbors, and model-based algorithms recommend by ﬁrst developing a model of user ratings. In this paper, we bring to surface factors that aﬀect CF process in order to identify existing false beliefs. In terms of accuracy, by being able to view the ‘‘big picture’’, we propose new approaches that substantially improve the performance of CF algorithms. For instance, we obtain more than 40\% increase in precision in comparison to widely-used CF algorithms. In terms of eﬃciency, we propose a model-based approach based on latent semantic indexing (LSI), that reduces execution times at least 50\% than the classic CF algorithms.},
	language = {en},
	number = {4},
	urldate = {2023-01-30},
	journal = {Expert Systems with Applications},
	author = {Symeonidis, P and Nanopoulos, A and Papadopoulos, A and Manolopoulos, Y},
	month = may,
	year = {2008},
	pages = {2995--3013},
	file = {Symeonidis et al. - 2008 - Collaborative recommender systems Combining effec.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BYXGAENF\\Symeonidis et al. - 2008 - Collaborative recommender systems Combining effec.pdf:application/pdf},
}

@inproceedings{jeunenEfficientSimilarityComputation2019,
	address = {Copenhagen Denmark},
	title = {Efficient similarity computation for collaborative filtering in dynamic environments},
	isbn = {978-1-4503-6243-6},
	url = {https://dl.acm.org/doi/10.1145/3298689.3347017},
	doi = {10.1145/3298689.3347017},
	abstract = {The problem of computing all pairwise similarities in a large collection of vectors is a well-known and common data mining task. As the number and dimensionality of these vectors keeps increasing, however, currently existing approaches are often unable to meet the strict efficiency requirements imposed by the environments they need to perform in. Real-time neighbourhood-based collaborative filtering (CF) is one example of such an environment in which performance is critical. In this work, we present a novel algorithm for efficient and exact similarity computation between sparse, high-dimensional vectors. Our approach exploits the sparsity that is inherent to implicit feedback data-streams, entailing significant gains compared to other methods. Furthermore, as our model learns incrementally, it is naturally suited for dynamic real-time CF environments. We propose a MapReduce-inspired parallellisation procedure along with our method, and show how even more speed-up can be achieved. Additionally, in many real-world systems, many items are actually not recommendable at any given time, due to recency, stock, seasonality, or enforced business rules. We exploit this fact to further improve the computational efficiency of our approach. Experimental evaluation on both real-world and publicly available datasets shows that our approach scales up to millions of processed user-item interactions per second, and well advances the state-of-the-art.},
	language = {en},
	urldate = {2023-01-30},
	booktitle = {Proceedings of the 13th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Jeunen, Olivier and Verstrepen, Koen and Goethals, Bart},
	month = sep,
	year = {2019},
	pages = {251--259},
	file = {Jeunen et al. - 2019 - Efficient similarity computation for collaborative.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HH5L5ZUA\\Jeunen et al. - 2019 - Efficient similarity computation for collaborative.pdf:application/pdf},
}

@article{altintasAmazonRecommenderSystem,
	title = {Amazon {Recommender} {System}},
	abstract = {Recommender systems are algorithms that suggest relevant items to users based on data. They generate large revenue for the modern e-commerce industry. 35\% of Amazon web sales were generated through their recommended items [source: McKinsey]. This study aims to construct an apparel recommender system for Amazon users through user-rating history, product images and product title text. Multiple deep learning models were built on both readily-available and engineered datasets resulting in a multi-step recommender system. Tableau and a web app are used to display results, along with evaluation measurements.},
	language = {en},
	author = {Altintas, Ilkay and McAuley, Julian},
	file = {Altintas and McAuley - Amazon Recommender System.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\H44NSPIM\\Altintas and McAuley - Amazon Recommender System.pdf:application/pdf},
}

@article{liuMultiinfoFusionBased2019,
	title = {Multi-info {Fusion} {Based} {Video} {Recommendation} {System}},
	volume = {1229},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1229/1/012010},
	doi = {10.1088/1742-6596/1229/1/012010},
	abstract = {The great progress in recommendation system help users discover more interesting items that satisfy their appetites. Considering the video recommendation is an increasing popular sub-field of recommendation, but the traditional recommendation techniques such as Collaborative Filtering and Content-based model simply exploit one information source that limits its performance. In this paper, we proposed a Multi-info fusion based recommendation system which integrates several different information sources to comprehensively model the similarity between videos. The information sources including the common user-item rating data and video’s textual content that consists of video’s genres and textual description. Experimental results on a public dataset show that the proposed system is of high quality and achieves significant improvements over the traditional Collaborative Filtering techniques.},
	language = {en},
	number = {1},
	urldate = {2023-01-30},
	journal = {Journal of Physics: Conference Series},
	author = {Liu, Yang and Zhang, Guijuan and Jin, Xiaoning and Yuan, Haifeng},
	month = may,
	year = {2019},
	pages = {012010},
	file = {Liu et al. - 2019 - Multi-info Fusion Based Video Recommendation Syste.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6ENE9LYP\\Liu et al. - 2019 - Multi-info Fusion Based Video Recommendation Syste.pdf:application/pdf},
}

@article{ResearchSimilarityMeasure2019,
	title = {A {Research} on {Similarity} {Measure} to {Identify} {Effective} {Similar} {Users} in {Recommender} {Systems}},
	volume = {8},
	issn = {2278-3075},
	url = {https://www.ijitee.org/wp-content/uploads/papers/v8i9S2/I11720789S219.pdf},
	doi = {10.35940/ijitee.I1172.0789S219},
	abstract = {In recent years there is a drastic increase in information over the internet. Users get confused to find out best product on the internet of one’s interest. Here the recommender system helps to filter the information and gives relevant recommendations to users so that the user community can find the item(s) of their interest from huge collection of available data. But filtering information from the users reviews given for various items seems to be a challenging task for recommending the user interested things. In general similarities between the users are considered for recommendations in collaborative filtering techniques. This paper describes a new collaborative filtering technique called Adaptive Similarity Measure Model [ASMM] to identify similarity between users for the selection of unseen items. Out of all the available items most similarities would be sorted out by ASMM for recommendation which varies from user to user.},
	language = {en},
	number = {9S2},
	urldate = {2023-01-30},
	journal = {International Journal of Innovative Technology and Exploring Engineering},
	month = aug,
	year = {2019},
	pages = {834--840},
	file = {2019 - A Research on Similarity Measure to Identify Effec.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\I4FXHC2B\\2019 - A Research on Similarity Measure to Identify Effec.pdf:application/pdf},
}

@article{shamjiManagementDegenerativeLumbar2015,
	title = {Management of {Degenerative} {Lumbar} {Spinal} {Stenosis} in the {Elderly}},
	volume = {77},
	issn = {0148-396X},
	url = {https://journals.lww.com/00006123-201510001-00007},
	doi = {10.1227/NEU.0000000000000943},
	abstract = {BACKGROUND: Lumbar spinal stenosis can cause symptomatic neurogenic claudication alongside radicular pain and weakness. In appropriately selected patients, surgical intervention has been demonstrated to provide for improvement in pain, disability, and quality of life. This systematic review sought to define the utility and safety of such decompression with or without arthrodesis in the management of symptomatic lumbar spinal stenosis for elderly patients older than 65 years of age.
METHODS: A systematic review was conducted using MEDLINE for literature published through December 2014. The first question focused on the effectiveness of lumbar spinal surgery for symptomatic lumbar spinal stenosis in elderly patients (over age 65 y). The second question focused on safety of surgical intervention on this elderly population with emphasis on perioperative complication rates.
RESULTS: Review of 11 studies reveals that the majority of elderly patients exhibit significant symptomatic improvement, with overall benefits observed for pain (change visual analog scale 4.4 points) and disability (change Oswestry Disability Index 23 points). Review of 11 studies reveals that perioperative complications were infrequent and acceptable with pooled estimates of mortality (0.5\%), inadvertent durotomy (5\%), and wound infection (2\%). Outcomes seem less favorable with greater complication rates among patients with diabetes or obesity.
CONCLUSION: Based on largely low-quality, retrospective evidence, we recommend that elderly patients should not be excluded from surgical intervention for symptomatic lumbar spinal stenosis.},
	language = {en},
	number = {Supplement 1},
	urldate = {2023-01-31},
	journal = {Neurosurgery},
	author = {Shamji, Mohammed F. and Mroz, Thomas and Hsu, Wellington and Chutkan, Norman},
	month = oct,
	year = {2015},
	pages = {S68--S74},
	file = {Shamji et al. - 2015 - Management of Degenerative Lumbar Spinal Stenosis .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SGWZ3YGP\\Shamji et al. - 2015 - Management of Degenerative Lumbar Spinal Stenosis .pdf:application/pdf},
}

@article{deyoTrendsMajorMedical2010,
	title = {Trends, {Major} {Medical} {Complications}, and {Charges} {Associated} {With} {Surgery} for {Lumbar} {Spinal} {Stenosis} in {Older} {Adults}},
	volume = {303},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2010.338},
	doi = {10.1001/jama.2010.338},
	language = {en},
	number = {13},
	urldate = {2023-01-31},
	journal = {JAMA},
	author = {Deyo, Richard A.},
	month = apr,
	year = {2010},
	pages = {1259},
	file = {Deyo_2010_Trends, Major Medical Complications, and Charges Associated With Surgery for.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XZN4IHTD\\Deyo_2010_Trends, Major Medical Complications, and Charges Associated With Surgery for.pdf:application/pdf},
}

@article{weinsteinSurgicalNonsurgicalTherapy2008,
	title = {Surgical versus {Nonsurgical} {Therapy} for {Lumbar} {Spinal} {Stenosis}},
	volume = {358},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/abs/10.1056/NEJMoa0707136},
	doi = {10.1056/NEJMoa0707136},
	language = {en},
	number = {8},
	urldate = {2023-01-31},
	journal = {New England Journal of Medicine},
	author = {Weinstein, James N. and Tosteson, Tor D. and Lurie, Jon D. and Tosteson, Anna N.A. and Blood, Emily and Hanscom, Brett and Herkowitz, Harry and Cammisa, Frank and Albert, Todd and Boden, Scott D. and Hilibrand, Alan and Goldberg, Harley and Berven, Sigurd and An, Howard},
	month = feb,
	year = {2008},
	pages = {794--810},
	file = {Weinstein et al_2008_Surgical versus Nonsurgical Therapy for Lumbar Spinal Stenosis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8VBWF2KF\\Weinstein et al_2008_Surgical versus Nonsurgical Therapy for Lumbar Spinal Stenosis.pdf:application/pdf},
}

@article{zainaSurgicalNonsurgicalTreatment2016,
	title = {Surgical {Versus} {Nonsurgical} {Treatment} for {Lumbar} {Spinal} {Stenosis}},
	volume = {41},
	issn = {0362-2436, 1528-1159},
	url = {https://journals.lww.com/00007632-201607150-00009},
	doi = {10.1097/BRS.0000000000001635},
	language = {en},
	number = {14},
	urldate = {2023-01-31},
	journal = {Spine},
	author = {Zaina, Fabio and Tomkins-Lane, Christy and Carragee, Eugene and Negrini, Stefano},
	month = jul,
	year = {2016},
	pages = {E857--E868},
	file = {Zaina et al_2016_Surgical Versus Nonsurgical Treatment for Lumbar Spinal Stenosis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3V6CCV32\\Zaina et al_2016_Surgical Versus Nonsurgical Treatment for Lumbar Spinal Stenosis.pdf:application/pdf},
}

@article{tsengChoiceSurgeryConservative2020,
	title = {Choice between {Surgery} and {Conservative} {Treatment} for {Patients} with {Lumbar} {Spinal} {Stenosis}: {Predicting} {Results} through {Data} {Mining} {Technology}},
	volume = {10},
	issn = {2076-3417},
	shorttitle = {Choice between {Surgery} and {Conservative} {Treatment} for {Patients} with {Lumbar} {Spinal} {Stenosis}},
	url = {https://www.mdpi.com/2076-3417/10/18/6406},
	doi = {10.3390/app10186406},
	abstract = {Currently, patients with lumbar spinal stenosis (LSS) have two treatment options: nonoperative conservative treatment and surgical treatment. Because surgery is invasive, patients often prefer conservative treatment as their first choice to avoid risks from surgery. However, the effectiveness of nonoperative conservative treatment for patients with LSS may be lower than expected because of individual differences. Rules to determine whether patients with LSS should undergo surgical treatment merits exploration. In addition, without a decision-making system to assist patients undergoing conservative treatment to decide whether to undergo surgical treatment, medical professionals may encounter difficulty in providing the best treatment advice. This study collected medical record data and magnetic resonance imaging diagnostic data from patients with LSS, analyzed and consolidated the data through data mining techniques, identified crucial factors and rules affecting the final outcome the patients with LSS who opted for conservative treatment and ultimately underwent surgical treatment, and, finally, established an effective prediction model. This study applied logistic regression (LGR) and decision tree algorithms to extract the crucial features and combined them with back propagation neural networks (BPNN) and support vector machines (SVM) to establish the prediction model. The crucial features obtained are as follows: reduction of the intervertebral disc height, age, blood pressure difference, leg pain, gender, etc. Among the models predicting whether patients with LSS ultimately underwent surgical treatment, the model combining LGR and the decision tree for feature selection with a BPNN has a testing accuracy rate of 94.87\%, sensitivity of 0.9, specificity of 1, and area under the receiver operating characteristic curve of 0.952. Adopting these data mining techniques to predict whether patients with LSS who opted for conservative treatment ultimately underwent surgical treatment may assist medical professionals in reaching a treatment decision and provide clearer treatment. This may effectively mitigate disease progression, aid the goals of precision medicine, and ultimately enhance the quality of health care.},
	language = {en},
	number = {18},
	urldate = {2023-01-31},
	journal = {Applied Sciences},
	author = {Tseng, Li-Ping and Pei, Yu-Cheng and Chen, Yen-Sheng and Hou, Tung-Hsu and Ou, Yang-Kun},
	month = sep,
	year = {2020},
	pages = {6406},
	file = {Tseng et al_2020_Choice between Surgery and Conservative Treatment for Patients with Lumbar.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XRKSXYPQ\\Tseng et al_2020_Choice between Surgery and Conservative Treatment for Patients with Lumbar.pdf:application/pdf},
}

@article{zainaSurgicalNonsurgicalTreatment2016a,
	title = {Surgical versus non-surgical treatment for lumbar spinal stenosis},
	volume = {2016},
	issn = {14651858},
	url = {http://doi.wiley.com/10.1002/14651858.CD010264.pub2},
	doi = {10.1002/14651858.CD010264.pub2},
	language = {en},
	number = {1},
	urldate = {2023-01-31},
	journal = {Cochrane Database of Systematic Reviews},
	author = {Zaina, Fabio and Tomkins-Lane, Christy and Carragee, Eugene and Negrini, Stefano},
	editor = {{Cochrane Back and Neck Group}},
	month = jan,
	year = {2016},
	file = {Zaina et al_2016_Surgical versus non-surgical treatment for lumbar spinal stenosis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YNHVWYP7\\Zaina et al_2016_Surgical versus non-surgical treatment for lumbar spinal stenosis.pdf:application/pdf},
}

@article{atlasLongTermOutcomesSurgical2005,
	title = {Long-{Term} {Outcomes} of {Surgical} and {Nonsurgical} {Management} of {Lumbar} {Spinal} {Stenosis}: 8 to 10 {Year} {Results} from the {Maine} {Lumbar} {Spine} {Study}:},
	volume = {30},
	issn = {0362-2436},
	shorttitle = {Long-{Term} {Outcomes} of {Surgical} and {Nonsurgical} {Management} of {Lumbar} {Spinal} {Stenosis}},
	url = {http://journals.lww.com/00007632-200504150-00017},
	doi = {10.1097/01.brs.0000158953.57966.c0},
	language = {en},
	number = {8},
	urldate = {2023-01-31},
	journal = {Spine},
	author = {Atlas, Steven J. and Keller, Robert B. and Wu, Yen A. and Deyo, Richard A. and Singer, Daniel E.},
	month = apr,
	year = {2005},
	pages = {936--943},
}

@article{yin100MostCited2022,
	title = {The 100 {Most} {Cited} {Articles} on {Lumbar} {Spinal} {Stenosis}: {A} {Bibliometric} {Analysis}},
	volume = {12},
	issn = {2192-5682, 2192-5690},
	shorttitle = {The 100 {Most} {Cited} {Articles} on {Lumbar} {Spinal} {Stenosis}},
	url = {http://journals.sagepub.com/doi/10.1177/2192568220952074},
	doi = {10.1177/2192568220952074},
	abstract = {Study Design:
              Bibliometric analysis.
            
            
              Objective:
              With the increasing literature of spine surgery, some pioneering research studies have had a significant impact on the field of lumbar spinal stenosis (LSS). The objective of the authors was to identify and analyze the most frequently cited 100 articles in this field.
            
            
              Methods:
              Web of Science was searched to identify 100 top-cited articles related to LSS from 2000 to 2019. Articles on the final list were filtered based on their titles and abstracts. The following information were recorded and analyzed with bibliometric method: article title, first author, year of publication, journal of publication, total number of citations, country, institution, and study topic.
            
            
              Results:
              The citation count for final articles on the list ranged from 71 to 2162, with a mean number of 207.7. The journal Spine contributed the maximum number of articles (37), followed by European Spine Journal (9) and Pain Physician (8). There were collectively 80 first authors contributing to articles on the final list. Twelve authors were represented multiple times in the top 100 articles. The most prolific years were 2008 and 2009, each had 11 articles published. With regard to country and region of origin, most articles were from the United States (58). The most cited article was published in Spine in 2000 by Fairbank and Pynsent, who discussed the role of the Oswestry Disability Index as an evaluation standard in spinal disorders, including LSS.
            
            
              Conclusion:
              The current study analyzed the 100 most cited articles on LSS. It no doubt developed a useful resource with detailed information for many, particularly orthopedic and neurosurgery physicians who want to assimilate research focus and advance of LSS within a relatively short period. Researchers may benefit from emphasis on citation count while citing and evaluating articles and realize the deficiencies when high-level articles appear.},
	language = {en},
	number = {3},
	urldate = {2023-01-31},
	journal = {Global Spine Journal},
	author = {Yin, Mengchen and Xu, Chongqing and Mo, Wen},
	month = apr,
	year = {2022},
	pages = {381--391},
	file = {Yin et al_2022_The 100 Most Cited Articles on Lumbar Spinal Stenosis.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\XT4J4NDT\\Yin et al_2022_The 100 Most Cited Articles on Lumbar Spinal Stenosis.pdf:application/pdf},
}

@article{weisslerRoleMachineLearning2021,
	title = {The role of machine learning in clinical research: transforming the future of evidence generation},
	volume = {22},
	issn = {1745-6215},
	shorttitle = {The role of machine learning in clinical research},
	url = {https://trialsjournal.biomedcentral.com/articles/10.1186/s13063-021-05489-x},
	doi = {10.1186/s13063-021-05489-x},
	abstract = {Abstract
            
              Background
              Interest in the application of machine learning (ML) to the design, conduct, and analysis of clinical trials has grown, but the evidence base for such applications has not been surveyed. This manuscript reviews the proceedings of a multi-stakeholder conference to discuss the current and future state of ML for clinical research. Key areas of clinical trial methodology in which ML holds particular promise and priority areas for further investigation are presented alongside a narrative review of evidence supporting the use of ML across the clinical trial spectrum.
            
            
              Results
              Conference attendees included stakeholders, such as biomedical and ML researchers, representatives from the US Food and Drug Administration (FDA), artificial intelligence technology and data analytics companies, non-profit organizations, patient advocacy groups, and pharmaceutical companies. ML contributions to clinical research were highlighted in the pre-trial phase, cohort selection and participant management, and data collection and analysis. A particular focus was paid to the operational and philosophical barriers to ML in clinical research. Peer-reviewed evidence was noted to be lacking in several areas.
            
            
              Conclusions
              ML holds great promise for improving the efficiency and quality of clinical research, but substantial barriers remain, the surmounting of which will require addressing significant gaps in evidence.},
	language = {en},
	number = {1},
	urldate = {2023-02-03},
	journal = {Trials},
	author = {Weissler, E. Hope and Naumann, Tristan and Andersson, Tomas and Ranganath, Rajesh and Elemento, Olivier and Luo, Yuan and Freitag, Daniel F. and Benoit, James and Hughes, Michael C. and Khan, Faisal and Slater, Paul and Shameer, Khader and Roe, Matthew and Hutchison, Emmette and Kollins, Scott H. and Broedl, Uli and Meng, Zhaoling and Wong, Jennifer L. and Curtis, Lesley and Huang, Erich and Ghassemi, Marzyeh},
	month = dec,
	year = {2021},
	pages = {537},
	file = {Weissler et al_2021_The role of machine learning in clinical research.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HXEZ6JLW\\Weissler et al_2021_The role of machine learning in clinical research.pdf:application/pdf},
}

@article{weichEffectAnxiolyticHypnotic2014,
	title = {Effect of anxiolytic and hypnotic drug prescriptions on mortality hazards: retrospective cohort study},
	volume = {348},
	issn = {1756-1833},
	shorttitle = {Effect of anxiolytic and hypnotic drug prescriptions on mortality hazards},
	doi = {10.1136/bmj.g1996},
	abstract = {OBJECTIVE: To test the hypothesis that people taking anxiolytic and hypnotic drugs are at increased risk of premature mortality, using primary care prescription records and after adjusting for a wide range of potential confounders.
DESIGN: Retrospective cohort study.
SETTING: 273 UK primary care practices contributing data to the General Practice Research Database.
PARTICIPANTS: 34,727 patients aged 16 years and older first prescribed anxiolytic or hypnotic drugs, or both, between 1998 and 2001, and 69,418 patients with no prescriptions for such drugs (controls) matched by age, sex, and practice. Patients were followed-up for a mean of 7.6 years (range 0.1-13.4 years).
MAIN OUTCOME: All cause mortality ascertained from practice records.
RESULTS: Physical and psychiatric comorbidities and prescribing of non-study drugs were significantly more prevalent among those prescribed study drugs than among controls. The age adjusted hazard ratio for mortality during the whole follow-up period for use of any study drug in the first year after recruitment was 3.46 (95\% confidence interval 3.34 to 3.59) and 3.32 (3.19 to 3.45) after adjusting for other potential confounders. Dose-response associations were found for all three classes of study drugs (benzodiazepines, Z drugs (zaleplon, zolpidem, and zopiclone), and other drugs). After excluding deaths in the first year, there were approximately four excess deaths linked to drug use per 100 people followed for an average of 7.6 years after their first prescription.
CONCLUSIONS: In this large cohort of patients attending UK primary care, anxiolytic and hypnotic drugs were associated with significantly increased risk of mortality over a seven year period, after adjusting for a range of potential confounders. As with all observational findings, however, these results are prone to bias arising from unmeasured and residual confounding.},
	language = {eng},
	journal = {BMJ (Clinical research ed.)},
	author = {Weich, Scott and Pearce, Hannah Louise and Croft, Peter and Singh, Swaran and Crome, Ilana and Bashford, James and Frisher, Martin},
	month = mar,
	year = {2014},
	pmid = {24647164},
	pmcid = {PMC3959619},
	keywords = {Humans, Female, Male, Middle Aged, Mortality, Anti-Anxiety Agents, Benzodiazepines, Hypnotics and Sedatives, Prescription Drugs, Proportional Hazards Models, Retrospective Studies, United Kingdom},
	pages = {g1996},
	file = {Weich et al_2014_Effect of anxiolytic and hypnotic drug prescriptions on mortality hazards.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\PK9HB3FT\\Weich et al_2014_Effect of anxiolytic and hypnotic drug prescriptions on mortality hazards.pdf:application/pdf},
}

@article{billiotidegageBenzodiazepineUseRisk2014,
	title = {Benzodiazepine use and risk of {Alzheimer}'s disease: case-control study},
	volume = {349},
	issn = {1756-1833},
	shorttitle = {Benzodiazepine use and risk of {Alzheimer}'s disease},
	doi = {10.1136/bmj.g5205},
	abstract = {OBJECTIVES: To investigate the relation between the risk of Alzheimer's disease and exposure to benzodiazepines started at least five years before, considering both the dose-response relation and prodromes (anxiety, depression, insomnia) possibly linked with treatment.
DESIGN: Case-control study.
SETTING: The Quebec health insurance program database (RAMQ).
PARTICIPANTS: 1796 people with a first diagnosis of Alzheimer's disease and followed up for at least six years before were matched with 7184 controls on sex, age group, and duration of follow-up. Both groups were randomly sampled from older people (age {\textgreater}66) living in the community in 2000-09.
MAIN OUTCOME MEASURE: The association between Alzheimer's disease and benzodiazepine use started at least five years before diagnosis was assessed by using multivariable conditional logistic regression. Ever exposure to benzodiazepines was first considered and then categorised according to the cumulative dose expressed as prescribed daily doses (1-90, 91-180, {\textgreater}180) and the drug elimination half life.
RESULTS: Benzodiazepine ever use was associated with an increased risk of Alzheimer's disease (adjusted odds ratio 1.51, 95\% confidence interval 1.36 to 1.69; further adjustment on anxiety, depression, and insomnia did not markedly alter this result: 1.43, 1.28 to 1.60). No association was found for a cumulative dose {\textless}91 prescribed daily doses. The strength of association increased with exposure density (1.32 (1.01 to 1.74) for 91-180 prescribed daily doses and 1.84 (1.62 to 2.08) for {\textgreater}180 prescribed daily doses) and with the drug half life (1.43 (1.27 to 1.61) for short acting drugs and 1.70 (1.46 to 1.98) for long acting ones).
CONCLUSION: Benzodiazepine use is associated with an increased risk of Alzheimer's disease. The stronger association observed for long term exposures reinforces the suspicion of a possible direct association, even if benzodiazepine use might also be an early marker of a condition associated with an increased risk of dementia. Unwarranted long term use of these drugs should be considered as a public health concern.},
	language = {eng},
	journal = {BMJ (Clinical research ed.)},
	author = {Billioti de Gage, Sophie and Moride, Yola and Ducruet, Thierry and Kurth, Tobias and Verdoux, Hélène and Tournier, Marie and Pariente, Antoine and Bégaud, Bernard},
	month = sep,
	year = {2014},
	pmid = {25208536},
	pmcid = {PMC4159609},
	keywords = {Humans, Depressive Disorder, Female, Male, Anxiety Disorders, Benzodiazepines, Aged, Aged, 80 and over, Alzheimer Disease, Case-Control Studies, Quebec, Risk Factors, Sleep Initiation and Maintenance Disorders},
	pages = {g5205},
	file = {Billioti de Gage et al_2014_Benzodiazepine use and risk of Alzheimer's disease.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DSZJCI8X\\Billioti de Gage et al_2014_Benzodiazepine use and risk of Alzheimer's disease.pdf:application/pdf},
}

@article{grayBenzodiazepineUsePhysical2006,
	title = {Benzodiazepine use and physical disability in community-dwelling older adults},
	volume = {54},
	issn = {0002-8614},
	doi = {10.1111/j.1532-5415.2005.00571.x},
	abstract = {OBJECTIVES: To determine whether benzodiazepine use is associated with incident disability in mobility and activities of daily living (ADLs) in older individuals.
DESIGN: A prospective cohort study.
SETTING: Four sites of the Established Populations for Epidemiologic Studies of the Elderly.
PARTICIPANTS: This study included 9,093 subjects (aged {\textgreater} or =65) who were not disabled in mobility or ADLs at baseline.
MEASUREMENTS: Mobility disability was defined as inability to walk half a mile or climb one flight of stairs. ADL disability was defined as inability to perform one or more basic ADLs (bathing, eating, dressing, transferring from a bed to a chair, using the toilet, or walking across a small room). Trained interviewers assessed outcomes annually.
RESULTS: At baseline, 5.5\% of subjects reported benzodiazepine use. In multivariable models, benzodiazepine users were 1.23 times as likely as nonusers (95\% confidence interval (CI) = 1.09-1.39) to develop mobility disability and 1.28 times as likely (95\% CI = 1.09-1.52) to develop ADL disability. Risk for incident mobility was increased with short- (hazard ratio (HR) = 1.27, 95\% CI = 1.08-1.50) and long-acting benzodiazepines (HR = 1.20, 95\% CI = 1.03-1.39) and no use. Risk for ADL disability was greater with short- (HR = 1.58, 95\% CI = 1.25-2.01) but not long-acting (HR = 1.11, 95\% CI = 0.89-1.39) agents than for no use.
CONCLUSION: Older adults taking benzodiazepines have a greater risk for incident mobility and ADL disability. Use of short-acting agents does not appear to confer any safety benefits over long-acting agents.},
	language = {eng},
	number = {2},
	journal = {Journal of the American Geriatrics Society},
	author = {Gray, Shelly L. and LaCroix, Andrea Z. and Hanlon, Joseph T. and Penninx, Brenda W. J. H. and Blough, David K. and Leveille, Suzanne G. and Artz, Margaret B. and Guralnik, Jack M. and Buchner, Dave M.},
	month = feb,
	year = {2006},
	pmid = {16460372},
	pmcid = {PMC2365497},
	keywords = {Humans, United States, Female, Male, Surveys and Questionnaires, Treatment Outcome, Prospective Studies, Benzodiazepines, Aged, Activities of Daily Living, Anxiety, Disabled Persons, Follow-Up Studies, Incidence, Population Surveillance, Time Factors},
	pages = {224--230},
	file = {Gray et al_2006_Benzodiazepine use and physical disability in community-dwelling older adults.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\DFFW5QZ9\\Gray et al_2006_Benzodiazepine use and physical disability in community-dwelling older adults.pdf:application/pdf},
}

@article{madhusoodananSafetyBenzodiazepinesGeriatric2004,
	title = {Safety of benzodiazepines in the geriatric population},
	volume = {3},
	issn = {1474-0338, 1744-764X},
	url = {http://www.tandfonline.com/doi/full/10.1517/14740338.3.5.485},
	doi = {10.1517/14740338.3.5.485},
	language = {en},
	number = {5},
	urldate = {2023-02-22},
	journal = {Expert Opinion on Drug Safety},
	author = {Madhusoodanan, Subramoniam and Bogunovic, Olivera J},
	month = sep,
	year = {2004},
	pages = {485--493},
}

@inproceedings{schelterEfficientIncrementalCooccurrence2019,
	address = {Santa Cruz CA USA},
	title = {Efficient {Incremental} {Cooccurrence} {Analysis} for {Item}-{Based} {Collaborative} {Filtering}},
	isbn = {978-1-4503-6216-0},
	url = {https://dl.acm.org/doi/10.1145/3335783.3335784},
	doi = {10.1145/3335783.3335784},
	abstract = {Recommender systems are ubiquitous in the modern internet, where they help users find items they might like. A widely deployed recommendation approach is item-based collaborative filtering. This approach relies on analyzing large item cooccurrence matrices that denote how many users interacted with a pair of items. The potentially quadratic number of items to compare poses a scalability bottleneck in analyzing such item cooccurrences. Additionally, this problem intensifies in real world use cases with incrementally growing datasets, especially when the recommendation model is regularly recomputed from scratch. We highlight the connection between the growing cost of item-based recommendation and densification processes in common interaction datasets. Based on our findings, we propose an efficient incremental algorithm for itembased collaborative filtering based on cooccurrence analysis. This approach restricts the number of interactions to consider from ‘power users’ and ‘ubiquitous items’ to guarantee a provably constant amount of work per user-item interaction to process. We discuss efficient implementations of our algorithm on a single machine as well as on a distributed stream processing engine, and present an extensive experimental evaluation. Our results confirm the asymptotic benefits of the incremental approach. Furthermore, we find that our implementation is an order of magnitude faster than existing open source recommender libraries on many datasets, and at the same time scales to high dimensional datasets which these existing recommenders fail to process.},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}},
	publisher = {ACM},
	author = {Schelter, Sebastian and Celebi, Ufuk and Dunning, Ted},
	month = jul,
	year = {2019},
	pages = {61--72},
	file = {Schelter et al. - 2019 - Efficient Incremental Cooccurrence Analysis for It.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BQ6B95RZ\\Schelter et al. - 2019 - Efficient Incremental Cooccurrence Analysis for It.pdf:application/pdf},
}

@inproceedings{sachdevaSamplingCollaborativeFiltering2022,
	title = {On {Sampling} {Collaborative} {Filtering} {Datasets}},
	url = {http://arxiv.org/abs/2201.04768},
	doi = {10.1145/3488560.3498439},
	abstract = {We study the practical consequences of dataset sampling strategies on the ranking performance of recommendation algorithms. Recommender systems are generally trained and evaluated on samples of larger datasets. Samples are often taken in a naïve or ad-hoc fashion: e.g. by sampling a dataset randomly or by selecting users or items with many interactions. As we demonstrate, commonly-used data sampling schemes can have significant consequences on algorithm performance. Following this observation, this paper makes three main contributions: (1) characterizing the effect of sampling on algorithm performance, in terms of algorithm and dataset characteristics (e.g. sparsity characteristics, sequential dynamics, etc.); (2) designing SVP-CF, which is a data-specific sampling strategy, that aims to preserve the relative performance of models after sampling, and is especially suited to long-tailed interaction data; and (3) developing an oracle, Data-genie, which can suggest the sampling scheme that is most likely to preserve model performance for a given dataset. The main benefit of Data-genie is that it will allow recommender system practitioners to quickly prototype and compare various approaches, while remaining confident that algorithm performance will be preserved, once the algorithm is retrained and deployed on the complete data. Detailed experiments show that using Data-genie, we can discard upto 5× more data than any sampling strategy with the same level of performance.},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the {Fifteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	author = {Sachdeva, Noveen and Wu, Carole-Jean and McAuley, Julian},
	month = feb,
	year = {2022},
	note = {arXiv:2201.04768 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	pages = {842--850},
	file = {Sachdeva et al. - 2022 - On Sampling Collaborative Filtering Datasets.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UMPCRLWV\\Sachdeva et al. - 2022 - On Sampling Collaborative Filtering Datasets.pdf:application/pdf},
}

@article{marlinRecommenderSystemsMissing,
	title = {Recommender {Systems}: {Missing} {Data} and {Statistical} {Model} {Estimation}},
	abstract = {The goal of rating-based recommender systems is to make personalized predictions and recommendations for individual users by leveraging the preferences of a community of users with respect to a collection of items like songs or movies. Recommender systems are often based on intricate statistical models that are estimated from data sets containing a very high proportion of missing ratings. This work describes evidence of a basic incompatibility between the properties of recommender system data sets and the assumptions required for valid estimation and evaluation of statistical models in the presence of missing data. We discuss the implications of this problem and describe extended modelling and evaluation frameworks that attempt to circumvent it. We present prediction and ranking results showing that models developed and tested under these extended frameworks can signiﬁcantly outperform standard models.},
	language = {en},
	author = {Marlin, Benjamin M and Zemel, Richard S and Roweis, Sam T and Slaney, Malcolm},
	file = {Marlin et al. - Recommender Systems Missing Data and Statistical .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VUYJK9PF\\Marlin et al. - Recommender Systems Missing Data and Statistical .pdf:application/pdf},
}

@inproceedings{aggarwalHortingHatchesEgg1999,
	address = {San Diego California USA},
	title = {Horting hatches an egg: a new graph-theoretic approach to collaborative filtering},
	isbn = {978-1-58113-143-7},
	shorttitle = {Horting hatches an egg},
	url = {https://dl.acm.org/doi/10.1145/312129.312230},
	doi = {10.1145/312129.312230},
	abstract = {This paper introduces a new and novel approach to ratingbased collaborative ltering. The new technique is most appropriate for e-commerce merchants o ering one or more groups of relatively homogeneous items such as compact disks, videos, books, software and the like. In contrast with other known collaborative ltering techniques, the new algorithm is graph-theoretic, based on the twin new concepts of and horting predictability. As is demonstrated in this paper, the technique is fast, scalable, accurate, and requires only a modest learning curve. It makes use of a hierarchical classi cation scheme in order to introduce context into the rating process, and uses so-called creative links in order to nd surprising and atypical items to recommend, perhaps even items which cross the group boundaries. The new technique is one of the key engines of the Intelligent Recommendation Algorithm IRA project, now being developed at IBM Research. In addition to several other recommendation engines, IRA contains a situation analyzer to determine the most appropriate mix of engines for a particular e-commerce merchant, as well as an engine for optimizing the placement of advertisements.},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the fifth {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Aggarwal, Charu C. and Wolf, Joel L. and Wu, Kun-Lung and Yu, Philip S.},
	month = aug,
	year = {1999},
	pages = {201--212},
	file = {Aggarwal et al. - 1999 - Horting hatches an egg a new graph-theoretic appr.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\AECK8VB8\\Aggarwal et al. - 1999 - Horting hatches an egg a new graph-theoretic appr.pdf:application/pdf},
}

@misc{mengPMDOptimalTransportationbased2019,
	title = {{PMD}: {An} {Optimal} {Transportation}-based {User} {Distance} for {Recommender} {Systems}},
	shorttitle = {{PMD}},
	url = {http://arxiv.org/abs/1909.04239},
	abstract = {Collaborative filtering, a widely-used recommendation technique, predicts a user’s preference by aggregating the ratings from similar users. Traditional similarity measures utilize ratings of only co-rated items while computing similarity between a pair of users. As a result, these measures cannot fully utilize the rating information and are not suitable for real world sparse data. To solve these issues, we propose a novel user distance measure named Preference Mover’s Distance (PMD) which makes full use of all ratings made by each user. Our proposed PMD can properly measure the distance between a pair of users even if they have no co-rated items. We show that this measure can be cast as an instance of the Earth Mover’s Distance, a wellstudied transportation problem for which several highly efficient solvers have been developed. Experimental results show that PMD can help achieve superior recommendation accuracy than state-ofthe-art methods, especially when training data is very sparse.},
	language = {en},
	urldate = {2023-03-05},
	publisher = {arXiv},
	author = {Meng, Yitong and Dai, Xinyan and Yan, Xiao and Cheng, James and Liu, Weiwen and Liao, Benben and Guo, Jun and Chen, Guangyong},
	month = dec,
	year = {2019},
	note = {arXiv:1909.04239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Meng et al. - 2019 - PMD An Optimal Transportation-based User Distance.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VJEE2TC2\\Meng et al. - 2019 - PMD An Optimal Transportation-based User Distance.pdf:application/pdf},
}

@article{mobasherImprovingEffectivenessCollaborative,
	title = {Improving the {Eﬀectiveness} of {Collaborative} {Filtering} on {Anonymous} {Web} {Usage} {Data}},
	abstract = {Recommender systems based on collaborative ﬁltering usually require real-time comparison of users’ ratings on objects. In the context of Web personalization, particularly at the early stages of a visitor’s interaction with the site (i.e., before registration or authentication), recommender systems must rely on anonymous clickstream data. The lack of explicit user ratings and the shear amount of data in such a setting poses serious challenges to standard collaborative ﬁltering techniques in terms of scalability and performance. Oﬄine clustering of users transactions can be used to improve the scalability of collaborative ﬁltering, however, this is often at the cost of reduced recommendation accuracy. In this paper we study the impact of various preprocessing techniques applied to clickstream data, such as clustering, normalization, and signiﬁcance ﬁltering, on collaborative ﬁltering. Our experimental results, performed on real usage data, indicate that with proper data preparation, the clustering-based approach to collaborative ﬁltering can achieve dramatic improvements in terms of recommendation eﬀectiveness, while maintaining the computational advantage over the direct approaches such as the k-NearestNeighbor technique.},
	language = {en},
	author = {Mobasher, Bamshad and Dai, Honghua and Luo, Tao and Nakagawa, Miki},
	file = {Mobasher et al. - Improving the Eﬀectiveness of Collaborative Filter.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6WRIV2LI\\Mobasher et al. - Improving the Eﬀectiveness of Collaborative Filter.pdf:application/pdf},
}

@article{diAttenuatedNormalizedItemitem2022,
	title = {Attenuated and normalized item-item product network for sequential recommendation},
	volume = {8},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-867},
	doi = {10.7717/peerj-cs.867},
	abstract = {Sequential recommendation has become a research trending that exploits user’s recent behaviors for recommendation. The user-item interactions contain a sequential dependency that we need to capture to better recommend. Item-item Product (IIP), which models item co-occurrence, has shown good potential by characterizing the pairwise item relationships. Generally, recent behaviors have a greater impact on the current than long-term historical behaviors. And the decaying rate of influence around infrequent behaviors is fast. However, IIP ignores such a phenomenon when considering item-item relevance and leads to suboptimal performance. In this paper, we propose an attenuated IIP mechanism which is position-aware and decays the influence of historical items at an exponential rate. Besides, In order to make up for scenarios where the influence is not in a monotonous decline trend, we add another normalized IIP mechanism to complement the attenuated IIP mechanism. It also strengthen the model’s ability in discriminating favorite items under the sparse data condition by enlarging the gap of matching degree between items. Experiments conducted on five real-world datasets demonstrate that our proposed model achieves better performance than a set of state-of-the-art sequential recommendation models.},
	language = {en},
	urldate = {2023-03-05},
	journal = {PeerJ Computer Science},
	author = {Di, Weiqiang and Wu, Zhihao and Lin, Youfang},
	month = jan,
	year = {2022},
	pages = {e867},
	file = {Di et al. - 2022 - Attenuated and normalized item-item product networ.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GAQRKLWV\\Di et al. - 2022 - Attenuated and normalized item-item product networ.pdf:application/pdf},
}

@article{adomaviciusStabilityCollaborativeFiltering,
	title = {Stability of {Collaborative} {Filtering} {Recommendation} {Algorithms}},
	abstract = {The paper explores stability as a new measure of recommender systems performance. Stability is defined to measure the extent to which a recommendation algorithm provides predictions that are consistent with each other. Specifically, for a stable algorithm, adding some of the algorithm’s own predictions to the algorithm’s training data (for example, if these predictions were confirmed as accurate by users) would not invalidate or change the other predictions. While stability is an interesting theoretical property that can provide additional understanding about recommendation algorithms, we believe stability to be a desired practical property for recommender systems designers as well, because unstable recommendations can potentially decrease users’ trust in recommender systems and, as a result, reduce users’ acceptance of recommendations. In this paper, we also provide an extensive empirical evaluation of stability for six popular recommendation algorithms on four real-world datasets. Our results suggest that stability performance of individual recommendation algorithms is consistent across a variety of datasets and settings. In particular, we find that model-based recommendation algorithms consistently demonstrate higher stability than neighborhood-based collaborative filtering techniques. In addition, we perform a comprehensive empirical analysis of many important factors (e.g., the sparsity of original rating data, normalization of input data, the number of new incoming ratings, the distribution of incoming ratings, the distribution of evaluation data, etc.) and report the impact they have on recommendation stability. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Information Filtering, H.2.8.d [Information Technology and Systems]: Database Applications – Data Mining, I.2.6 [Artificial Intelligence]: Learning. General Terms: Algorithms, Measurement, Performance, Reliability.},
	language = {en},
	author = {Adomavicius, Gediminas and Zhang, Jingjing},
	file = {Adomavicius and Zhang - Stability of Collaborative Filtering Recommendatio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\TVH3BQ9U\\Adomavicius and Zhang - Stability of Collaborative Filtering Recommendatio.pdf:application/pdf},
}

@article{maNormalizingItemBasedCollaborative2017,
	title = {Normalizing {Item}-{Based} {Collaborative} {Filter} {Using} {Context}-{Aware} {Scaled} {Baseline} {Predictor}},
	volume = {2017},
	issn = {1024-123X, 1563-5147},
	url = {https://www.hindawi.com/journals/mpe/2017/6562371/},
	doi = {10.1155/2017/6562371},
	abstract = {Item-based collaborative filter algorithms play an important role in modern commercial recommendation systems (RSs). To improve the recommendation performance, normalization is always used as a basic component for the predictor models. Among a lot of normalizing methods, subtracting the baseline predictor (BLP) is the most popular one. However, the BLP uses a statistical constant without considering the context. We found that slightly scaling the different components of the BLP separately could dramatically improve the performance. This paper proposed some normalization methods based on the scaled baseline predictors according to different context information. The experimental results show that using context-aware scaled baseline predictor for normalization indeed gets better recommendation performance, including RMSE, MAE, precision, recall, and nDCG.},
	language = {en},
	urldate = {2023-03-05},
	journal = {Mathematical Problems in Engineering},
	author = {Ma, Wenming and Shi, Junfeng and Zhao, Ruidong},
	year = {2017},
	pages = {1--9},
	file = {Ma et al. - 2017 - Normalizing Item-Based Collaborative Filter Using .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\APSP2PPL\\Ma et al. - 2017 - Normalizing Item-Based Collaborative Filter Using .pdf:application/pdf},
}

@article{jannachConversationalRecommendationGrand2022,
	title = {Conversational recommendation: {A} grand {AI} challenge},
	volume = {43},
	issn = {0738-4602, 2371-9621},
	shorttitle = {Conversational recommendation},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/aaai.12059},
	doi = {10.1002/aaai.12059},
	abstract = {Animated avatars, which look and talk like humans, are iconic visions of the future of AI-powered systems. Through many sci-fi movies, we are acquainted with the idea of speaking to such virtual personalities as if they were humans. Today, we talk more and more to machines like Apple’s Siri, for example, to ask them for the weather forecast. However, when asked for recommendations, for example, for a restaurant to go to, the limitations of such devices quickly become obvious. They do not engage in a conversation to find out what we might prefer, they often do not provide explanations for what they recommend, and they may have difficulties remembering what was said 1 min earlier. Conversational recommender systems (CRS) promise to address these limitations. In this paper, we review existing approaches to building such systems, which developments we observe today, which challenges are still open and why the development of conversational recommenders represents one of the next grand challenges of AI.},
	language = {en},
	number = {2},
	urldate = {2023-03-05},
	journal = {AI Magazine},
	author = {Jannach, Dietmar and Chen, Li},
	month = jun,
	year = {2022},
	pages = {151--163},
	file = {Jannach and Chen - 2022 - Conversational recommendation A grand AI challeng.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\29CPSBNX\\Jannach and Chen - 2022 - Conversational recommendation A grand AI challeng.pdf:application/pdf},
}

@article{castellsOfflineRecommenderSystem2022,
	title = {Offline recommender system evaluation: {Challenges} and new directions},
	volume = {43},
	issn = {0738-4602, 2371-9621},
	shorttitle = {Offline recommender system evaluation},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/aaai.12051},
	doi = {10.1002/aaai.12051},
	abstract = {Offline evaluation is an essential complement to online experiments in the selection, improvement, tuning, and deployment of recommender systems. Offline methodologies for recommender system evaluation evolved from experimental practice in Machine Learning (ML) and Information Retrieval (IR). However, evaluating recommendations involves particularities that pose challenges to the assumptions upon which the ML and IR methodologies were developed. We recap and reflect on the development and current status of recommender system evaluation, providing an updated perspective. With a focus on offline evaluation, we review the adaptation of IR principles, procedures and metrics, and the implications of those techniques when applied to recommender systems. At the same time, we identify the singularities of recommendation that require different responses, or involve specific new needs. In addition, we provide an overview of important choices in the configuration of experiments that require particular care and understanding; discuss broader perspectives of evaluation such as recommendation value beyond accuracy; and survey open challenges such as experimental biases, and the cyclic dimension of recommendation.},
	language = {en},
	number = {2},
	urldate = {2023-03-05},
	journal = {AI Magazine},
	author = {Castells, Pablo and Moffat, Alistair},
	month = jun,
	year = {2022},
	pages = {225--238},
	file = {Castells and Moffat - 2022 - Offline recommender system evaluation Challenges .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8EWIK7XW\\Castells and Moffat - 2022 - Offline recommender system evaluation Challenges .pdf:application/pdf},
}

@article{afcharExplainabilityMusicRecommender2022,
	title = {Explainability in {Music} {Recommender} {Systems}},
	volume = {43},
	issn = {0738-4602, 2371-9621},
	url = {http://arxiv.org/abs/2201.10528},
	doi = {10.1002/aaai.12056},
	abstract = {The most common way to listen to recorded music nowadays is via streaming platforms which provide access to tens of millions of tracks. To assist users in effectively browsing these large catalogs, the integration of Music Recommender Systems (MRSs) has become essential. Current real-world MRSs are often quite complex and optimized for recommendation accuracy. They combine several building blocks based on collaborative filtering and content-based recommendation. This complexity can hinder the ability to explain recommendations to end users, which is particularly important for recommendations perceived as unexpected or inappropriate. While pure recommendation performance often correlates with user satisfaction, explainability has a positive impact on other factors such as trust and forgiveness, which are ultimately essential to maintain user loyalty. In this article, we discuss how explainability can be addressed in the context of MRSs. We provide perspectives on how explainability could improve music recommendation algorithms and enhance user experience. First, we review common dimensions and goals of recommenders’ explainability and in general of eXplainable Artificial Intelligence (XAI), and elaborate on the extent to which these apply – or need to be adapted – to the specific characteristics of music consumption and recommendation. Then, we show how explainability components can be integrated within a MRS and in what form explanations can be provided. Since the evaluation of explanation quality is decoupled from pure accuracy-based evaluation criteria, we also discuss requirements and strategies for evaluating explanations of music recommendations. Finally, we describe the current challenges for introducing explainability within a large-scale industrial music recommender system and provide research perspectives.},
	language = {en},
	number = {2},
	urldate = {2023-03-05},
	journal = {AI Magazine},
	author = {Afchar, Darius and Melchiorre, Alessandro B. and Schedl, Markus and Hennequin, Romain and Epure, Elena V. and Moussallam, Manuel},
	month = jun,
	year = {2022},
	note = {arXiv:2201.10528 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {190--208},
	file = {Afchar et al. - 2022 - Explainability in Music Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\4VP2AUKH\\Afchar et al. - 2022 - Explainability in Music Recommender Systems.pdf:application/pdf},
}

@article{jannachEscapingMcNamaraFallacy,
	title = {Escaping the {McNamara} {Fallacy}: {Toward} {More} {Impactful} {Recommender} {Systems} {Research}},
	language = {en},
	author = {Jannach, Dietmar and Bauer, Christine},
	file = {Jannach and Bauer - Escaping the McNamara Fallacy Toward More Impactf.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UAFATVHV\\Jannach and Bauer - Escaping the McNamara Fallacy Toward More Impactf.pdf:application/pdf},
}

@article{chenEfficientHeterogeneousCollaborative2020,
	title = {Efficient {Heterogeneous} {Collaborative} {Filtering} without {Negative} {Sampling} for {Recommendation}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5329},
	doi = {10.1609/aaai.v34i01.5329},
	abstract = {Recent studies on recommendation have largely focused on exploring state-of-the-art neural networks to improve the expressiveness of models, while typically apply the Negative Sampling (NS) strategy for efﬁcient learning. Despite effectiveness, two important issues have not been well-considered in existing methods: 1) NS suffers from dramatic ﬂuctuation, making sampling-based methods difﬁcult to achieve the optimal ranking performance in practical applications; 2) although heterogeneous feedback (e.g., view, click, and purchase) is widespread in many online systems, most existing methods leverage only one primary type of user feedback such as purchase. In this work, we propose a novel nonsampling transfer learning solution, named Efﬁcient Heterogeneous Collaborative Filtering (EHCF) for Top-N recommendation. It can not only model ﬁne-grained user-item relations, but also efﬁciently learn model parameters from the whole heterogeneous data (including all unlabeled data) with a rather low time complexity. Extensive experiments on three real-world datasets show that EHCF signiﬁcantly outperforms state-of-the-art recommendation methods in both traditional (single-behavior) and heterogeneous scenarios. Moreover, EHCF shows signiﬁcant improvements in training efﬁciency, making it more applicable to real-world large-scale systems. Our implementation has been released 1 to facilitate further developments on efﬁcient whole-data based neural methods.},
	language = {en},
	number = {01},
	urldate = {2023-03-05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Chong and Zhang, Min and Zhang, Yongfeng and Ma, Weizhi and Liu, Yiqun and Ma, Shaoping},
	month = apr,
	year = {2020},
	pages = {19--26},
	file = {Chen et al. - 2020 - Efficient Heterogeneous Collaborative Filtering wi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\M5MW253F\\Chen et al. - 2020 - Efficient Heterogeneous Collaborative Filtering wi.pdf:application/pdf},
}

@misc{zamaniAnalysisApproachesTaken2019,
	title = {An {Analysis} of {Approaches} {Taken} in the {ACM} {RecSys} {Challenge} 2018 for {Automatic} {Music} {Playlist} {Continuation}},
	url = {http://arxiv.org/abs/1810.01520},
	abstract = {The ACM Recommender Systems Challenge 2018 focused on the task of automatic music playlist continuation, which is a form of the more general task of sequential recommendation. Given a playlist of arbitrary length with some additional meta-data, the task was to recommend up to 500 tracks that fit the target characteristics of the original playlist. For the RecSys Challenge, Spotify released a dataset of one million user-generated playlists. Participants could compete in two tracks, i.e., main and creative tracks. Participants in the main track were only allowed to use the provided training set, however, in the creative track, the use of external public sources was permitted. In total, 113 teams submitted 1,228 runs to the main track; 33 teams submitted 239 runs to the creative track. The highest performing team in the main track achieved an R-precision of 0.2241, an NDCG of 0.3946, and an average number of recommended songs clicks of 1.784. In the creative track, an R-precision of 0.2233, an NDCG of 0.3939, and a click rate of 1.785 was obtained by the best team. This article provides an overview of the challenge, including motivation, task definition, dataset description, and evaluation. We further report and analyze the results obtained by the top performing teams in each track and explore the approaches taken by the winners. We finally summarize our key findings, discuss generalizability of approaches and results to domains other than music, and list the open avenues and possible future directions in the area of automatic playlist continuation.},
	language = {en},
	urldate = {2023-03-05},
	publisher = {arXiv},
	author = {Zamani, Hamed and Schedl, Markus and Lamere, Paul and Chen, Ching-Wei},
	month = aug,
	year = {2019},
	note = {arXiv:1810.01520 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval, Computer Science - Multimedia},
	file = {Zamani et al. - 2019 - An Analysis of Approaches Taken in the ACM RecSys .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y7R35CE2\\Zamani et al. - 2019 - An Analysis of Approaches Taken in the ACM RecSys .pdf:application/pdf},
}

@article{isinkayeRecommendationSystemsPrinciples2015,
	title = {Recommendation systems: {Principles}, methods and evaluation},
	volume = {16},
	issn = {11108665},
	shorttitle = {Recommendation systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1110866515000341},
	doi = {10.1016/j.eij.2015.06.005},
	abstract = {On the Internet, where the number of choices is overwhelming, there is need to ﬁlter, prioritize and efﬁciently deliver relevant information in order to alleviate the problem of information overload, which has created a potential problem to many Internet users. Recommender systems solve this problem by searching through large volume of dynamically generated information to provide users with personalized content and services. This paper explores the different characteristics and potentials of different prediction techniques in recommendation systems in order to serve as a compass for research and practice in the ﬁeld of recommendation systems.},
	language = {en},
	number = {3},
	urldate = {2023-03-05},
	journal = {Egyptian Informatics Journal},
	author = {Isinkaye, F.O. and Folajimi, Y.O. and Ojokoh, B.A.},
	month = nov,
	year = {2015},
	pages = {261--273},
	file = {Isinkaye et al. - 2015 - Recommendation systems Principles, methods and ev.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MLYI7ZD3\\Isinkaye et al. - 2015 - Recommendation systems Principles, methods and ev.pdf:application/pdf},
}

@article{duEBCREmpiricalBayes2021,
	title = {{EBCR}: {Empirical} {Bayes} concordance ratio method to improve similarity measurement in memory-based collaborative filtering},
	volume = {16},
	issn = {1932-6203},
	shorttitle = {{EBCR}},
	url = {https://dx.plos.org/10.1371/journal.pone.0255929},
	doi = {10.1371/journal.pone.0255929},
	abstract = {Recommender systems aim to provide users with a selection of items, based on predicting their preferences for items they have not yet rated, thus helping them filter out irrelevant ones from a large product catalogue. Collaborative filtering is a widely used mechanism to predict a particular user’s interest in a given item, based on feedback from neighbour users with similar tastes. The way the user’s neighbourhood is identified has a significant impact on prediction accuracy. Most methods estimate user proximity from ratings they assigned to co-rated items, regardless of their number. This paper introduces a similarity adjustment taking into account the number of co-ratings. The proposed method is based on a concordance ratio representing the probability that two users share the same taste for a new item. The probabilities are further adjusted by using the Empirical Bayes inference method before being used to weight similarities. The proposed approach improves existing similarity measures without increasing time complexity and the adjustment can be combined with all existing similarity measures. Experiments conducted on benchmark datasets confirmed that the proposed method systematically improved the recommender system’s prediction accuracy performance for all considered similarity measures.},
	language = {en},
	number = {8},
	urldate = {2023-03-05},
	journal = {PLOS ONE},
	author = {Du, Yu and Sutton-Charani, Nicolas and Ranwez, Sylvie and Ranwez, Vincent},
	editor = {Xie, Haoran},
	month = aug,
	year = {2021},
	pages = {e0255929},
	file = {Du et al. - 2021 - EBCR Empirical Bayes concordance ratio method to .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6GR736WJ\\Du et al. - 2021 - EBCR Empirical Bayes concordance ratio method to .pdf:application/pdf},
}

@article{jannachRecommenderSystemsTrends2022,
	title = {Recommender systems: {Trends} and frontiers},
	volume = {43},
	issn = {0738-4602, 2371-9621},
	shorttitle = {Recommender systems},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/aaai.12050},
	doi = {10.1002/aaai.12050},
	abstract = {Recommender systems (RSs), as used by Netflix, YouTube, or Amazon, are one of the most compelling success stories of AI. Enduring research activity in this area has led to a continuous improvement of recommendation techniques over the years, and today’s RSs are indeed often capable to make astonishingly good suggestions. With countless papers being published on the topic each year, one might think the recommendation problem is almost solved. In reality, however, the large majority of published works focuses on algorithmic improvements and relies on data-based evaluation procedures which may sometimes tell us little regarding the effects new algorithms will have in practice. This special issue contains a set of papers which address some of the open challenges and frontiers in RSs research: (i) building interactive and conversational solutions, (ii) understanding recommender systems as socio-technical systems with longitudinal dynamics, (iii) avoiding abstraction traps, and (iv) finding better ways of assessing the impact and value of recommender systems without field tests.},
	language = {en},
	number = {2},
	urldate = {2023-03-05},
	journal = {AI Magazine},
	author = {Jannach, Dietmar and Pu, Pearl and Ricci, Francesco and Zanker, Markus},
	month = jun,
	year = {2022},
	pages = {145--150},
	file = {Jannach et al. - 2022 - Recommender systems Trends and frontiers.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CMADCEBR\\Jannach et al. - 2022 - Recommender systems Trends and frontiers.pdf:application/pdf},
}

@article{ihemelanduStatisticalInferenceMissing,
	title = {Statistical {Inference}: {The} {Missing} {Piece} of {RecSys} {Experiment} {Reliability} {Discourse}},
	abstract = {This paper calls attention to the missing component of the recommender system evaluation process: Statistical Inference. There is active research in several components of the recommender system evaluation process: selecting baselines, standardizing benchmarks, and target item sampling. However, there has not yet been significant work on the role and use of statistical inference for analyzing recommender system evaluation results.},
	language = {en},
	author = {Ihemelandu, Ngozi and Ekstrand, Michael D},
	file = {Ihemelandu and Ekstrand - Statistical Inference The Missing Piece of RecSys.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\R6N5QCU6\\Ihemelandu and Ekstrand - Statistical Inference The Missing Piece of RecSys.pdf:application/pdf},
}

@article{mckeagThesisSubmittedPartial2021,
	title = {A thesis submitted in partial satisfaction of the requirements for the degree {Master} of {Science} in {Statistics}},
	language = {en},
	author = {McKeag, Candace Jennifer},
	year = {2021},
	file = {McKeag - 2021 - A thesis submitted in partial satisfaction of the .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\MHZF5IS7\\McKeag - 2021 - A thesis submitted in partial satisfaction of the .pdf:application/pdf},
}

@article{marlinRecommenderSystemsMissinga,
	title = {Recommender {Systems}: {Missing} {Data} and {Statistical} {Model} {Estimation}},
	abstract = {The goal of rating-based recommender systems is to make personalized predictions and recommendations for individual users by leveraging the preferences of a community of users with respect to a collection of items like songs or movies. Recommender systems are often based on intricate statistical models that are estimated from data sets containing a very high proportion of missing ratings. This work describes evidence of a basic incompatibility between the properties of recommender system data sets and the assumptions required for valid estimation and evaluation of statistical models in the presence of missing data. We discuss the implications of this problem and describe extended modelling and evaluation frameworks that attempt to circumvent it. We present prediction and ranking results showing that models developed and tested under these extended frameworks can signiﬁcantly outperform standard models.},
	language = {en},
	author = {Marlin, Benjamin M and Zemel, Richard S and Roweis, Sam T and Slaney, Malcolm},
	file = {Marlin et al. - Recommender Systems Missing Data and Statistical .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Y8C3EK8L\\Marlin et al. - Recommender Systems Missing Data and Statistical .pdf:application/pdf},
}

@article{marlinRecommenderSystemsMissingb,
	title = {Recommender {Systems}: {Missing} {Data} and {Statistical} {Model} {Estimation}},
	abstract = {The goal of rating-based recommender systems is to make personalized predictions and recommendations for individual users by leveraging the preferences of a community of users with respect to a collection of items like songs or movies. Recommender systems are often based on intricate statistical models that are estimated from data sets containing a very high proportion of missing ratings. This work describes evidence of a basic incompatibility between the properties of recommender system data sets and the assumptions required for valid estimation and evaluation of statistical models in the presence of missing data. We discuss the implications of this problem and describe extended modelling and evaluation frameworks that attempt to circumvent it. We present prediction and ranking results showing that models developed and tested under these extended frameworks can signiﬁcantly outperform standard models.},
	language = {en},
	author = {Marlin, Benjamin M and Zemel, Richard S and Roweis, Sam T and Slaney, Malcolm},
	file = {Marlin et al. - Recommender Systems Missing Data and Statistical .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3MFKWRLU\\Marlin et al. - Recommender Systems Missing Data and Statistical .pdf:application/pdf},
}

@article{sunEstimatingProbabilitiesRecommendation2012,
	title = {Estimating probabilities in recommendation systems: {Probabilities} in {Recommendation} {Systems}},
	volume = {61},
	issn = {00359254},
	shorttitle = {Estimating probabilities in recommendation systems},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2011.01027.x},
	doi = {10.1111/j.1467-9876.2011.01027.x},
	abstract = {Modeling ranked data is an essential component in a number of important applications including recommendation systems and websearch. In many cases, judges omit preference among unobserved items and between unobserved and observed items. This case of analyzing incomplete rankings is very important from a practical perspective and yet has not been fully studied due to considerable computational diﬃculties. We show how to avoid such computational diﬃculties and eﬃciently construct a non-parametric model for rankings with missing items. We demonstrate our approach and show how it applies in the context of collaborative ﬁltering.},
	language = {en},
	number = {3},
	urldate = {2023-03-05},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Sun, Mingxuan and Lebanon, Guy and Kidwell, Paul},
	month = may,
	year = {2012},
	pages = {471--492},
	file = {Sun et al. - 2012 - Estimating probabilities in recommendation systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\CWD7IHJH\\Sun et al. - 2012 - Estimating probabilities in recommendation systems.pdf:application/pdf},
}

@article{yaoReviewOptimalSubsampling2021,
	title = {A {Review} on {Optimal} {Subsampling} {Methods} for {Massive} {Datasets}},
	issn = {1680-743X, 1683-8602},
	url = {https://jds-online.org/doi/10.6339/21-JDS999},
	doi = {10.6339/21-JDS999},
	abstract = {Subsampling is an eﬀective way to deal with big data problems and many subsampling approaches have been proposed for diﬀerent models, such as leverage sampling for linear regression models and local case control sampling for logistic regression models. In this article, we focus on optimal subsampling methods, which draw samples according to optimal subsampling probabilities formulated by minimizing some function of the asymptotic distribution. The optimal subsampling methods have been investigated to include logistic regression models, softmax regression models, generalized linear models, quantile regression models, and quasi-likelihood estimation. Real data examples are provided to show how optimal subsampling methods are applied.},
	language = {en},
	urldate = {2023-03-05},
	journal = {Journal of Data Science},
	author = {Yao, Yaqiong and Wang, HaiYing},
	year = {2021},
	pages = {151--172},
	file = {Yao and Wang - 2021 - A Review on Optimal Subsampling Methods for Massiv.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\T4EI7XTV\\Yao and Wang - 2021 - A Review on Optimal Subsampling Methods for Massiv.pdf:application/pdf},
}

@article{maryDataDrivenRecommenderSystems,
	title = {Data-{Driven} {Recommender} {Systems}},
	language = {en},
	author = {Mary, Jérémie},
	file = {Mary - Data-Driven Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ZPWG9UL8\\Mary - Data-Driven Recommender Systems.pdf:application/pdf},
}

@article{poudelOptimalDependencePerformance2022,
	title = {Optimal {Dependence} of {Performance} and {Efficiency} of {Collaborative} {Filtering} on {Random} {Stratified} {Subsampling}},
	volume = {5},
	issn = {2096-0654},
	url = {https://ieeexplore.ieee.org/document/9793360/},
	doi = {10.26599/BDMA.2021.9020032},
	abstract = {Dropping fractions of users or items judiciously can reduce the computational cost of Collaborative Filtering (CF) algorithms. The effect of this subsampling on the computing time and accuracy of CF is not fully understood, and clear guidelines for selecting optimal or even appropriate subsampling levels are not available. In this paper, we present a Density-based Random Stratiﬁed Subsampling using Clustering (DRSC) algorithm in which the desired Fraction of Users Dropped (FUD) and Fraction of Items Dropped (FID) are speciﬁed, and the overall density during subsampling is maintained. Subsequently, we develop simple models of the Training Time Improvement (TTI) and the Accuracy Loss (AL) as functions of FUD and FID, based on extensive simulations of seven standard CF algorithms as applied to various primary matrices from MovieLens, Yahoo Music Rating, and Amazon Automotive data. Simulations show that both TTI and a scaled AL are bi-linear in FID and FUD for all seven methods. The TTI linear regression of a CF method appears to be same for all datasets. Extensive simulations illustrate that TTI can be estimated reliably with FUD and FID only, but AL requires considering additional dataset characteristics. The derived models are then used to optimize the levels of subsampling addressing the tradeoff between TTI and AL. A simple sub-optimal approximation was found, in which the optimal AL is proportional to the optimal Training Time Reduction Factor (TTRF) for higher values of TTRF, and the optimal subsampling levels, like optimal FID/(1–FID), are proportional to the square root of TTRF.},
	language = {en},
	number = {3},
	urldate = {2023-03-05},
	journal = {Big Data Mining and Analytics},
	author = {Poudel, Samin and Bikdash, Marwan},
	month = sep,
	year = {2022},
	pages = {192--205},
	file = {Poudel and Bikdash - 2022 - Optimal Dependence of Performance and Efficiency o.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\93IFSHAD\\Poudel and Bikdash - 2022 - Optimal Dependence of Performance and Efficiency o.pdf:application/pdf},
}

@article{kammounGenerativeAdversarialNetworks2022a,
	title = {Generative {Adversarial} {Networks} for face generation: {A} survey},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Generative {Adversarial} {Networks} for face generation},
	url = {https://dl.acm.org/doi/10.1145/1122445.1122456},
	doi = {10.1145/1122445.1122456},
	abstract = {Deep learning models in large-scale machine learning systems are often continuously trained with enormous data from production environments. The sheer volume of streaming training data poses a significant challenge to real-time training subsystems and ad-hoc sampling is the standard practice. Our key insight is that these deployed ML systems continuously perform forward passes on data instances during inference, but ad-hoc sampling does not take advantage of this substantial computational effort. Therefore, we propose to record a constant amount of information per instance from these forward passes. The extra information measurably improves the selection of which data instances should participate in forward and backward passes. A novel optimization framework is proposed to analyze this problem and we provide an efficient approximation algorithm under the framework of minibatch SGD as a practical solution. We also demonstrate the effectiveness of our framework and algorithm on several large-scale classification and regression tasks, when compared with competitive baselines widely used in industry.},
	language = {en},
	urldate = {2023-03-05},
	journal = {ACM Computing Surveys},
	author = {Kammoun, Amina and Slama, Rim and Tabia, Hedi and Ouni, Tarek and Abid, Mohmed},
	month = mar,
	year = {2022},
	pages = {1122445.1122456},
	file = {Kammoun et al. - 2022 - Generative Adversarial Networks for face generatio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3DQIRQAC\\Kammoun et al. - 2022 - Generative Adversarial Networks for face generatio.pdf:application/pdf},
}

@inproceedings{sachdevaSamplingCollaborativeFiltering2022a,
	title = {On {Sampling} {Collaborative} {Filtering} {Datasets}},
	url = {http://arxiv.org/abs/2201.04768},
	doi = {10.1145/3488560.3498439},
	abstract = {We study the practical consequences of dataset sampling strategies on the ranking performance of recommendation algorithms. Recommender systems are generally trained and evaluated on samples of larger datasets. Samples are often taken in a naïve or ad-hoc fashion: e.g. by sampling a dataset randomly or by selecting users or items with many interactions. As we demonstrate, commonly-used data sampling schemes can have significant consequences on algorithm performance. Following this observation, this paper makes three main contributions: (1) characterizing the effect of sampling on algorithm performance, in terms of algorithm and dataset characteristics (e.g. sparsity characteristics, sequential dynamics, etc.); (2) designing SVP-CF, which is a data-specific sampling strategy, that aims to preserve the relative performance of models after sampling, and is especially suited to long-tailed interaction data; and (3) developing an oracle, Data-genie, which can suggest the sampling scheme that is most likely to preserve model performance for a given dataset. The main benefit of Data-genie is that it will allow recommender system practitioners to quickly prototype and compare various approaches, while remaining confident that algorithm performance will be preserved, once the algorithm is retrained and deployed on the complete data. Detailed experiments show that using Data-genie, we can discard upto 5× more data than any sampling strategy with the same level of performance.},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the {Fifteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	author = {Sachdeva, Noveen and Wu, Carole-Jean and McAuley, Julian},
	month = feb,
	year = {2022},
	note = {arXiv:2201.04768 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	pages = {842--850},
	file = {Sachdeva et al. - 2022 - On Sampling Collaborative Filtering Datasets.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JRICTJ6X\\Sachdeva et al. - 2022 - On Sampling Collaborative Filtering Datasets.pdf:application/pdf},
}

@article{schaferCollaborativeFilteringRecommender,
	title = {9 {Collaborative} {Filtering} {Recommender} {Systems}},
	abstract = {One of the potent personalization technologies powering the adaptive web is collaborative filtering. Collaborative filtering (CF) is the process of filtering or evaluating items through the opinions of other people. CF technology brings together the opinions of large interconnected communities on the web, supporting filtering of substantial quantities of data. In this chapter we introduce the core concepts of collaborative filtering, its primary uses for users of the adaptive web, the theory and practice of CF algorithms, and design decisions regarding rating systems and acquisition of ratings. We also discuss how to evaluate CF systems, and the evolution of rich interaction interfaces. We close the chapter with discussions of the challenges of privacy particular to a CF recommendation service and important open research questions in the field.},
	language = {en},
	author = {Schafer, J Ben and Frankowski, Dan and Herlocker, Jon and Sen, Shilad},
	file = {Schafer et al. - 9 Collaborative Filtering Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\92ZWSYVB\\Schafer et al. - 9 Collaborative Filtering Recommender Systems.pdf:application/pdf},
}

@article{sachdevaDataCentricApproachesRecommendation,
	title = {Data-{Centric} {Approaches} to {Recommendation}},
	language = {en},
	author = {Sachdeva, Noveen},
	file = {Sachdeva - Data-Centric Approaches to Recommendation.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YJ83DA28\\Sachdeva - Data-Centric Approaches to Recommendation.pdf:application/pdf},
}

@article{muSurveyRecommenderSystems2018,
	title = {A {Survey} of {Recommender} {Systems} {Based} on {Deep} {Learning}},
	volume = {6},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8529185/},
	doi = {10.1109/ACCESS.2018.2880197},
	abstract = {In recent years, deep learning’s revolutionary advances in speech recognition, image analysis, and natural language processing have gained signiﬁcant attention. Deep learning technology has become a hotspot research ﬁeld in the artiﬁcial intelligence and has been applied into recommender system. In contrast to traditional recommendation models, deep learning is able to effectively capture the non-linear and non-trivial user-item relationships and enables the codiﬁcation of more complex abstractions as data representations in the higher layers. In this paper, we provide a comprehensive review of the related research contents of deep learning-based recommender systems. First, we introduce the basic terminologies and the background concepts of recommender systems and deep learning technology. Second, we describe the main current research on deep learning-based recommender systems. Third, we provide the possible research directions of deep learning-based recommender systems in the future. Finally, concludes this paper.},
	language = {en},
	urldate = {2023-03-05},
	journal = {IEEE Access},
	author = {Mu, Ruihui},
	year = {2018},
	pages = {69009--69022},
	file = {Mu - 2018 - A Survey of Recommender Systems Based on Deep Lear.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\7DABS8SP\\Mu - 2018 - A Survey of Recommender Systems Based on Deep Lear.pdf:application/pdf},
}

@inproceedings{umyarovImprovingCollaborativeFiltering2008,
	address = {Pisa, Italy},
	title = {Improving {Collaborative} {Filtering} {Recommendations} {Using} {External} {Data}},
	isbn = {978-0-7695-3502-9},
	url = {http://ieeexplore.ieee.org/document/4781157/},
	doi = {10.1109/ICDM.2008.44},
	abstract = {This paper describes an approach for incorporating externally speciﬁed aggregate ratings information into certain types of collaborative ﬁltering (CF) methods. For a statistical model-based CF approach, we formally showed that this additional aggregated information provides more accurate recommendations of individual items to individual users. Furthermore, theoretical insights gained from the analysis of this model-based method suggested a way to incorporate aggregate information into the heuristic itembased CF method. Both the model-based and the heuristic item-based CF methods were empirically tested on several datasets, and the experiments uniformly conﬁrmed that the aggregate rating information indeed improves CF recommendations. These results also show the power of theory by demonstrating how the insights gained from theoretical developments can shed light on proper selection of good heuristic methods. We also showed the way to introduce scalability and parallelization into the estimation procedure and reported the running time for steps of the estimation procedure for large datasets.},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {2008 {Eighth} {IEEE} {International} {Conference} on {Data} {Mining}},
	publisher = {IEEE},
	author = {Umyarov, Akhmed and Tuzhilin, Alexander},
	month = dec,
	year = {2008},
	pages = {618--627},
	file = {Umyarov and Tuzhilin - 2008 - Improving Collaborative Filtering Recommendations .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VAELB3FE\\Umyarov and Tuzhilin - 2008 - Improving Collaborative Filtering Recommendations .pdf:application/pdf},
}

@incollection{amatriainDataMiningMethods2011,
	address = {Boston, MA},
	title = {Data {Mining} {Methods} for {Recommender} {Systems}},
	isbn = {978-0-387-85819-7 978-0-387-85820-3},
	url = {https://link.springer.com/10.1007/978-0-387-85820-3_2},
	abstract = {In this chapter, we give an overview of the main Data Mining techniques used in the context of Recommender Systems. We ﬁrst describe common preprocessing methods such as sampling or dimensionality reduction. Next, we review the most important classiﬁcation techniques, including Bayesian Networks and Support Vector Machines. We describe the k-means clustering algorithm and discuss several alternatives. We also present association rules and related algorithms for an efﬁcient training process. In addition to introducing these techniques, we survey their uses in Recommender Systems and present cases where they have been successfully applied.},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Recommender {Systems} {Handbook}},
	publisher = {Springer US},
	author = {Amatriain, Xavier and Jaimes*, Alejandro and Oliver, Nuria and Pujol, Josep M.},
	editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
	year = {2011},
	doi = {10.1007/978-0-387-85820-3_2},
	pages = {39--71},
	file = {Amatriain et al. - 2011 - Data Mining Methods for Recommender Systems.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\BKV9GGEW\\Amatriain et al. - 2011 - Data Mining Methods for Recommender Systems.pdf:application/pdf},
}

@article{niyigenaEfficientPairwiseDocument2015,
	title = {Efficient {Pairwise} {Document} {Similarity} {Computation} in {Big} {Datasets}},
	volume = {8},
	issn = {20054270, 20054270},
	url = {http://article.nadiapub.com/IJDTA/vol8_no4/7.pdf},
	doi = {10.14257/ijdta.2015.8.4.07},
	abstract = {Document similarity is a common task to a variety of problems such as clustering, unsupervised learning and text retrieval. It has been seen that document with the very similar content provides little or no new information to the user. This work tackles this problem focusing on detecting near duplicates documents in large corpora. In this paper, we are presenting a new method to compute pairwise document similarity in a corpus which will reduce the time execution and save space execution resources. Our method group shingles of all documents of a corpus in a relation, with an advantage of efficiently manage up to millions of records and ease counting and aggregating. Three algorithms are introduced to reduce the candidates shingles to be compared: one creates the relation of shingles to be considered, the second one creates the set of triples and the third one gives the similarity of documents by efficiently counting the shared shingles between documents. The experiment results show that our method reduces the number of candidates pairs to be compared from which reduce also the execution time and space compared with existing algorithms which consider the computation of all pairs candidates.},
	language = {en},
	number = {4},
	urldate = {2023-03-05},
	journal = {International Journal of Database Theory and Application},
	author = {Niyigena, Papias and Zuping, Zhang and Li, Weiqi and Long, Jun},
	month = aug,
	year = {2015},
	pages = {59--70},
	file = {Niyigena et al. - 2015 - Efficient Pairwise Document Similarity Computation.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6F784SBR\\Niyigena et al. - 2015 - Efficient Pairwise Document Similarity Computation.pdf:application/pdf},
}

@inproceedings{liuAugmentingSequentialRecommendation2021,
	title = {Augmenting {Sequential} {Recommendation} with {Pseudo}-{Prior} {Items} via {Reversely} {Pre}-training {Transformer}},
	url = {http://arxiv.org/abs/2105.00522},
	doi = {10.1145/3404835.3463036},
	abstract = {Sequential Recommendation characterizes the evolving patterns by modeling item sequences chronologically. The essential target of it is to capture the item transition correlations. The recent developments of transformer inspire the community to design effective sequence encoders, e.g., SASRec and BERT4Rec. However, we observe that these transformer-based models suffer from the cold-start issue, i.e., performing poorly for short sequences. Therefore, we propose to augment short sequences while still preserving original sequential correlations. We introduce a new framework for Augmenting Sequential Recommendation with Pseudo-prior items (ASReP). We firstly pre-train a transformer with sequences in a reverse direction to predict prior items. Then, we use this transformer to generate fabricated historical items at the beginning of short sequences. Finally, we fine-tune the transformer using these augmented sequences from the time order to predict the next item. Experiments on two real-world datasets verify the effectiveness of ASReP. The code is available on https://github.com/DyGRec/ASReP.},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Liu, Zhiwei and Fan, Ziwei and Wang, Yu and Yu, Philip S.},
	month = jul,
	year = {2021},
	note = {arXiv:2105.00522 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	pages = {1608--1612},
	file = {Liu et al. - 2021 - Augmenting Sequential Recommendation with Pseudo-P.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\W5GA6C8P\\Liu et al. - 2021 - Augmenting Sequential Recommendation with Pseudo-P.pdf:application/pdf},
}

@inproceedings{palLSHBasedProbabilisticPruning2017,
	address = {Chicago IL USA},
	title = {{LSH}-{Based} {Probabilistic} {Pruning} of {Inverted} {Indices} for {Sets} and {Ranked} {Lists}},
	isbn = {978-1-4503-4983-3},
	url = {https://dl.acm.org/doi/10.1145/3068839.3068845},
	doi = {10.1145/3068839.3068845},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the 20th {International} {Workshop} on the {Web} and {Databases}},
	publisher = {ACM},
	author = {Pal, Koninika and Michel, Sebastian},
	month = may,
	year = {2017},
	pages = {23--28},
	file = {Pal and Michel - 2017 - LSH-Based Probabilistic Pruning of Inverted Indice.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\KGH4MZJC\\Pal and Michel - 2017 - LSH-Based Probabilistic Pruning of Inverted Indice.pdf:application/pdf},
}

@inproceedings{linBruteForceIndexed2009,
	address = {Boston MA USA},
	title = {Brute force and indexed approaches to pairwise document similarity comparisons with {MapReduce}},
	isbn = {978-1-60558-483-6},
	url = {https://dl.acm.org/doi/10.1145/1571941.1571970},
	doi = {10.1145/1571941.1571970},
	abstract = {This paper explores the problem of computing pairwise similarity on document collections, focusing on the application of “more like this” queries in the life sciences domain. Three MapReduce algorithms are introduced: one based on brute force, a second where the problem is treated as large-scale ad hoc retrieval, and a third based on the Cartesian product of postings lists. Each algorithm supports one or more approximations that trade eﬀectiveness for eﬃciency, the characteristics of which are studied experimentally. Results show that the brute force algorithm is the most eﬃcient of the three when exact similarity is desired. However, the other two algorithms support approximations that yield large efﬁciency gains without signiﬁcant loss of eﬀectiveness.},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the 32nd international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {ACM},
	author = {Lin, Jimmy},
	month = jul,
	year = {2009},
	pages = {155--162},
	file = {Lin - 2009 - Brute force and indexed approaches to pairwise doc.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\QGHFFMRP\\Lin - 2009 - Brute force and indexed approaches to pairwise doc.pdf:application/pdf},
}

@article{jallouliDesigningRecommenderSystem2017,
	title = {Designing {Recommender} {System}: {Conceptual} {Framework} and {Practical} {Implementation}},
	volume = {112},
	issn = {18770509},
	shorttitle = {Designing {Recommender} {System}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050917315909},
	doi = {10.1016/j.procs.2017.08.195},
	language = {en},
	urldate = {2023-03-05},
	journal = {Procedia Computer Science},
	author = {Jallouli, Maryam and Lajmi, Sonia and Amous, Ikram},
	year = {2017},
	pages = {1701--1710},
	file = {Jallouli et al. - 2017 - Designing Recommender System Conceptual Framework.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\IZWN6B26\\Jallouli et al. - 2017 - Designing Recommender System Conceptual Framework.pdf:application/pdf},
}

@inproceedings{liangFactorizationMeetsItem2016,
	address = {Boston Massachusetts USA},
	title = {Factorization {Meets} the {Item} {Embedding}: {Regularizing} {Matrix} {Factorization} with {Item} {Co}-occurrence},
	isbn = {978-1-4503-4035-9},
	shorttitle = {Factorization {Meets} the {Item} {Embedding}},
	url = {https://dl.acm.org/doi/10.1145/2959100.2959182},
	doi = {10.1145/2959100.2959182},
	abstract = {Matrix factorization (MF) models and their extensions are standard in modern recommender systems. MF models decompose the observed user-item interaction matrix into user and item latent factors. In this paper, we propose a cofactorization model, CoFactor, which jointly decomposes the user-item interaction matrix and the item-item co-occurrence matrix with shared item latent factors. For each pair of items, the co-occurrence matrix encodes the number of users that have consumed both items. CoFactor is inspired by the recent success of word embedding models (e.g., word2vec) which can be interpreted as factorizing the word co-occurrence matrix. We show that this model signiﬁcantly improves the performance over MF models on several datasets with little additional computational overhead. We provide qualitative results that explain how CoFactor improves the quality of the inferred factors and characterize the circumstances where it provides the most signiﬁcant improvements.},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the 10th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Liang, Dawen and Altosaar, Jaan and Charlin, Laurent and Blei, David M.},
	month = sep,
	year = {2016},
	pages = {59--66},
	file = {Liang et al. - 2016 - Factorization Meets the Item Embedding Regularizi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\LA8Q3VR2\\Liang et al. - 2016 - Factorization Meets the Item Embedding Regularizi.pdf:application/pdf},
}

@inproceedings{patilInvertedIndexesPhrases2011,
	address = {Beijing China},
	title = {Inverted indexes for phrases and strings},
	isbn = {978-1-4503-0757-4},
	url = {https://dl.acm.org/doi/10.1145/2009916.2009992},
	doi = {10.1145/2009916.2009992},
	abstract = {Inverted indexes are the most fundamental and widely used data structures in information retrieval. For each unique word occurring in a document collection, the inverted index stores a list of the documents in which this word occurs. Compression techniques are often applied to further reduce the space requirement of these lists. However, the index has a shortcoming, in that only predeﬁned pattern queries can be supported eﬃciently. In terms of string documents where word boundaries are undeﬁned, if we have to index all the substrings of a given document, then the storage quickly becomes quadratic in the data size. Also, if we want to apply the same type of indexes for querying phrases or sequence of words, then the inverted index will end up storing redundant information. In this paper, we show the ﬁrst set of inverted indexes which work naturally for strings as well as phrase searching. The central idea is to exclude document d in the inverted list of a string P if every occurrence of P in d is subsumed by another string of which P is a preﬁx. With this we show that our space utilization is close to the optimal. Techniques from succinct data structures are deployed to achieve compression while allowing fast access in terms of frequency and document id based retrieval. Compression and speed tradeoﬀs are evaluated for diﬀerent variants of the proposed index. For phrase searching, we show that our indexes compare favorably against a typical inverted index deploying position-wise intersections. We also show eﬃcient top-k based retrieval under relevance metrics like frequency and tf-idf.},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the 34th international {ACM} {SIGIR} conference on {Research} and development in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Patil, Manish and Thankachan, Sharma V. and Shah, Rahul and Hon, Wing-Kai and Vitter, Jeffrey Scott and Chandrasekaran, Sabrina},
	month = jul,
	year = {2011},
	pages = {555--564},
	file = {Patil et al. - 2011 - Inverted indexes for phrases and strings.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\44GWBPNH\\Patil et al. - 2011 - Inverted indexes for phrases and strings.pdf:application/pdf},
}

@misc{milchevskiSweetSpotInverted2015,
	title = {The {Sweet} {Spot} between {Inverted} {Indices} and {Metric}-{Space} {Indexing} for {Top}-{K}-{List} {Similarity} {Search}},
	url = {https://openproceedings.org/2015/conf/edbt/paper-65.pdf},
	abstract = {We consider the problem of processing similarity queries over a set of top-k rankings where the query ranking and the similarity threshold are provided at query time. Spearman’s Footrule distance is used to compute the similarity between rankings, considering how well rankings agree on the positions (ranks) of ranked items (i.e., the L1 distance). This setup allows the application of metric index structures such as M- or BK-trees and, alternatively, enables the use of traditional inverted indices for retrieving rankings that overlap (in items) with the query. Although both techniques are reasonable, they come with individual drawbacks for our speciﬁc problem. In this paper, we propose a hybrid indexing strategy, which blends inverted indices and metric space indexing, resulting in a structure that resembles both indexing methods with tunable emphasis on one or the other. To ﬁnd the sweet spot, we propose an assumption-lean but highly accurate (empirically validated) cost model through theoretical analysis. We further present optimizations to the inverted index component, for early termination and minimizing bookkeeping. The performance of the proposed algorithms, hybrid variants, and competitors is studied in a comprehensive evaluation using real-world benchmark data consisting of Web-search–result rankings and entity rankings based on Wikipedia.},
	language = {en},
	urldate = {2023-03-05},
	publisher = {OpenProceedings.org},
	author = {Milchevski, Evica and Anand, Avishek and Michel, Sebastian},
	year = {2015},
	doi = {10.5441/002/EDBT.2015.23},
	note = {Type: dataset},
	keywords = {Database Technology},
	file = {Milchevski et al. - 2015 - The Sweet Spot between Inverted Indices and Metric.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YCVLP377\\Milchevski et al. - 2015 - The Sweet Spot between Inverted Indices and Metric.pdf:application/pdf},
}

@article{zhangMultipleComplementaryInverted2019,
	title = {Multiple complementary inverted indexing based on multiple metrics},
	volume = {78},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-018-6439-x},
	doi = {10.1007/s11042-018-6439-x},
	abstract = {Inverted indexing based on vector quantization has been a popular technique in large scale information retrieval. With vector quantization based on a certain similarity metric, the sample space is partitioned into some voronoi cells, and samples in each cell are indexed by an inverted list. The nearest neighbors of a query are efficiently identified by looking up the cell where the query is located. To improve the recall, the sample space partitioning has been performed multiple times with different initializations of k-means to build multiple inverted indexes. While with the single similarity metric, e.g., Euclidean distance, high correlation may exist between multiple inverted indexes, which constrains the possible gain in recall. A new multiple inverted indexing method based on multiple sample space partitioning with multiple different similarity metrics is presented in this paper. Furthermore, several techniques for defining multiple metrics are investigated empirically. Experiments are conducted on 3 representative datasets, million-scale SIFT and GIST feature sets and a deep-learning-produced feature set, to properly evaluate the effectiveness of the proposed method. Experiment results show that the proposed method has competitive performance compared with the state-of-the-art inverted indexing methods in terms of recall and retrieval time, and the Latin-Hypercube weighting method can generate better diverse multiple metrics and get better gain in recall.},
	language = {en},
	number = {6},
	urldate = {2023-03-05},
	journal = {Multimedia Tools and Applications},
	author = {Zhang, Kai and Zhou, Wengang and Sun, Shaoyan and Li, Bin},
	month = mar,
	year = {2019},
	pages = {7727--7747},
	file = {Zhang et al. - 2019 - Multiple complementary inverted indexing based on .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\6C8VNEV4\\Zhang et al. - 2019 - Multiple complementary inverted indexing based on .pdf:application/pdf},
}

@inproceedings{palLSHBasedProbabilisticPruning2017a,
	address = {Chicago IL USA},
	title = {{LSH}-{Based} {Probabilistic} {Pruning} of {Inverted} {Indices} for {Sets} and {Ranked} {Lists}},
	isbn = {978-1-4503-4983-3},
	url = {https://dl.acm.org/doi/10.1145/3068839.3068845},
	doi = {10.1145/3068839.3068845},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the 20th {International} {Workshop} on the {Web} and {Databases}},
	publisher = {ACM},
	author = {Pal, Koninika and Michel, Sebastian},
	month = may,
	year = {2017},
	pages = {23--28},
	file = {Pal and Michel - 2017 - LSH-Based Probabilistic Pruning of Inverted Indice.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\HJ6WFSKJ\\Pal and Michel - 2017 - LSH-Based Probabilistic Pruning of Inverted Indice.pdf:application/pdf},
}

@article{noffsingerPredictiveAccuracyRecommender,
	title = {Predictive {Accuracy} of {Recommender} {Algorithms}},
	language = {en},
	author = {Noffsinger, William Blake},
	file = {Noffsinger - Predictive Accuracy of Recommender Algorithms.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\INTWAH34\\Noffsinger - Predictive Accuracy of Recommender Algorithms.pdf:application/pdf},
}

@article{liuNewParallelItemBased2015,
	title = {A {New} {Parallel} {Item}-{Based} {Collaborative} {Filtering} {Algorithm} {Based} on {Hadoop}},
	volume = {10},
	issn = {1796217X},
	url = {http://www.jsoftware.us/index.php?m=content&c=index&a=show&catid=151&id=2315},
	doi = {10.17706/jsw.10.4.416-426},
	abstract = {With the appearance of big data’s era, some problems caused in recommendation systems are needed to solve immediately. So it is very useful to design parallel recommendation algorithms. An improved parallel item-based collaborative filtering (IP\_Item-basedCF) algorithm based on Hadoop is proposed in this paper. In order to consider the influence of user’s activity, a new parameter called IUF is introduced that can give the active users soft punishment. And the user’s rating is also considered in prediction model. Finally, we evaluate the performance of our approach by using two real datasets –MovieLens and Douban. The experimental results show that this new parallel algorithm outperforms the algorithms existed and has a good scalability and speedup.},
	language = {en},
	number = {4},
	urldate = {2023-03-05},
	journal = {Journal of Software},
	author = {Liu, Qun and Li, Xiaobing},
	month = apr,
	year = {2015},
	pages = {416--426},
	file = {Liu and Li - 2015 - A New Parallel Item-Based Collaborative Filtering .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JBX82834\\Liu and Li - 2015 - A New Parallel Item-Based Collaborative Filtering .pdf:application/pdf},
}

@article{siomosParallelImplementationBasic,
	title = {Parallel {Implementation} of {Basic} {Recommendation} {Algorithms}},
	language = {en},
	author = {Siomos, Theodosios},
	file = {Siomos - Parallel Implementation of Basic Recommendation Al.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\R4L2RA9A\\Siomos - Parallel Implementation of Basic Recommendation Al.pdf:application/pdf},
}

@article{xiaoGeneralOfflineReinforcement2021,
	title = {A {General} {Offline} {Reinforcement} {Learning} {Framework} for {Interactive} {Recommendation}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16579},
	doi = {10.1609/aaai.v35i5.16579},
	abstract = {This paper studies the problem of learning interactive recommender systems from logged feedbacks without any exploration in online environments. We address the problem by proposing a general ofﬂine reinforcement learning framework for recommendation, which enables maximizing cumulative user rewards without online exploration. Speciﬁcally, we ﬁrst introduce a probabilistic generative model for interactive recommendation, and then propose an effective inference algorithm for discrete and stochastic policy learning based on logged feedbacks. In order to perform ofﬂine learning more effectively, we propose ﬁve approaches to minimize the distribution mismatch between the logging policy and recommendation policy: support constraints, supervised regularization, policy constraints, dual constraints and reward extrapolation. We conduct extensive experiments on two public realworld datasets, demonstrating that the proposed methods can achieve superior performance over existing supervised learning and reinforcement learning methods for recommendation.},
	language = {en},
	number = {5},
	urldate = {2023-03-05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Xiao, Teng and Wang, Donglin},
	month = may,
	year = {2021},
	pages = {4512--4520},
	file = {Xiao and Wang - 2021 - A General Offline Reinforcement Learning Framework.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3HPK5K4C\\Xiao and Wang - 2021 - A General Offline Reinforcement Learning Framework.pdf:application/pdf},
}

@inproceedings{sweidanAssociationRulesOfflinedatabased2020,
	address = {Cologne, Germany},
	title = {Association rules and offline-data-based recommender systems},
	isbn = {9789811223327 9789811223334},
	url = {https://www.worldscientific.com/doi/abs/10.1142/9789811223334_0064},
	doi = {10.1142/9789811223334_0064},
	abstract = {Association rules are rules that deﬁne relationships between items in sales databases. They have been used primarily to organize relevant products in stores in a way to makes them more visible to consumers, which may increase sales and proﬁts. On the other hand, it has been rarely used in recommender systems where algorithms provide instant recommendations by processing consumers’ interests that are gathered when browsing online. However, the vast amount of information collected from transaction data saved on backup servers is poorly taken advantage of, because it is not connected to the Internet, although interesting and personalized recommendations can be created after ﬁnding the collections of most frequent items, or most interesting rules in such databases. In this paper, we do a critique of the existing research on both recommender systems along with showing their drawbacks, and the association rules with detailed explanations on their advantages. Finally, draw up with several solutions for producing high quality as well as accurate recommendations by applying novel combinations of techniques observed in this research area including the association-rules-based recommender systems.},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Developments of {Artificial} {Intelligence} {Technologies} in {Computation} and {Robotics}},
	publisher = {WORLD SCIENTIFIC},
	author = {Sweidan, Dirar},
	month = oct,
	year = {2020},
	pages = {530--539},
	file = {Sweidan - 2020 - Association rules and offline-data-based recommend.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\YTKJ5YZG\\Sweidan - 2020 - Association rules and offline-data-based recommend.pdf:application/pdf},
}

@article{smithTwoDecadesRecommender2017,
	title = {Two {Decades} of {Recommender} {Systems} at {Amazon}.com},
	volume = {21},
	issn = {1089-7801},
	url = {http://ieeexplore.ieee.org/document/7927889/},
	doi = {10.1109/MIC.2017.72},
	language = {en},
	number = {3},
	urldate = {2023-03-05},
	journal = {IEEE Internet Computing},
	author = {Smith, Brent and Linden, Greg},
	month = may,
	year = {2017},
	pages = {12--18},
	file = {Smith and Linden - 2017 - Two Decades of Recommender Systems at Amazon.com.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\Z9CUULXN\\Smith and Linden - 2017 - Two Decades of Recommender Systems at Amazon.com.pdf:application/pdf},
}

@inproceedings{UsingNeighborhoodPrecomputation2012,
	address = {Barcelona, Spain},
	title = {Using {Neighborhood} {Pre}-computation to {Increase} {Recommendation} {Efficiency}:},
	isbn = {978-989-8565-29-7},
	shorttitle = {Using {Neighborhood} {Pre}-computation to {Increase} {Recommendation} {Efficiency}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0004139703330335},
	doi = {10.5220/0004139703330335},
	language = {en},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the {International} {Conference} on {Knowledge} {Discovery} and {Information} {Retrieval}},
	publisher = {SciTePress - Science and and Technology Publications},
	year = {2012},
	pages = {333--335},
	file = {2012 - Using Neighborhood Pre-computation to Increase Rec.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\ND77MTZ4\\2012 - Using Neighborhood Pre-computation to Increase Rec.pdf:application/pdf},
}

@inproceedings{liangVariationalAutoencodersCollaborative2018,
	address = {Lyon, France},
	title = {Variational {Autoencoders} for {Collaborative} {Filtering}},
	isbn = {978-1-4503-5639-8},
	url = {http://dl.acm.org/citation.cfm?doid=3178876.3186150},
	doi = {10.1145/3178876.3186150},
	abstract = {We extend variational autoencoders (vaes) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research. We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.},
	language = {en},
	urldate = {2023-03-18},
	booktitle = {Proceedings of the 2018 {World} {Wide} {Web} {Conference} on {World} {Wide} {Web} - {WWW} '18},
	publisher = {ACM Press},
	author = {Liang, Dawen and Krishnan, Rahul G. and Hoffman, Matthew D. and Jebara, Tony},
	year = {2018},
	pages = {689--698},
	file = {Liang et al. - 2018 - Variational Autoencoders for Collaborative Filteri.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\WXFS6MYG\\Liang et al. - 2018 - Variational Autoencoders for Collaborative Filteri.pdf:application/pdf},
}

@inproceedings{michielsRecPackOtherExperimentation2022,
	address = {Seattle WA USA},
	title = {{RecPack}: {An}(other) {Experimentation} {Toolkit} for {Top}-{N} {Recommendation} using {Implicit} {Feedback} {Data}},
	isbn = {978-1-4503-9278-5},
	shorttitle = {{RecPack}},
	url = {https://dl.acm.org/doi/10.1145/3523227.3551472},
	doi = {10.1145/3523227.3551472},
	abstract = {RecPack is an easy-to-use, flexible and extensible toolkit for top-N recommendation with implicit feedback data. Its goal is to support researchers with the development of their recommendation algorithms, from similarity-based to deep learning algorithms, and allow for correct, reproducible and reusable experimentation. In this demo, we give an overview of the package and show how researchers can use it to their advantage when developing recommendation algorithms.},
	language = {en},
	urldate = {2023-03-18},
	booktitle = {Sixteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Michiels, Lien and Verachtert, Robin and Goethals, Bart},
	month = sep,
	year = {2022},
	pages = {648--651},
	file = {Michiels et al. - 2022 - RecPack An(other) Experimentation Toolkit for Top.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\VDYNHITI\\Michiels et al. - 2022 - RecPack An(other) Experimentation Toolkit for Top.pdf:application/pdf},
}

@inproceedings{volkovsTwostageModelAutomatic2018,
	address = {Vancouver BC Canada},
	title = {Two-stage {Model} for {Automatic} {Playlist} {Continuation} at {Scale}},
	isbn = {978-1-4503-6586-4},
	url = {https://dl.acm.org/doi/10.1145/3267471.3267480},
	doi = {10.1145/3267471.3267480},
	abstract = {Automatic playlist continuation is a prominent problem in music recommendation. Significant portion of music consumption is now done online through playlists and playlist-like online radio stations. Manually compiling playlists for consumers is a highly time consuming task that is difficult to do at scale given the diversity of tastes and the large amount of musical content available. Consequently, automated playlist continuation has received increasing attention recently [1, 7, 11]. The 2018 ACM RecSys Challenge [14] is dedicated to evaluating and advancing current state-of-the-art in automated playlist continuation using a large scale dataset released by Spotify. In this paper we present our approach to this challenge. We use a two-stage model where the first stage is optimized for fast retrieval, and the second stage re-ranks retrieved candidates maximizing the accuracy at the top of the recommended list. Our team vl6 achieved 1’st place in both main and creative tracks out of over 100 teams.},
	language = {en},
	urldate = {2023-03-18},
	booktitle = {Proceedings of the {ACM} {Recommender} {Systems} {Challenge} 2018},
	publisher = {ACM},
	author = {Volkovs, Maksims and Rai, Himanshu and Cheng, Zhaoyue and Wu, Ga and Lu, Yichao and Sanner, Scott},
	month = oct,
	year = {2018},
	pages = {1--6},
	file = {Volkovs et al. - 2018 - Two-stage Model for Automatic Playlist Continuatio.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\8MEBS8UL\\Volkovs et al. - 2018 - Two-stage Model for Automatic Playlist Continuatio.pdf:application/pdf},
}

@misc{zamaniAnalysisApproachesTaken2019a,
	title = {An {Analysis} of {Approaches} {Taken} in the {ACM} {RecSys} {Challenge} 2018 for {Automatic} {Music} {Playlist} {Continuation}},
	url = {http://arxiv.org/abs/1810.01520},
	abstract = {The ACM Recommender Systems Challenge 2018 focused on the task of automatic music playlist continuation, which is a form of the more general task of sequential recommendation. Given a playlist of arbitrary length with some additional meta-data, the task was to recommend up to 500 tracks that fit the target characteristics of the original playlist. For the RecSys Challenge, Spotify released a dataset of one million user-generated playlists. Participants could compete in two tracks, i.e., main and creative tracks. Participants in the main track were only allowed to use the provided training set, however, in the creative track, the use of external public sources was permitted. In total, 113 teams submitted 1,228 runs to the main track; 33 teams submitted 239 runs to the creative track. The highest performing team in the main track achieved an R-precision of 0.2241, an NDCG of 0.3946, and an average number of recommended songs clicks of 1.784. In the creative track, an R-precision of 0.2233, an NDCG of 0.3939, and a click rate of 1.785 was obtained by the best team. This article provides an overview of the challenge, including motivation, task definition, dataset description, and evaluation. We further report and analyze the results obtained by the top performing teams in each track and explore the approaches taken by the winners. We finally summarize our key findings, discuss generalizability of approaches and results to domains other than music, and list the open avenues and possible future directions in the area of automatic playlist continuation.},
	language = {en},
	urldate = {2023-03-18},
	publisher = {arXiv},
	author = {Zamani, Hamed and Schedl, Markus and Lamere, Paul and Chen, Ching-Wei},
	month = aug,
	year = {2019},
	note = {arXiv:1810.01520 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval, Computer Science - Multimedia},
	file = {Zamani et al. - 2019 - An Analysis of Approaches Taken in the ACM RecSys .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\JFWHWF6E\\Zamani et al. - 2019 - An Analysis of Approaches Taken in the ACM RecSys .pdf:application/pdf},
}

@article{juanSurveyRecommendationBased2019,
	title = {Survey of {Recommendation} {Based} on {Collaborative} {Filtering}},
	volume = {1314},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1314/1/012078},
	doi = {10.1088/1742-6596/1314/1/012078},
	abstract = {This paper introduces the domestic and international research of collaborative filtering, and discusses the main problems of collaborative filtering algorithm, including data sparsity, cold start and accuracy of similarity measure.Then, future research and development trends of integrating deep learning to recommender systems are pointed out．In order to solve the data sparsity and cold start problems in the personalized recommendation system, a hybrid collaborative filtering recommendation algorithm is proposed, which combines the KNN model and XGBoost model. When deep learning is applied to recommendation system by integrating massive multi-sources heterogeneous data,it could improve the performance of the recommendation system.},
	language = {en},
	number = {1},
	urldate = {2023-03-18},
	journal = {Journal of Physics: Conference Series},
	author = {Juan, Wang and Yue-xin, Lan and Chun-ying, Wu},
	month = oct,
	year = {2019},
	pages = {012078},
	file = {Juan et al. - 2019 - Survey of Recommendation Based on Collaborative Fi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\GAL7UBDQ\\Juan et al. - 2019 - Survey of Recommendation Based on Collaborative Fi.pdf:application/pdf},
}

@misc{pengSurveyModernRecommendation2022,
	title = {A {Survey} on {Modern} {Recommendation} {System} based on {Big} {Data}},
	url = {http://arxiv.org/abs/2206.02631},
	abstract = {Recommendation systems have become very popular in recent years and are used in various web applications. Modern recommendation systems aim at providing users with personalized recommendations of online products or services. Various recommendation techniques, such as content-based, collaborative ﬁltering-based, knowledge-based, and hybrid-based recommendation systems, have been developed to fulﬁll the needs in different scenarios. This paper presents a comprehensive review of historical and recent state-of-the-art recommendation approaches, followed by an in-depth analysis of groundbreaking advances in modern recommendation systems based on big data. Furthermore, this paper reviews the issues faced in modern recommendation systems such as sparsity, scalability, and diversity and illustrates how these challenges can be transformed into proliﬁc future research avenues.},
	language = {en},
	urldate = {2023-03-18},
	publisher = {arXiv},
	author = {Peng, Yuanzhe},
	month = may,
	year = {2022},
	note = {arXiv:2206.02631 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {Peng - 2022 - A Survey on Modern Recommendation System based on .pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\FKNKZSJI\\Peng - 2022 - A Survey on Modern Recommendation System based on .pdf:application/pdf},
}

@article{fkihSimilarityMeasuresCollaborative2022,
	title = {Similarity measures for {Collaborative} {Filtering}-based {Recommender} {Systems}: {Review} and experimental comparison},
	volume = {34},
	issn = {13191578},
	shorttitle = {Similarity measures for {Collaborative} {Filtering}-based {Recommender} {Systems}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1319157821002652},
	doi = {10.1016/j.jksuci.2021.09.014},
	abstract = {Collaborative Filtering (CF) ﬁlters the ﬂow of data that can be recommended, by a Recommender System (RS), to a target user according to his taste and his preferences. The target user’s proﬁle is built based on his similarity with other users. For this reason, CF technique is very sensitive to the similarity measure used to quantify the dependency strength between two users (or two items). In this paper we provide an in-depth review on similarity measures used for CF-based RS. For each measure, we outline its fundamental background and we test its performance through an experimental study. Experiments are carried out on three standard datasets (MovieLens100k, MovieLens1M and Jester) and reveal many important conclusions. In fact, results show that ITR and IPWR are the most suitable similarity measures for a user-based RS while AMI is the best choice for an item-based RS. Evaluation metrics show that under the user-based approach, ITR obtains an MAE equal to 0.786 and 0.731 on MovieLens100k and MovieLens1M, respectively. Whereas, IPWR reach an MAE equal to 3.256 on Jester. Also, AMI gets under the item-based approach an MAE equal to 0.745, 0.724 and 3.281 on MovieLens100k, MovieLens1M and Jester, respectively.},
	language = {en},
	number = {9},
	urldate = {2023-03-18},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Fkih, Fethi},
	month = oct,
	year = {2022},
	pages = {7645--7669},
	file = {Fkih - 2022 - Similarity measures for Collaborative Filtering-ba.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\K4Z4WB46\\Fkih - 2022 - Similarity measures for Collaborative Filtering-ba.pdf:application/pdf},
}

@article{widiyaningtyasRecommendationAlgorithmUsing2022,
	title = {Recommendation {Algorithm} {Using} {SVD} and {Weight} {Point} {Rank} ({SVD}-{WPR})},
	volume = {6},
	issn = {2504-2289},
	url = {https://www.mdpi.com/2504-2289/6/4/121},
	doi = {10.3390/bdcc6040121},
	abstract = {One of the most prevalent recommendation systems is ranking-oriented collaborative ﬁltering which employs ranking aggregation. The collaborative ﬁltering study recently applied the ranking aggregation that considers the weight point of items to achieve a more accurate recommended ranking. However, this algorithm suffers in the execution time with an increased number of items. Therefore, this study proposes a new recommendation algorithm that combines the matrix decomposition method and ranking aggregation to reduce the time complexity. The matrix decomposition method utilizes singular decomposition value (SVD) to predict the unrated items. The ranking aggregation method applies weight point rank (WPR) to obtain the recommended items. The experimental results with the MovieLens 100K dataset result in a faster running time of 13.502 s. In addition, the normalized discounted cumulative gain (NDCG) score increased by 27.11\% compared to the WP-Rank algorithm.},
	language = {en},
	number = {4},
	urldate = {2023-03-18},
	journal = {Big Data and Cognitive Computing},
	author = {Widiyaningtyas, Triyanna and Ardiansyah, Muhammad Iqbal and Adji, Teguh Bharata},
	month = oct,
	year = {2022},
	pages = {121},
	file = {Widiyaningtyas et al. - 2022 - Recommendation Algorithm Using SVD and Weight Poin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\UF33MNBY\\Widiyaningtyas et al. - 2022 - Recommendation Algorithm Using SVD and Weight Poin.pdf:application/pdf},
}

@article{widiyaningtyasRecommendationAlgorithmUsing2022a,
	title = {Recommendation {Algorithm} {Using} {SVD} and {Weight} {Point} {Rank} ({SVD}-{WPR})},
	volume = {6},
	issn = {2504-2289},
	url = {https://www.mdpi.com/2504-2289/6/4/121},
	doi = {10.3390/bdcc6040121},
	abstract = {One of the most prevalent recommendation systems is ranking-oriented collaborative ﬁltering which employs ranking aggregation. The collaborative ﬁltering study recently applied the ranking aggregation that considers the weight point of items to achieve a more accurate recommended ranking. However, this algorithm suffers in the execution time with an increased number of items. Therefore, this study proposes a new recommendation algorithm that combines the matrix decomposition method and ranking aggregation to reduce the time complexity. The matrix decomposition method utilizes singular decomposition value (SVD) to predict the unrated items. The ranking aggregation method applies weight point rank (WPR) to obtain the recommended items. The experimental results with the MovieLens 100K dataset result in a faster running time of 13.502 s. In addition, the normalized discounted cumulative gain (NDCG) score increased by 27.11\% compared to the WP-Rank algorithm.},
	language = {en},
	number = {4},
	urldate = {2023-03-18},
	journal = {Big Data and Cognitive Computing},
	author = {Widiyaningtyas, Triyanna and Ardiansyah, Muhammad Iqbal and Adji, Teguh Bharata},
	month = oct,
	year = {2022},
	pages = {121},
	file = {Widiyaningtyas et al. - 2022 - Recommendation Algorithm Using SVD and Weight Poin.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\SM2IQG27\\Widiyaningtyas et al. - 2022 - Recommendation Algorithm Using SVD and Weight Poin.pdf:application/pdf},
}

@article{canoHybridRecommenderSystems2017,
	title = {Hybrid recommender systems: {A} systematic literature review},
	volume = {21},
	issn = {1088467X, 15714128},
	shorttitle = {Hybrid recommender systems},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IDA-163209},
	doi = {10.3233/IDA-163209},
	abstract = {Recommender systems are software tools used to generate and provide suggestions for items and other entities to the users by exploiting various strategies. Hybrid recommender systems combine two or more recommendation strategies in different ways to beneﬁt from their complementary advantages. This systematic literature review presents the state of the art in hybrid recommender systems of the last decade. It is the ﬁrst quantitative review work completely focused in hybrid recommenders. We address the most relevant problems considered and present the associated data mining and recommendation techniques used to overcome them. We also explore the hybridization classes each hybrid recommender belongs to, the application domains, the evaluation process and proposed future research directions. Based on our ﬁndings, most of the studies combine collaborative ﬁltering with another technique often in a weighted way. Also cold-start and data sparsity are the two traditional and top problems being addressed in 23 and 22 studies each, while movies and movie datasets are still widely used by most of the authors. As most of the studies are evaluated by comparisons with similar methods using accuracy metrics, providing more credible and user oriented evaluations remains a typical challenge. Besides this, newer challenges were also identiﬁed such as responding to the variation of user context, evolving user tastes or providing cross-domain recommendations. Being a hot topic, hybrid recommenders represent a good basis with which to respond accordingly by exploring newer opportunities such as contextualizing recommendations, involving parallel hybrid algorithms, processing larger datasets, etc.},
	language = {en},
	number = {6},
	urldate = {2023-03-18},
	journal = {Intelligent Data Analysis},
	author = {Çano, Erion and Morisio, Maurizio},
	month = nov,
	year = {2017},
	pages = {1487--1524},
	file = {Çano and Morisio - 2017 - Hybrid recommender systems A systematic literatur.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3V7GJGJT\\Çano and Morisio - 2017 - Hybrid recommender systems A systematic literatur.pdf:application/pdf},
}

@article{suSurveyCollaborativeFiltering2009,
	title = {A {Survey} of {Collaborative} {Filtering} {Techniques}},
	volume = {2009},
	issn = {1687-7470, 1687-7489},
	url = {https://www.hindawi.com/journals/aai/2009/421425/},
	doi = {10.1155/2009/421425},
	abstract = {As one of the most successful approaches to building recommender systems, collaborative filtering (
              CF
              ) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. In this paper, we first introduce CF tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc., and their possible solutions. We then present three main categories of CF techniques: memory-based, model-based, and hybrid CF algorithms (that combine CF with other recommendation techniques), with examples for representative algorithms of each category, and analysis of their predictive performance and their ability to address the challenges. From basic techniques to the state-of-the-art, we attempt to present a comprehensive survey for CF techniques, which can be served as a roadmap for research and practice in this area.},
	language = {en},
	urldate = {2023-03-18},
	journal = {Advances in Artificial Intelligence},
	author = {Su, Xiaoyuan and Khoshgoftaar, Taghi M.},
	month = oct,
	year = {2009},
	pages = {1--19},
	file = {Su and Khoshgoftaar - 2009 - A Survey of Collaborative Filtering Techniques.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\5FKPEW5W\\Su and Khoshgoftaar - 2009 - A Survey of Collaborative Filtering Techniques.pdf:application/pdf},
}

@inproceedings{anelliImportanceBeingDissimilar2019,
	title = {The importance of being dissimilar in {Recommendation}},
	url = {http://arxiv.org/abs/1807.04207},
	doi = {10.1145/3297280.3297360},
	abstract = {Similarity measures play a fundamental role in memory-based nearest neighbors approaches. They recommend items to a user based on the similarity of either items or users in a neighborhood. In this paper we argue that, although it keeps a leading importance in computing recommendations, similarity between users or items should be paired with a value of dissimilarity (computed not just as the complement of the similarity one). We formally modeled and injected this notion in some of the most used similarity measures and evaluated our approach showing its effectiveness in terms of accuracy results.},
	language = {en},
	urldate = {2023-03-18},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	author = {Anelli, Vito Walter and Trotta, Joseph and Di Noia, Tommaso and Di Sciascio, Eugenio and Ragone, Azzurra},
	month = apr,
	year = {2019},
	note = {arXiv:1807.04207 [cs]},
	keywords = {Computer Science - Information Retrieval},
	pages = {816--821},
	file = {Anelli et al. - 2019 - The importance of being dissimilar in Recommendati.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\NVHHU8P6\\Anelli et al. - 2019 - The importance of being dissimilar in Recommendati.pdf:application/pdf},
}

@inproceedings{jinAutomaticWeightingScheme2004,
	address = {Sheffield United Kingdom},
	title = {An automatic weighting scheme for collaborative filtering},
	isbn = {978-1-58113-881-8},
	url = {https://dl.acm.org/doi/10.1145/1008992.1009051},
	doi = {10.1145/1008992.1009051},
	abstract = {Collaborative filtering identifies information interest of a particular user based on the information provided by other similar users. The memory-based approaches for collaborative filtering (e.g., Pearson correlation coefficient approach) identify the similarity between two users by comparing their ratings on a set of items. In these approaches, different items are weighted either equally or by some predefined functions. The impact of rating discrepancies among different users has not been taken into consideration. For example, an item that is highly favored by most users should have a smaller impact on the user-similarity than an item for which different types of users tend to give different ratings. Even though simple weighting methods such as variance weighting try to address this problem, empirical studies have shown that they are ineffective in improving the performance of collaborative filtering. In this paper, we present an optimization algorithm to automatically compute the weights for different items based on their ratings from training users. More specifically, the new weighting scheme will create a clustered distribution for user vectors in the item space by bringing users of similar interests closer and separating users of different interests more distant. Empirical studies over two datasets have shown that our new weighting scheme substantially improves the performance of the Pearson correlation coefficient method for collaborative filtering.},
	language = {en},
	urldate = {2023-03-18},
	booktitle = {Proceedings of the 27th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {ACM},
	author = {Jin, Rong and Chai, Joyce Y. and Si, Luo},
	month = jul,
	year = {2004},
	pages = {337--344},
	file = {Jin et al. - 2004 - An automatic weighting scheme for collaborative fi.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\F7P36M8V\\Jin et al. - 2004 - An automatic weighting scheme for collaborative fi.pdf:application/pdf},
}

@article{breeseEmpiricalAnalysisPredictive,
	title = {Empirical {Analysis} of {Predictive} {Algorithms} for {Collaborative} {Filtering}},
	language = {en},
	author = {Breese, John S and Heckerman, David and Kadie, Carl},
	file = {Breese et al. - Empirical Analysis of Predictive Algorithms for Co.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\E6W9WTW3\\Breese et al. - Empirical Analysis of Predictive Algorithms for Co.pdf:application/pdf},
}

@misc{nikolakopoulosTrustYourNeighbors2021,
	title = {Trust your neighbors: {A} comprehensive survey of neighborhood-based methods for recommender systems},
	shorttitle = {Trust your neighbors},
	url = {http://arxiv.org/abs/2109.04584},
	abstract = {Collaborative recommendation approaches based on nearest-neighbors are still highly popular today due to their simplicity, their eﬃciency, and their ability to produce accurate and personalized recommendations. This chapter oﬀers a comprehensive survey of neighborhood-based methods for the item recommendation problem. It presents the main characteristics and beneﬁts of such methods, describes key design choices for implementing a neighborhood-based recommender system, and gives practical information on how to make these choices. A broad range of methods is covered in the chapter, including traditional algorithms like knearest neighbors as well as advanced approaches based on matrix factorization, sparse coding and random walks.},
	language = {en},
	urldate = {2023-03-18},
	publisher = {arXiv},
	author = {Nikolakopoulos, Athanasios N. and Ning, Xia and Desrosiers, Christian and Karypis, George},
	month = sep,
	year = {2021},
	note = {arXiv:2109.04584 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	file = {Nikolakopoulos et al. - 2021 - Trust your neighbors A comprehensive survey of ne.pdf:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\9MMC5RZQ\\Nikolakopoulos et al. - 2021 - Trust your neighbors A comprehensive survey of ne.pdf:application/pdf},
}
